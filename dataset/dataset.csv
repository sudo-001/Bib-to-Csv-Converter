ID,Author,Title,Journal,Volume,Issue,Year,DOI,ISSN,Abstract
Soltana2020,Ghanem Soltana and Mehrdad Sabetzadeh and Lionel C. Briand,Practical Constraint Solving for Generating System Test Data,ACM Transactions on Software Engineering and Methodology,29,2,2020,10.1145/3381032,15577392,"The ability to generate test data is often a necessary prerequisite for automated software testing. For the generated data to be fit for their intended purpose, the data usually have to satisfy various logical constraints. When testing is performed at a system level, these constraints tend to be complex and are typically captured in expressive formalisms based on first-order logic. Motivated by improving the feasibility and scalability of data generation for system testing, we present a novel approach, whereby we employ a combination of metaheuristic search and Satisfiability Modulo Theories (SMT) for constraint solving. Our approach delegates constraint solving tasks to metaheuristic search and SMT in such a way as to take advantage of the complementary strengths of the two techniques. We ground our work on test data models specified in UML, with OCL used as the constraint language. We present tool support and an evaluation of our approach over three industrial case studies. The results indicate that, for complex system test data generation problems, our approach presents substantial benefits over the state-of-the-art in terms of applicability and scalability."
Dixit2022,Marm Dixit and Anand Parejiya and Rachid Essehli and Nitin Muralidharan and Shomaz Ul Haq and Ruhul Amin and Ilias Belharouak,SolidPAC is an interactive battery-on-demand energy density estimator for solid-state batteries,Cell Reports Physical Science,3,2,2022,10.1016/j.xcrp.2022.100756,26663864,"Solid-state batteries hold the promise to be highly impactful next-generation technologies for high-energy and -power-density rechargeable battery applications. It is crucial to identify the metrics that an emerging battery technology should fulfill to achieve parity with conventional Li-ion batteries, primarily in terms of energy density. However, limited approaches exist today to assess and extrapolate the impact of battery designs and choices of cell components on the cell-level energy density of a solid-state battery. Herein, we introduce the Solid-State Battery Performance Analyzer and Calculator (SolidPAC), an interactive experimental toolkit to enable the design of a solid-state battery for user-specified application requirements. The toolkit is flexible enough to assist the battery community in quantifying the impact of materials chemistry and fractions, electrode thicknesses and loadings, and electron flows on cell energy density and costs and in utilizing inverse engineering concepts to correlate the cell energy density output to materials and cell design inputs."
Wellsandt2022,Stefan Wellsandt and Konstantin Klein and Karl Hribernik and Marco Lewandowski and Alexandros Bousdekis and Gregoris Mentzas and Klaus Dieter Thoben,Hybrid-augmented intelligence in predictive maintenance with digital intelligent assistants,Annual Reviews in Control,53,,2022,10.1016/j.arcontrol.2022.04.001,13675788,"Industrial maintenance strategies increasingly rely on artificial intelligence to predict asset conditions and prescribe maintenance actions. The related maintenance software and human maintenance actors can form a hybrid-augmented intelligence system where each side benefits from and enhances the other side's intelligence. This system requires optimized human-machine interfaces to help users express their knowledge and retrieve information from difficult-to-use software. Therefore, this article proposes a novel approach for maintenance experts and operators to interact with a predictive maintenance system through a digital intelligent assistant. This assistant is artificial intelligence (AI) that could help its users interact with the system via natural language and collect their feedback about the success of maintenance interventions. Implementing hybrid-augmented intelligence in a predictive maintenance system faces several technical, social, economic, organizational, and legal challenges. The benefits, limitations, and risks of hybrid-augmented intelligence must be clear to all employees to advocate its use. AI-focused change management and employee training could be techniques to address these challenges. The success of the proposed approach also relies on the continuous improvement of natural language understanding. Such a process will need conversation-driven development where actual interactions with the assistant provide accurate training data for language and dialog models. Future research has to be interdisciplinary and may cover the integration of explainable AI, suitable AI laws, operationalized trustworthy AI, efficient design for human-computer interaction, and natural language processing adapted to predictive maintenance."
Shrestha2020,Alen Shrestha and Linkon Bhattacharjee and Sudip Baral and Balbhadra Thakur and Neekita Joshi and Ajay Kalra and Ritu Gupta,Understanding Suitability of MIKE 21 and HEC-RAS for 2D Floodplain Modeling,,,,2020,10.1061/9780784482971.024,,"Urbanization leads to increase in impervious area and eventually increasing vulnerability to urban flooding. Deer Creek in the city of Brentwood, Missouri, has been experiencing major significant high flows in the past and has flooded the neighboring communities several times. A major step in mitigating the flood is to evaluate the flooding extent. The current research utilizes the Hydrologic Engineering Center River Analysis System (HEC-RAS) and hydrodynamic modelling software-MIKE 21 to study the extent of floodplain in the urbanized area. A comparative study between the models to evaluate the modelling advantages of each model is performed. Moreover, the study focuses on two-dimensional (2D) modelling capabilities of each model as 2D models are more useful in urban areas with more numbers of discrete features. The tradeoff between the accuracy and resources is evaluated while comparing the 2D floodplain modelling capabilities of each models. Channel morphology and flow characteristics for different return period will be incorporated while modeling and the USGS gage data will be utilized for calibration of the models. The models will be calibrated for peak stages along the Deer Creek, with downstream boundary condition of normal depth and the unsteady flow simulations will be performed using the relevant flow hydrographs. The streamflow properties simulated will be derived from the measurements of USGS gaging site near Brentwood, MO."
Bahari2021,Mehran Bahari and Abolfazl Ahmadi and Reza Dashti,Exergo-economic analysis and optimization of a combined solar collector with steam and Organic Rankine Cycle using particle swarm optimization (PSO) algorithm,Cleaner Engineering and Technology,4,,2021,10.1016/j.clet.2021.100221,26667908,"With the diminution of fossil fuel sources and the substantial importance of CO2 and other greenhouse gasses emission, the usage of enhanced thermal power plants coupled with renewable energies, such as solar, becomes more vital and promising. This paper proposes a novel configuration of the power generation system, featuring a solar collector to supply the heat for a two-stage steam turbine with inter heating and an Organic Rankine cycle as bottoming cycle of the steam turbine. The proposed system has been simulated and optimized using the particle swarm optimization algorithm. A heat storage system with NaNO3 and KNO3 in 3:2 ratios is used to store the extra heat in daylight to extend the operation at nighttime. For achieving the best working conditions for the proposed hybrid system, we employed a multi-objective optimization to maximizing the exergy efficiency while minimizing the levelized cost of electricity production. The simulation was performed using the Engineering Equation Solver (EES), and MATLAB software which is used for receiving the simulation results from EES and optimizing the key design parameters of the system using the PSO algorithm to select the best design variables. The optimization showed that at the optimum point, the exergy efficiency of the system and the levelized cost of electricity production reach to be 63.89% and 0.1529 USD/kWh, respectively. Results also showed that in the proposed system, the solar collector is the most important source of exergy destruction, in which more than 59% of the total destructed exergy happens in it. Sensitivity analysis also revealed that decreasing the turbine's inlet temperature will increase the production cost of electricity due to lower efficiency. Also, any changes (deviation from design point) in the back pressure of the low-pressure turbine will decrease the efficiency; while the production cost of electricity increases if this back pressure increases and vice versa."
Martnez-Estvez2023,I. Martínez-Estévez and J. M. Domínguez and B. Tagliafierro and R. B. Canelas and O. García-Feal and A. J.C. Crespo and M. Gómez-Gesteira,Coupling of an SPH-based solver with a multiphysics library,Computer Physics Communications,283,,2023,10.1016/j.cpc.2022.108581,00104655,"A two-way coupling between the Smoothed Particle Hydrodynamics-based (SPH) code with a multiphysics library to solve complex fluid-solid interaction problems is proposed. This work provides full access to the package for the use of this coupling by releasing the source code, completed with guidelines for its compilation and utilization, and self-contained template setups for practical uses of the novel implemented features, is provided here. The presented coupling expands the applicability of two different solvers allowing to simulate fluids, multibody systems, collisions with frictional contacts using either non-smooth contact (NSC) or smooth contact (SMC) methods, all integrated under the same framework. The fluid solver is the open-source code DualSPHysics, highly optimised for simulating free-surface phenomena and structure interactions, uniquely positioned as a general-purpose Computational Fluid Dynamics (CFD) software with a GPU-accelerated solver. Mechanical systems that comprise collision detection and/or multibody dynamics are solved by the multiphysics library Project Chrono, which uses a Discrete Element Method (DEM). Therefore, this SPH-DEM coupling approach can manage interactions between fluid and complex multibody systems with relative constraints, springs, or mechanical joints. Program summary: Program title: DualSPHysics-Chrono CPC Library link to program files: https://doi.org/10.17632/g2cc37dw4f.1 Licensing provisions: DualSPHysics and DSPHChronoLib under GNU Lesser General Public License (LGPL); Project Chrono under BSD-3-Clause License. Programming language: C++ and CUDA Nature of problem: The simulation of turbulent free-surface flows in interaction with complex fixed or floating structures is essential to address typical marine and coastal engineering problems. The Smoothed Particle Hydrodynamics (SPH) method is particularly suitable for solving this type of nonlinear problems. However, this type of application usually requires mechanical restrictions between the different structural elements (spherical joints, hinges, or springs), as well as the correct simulation of collisions between solid objects. In these cases, it is necessary to combine the SPH method with other numerical methods that allow performing these multiphysics simulations. Solution method: DualSPHysics-Chrono is a two-way coupling between an SPH solver and a multiphysics library that combines fluid simulation using a Lagrangian approximation and interactions between solids using Discrete Element Method (DEM). DualSPHysics solver is a GPU-optimized implementation of the SPH method that allows efficient simulation of fluid-structure interaction problems. Whereas Project Chrono is a DEM implementation for simulating multibody dynamics that includes collision detection and numerous mechanical constraints to solve complex mechanisms. The coupling of DualSPHysics and Project Chrono combines the capabilities of both models under a free software framework. Additional comments including restrictions and unusual features: The SPH solver includes a version implemented with CUDA (Compute Unified Device Architecture) to exploit the parallelism of NVIDIA graphics processing units (GPUs)."
Coelho2020,Carlos Coelho and Pedro Narra and Bárbara Marinho and Márcia Lima,Coastal management software to support the decision-makers to mitigate coastal erosion,Journal of Marine Science and Engineering,8,1,2020,10.3390/JMSE8010037,20771312,"There are no sequential and integrated approaches that include the steps needed to perform an adequate management and planning of the coastal zones to mitigate coastal erosion problems and climate change efects. Important numerical model packs are available for users, but often looking deeply to the physical processes, demanding big computational eforts and focusing on specific problems. Thus, it is important to provide adequate tools to the decision-makers, which can be easily interpreted by populations, promoting discussions of optimal intervention scenarios in medium to long-term horizons. COMASO (coastal management software) intends to fill this gap, presenting a group of tools that can be applied in standalone mode, or in a sequential order. The first tool should map the coastal erosion vulnerability and risk, also including the climate change effects, defining a hierarchy of priorities where coastal defense interventions should be performed, or limiting/constraining some land uses or activities. In the locations identified as priorities, a more detailed analysis should consider the application of shoreline and cross-shore evolution models (second tool), allowing discussing intervention scenarios, in medium to long-term horizons. After the defined scenarios, the design of the intervention should be discussed, both in case of being a hard coastal structure or an artificial nourishment (third type of tools). Finally, a cost-benefit assessment tool should optimize the decisions, forecasting costs and benefits for each diferent scenario, through definition of economic values to the interventions and to the land/services/ecosystems, weighting all the environmental, cultural, social and historical aspects. It is considered that COMASO tools can help giving answers to the major problems of the coastal planning and management entities, integrating transversal knowledge in risk assessment, physical processes, engineering and economic evaluations. The integrated coastal zone management needs these tools to ensure sustainable coastal zones, mitigating erosion and climate change effects."
Shi2020,Yusheng Shi and Hongzhi Wu and Chunze Yan and Xiao Yang and Daobing Chen and Ce Zhang and Bin Su and Bo Song and Zhongwei Li and Shengyong Pang and Shifeng Wen and Bo Liang and Qingliang Zhao and Jiankang He and Shuquan Zhang and Yintang Wen,Four-dimensional Printing -- the Additive Manufacturing Technology of Intelligent Components,Jixie Gongcheng Xuebao/Journal of Mechanical Engineering,56,15,2020,10.3901/JME.2020.15.001,05776686,"4D printing technology has aroused widespread attention in the academic and industrial communities since its conceptualization in 2013. It is an additive manufacturing technology for fabricating intelligent components and a revolutionary manufacturing technology based on the high degree of cross-integration of the disciplines including materials, mechanical engineering, dynamics, information, etc. This review paper clarifies the concept and connotation of 4D printing and introduces the application prospects of 4D printing in aerospace, automotive, biomedical and software robots. The state of the art of intelligent component design, simulation, materials, manufacturing processes & equipment, and functional evaluation of intelligent components in 4D printing are also describe. Some existing problems are pointed out, and the research thoughts on 4D printing are put forward. Finally, we propose the development direction and research focus of 4D printing in the future."
Alfadil2022,Mohammad Omar Alfadil and Mukhtar A. Kassem and Kherun Nita Ali and Wael Alaghbari,Construction Industry from Perspective of Force Majeure and Environmental Risk Compared to the COVID-19 Outbreak: A Systematic Literature Review,Sustainability (Switzerland),14,3,2022,10.3390/su14031135,20711050,"The COVID-19 pandemic represents a type of force majeure that significantly and unex-pectedly affected all human lifestyles. This study includes an integrative review of articles published across Scopus and Web of Science journals and compiled using the systematic review methodology based on the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) Statement and VOSreview (visualization of similarities) software by defining keywords that include “construction industry” and “force majeure” and “environmental risks” as a starting point. Moreover, the research years and the countries covered by this research were determined in a second stage. Finally, the abstracts of selected studies were reviewed in order to extract factors similar to the pandemic conditions of COVID-19 along with the brief results of the research. Out of 6384 publications identified and 56 publications reporting, 20 studies fulfilled the inclusion criteria with full text. Based on our findings, there has been a continuous growth of publications on construction risk and environmental research since 2010. Malaysia had the greatest contribution to the research topic of the countries covered by the study, followed by Egypt. The Engineering, Construction and Architectural Management journal published the greatest number of publications related to the research topic. In this review, the most important previous studies are classified according to their handling of force majeure and environmental risks and the most important factors mentioned in these studies are identified. In addition, recommendations are made for dealing with the COVID-19 pandemic and for mitigating its effects on the construction industry in the Arab world and Malaysia. The results of this review will benefit researchers and construction companies alike in furthering research on reducing the risks of COVID-19 to construction projects and avoiding the significant economic loss that results from stopping these projects."
Gunathilake2021,Miyuru B. Gunathilake and Chamaka Karunanayake and Anura S. Gunathilake and Niranga Marasingha and Jayanga T. Samarasinghe and Isuru M. Bandara and Upaka Rathnayake,Hydrological Models and Artificial Neural Networks (ANNs) to Simulate Streamflow in a Tropical Catchment of Sri Lanka,Applied Computational Intelligence and Soft Computing,2021,,2021,10.1155/2021/6683389,16879732,"Accurate streamflow estimations are essential for planning and decision-making of many development activities related to water resources. Hydrological modelling is a frequently adopted and a matured technique to simulate streamflow compared to the data driven models such as artificial neural networks (ANNs). In addition, usage of ANNs is minimum to simulate streamflow in the context of Sri Lanka. Therefore, this study presents an intercomparison between streamflow estimations from conventional hydrological modelling and ANN analysis for Seethawaka River Basin located in the upstream part of the Kelani River Basin, Sri Lanka. The hydrological model was developed using the Hydrologic Engineering Centre-Hydrologic Modelling System (HEC-HMS), while the data-driven ANN model was developed in MATLAB. The rainfall and streamflows' data for 2003-2010 period have been used. The simulations by HEC-HMS were performed by four types of input rainfall data configurations, including observed rainfall data sets and three satellite-based precipitation products (SbPPs), namely, PERSIANN, PERSIANN-CCS, and PERSIANN-CDR. The ANN model was trained using three well-known training algorithms, namely, Levenberg-Marquadt (LM), Bayesian regularization (BR), and scaled conjugate gradient (SCG). Results revealed that the simulated hydrological model based on observed rainfall outperformed those of based on remotely sensed SbPPs. BR algorithm-based ANN algorithm was found to be superior among the data-driven models in the context of ANN model simulations. However, none of the above developed models were able to capture several peak discharges recorded in the Seethawaka River. The results of this study indicate that ANN models can be used to simulate streamflow to an acceptable level, despite presence of intensive spatial and temporal data sets, which are often required for hydrologic software. Hence, the results of the current study provide valuable feedback for water resources' planners in the developing region which lack multiple data sets for hydrologic software."
Lane2020,J. W. Lane and J. M. Link and J. M. King and T. L. George and S. W. Claybrook,Benchmark of GOTHIC to EBR-II SHRT-17 and SHRT-45R Tests,Nuclear Technology,206,7,2020,10.1080/00295450.2019.1698896,19437471,"GOTHIC™ has been used to simulate the Experimental Breeder Reactor–II (EBR-II) Shutdown Heat Removal Test 17 (SHRT-17) and Shutdown Heat Removal Test 45R (SHRT-45R), which correspond to protected and unprotected loss-of-flow events, respectively. GOTHIC is a versatile general-purpose, thermal-hydraulic software package that is a hybrid between traditional system thermal-hydraulic and computational fluid dynamics codes. It is a practical engineering tool that has been used for the design and licensing of existing plants, small modular reactors (SMRs), and next-generation plant designs. Historically, the software has been applied for containment analysis and operability assessments for light water reactors (LWRs), but the recent improvements included in GOTHIC 8.3(QA) allow for the software to be used to simulate advanced, non-LWR concepts currently being developed such as sodium, molten salt, lead, and gas–cooled designs. It will be demonstrated in this paper that GOTHIC includes both the required attributes to model EBR-II and the appropriate physics to accurately simulate the steady-state operating conditions as well as SHRT-17 and SHRT-45R. The GOTHIC model of EBR-II was developed using only publicly available information. The nodalization was selected not only to capture the important phenomena but also to remain computationally efficient. The GOTHIC results show good agreement in both magnitude and trend with the experimental data. Differences are within the bounds of experimental uncertainty and required engineering assumptions applied in the model to fill in gaps in information, particularly for the various leakage paths that existed throughout the primary side of EBR-II, and were not well characterized during the tests."
Xue2021,Jie Xue and Genserik Reniers and Jie Li and Ming Yang and Chaozhong Wu and P. H.A.J.M. van Gelder,A bibliometric and visualized overview for the evolution of process safety and environmental protection,International Journal of Environmental Research and Public Health,18,11,2021,10.3390/ijerph18115985,16604601,"This paper presents a bibliometric overview of the publications in the principal international journal Process Safety and Environmental Protection (PSEP) from 1990 to 2020 retrieved in the Web of Science (WoS) database to explore the evolution in safety and environmental engineering design and practice, as well as experimental or theoretical innovative research. Therefore, based on the WoS database and the visualization of similarities (VOS) viewer software, the bibliometric analysis and scientometric mapping of the literature have been performed from the perspectives of doc-ument types, publication and citation distribution over time, leading authors, countries (regions), institutions, the corresponding collaboration networks, most cited publications and references, fo-cused research fields and topics, research trend evolution over time, etc. The paper provides a comprehensive and quantitative overview and significant picture representation for the journal’s leading and evolutionary trends by employing specific aforementioned bibliometric analysis factors. In addition, by reviewing the evolutionary trends of the journal and the proposed investigated factors, such as the influential works, main research topics, and the research frontiers, this paper reveals the scientific literature production’s main research objectives and directions that could be addressed and explored in future studies."
Frize2021,Monique Frize and Lenka Lhotska and Loredana G. Marcu and Magdalena Stoeva and Gilda Barabino and Fatimah Ibrahim and Sierin Lim and Eleni Kaldoudi and Ana Maria Marques da Silva and Peck Ha Tan and Virginia Tsapaki and Eva Bezak,The impact of COVID-19 pandemic on gender-related work from home in STEM fields—Report of the WiMPBME Task Group,"Gender, Work and Organization",28,S2,2021,10.1111/gwao.12690,14680432,"The COVID-19 pandemic has forced many people, including those in the fields of science and engineering, to work from home. The new working environment caused by the pandemic is assumed to have a different impact on the amount of work that women and men can do from home. Particularly, if the major burden of child and other types of care is still predominantly on the shoulders of women. As such, a survey was conducted to assess the main issues that biomedical engineers, medical physicists (academics and professionals), and other similar professionals have been facing when working from home during the pandemic. A survey was created and disseminated worldwide. It originated from a committee of International Union for Physical and Engineering Sciences in Medicine (IUPESM; Women in Medical Physics and Biomedical Engineering Task Group) and supported by the Union. The ethics clearance was received from Carleton University. The survey was deployed on the Survey Monkey platform and the results were analyzed using IBM SPSS software. The analyses mainly consisted of frequency of the demographic parameters and the cross-tabulation of gender with all relevant variables describing the impact of work at home. A total of 921 responses from biomedical professions in 76 countries were received: 339 males, 573 females, and nine prefer-not-to-say/other. Regarding marital/partnership status, 85% of males were married or in partnership, and 15% were single, whereas 72% of females were married or in partnership, and 26% were single. More women were working from home during the pandemic (68%) versus 50% of men. More men had access to an office at home (68%) versus 64% for women. The proportion of men spending more than 3 h on child care and schooling per day was 12%, while for women it was 22%; for household duties, 8% of men spent more than 3 h; for women, this was 12.5%. It is interesting to note that 44% of men spent between 1 and 3 h per day on household duties, while for women, it was 55%. The high number of survey responses can be considered excellent. It is interesting to note that men participate in childcare and household duties in a relatively high percentage; although this corresponds to less hours daily than for women. It is far more than can be found 2 and 3 decades ago. This may reflect the situation in the developed countries only—as majority of responses (75%) was received from these countries. It is evident that the burden of childcare and household duties will have a negative impact on the careers of women if the burden is not more similar for both sexes. It is important to recognize that a change in policies of organizations that hire them may be required to provide accommodation and compensation to minimize the negative impact on the professional status and career of men and women who work in STEM fields."
Caas2020,José M. Cañas and Diego Martín-Martín and Pedro Arias and Julio Vega and David Roldán-álvarez and Lía García-Pérez and Jesús Fernández-Conde,Open-source drone programming course for distance engineering education,Electronics (Switzerland),9,12,2020,10.3390/electronics9122163,20799292,"This article presents a full course for autonomous aerial robotics inside the RoboticsAcademy framework. This “drone programming” course is open-access and ready-to-use for any teacher/student to teach/learn drone programming with it for free. The students may program diverse drones on their computers without a physical presence in this course. Unmanned aerial vehicles (UAV) applications are essentially practical, as their intelligence resides in the software part. Therefore, the proposed course emphasizes drone programming through practical learning. It comprises a collection of exercises resembling drone applications in real life, such as following a road, visual landing, and people search and rescue, including their corresponding background theory. The course has been successfully taught for five years to students from several university engineering degrees. Some exercises from the course have also been validated in three aerial robotics competitions, including an international one. RoboticsAcademy is also briefly presented in the paper. It is an open framework for distance robotics learning in engineering degrees. It has been designed as a practical complement to the typical online videos of massive open online courses (MOOCs). Its educational contents are built upon robot operating system (ROS) middleware (de facto standard in robot programming), the powerful 3D Gazebo simulator, and the widely used Python programming language. Additionally, RoboticsAcademy is a suitable tool for gamified learning and online robotics competitions, as it includes several competitive exercises and automatic assessment tools."
Asiri2023,Sultan Asiri and Yang Xiao and Saleh Alzahrani and Shuhui Li and Tieshan Li,A Survey of Intelligent Detection Designs of HTML URL Phishing Attacks,IEEE Access,11,,2023,10.1109/ACCESS.2023.3237798,21693536,"Phishing attacks are a type of cybercrime that has grown in recent years. It is part of social engineering attacks where an attacker deceives users by sending fake messages using social media platforms or emails. Phishing attacks steal users' information or download and install malicious software. They are hard to detect because attackers can design a phishing message that looks legitimate to a user. This message may contain a phishing URL so that even an expert can be a victim. This URL leads the victim to a fake website that steals information, such as login information, payment information, etc. Researchers and engineers work to develop methods to detect phishing attacks without the need for the eyes of experts. Even though many papers discuss HTML and URL-based phishing detection methods, there is no comprehensive survey to discuss these methods. Therefore, this paper comprehensively surveys HTML and URL phishing attacks and detection methods. We review the current state-of-art deep learning models to detect URL-based and hybrid-based phishing attacks in detail. We compare each model based on its data preprocessing, feature extraction, model design, and performance."
Shoufan2021,Abdulhadi Shoufan,Active Distance Learning of Embedded Systems,IEEE Access,9,,2021,10.1109/ACCESS.2021.3065248,21693536,"The move from face-to-face to distance learning poses a challenge for courses that rely on hands-on experience such as embedded systems. In this course, students need to work with hardware and software to achieve various learning objectives. For full advantage, the hands-on experience should be aligned with the acquisition of related concepts and procedural knowledge. The alignment of conceptual learning with hands-on experience is a big challenge, in general, and for distance learning, in particular. This article describes how different learning technologies can be integrated to achieve such alignment for embedded systems in a distance learning mode. A framework for active, lecture-free learning was established using a learning management system, YouTube, various web resources, a hardware kit, and a software development environment. The learning activities were implemented as ungraded quizzes on Moodle with different types of questions. These include review questions, conceptual questions, procedural questions, brainstorming questions, code analysis questions, and code creation questions. Our students used the provided hardware kit and the software development environment to complete the learning activities throughout the semester without listening to any live or recorded lecture from our end. This instructional design was evaluated by analyzing learning data generated by Moodle as well as self-report data. The results show high student engagement and positive perceptions of the course content and the learning method. We believe that the proposed pedagogical framework of this design is of general value and can be adopted in other engineering courses with similar requirements of hands-on experience in distance learning."
Dziubak2021,Tadeusz Dziubak and Leszek Bąkała,Computational and experimental analysis of axial flow cyclone used for intake air filtration in internal combustion engines,Energies,14,8,2021,10.3390/en14082285,19961073,"The properties and advantages of axial flow cyclones are presented; several dozen of them are already widely used as the first stage of inlet air filtration in internal combustion motor vehicle engines, work machines and helicopters. The necessity to conduct research on cyclones to improve separation efficiency has been demonstrated. Using the commercial engineering software Ansys Fluent, at a constant inlet velocity of 10 m/s, an assessment was made on the effect of the separation length and inlet diameter of the outlet tube on changes in separation efficiency in axial flow cyclone. Each of the examined parameters was variable while maintaining other factors at a constant level. In the numerical calculations, test dust was used, which was the equivalent of AC fine dust, the particle size composition of which was taken into account using the Rosin– Rammler model. Increase in the separation efficiency was observed with an increase in the separation length and a decrease in the diameter of the cyclone inlet tube. For the cyclone model with an increased separation length and reduced diameter of the inlet pipe, numerical tests of separation efficiency and pressure drop were performed for various velocities at cyclone inlet in the range of 2.5–15 m/s. The obtained characteristics of modified axial flow cyclone were experimentally verified on a laboratory stand during cyclone prototype tests, the model of which was printed using the additive manufacturing technique."
Tribst2020,João Paulo Mendes Tribst and Amanda Maria de Oliveira Dal Piva and Alexandre Luiz Souto Borges and Vinicius Anéas Rodrigues and Marco Antonio Bottino and Cornelis Johannes Kleverlaan,Does the prosthesis weight matter? 3D finite element analysis of a fixed implant-supported prosthesis at different weights and implant numbers,Journal of Advanced Prosthodontics,12,2,2020,10.4047/jap.2020.12.2.67,20057814,"PURPOSE. This study evaluated the influence of prosthesis weight and number of implants on the bone tissue microstrain. MATERIALS AND METHODS. Fifteen (15) fixed full-arch implant-supported prosthesis designs were created using a modeling software with different numbers of implants (4, 6, or 8) and prosthesis weights (10, 15, 20, 40, or 60 g). Each solid was imported to the computer aided engineering software and tetrahedral elements formed the mesh. The material properties were assigned to each solid with isotropic and homogeneous behavior. The friction coefficient was set as 0.3 between all the metallic interfaces, 0.65 for the cortical bone-implant interface, and 0.77 for the cancellous bone-implant interface. The standard earth gravity was defined along the Z-axis and the bone was fixed. The resulting equivalent strain was assumed as failure criteria. RESULTS. The prosthesis weight was related to the bone strain. The more implants installed, the less the amount of strain generated in the bone. The most critical situation was the use of a 60 g prosthesis supported by 4 implants with the largest calculated magnitude of 39.9 mm/mm, thereby suggesting that there was no group able to induce bone remodeling simply due to the prosthesis weight. CONCLUSION. Heavier prostheses under the effect of gravity force are related to more strain being generated around the implants. Installing more implants to support the prosthesis enables attenuating the effects observed in the bone. The simulated prostheses were not able to generate harmful values of peri-implant bone strain."
Liu2021,Jindong Liu,Social Robots as the Bride? Understanding the Construction of Gender in a Japanese Social Robot Product,Human-Machine Communication,2,1,2021,10.30658/hmc.2.5,26386038,"This study critically investigates the construction of gender on a Japanese hologram animestyle social robot Azuma Hikari. By applying a mixed method merging the visual semiotic method and heterogeneous engineering approach in software studies, the signs in Azuma Hikari's anthropomorphized image and the interactivity enabled by the multimedia interface have been analyzed and discussed. The analysis revealed a stereotyped representation of a Japanese ""ideal bride""who should be cute, sexy, comforting, good at housework, and subordinated to ""Master""-like husband. Moreover, the device interface disciplines users to play the role of ""wage earner""in the simulated marriage and reconstructs the gender relations in reality. It suggests the humanization of the objects is often associated with the dehumanization and objectification of the human in reverse."
Jin2020,Xianhao Jin and Francisco Servant,A cost-efficient approach to building in continuous integration,,,,2020,10.1145/3377811.3380437,02705257,"Continuous integration (CI) is a widely used practice in modern software engineering. Unfortunately, it is also an expensive practice -Google and Mozilla estimate their CI systems in millions of dollars. In this paper, we propose a novel approach for reducing the cost of CI. The cost of CI lies in the computing power to run builds and its value mostly lies on letting developers find bugs early - when their size is still small. Thus, we target reducing the number of builds that CI executes by still executing as many failing builds as early as possible. To achieve this goal, we propose SmartBuildSkip, a technique which predicts the first builds in a sequence of build failures and the remaining build failures separately. SmartBuildSkip is customizable, allowing developers to select different preferred trade-offs of saving many builds vs. observing build failures early. We evaluate the motivating hypothesis of SmartBuildSkip, its prediction power, and its cost savings in a realistic scenario. In its most conservative configuration, SmartBuildSkip saved a median 30% of builds by only incurring a median delay of 1 build in a median of 15% failing builds."
Zhang2020,Ziqi Zhang and Yuanchun Li and Yao Guo and Xiangqun Chen and Yunxin Liu,Dynamic slicing for deep neural networks,,,,2020,10.1145/3368089.3409676,,"Program slicing has been widely applied in a variety of software engineering tasks. However, existing program slicing techniques only deal with traditional programs that are constructed with instructions and variables, rather than neural networks that are composed of neurons and synapses. In this paper, we introduce NNSlicer, the first approach for slicing deep neural networks based on data-flow analysis. Our method understands the reaction of each neuron to an input based on the difference between its behavior activated by the input and the average behavior over the whole dataset. Then we quantify the neuron contributions to the slicing criterion by recursively backtracking from the output neurons, and calculate the slice as the neurons and the synapses with larger contributions. We demonstrate the usefulness and effectiveness of NNSlicer with three applications, including adversarial input detection, model pruning, and selective model protection. In all applications, NNSlicer significantly outperforms other baselines that do not rely on data flow analysis."
Gambi2022,Alessio Gambi and Gunel Jahangirova and Vincenzo Riccio and Fiorella Zampetti,SBST Tool Competition 2022,,,,2022,10.1145/3526072.3527538,,"We report on the organization, challenges, and results of the tenth edition of the Java Unit Testing Competition as well as the second edition of the Cyber-Physical Systems (CPS) Testing Competition. Java Unit Testing Competition. Seven tools, i.e., BBC, EvoSuite, Kex, Kex-Reflection, Randoop, UTBot, and UTBot-Mocks, were executed on a benchmark with 65 classes sampled from four open-source Java projects, for two time budgets: 30 and 120 seconds. CPS Testing Tool Competition. Six tools, i.e., AdaFrenetic, AmbieGen, FreneticV, GenRL, EvoMBT and WOGAN competed on testing two driving agents by generating simulation-based tests. We considered one configuration for each test subject and evaluated the tools' effectiveness and efficiency as well as the failure diversity. This paper describes our methodology, the statistical analysis of the results together with the competing tools, and the challenges faced while running the competition experiments."
Ardito2020,Luca Ardito and Riccardo Coppola and Luca Barbato and Diego Verga,A Tool-Based Perspective on Software Code Maintainability Metrics: A Systematic Literature Review,Scientific Programming,2020,,2020,10.1155/2020/8840389,10589244,"Software maintainability is a crucial property of software projects. It can be defined as the ease with which a software system or component can be modified to be corrected, improved, or adapted to its environment. The software engineering literature proposes many models and metrics to predict the maintainability of a software project statically. However, there is no common accordance with the most dependable metrics or metric suites to evaluate such nonfunctional property. The goals of the present manuscript are as follows: (i) providing an overview of the most popular maintainability metrics according to the related literature; (ii) finding what tools are available to evaluate software maintainability; and (iii) linking the most popular metrics with the available tools and the most common programming languages. To this end, we performed a systematic literature review, following Kitchenham's SLR guidelines, on the most relevant scientific digital libraries. The SLR outcome provided us with 174 software metrics, among which we identified a set of 15 most commonly mentioned ones, and 19 metric computation tools available to practitioners. We found optimal sets of at most five tools to cover all the most commonly mentioned metrics. The results also highlight missing tool coverage for some metrics on commonly used programming languages and minimal coverage of metrics for newer or less popular programming languages. We consider these results valuable for researchers and practitioners who want to find the best selection of tools to evaluate the maintainability of their projects or to bridge the discussed coverage gaps for newer programming languages."
Chotisarn2020,Noptanit Chotisarn and Leonel Merino and Xu Zheng and Supaporn Lonapalawong and Tianye Zhang and Mingliang Xu and Wei Chen,A systematic literature review of modern software visualization,Journal of Visualization,23,4,2020,10.1007/s12650-020-00647-w,18758975,"Abstract: We report on the state-of-the-art of software visualization.To ensure reproducibility, we adopted the Systematic Literature Review methodology. That is, we analyzed 1440 entries from IEEE Xplore and ACM Digital Library databases. We selected 105 relevant full papers published in 2013–2019, which we classified based on the aspect of the software system that is supported (i.e., structure, behavior, and evolution). For each paper, we extracted main dimensions that characterize software visualizations, such as software engineering tasks, roles of users, information visualization techniques, and media used to display visualizations. We provide researchers in the field an overview of the state-of-the-art in software visualization and highlight research opportunities. We also help developers to identify suitable visualizations for their particular context by matching software visualizations to development concerns and concrete details to obtain available visualization tools. Graphic abstract: [Figure not available: see fulltext.]."
Palma2022,Stefano Dalla Palma and Dario Di Nucci and Fabio Palomba and Damian A. Tamburri,Within-Project Defect Prediction of Infrastructure-as-Code Using Product and Process Metrics,IEEE Transactions on Software Engineering,48,6,2022,10.1109/TSE.2021.3051492,19393520,"Infrastructure-as-code (IaC) is the DevOps practice enabling management and provisioning of infrastructure through the definition of machine-readable files, hereinafter referred to as IaC scripts. Similarly to other source code artefacts, these files may contain defects that can preclude their correct functioning. In this paper, we aim at assessing the role of product and process metrics when predicting defective IaC scripts. We propose a fully integrated machine-learning framework for IaC Defect Prediction, that allows for repository crawling, metrics collection, model building, and evaluation. To evaluate it, we analyzed 104 projects and employed five machine-learning classifiers to compare their performance in flagging suspicious defective IaC scripts. The key results of the study report Random Forest as the best-performing model, with a median AUC-PR of 0.93 and MCC of 0.80. Furthermore, at least for the collected projects, product metrics identify defective IaC scripts more accurately than process metrics. Our findings put a baseline for investigating IaC Defect Prediction and the relationship between the product and process metrics, and IaC scripts' quality."
Cortellessa2022,Vittorio Cortellessa and Daniele Di Pompeo and Romina Eramo and Michele Tucci,A model-driven approach for continuous performance engineering in microservice-based systems,Journal of Systems and Software,183,,2022,10.1016/j.jss.2021.111084,01641212,"Microservices are quite widely impacting on the software industry in recent years. Rapid evolution and continuous deployment represent specific benefits of microservice-based systems, but they may have a significant impact on non-functional properties like performance. Despite the obvious relevance of this property, there is still a lack of systematic approaches that explicitly take into account performance issues in the lifecycle of microservice-based systems. In such a context of evolution and re-deployment, Model-Driven Engineering techniques can provide major support to various software engineering activities, and in particular they can allow managing the relationships between a running system and its architectural model. In this paper, we propose a model-driven integrated approach that exploits traceability relationships between the monitored data of a microservice-based running system and its architectural model to derive recommended refactoring actions that lead to performance improvement. The approach has been applied and validated on two microservice-based systems, in the domain of e-commerce and ticket reservation, respectively, whose architectural models have been designed in UML profiled with MARTE."
Shi2020,Lin Shi and Mingzhe Xing and Mingyang Li and Yawen Wang and Shoubin Li and Qing Wang,Detection of hidden feature requests from massive chat messages via deep siamese network,,,,2020,10.1145/3377811.3380356,02705257,"Online chatting is gaining popularity and plays an increasingly significant role in software development. When discussing functionalities, developers might reveal their desired features to other developers. Automated mining techniques towards retrieving feature requests from massive chat messages can benefit the requirements gathering process. But it is quite challenging to perform such techniques because detecting feature requests from dialogues requires a thorough understanding of the contextual information, and it is also extremely expensive on annotating feature-request dialogues for learning. To bridge that gap, we recast the traditional text classification task of mapping single dialog to its class into the task of determining whether two dialogues are similar or not by incorporating few-shot learning. We propose a novel approach, named FRMiner, which can detect feature-request dialogues from chat messages via deep Siamese network. We design a BiLSTMbased dialog model that can learn the contextual information of a dialog in both forward and reverse directions. Evaluation on the realworld projects shows that our approach achieves average precision, recall and F1-score of 88.52%, 88.50% and 88.51%, which confirms that our approach could effectively detect hidden feature requests from chat messages, thus can facilitate gathering comprehensive requirements from the crowd in an automated way."
Mykhailova2020,Mariia Mykhailova and Krysta M. Svore,Teaching quantum computing through a practical software-driven approach experience report,,,,2020,10.1145/3328778.3366952,,"Quantum computing harnesses quantum laws of nature to enable new types of algorithms, not efficiently possible on traditional computers, that may lead to breakthroughs in crucial areas like materials science and chemistry. There is rapidly growing demand for a quantum workforce educated in the basics of quantum computing, in particular in quantum programming. However, there are few offerings for non-specialists and little information on best practices for training computer science and engineering students. In this report we describe our experience teaching an undergraduate course on quantum computing using a practical, softwaredriven approach.We centered our course around teaching quantum algorithms through hands-on programming, reducing the significance of traditional written assignments and relying instead on self-paced programming exercises (Quantum Katas), a variety of programming assignments, and a final project. We observed that the programming sections of the course helped students internalize theoretical material presented during the lectures. In the survey results, students indicated that the programming exercises and the final project contributed the most to their learning process. We describe the motivation for centering the course around quantum programming, discuss major artifacts used in this course, and present our lessons learned and best practices for a future improved course offering.We hope that our experience will help guide instructors who want to adopt a practical approach to teaching quantum computing and will enable more undergraduate programs to offer quantum programming as an elective."
Dero2020,Sumera Dero and Azizah Mohd Rohni and Azizan Saaban,Stability analysis of Cu−C6H9NaO7 and Ag−C6H9NaO7 nanofluids with effect of viscous dissipation over stretching and shrinking surfaces using a single phase model,Heliyon,6,3,2020,10.1016/j.heliyon.2020.e03510,24058440,"A mathematical analysis is performed to study the flow and heat transfer phenomena of Casson based nanofluid with effects of the porosity parameter and viscous dissipation over the exponentially permeable stretching and shrinking surface. The considered nanofluid comprises Casson as a base fluid that contains silver (Ag) and copper (Cu) solid nanoparticles. The system of the nonlinear governing partial differential equations (PDEs) are converted into ordinary differential equations (ODEs) by applying similarity transformation. The obtained ODEs are solved by using shooting technique in Maple software. Numerically obtained results reveal dual solutions for various values of pertinent parameters. Due to occurrence of dual solutions, the stability analysis is done in order to find stable solution. Positive signs of smallest eigenvalues point out that the first solution is stable and second unstable. The variation of the velocity and the temperature profiles with coefficient of the skin friction and the Nusselt number are shown graphically. Both temperature profiles and its boundary layer thicknesses increase as volume fraction of nanoparticles of Ag and Cu are increased in the Casson fluid. Velocity profiles and corresponding boundary layer thicknesses decrease by suspension of nanoparticles of silver and copper, whereas the silver Ag nanoparticles show the greater rate of heat transfer enhancement as compared to copper Cu nanoparticles when suspended in Casson fluid."
Zhang2020,Tao Zhang and Kenneth Koltermann and Dmitry Evtyushkin,Exploring branch predictors for constructing transient execution trojans,,,,2020,10.1145/3373376.3378526,,"Transient execution is one of the most critical features used in CPUs to achieve high performance. Recent Spectre attacks demonstrated how this feature can be manipulated to force applications to reveal sensitive data. The industry quickly responded with a series of software and hardware mitigations among which microcode patches are the most prevalent and trusted. In this paper, we argue that currently deployed protections still leave room for constructing attacks. We do so by presenting transient trojans, software modules that conceal their malicious activity within transient execution mode. They appear completely benign, pass static and dynamic analysis checks, but reveal sensitive data when triggered. To construct these trojans, we perform a detailed analysis of the attack surface currently present in today's systems with respect to the recommended mitigation techniques. We reverse engineer branch predictors in several recent x86_64 processors which allows us to uncover previously unknown exploitation techniques. Using these techniques, we construct three types of transient trojans and demonstrate their stealthiness and practicality."
Wu2021,Rui Wu and Penghui Zhang and Pinnaduwa H.S.W. Kulatilake and Hao Luo and Qingyuan He,Stress and deformation analysis of gob-side pre-backfill driving procedure of longwall mining: a case study,International Journal of Coal Science and Technology,8,6,2021,10.1007/s40789-021-00460-2,21987823,"At present, non-pillar entry protection in longwall mining is mainly achieved through either the gob-side entry retaining (GER) procedure or the gob-side entry driving (GED) procedure. The GER procedure leads to difficulties in maintaining the roadway in mining both the previous and current panels. A narrow coal pillar about 5–7 m must be left in the GED procedure; therefore, it causes permanent loss of some coal. The gob-side pre-backfill driving (GPD) procedure effectively removes the wasting of coal resources that exists in the GED procedure and finds an alternative way to handle the roadway maintenance problem that exists in the GER procedure. The FLAC3D software was used to numerically investigate the stress and deformation distributions and failure of the rock mass surrounding the previous and current panel roadways during each stage of the GPD procedure which requires ""twice excavation and mining"". The results show that the stress distribution is slightly asymmetric around the previous panel roadway after the “primary excavation”. The stronger and stiffer backfill compared to the coal turned out to be the main bearing body of the previous panel roadway during the ""primary mining"". The highest vertical stresses of 32.6 and 23.1 MPa, compared to the in-situ stress of 10.5 MPa, appeared in the backfill wall and coal seam, respectively. After the ""primary mining"", the peak vertical stress under the coal seam at the floor level was slightly higher (18.1 MPa) than that under the backfill (17.8 MPa). After the ""secondary excavation"", the peak vertical stress under the coal seam at the floor level was slightly lower (18.7 MPa) than that under the backfill (19.8 MPa); the maximum floor heave and maximum roof sag of the current panel roadway were 252.9 and 322.1 mm, respectively. During the ""secondary mining"", the stress distribution in the rock mass surrounding the current panel roadway was mainly affected by the superposition of the front abutment pressure from the current panel and the side abutment pressure from the previous panel. The floor heave of the current panel roadway reached a maximum of 321.8 mm at 5 m ahead of the working face; the roof sag increased to 828.4 mm at the working face. The peak abutment pressure appeared alternately in the backfill and the coal seam during the whole procedure of ""twice excavation and mining"" of the GPD procedure. The backfill provided strong bearing capacity during all stages of the GPD procedure and exhibited reliable support for the roadway. The results provide scientific insight for engineering practice of the GPD procedure."
Gonzlez2021,Evelio González and José Demetrio Piñeiro and Jonay Toledo and Rafael Arnay and Leopoldo Acosta,An approach based on the ifcOWL ontology to support indoor navigation,Egyptian Informatics Journal,22,1,2021,10.1016/j.eij.2020.02.008,11108665,"This paper presents an indoor navigation support system based on the Building Information Models (BIM) paradigm. Although BIM is initially defined for the Architecture, Engineering and Construction/Facility Management (AEC/FM) industry, the authors believe that it can provide added value in this context. To this end, the authors will focus on the Industry Foundation Classes (IFC) standard for the formal representation of BIM. The approach followed in this paper will be based on the ifcOWL ontology, which translates the IFC schemas into Ontology Web Language (OWL). Several modifications of this ontology have been proposed, consisting of the inclusion of new items, SWRL rules and SQWRL searches. This way of expressing the elements of a building can be used to code information that is very useful for navigation, such as the location of elements related to the actions desired by the user. It is important to note that this design is intended to be used as a complement to other well-known tools and techniques for indoor navigation. The proposed modifications have been successfully tested in a variety of simulated and real scenarios. The main limitation of the proposal is the immense amount of information contained in the ifcOWL ontology, which causes difficulties involving its processing and the time necessary to perform operations on it. Those elements that are considered important have been selected, removing those that seem secondary to navigation. This procedure will result in a significant reduction in the storage and semantic processing of the information. Thus, for a system with 1000 individuals (in the ontological sense), the processing time is about 90 s. The authors regard this time as acceptable, since in most cases the tasks involved can be considered part of the system initialization, meaning they will only be executed once at the beginning of the process."
Dennis2021,Adam A. Dennis and Jordan J. Pannell and Danny J. Smyl and Samuel E. Rigby,Prediction of blast loading in an internal environment using artificial neural networks,International Journal of Protective Structures,12,3,2021,10.1177/2041419620970570,2041420X,"Explosive loading in a confined internal environment is highly complex and is driven by nonlinear physical processes associated with reflection and coalescence of multiple shock fronts. Prediction of this loading is not currently feasible using simple tools, and instead specialist computational software or practical testing is required, which are impractical for situations with a wide range of input variables. There is a need to develop a tool which balances the accuracy of experiments or physics-based numerical schemes with the simplicity and low computational cost of an engineering-level predictive approach. Artificial neural networks (ANNs) are formed of a collection of neurons that process information via a series of connections. When fully trained, ANNs are capable of replicating and generalising multi-parameter, high-complexity problems and are able to generate new predictions for unseen problems (within the bounds of the training variables). This article presents the development and rigorous testing of an ANN to predict blast loading in a confined internal environment. The ANN was trained using validated numerical modelling data, and key parameters relating to formulation of the training data and network structure were critically analysed in order to maximise the predictive capability of the network. The developed network was generally able to predict specific impulses to within 10% of the numerical data: 90% of specific impulses in the unseen testing data, and between 81% and 87% of specific impulses for data from four additional unseen test models, were predicted to this accuracy. The network was highly capable of generalising in areas adjacent to reflecting surfaces and as those close to ambient outflow boundaries. It is shown that ANNs are highly suited to modelling blast loading in a confined internal environment, with significant improvements in accuracy achievable if a robust, well distributed training dataset is used with a network structure that is tailored to the problem being solved."
Nanehkaran2023,Yaser A. Nanehkaran and Zhu Licai and Jin Chengyong and Junde Chen and Sheraz Anwar and Mohammad Azarafza and Reza Derakhshani,Comparative Analysis for Slope Stability by Using Machine Learning Methods,Applied Sciences (Switzerland),13,3,2023,10.3390/app13031555,20763417,"Featured Application: The presented paper conducted a comparative analysis based on well-known MLP, SVM, DT, and RF learning methods to assess/predict the safety factor (F.S) of earthslopes. Earth slopes’ stability analysis is a key task in geotechnical engineering that provides a detailed view of the slope conditions used to implement appropriate stabilizations. In the stability analysis process, calculating the safety factor (F.S) plays an essential part in the stability assessment, which guarantees operations’ success. Providing accurate and reliable F.S can be used to improve the stability analysis procedure as well as stabilizations. In this regard, researchers used computational intelligent methodologies to reach highly accurate F.S calculations. The presented study focused on the F.S estimation process and attempted to provide a comparative analysis based on computational intelligence and machine learning methods. In this regard, the well-known multilayer perceptron (MLP), decision tree (DT), support vector machines (SVM), and random forest (RF) learning algorithms were used to predict/calculate F.S for the earth slopes. These machine learning classifiers have a strong capability predict the F.S under certain conditions for slope failures and uncertainties. These models were implemented on a dataset containing 100 earth slopes’ stabilities, recorded based on F.S from various locations in the provinces of Fars, Isfahan, and Tehran in Iran, which were randomly divided into the training and testing datasets. These predictive models were validated by Janbu’s limit equilibrium analysis method (LEM) and GeoStudio commercial software. Regarding the study’s results, MLP (accuracy = 0.901/precision = 0.90) provides more accurate results to predict the F.S than other classifiers, with good agreement with LEM results. The SVM algorithm follows MLP (accuracy = 0.873/precision = 0.85). Regarding the estimated loss function, MLP obtained a 0.29 average loss in the F.S prediction process, which is the lowest rate. The SVM, DT, and RF obtained 0.41, 0.62, and 0.45 losses, respectively. This article tried to fill the gap in traditional analysis procedures based on advanced procedures in slope stability assessments."
Ghafouri2022,Mehran Ghafouri and Antti Ahola and Joseph Ahn and Timo Björk,Numerical and experimental investigations on the welding residual stresses and distortions of the short fillet welds in high strength steel plates,Engineering Structures,260,,2022,10.1016/j.engstruct.2022.114269,18737323,"The current study investigates the influence of different welding sequences and external constraints on development of welding- induced angular distortion and residual stresses in the short fillet welds made of the high strength steel (HSS), S700, using the finite element (FE) method. Three-dimensional thermo-elastic-plastic FE modeling was performed in the ABAQUS FE software based on the double-ellipsoidal heat source model, temperature-dependent material properties, and considering geometrical non-linearity. The accuracy of the FE models were verified by comparing the simulation results and experimentally measured data. Both the numerical method and measurements show that external constraint has relatively larger influence than welding sequence on the development of angular distortion and the peak magnitude of residual stresses. To determine the significance of short fillet welds in terms of induced distortion and residual stress fields, the results captured by simulation and validated by measurements were compared to those of continuous fillet welds. From a numerical point of view, the results of this study are meaningful to understand the reasonable accuracy required to capture the necessary details of the welding process. From an engineering prospect, the results of this study can be important as they indicate that transverse residual stress fields due to short fillet welds are localized with considerably larger peak magnitudes compared to continuous fillet welds. With respect to the angular distortion, the results of this study show that stiffness of external constraint has a greater impact on prevention of angular distortion in continuous fillet welds than in short fillet welds. In the case of fillet welds with small lengths, beside proper clamping, other deformation control techniques such as pre-alignment of the welded members need to be considered."
Deng2020,Yongliang Deng and Jinyun Li and Qiuting Wu and Shuangshuang Pei and Na Xu and Guodong Ni,Using network theory to explore bim application barriers for BIM sustainable development in China,Sustainability (Switzerland),12,8,2020,10.3390/SU12083190,20711050,"Building Information Modeling (BIM) technology has promoted the development of the architecture, engineering, and construction (AEC) industry, but has encountered many barriers to its application in China. Therefore, identifying the barriers to BIM application and capturing their interactions are essential in order to control and eliminate the determined barriers. From this standpoint, 23 BIM application barriers were identified through a literature review and expert interviews. Furthermore, the interactions among them were determined based on the Delphi method, which was the foundation for establishing the BIM application barrier network (BABN). Then, the software Pajek was employed to construct the network model and reveal its topological characteristics based on complex network theory, including degree, betweenness, eigenvector, clustering coe°cient, network diameter, and average path length. As indicated by the results, BABN possesses scale-free network property because its cumulative degree distribution obeys power-law distribution. BABN is also a small-world network, due to its relatively high clustering coe°cient as well as small average path length, implying that barrier propagation in BABN is fast. In addition, the results are discussed and recommendations are proposed. This research will help BIM stakeholders to develop coping strategies to control and eliminate BIM application barriers for the sake of driving BIM sustainable development."
Kraus2020,M. A. Kraus and M. Drass,"Artificial intelligence for structural glass engineering applications — overview, case studies and future potentials",Glass Structures and Engineering,5,3,2020,10.1007/s40940-020-00132-8,23635150,"’Big data’ and the use of ’Artificial Intelligence’ (AI) is currently advancing due to the increasing and even cheaper data collection and processing capabilities. Social and economical change is predicted by numerous company leaders, politicians and researchers. Machine and Deep Learning (ML/DL) are sub-types of AI, which are gaining high interest within the community of data scientists and engineers worldwide. Obviously, this global trend does not stop at structural glass engineering, so that, the first part of the present paper is concerned with introducing the basic theoretical frame of AI and its sub-classes of ML and DL while the specific needs and requirements for the application in a structural engineering context are highlighted. Then this paper explores potential applications of AI for different subjects within the design, verification and monitoring of façades and glass structures. Finally, the current status of research as well as successfully conducted industry projects by the authors are presented. The discussion of specific problems ranges from supervised ML in case of the material parameter identification of polymeric interlayers used in laminated glass or the prediction of cut-edge strength based on the process parameters of a glass cutting machine and prediction of fracture patterns of tempered glass to the application of computer vision DL methods to image classification of the Pummel test and the use of semantic segmentation for the detection of cracks at the cut edge of glass. In the summary and conclusion section, the main findings for the applicability and impact of AI for the presented structural glass research and industry problems are compiled. It can be seen that in many cases AI, data, software and computing resources are already available today to successfully implement AI projects in the glass industry, which is demonstrated by the many current examples mentioned. Future research directories however will need to concentrate on how to introduce further glass-specific theoretical and human expert knowledge in the AI training process on the one hand and on the other hand more pronunciation has to be laid on the thorough digitization of workflows associated with the structural glass problem at hand in order to foster the further use of AI within this domain in both research and industry."
Busto2021,Saray Busto and Michael Dumbser and Elena Gaburro,A simple but efficient concept of blended teaching of mathematics for engineering students during the covid-19 pandemic,Education Sciences,11,2,2021,10.3390/educsci11020056,22277102,"In this article we present a case study concerning a simple but efficient technical and logistic concept for the realization of blended teaching of mathematics and its applications in theoretical mechanics that was conceived, tested and implemented at the Department of Civil, Environmental and Mechanical Engineering (DICAM) of the University of Trento, Italy, during the COVID-19 pandemic. The concept foresees traditional blackboard lectures with a reduced number of students physically present in the lecture hall, while the same lectures are simultaneously made available to the remaining students, who cannot be present, via high-quality low-bandwidth online streaming. The case study presented in this paper was implemented in a single University Department and was carried out with a total of n = 1011 students and n = 68 professors participating in the study. Based on our first key assumption that traditional blackboard lectures, including the gestures and the facial expressions of the professor, are even nowadays still a very efficient and highly appreciated means of teaching mathematics at the university, this paper deliberately does not want to propose a novel pedagogical concept of how to teach mathematics at the undergraduate level, but rather presents a technical concept of how to preserve the quality of traditional blackboard lectures even during the COVID-19 pandemic and how to make them available to the students at home via online streaming with adequate audio and video quality even at low internet bandwidth. The second key assumption of this paper is that the teaching of mathematics is a dynamic creative process that requires the physical presence of students in the lecture hall as audience so that the professor can instantaneously fine-tune the evolution of the lecture according to his/her perception of the level of attention and the facial expressions of the students. The third key assumption of this paper is that students need to have the possibility to interact with each other personally, especially in the first years at the university. We report on the necessary hardware, software and logistics, as well as on the perception of the proposed blended lectures by undergraduate students from civil and environmental engineering at the University of Trento, Italy, compared to traditional lectures and also compared to the pure online lectures that were needed as emergency measure at the beginning of the COVID-19 pandemic. The evaluation of the concept was carried out with the aid of quantitative internet bandwidth measurements, direct comparison of transmitted video signals and a careful analysis of ex ante and ex post online questionnaires sent to students and professors."
Apostolopoulos2020,Stefanos Apostolopoulos and Jazmín Salas and José L.P. Ordóñez and Shern Shiou Tan and Carlos Ciller and Andreas Ebneter and Martin Zinkernagel and Raphael Sznitman and Sebastian Wolf and Sandro De Zanet and Marion R. Munk,Automatically Enhanced OCT Scans of the Retina: A proof of concept study,Scientific Reports,10,1,2020,10.1038/s41598-020-64724-8,20452322,"In this work we evaluated a postprocessing, customized automatic retinal OCT B-scan enhancement software for noise reduction, contrast enhancement and improved depth quality applicable to Heidelberg Engineering Spectralis OCT devices. A trained deep neural network was used to process images from an OCT dataset with ground truth biomarker gradings. Performance was assessed by the evaluation of two expert graders who evaluated image quality for B-scan with a clear preference for enhanced over original images. Objective measures such as SNR and noise estimation showed a significant improvement in quality. Presence grading of seven biomarkers IRF, SRF, ERM, Drusen, RPD, GA and iRORA resulted in similar intergrader agreement. Intergrader agreement was also compared with improvement in IRF and RPD, and disagreement in high variance biomarkers such as GA and iRORA."
Ferreira2020,Fernando Ferreira and Carlos Moutinho and Álvaro Cunha and Elsa Caetano,An artificial accelerogram generator code written in Matlab,Engineering Reports,2,3,2020,10.1002/eng2.12129,25778196,"Artificial accelerograms are a very useful tool for designing structures against seismic hazards; in particular, when structures exhibit nonlinear behavior. This paper presents a Matlab code for generating artificial accelerograms to match code-defined spectra based on the modification of actual earthquake ground motions. In this case, while the code is written to generate Eurocode 8-compatible accelerograms, it can be easily adapted to any other code in the world. The proposed code determines the earthquake spectra and Fourier Transform using the Fast Fourier Transform algorithm and then modifies the Fourier Transform in order to match the predefined spectra. The code also includes instructions for outlining different actual ground motions, soil, and earthquake type. The code is geared to students and researchers in the field of Structural and Earthquake Engineering. The objective of this work is to provide a Matlab code that can be integrated in other research programs such as seismic structural analysis, structural control software, or other codes."
Mohammadi2020,Mehdi Mohammadi and Maryam Shafiei Sarvestani and Sahar Nouroozi,Mobile Phone Use in Education and Learning by Faculty Members of Technical-Engineering Groups: Concurrent Mixed Methods Design,Frontiers in Education,5,,2020,10.3389/feduc.2020.00016,2504284X,"The purpose of this study was to evaluate Mobile Learning Acceptance among faculty members. The research methodology was a concurrent mixed methods design. The research method in the quantitative part was descriptive-survey, and in the qualitative part a phenomenological approach was applied. In the quantitative part, the study population consisted of all female and male faculty members of technical-engineering groups at Shiraz University (N = 147), where 87 participants were selected using stratified random sampling method. In the qualitative part, the participants included a small fraction of the same faculty members who had also participated in the quantitative part, and were selected by purposive sampling approach with criteria technique. Research instruments consisted of a researcher-made scale of mobile learning acceptance. Upon verifying the validity and reliability of the scale, it was distributed among the subjects, and the collected data were analyzed using SPSS 21 software. Qualitative data were collected from semi-structured interviews with faculty members who had an experience of mobile learning. The quantitative results indicated that in all areas, except for Usefulness, the faculty members' acceptance of mobile learning was higher than average, and Usefulness was at a moderate level. In the qualitative part, after integrating and summarizing the data, a total of 17 basic themes and three organizing themes were extracted, including the benefits of mobile learning, the barriers and limitations of mobile learning, and the required infrastructure for effective implementation of mobile learning."
CihanSorkun2022,Murat Cihan Sorkun and Dajt Mullaj and J. M.Vianney A. Koelman and Süleyman Er,"ChemPlot, a Python Library for Chemical Space Visualization**",Chemistry-Methods,2,7,2022,10.1002/cmtd.202200005,26289725,"Visualizing chemical spaces streamlines the analysis of molecular datasets by reducing the information to human perception level, hence it forms an integral piece of molecular engineering, including chemical library design, high-throughput screening, diversity analysis, and outlier detection. We present here ChemPlot, which enables users to visualize the chemical space of molecular datasets in both static and interactive ways. ChemPlot features structural and tailored similarity methods, together with three different dimensionality reduction methods: PCA, t-SNE, and UMAP. ChemPlot is the first visualization software that tackles the activity/property cliff problem by incorporating tailored similarity. With tailored similarity, the chemical space is constructed in a supervised manner considering target properties. Additionally, we propose a metric, the Distance Property Relationship score, to quantify the property difference of similar (i. e. close) molecules in the visualized chemical space. ChemPlot can be installed via Conda or PyPI (pip) and a web application is freely accessible at https://www.amdlab.nl/chemplot/."
Das2021,Subrat Das and Gregory P. Siroky and Shawn Lee and Davendra Mehta and Ranjit Suri,Cybersecurity: The need for data and patient safety with cardiac implantable electronic devices,Heart Rhythm,18,3,2021,10.1016/j.hrthm.2020.10.009,15563871,"Remote monitoring of cardiac implantable electronic devices (CIEDs) has become routine practice as a result of the advances in biomedical engineering, the advent of interconnectivity between the devices through the Internet, and the demonstrated improvement in patient outcomes, survival, and hospitalizations. However, this increased dependency on the Internet of Things comes with risks in the form of cybersecurity lapses and possible attacks. Although no cyberattack leading to patient harm has been reported to date, the threat is real and has been demonstrated in research laboratory scenarios and echoed in patient concerns. The CIED universe comprises a complex interplay of devices, connectivity protocols, and sensitive information flow between the devices and the central cloud server. Various manufacturers use proprietary software and black-box connectivity protocols that are susceptible to hacking. Here we discuss the fundamentals of the CIED ecosystem, the potential security vulnerabilities, a historical overview of such vulnerabilities reported in the literature, and recommendations for improving the security of the CIED ecosystem and patient safety."
Laa2021,Ibai Laña and Javier J. Sanchez-Medina and Eleni I. Vlahogianni and Javier Del Ser,From data to actions in intelligent transportation systems: A prescription of functional requirements for model actionability,Sensors (Switzerland),21,4,2021,10.3390/s21041121,14248220,"Advances in Data Science permeate every field of Transportation Science and Engineering, resulting in developments in the transportation sector that are data-driven. Nowadays, Intelligent Transportation Systems (ITS) could be arguably approached as a “story” intensively producing and consuming large amounts of data. A diversity of sensing devices densely spread over the infras-tructure, vehicles or the travelers’ personal devices act as sources of data flows that are eventually fed into software running on automatic devices, actuators or control systems producing, in turn, complex information flows among users, traffic managers, data analysts, traffic modeling scientists, etc. These information flows provide enormous opportunities to improve model development and decision-making. This work aims to describe how data, coming from diverse ITS sources, can be used to learn and adapt data-driven models for efficiently operating ITS assets, systems and processes; in other words, for data-based models to fully become actionable. Grounded in this described data modeling pipeline for ITS, we define the characteristics, engineering requisites and challenges intrin-sic to its three compounding stages, namely, data fusion, adaptive learning and model evaluation. We deliberately generalize model learning to be adaptive, since, in the core of our paper is the firm conviction that most learners will have to adapt to the ever-changing phenomenon scenario underly-ing the majority of ITS applications. Finally, we provide a prospect of current research lines within Data Science that can bring notable advances to data-based ITS modeling, which will eventually bridge the gap towards the practicality and actionability of such models."
Dezhdar2023,Ali Dezhdar and Ehsanolah Assareh and Sajjad Keykhah and Ali Bedakhanian and Moonyong Lee,A transient model for clean electricity generation using Solar energy and ocean thermal energy conversion (OTEC) - case study: Karkheh dam - southwest Iran,Energy Nexus,9,,2023,10.1016/j.nexus.2023.100176,27724271,"Given the limitations of fossil fuels, humans must develop alternate energy sources. Solar energy and ocean thermal energy are known as a safe, secure, promising, and clean instrument for this purpose due to their enormous potential. A hybrid system of solar energy and ocean thermal energy with a thermoelectric generator is examined in this study to generate clean electricity and utilize the water temperature difference of the Karkheh dam. The system under consideration is made up of flat panel solar collector subsystems and a Rankin organic cycle. To model the analyzed system and acquire system analysis results, thermodynamic software of EES engineering equations solving was employed. The collector area, solar radiation intensity, ambient temperature, and input temperature to the turbine have all been studied as parameters affecting system outputs. The system's economic research revealed that the solar unit, evaporator, and ORC turbine have the highest cost rates among the system components. Furthermore, the system's exergy study revealed that the solar unit and evaporator suffered the highest exergy destruction. For one year, a case study was conducted on the water temperature difference at Karkheh Dam, and the findings of system performance were assessed. The results demonstrated that the best system performance is obtained under summer weather conditions in Andimeshk. The results showed that the proposed system's total output power in relation to the difference in water temperature of Karkheh Dam is 1,192,607/9 kW per year, and that this system can meet the energy needs of 111 households in Andimeshk throughout the year. According to the results, the system described in this study is suitable for the required applications."
Wohlrab2020,Rebekka Wohlrab and Eric Knauss and Jan Philipp Steghöfer and Salome Maro and Anthony Anjorin and Patrizio Pelliccione,"Collaborative traceability management: a multiple case study from the perspectives of organization, process, and culture",Requirements Engineering,25,1,2020,10.1007/s00766-018-0306-1,1432010X,"Traceability is crucial for many activities in software and systems engineering including monitoring the development progress, and proving compliance with standards. In practice, the use and maintenance of trace links are challenging as artifacts undergo constant change, and development takes place in distributed scenarios with multiple collaborating stakeholders. Although traceability management in general has been addressed in previous studies, there is a need for empirical insights into the collaborative aspects of traceability management and how it is situated in existing development contexts. The study reported in this paper aims to close this gap by investigating the relation of collaboration and traceability management, based on an understanding of characteristics of the development effort. In our multiple exploratory case study, we conducted semi-structured interviews with 24 individuals from 15 industrial projects. We explored which challenges arise, how traceability management can support collaboration, how collaboration relates to traceability management approaches, and what characteristics of the development effort influence traceability management and collaboration. We found that practitioners struggle with the following challenges: (1) collaboration across team and tool boundaries, (2) conveying the benefits of traceability, and (3) traceability maintenance. If these challenges are addressed, we found that traceability can facilitate communication and knowledge management in distributed contexts. Moreover, there exist multiple approaches to traceability management with diverse collaboration approaches, i.e., requirements-centered, developer-driven, and mixed approaches. While traceability can be leveraged in software development with both agile and plan-driven paradigms, a certain level of rigor is needed to realize its benefits and overcome challenges. To support practitioners, we provide principles of collaborative traceability management. The main contribution of this paper is empirical evidence of how culture, processes, and organization impact traceability management and collaboration, and principles to support practitioners with collaborative traceability management. We show that collaboration and traceability management have the potential to be mutually beneficial—when investing in one, also the other one is positively affected."
Kim2020,Do Kyun Kim and Eileen Wee Chin Wong and Nak Kyun Cho,"An advanced technique to predict time-dependent corrosion damage of onshore, offshore, nearshore and ship structures: Part I = generalisation",International Journal of Naval Architecture and Ocean Engineering,12,,2020,10.1016/j.ijnaoe.2020.06.007,20926790,"A reliable and cost-effective technique for the development of corrosion damage model is introduced to predict nonlinear time-dependent corrosion wastage of steel structures. A detailed explanation on how to propose a generalised mathematical formulation of the corrosion model is investigated in this paper (Part I), and verification and application of the developed method are covered in the following paper (Part II) by adopting corrosion data of a ship's ballast tank structure. In this study, probabilistic approaches including statistical analysis were applied to select the best fit probability density function (PDF) for the measured corrosion data. The sub-parameters of selected PDF, e.g., the largest extreme value distribution consisting of scale, and shape parameters, can be formulated as a function of time using curve fitting method. The proposed technique to formulate the refined time-dependent corrosion wastage model (TDCWM) will be useful for engineers as it provides an easy and accurate prediction of the 1) starting time of corrosion, 2) remaining life of the structure, and 3) nonlinear corrosion damage amount over time. In addition, the obtained outcome can be utilised for the development of simplified engineering software shown in Appendix B."
Ren2020,Cheng Ren and Shiwei Bai and Yu Wang and Yaxin Li,Achieving Near-Optimal Traffic Engineering Using a Distributed Algorithm in Hybrid SDN,IEEE Access,8,,2020,10.1109/ACCESS.2020.2972103,21693536,"To empower advanced traffic engineering (TE) mechanism, while considering the infeasibility of one-step migration to software-defined networking (SDN), SDN nodes are incrementally deployed into legacy network, which gives rise to hybrid SDN. In hybrid SDN, redirecting flow of every source-destination pair through at least one SDN node, can enhance TE performance and obtain flow manageability, while on the other hand leading to increasing demands of TCAM resources in SDN nodes. In this paper, we make minimization of maximum link utilization as the TE objective, and comply with SDN waypoint enforcement and TCAM resource limitation. We first formulate the TE problem as an integer linear programming (ILP) model and solve it in a centralized manner, where SDN waypoint selection and splitting fractions for each flow are jointly determined. Then, based on a fact that the logically centralized control plane in hybrid SDN is composed of multiple physically decentralized controllers, each of which manages part of SDN nodes, as well as considering a real situation that a centralized solution is infeasible or too fragile for large-scale network, we develop a distributed algorithm deriving from Lagrangian decomposition theory to effectively solve the TE problem. The simulation results indicate that, when 30% of the SDN nodes are deployed, the proposed traffic engineering-aware distributed routing (TEDR) algorithm obtains maximum link utilization comparable to that of full SDN, and has a limited influence on the routing efficiency."
Weichbroth2020,Paweł Weichbroth and Łukasz Łysik,Mobile Security: Threats and Best Practices,Mobile Information Systems,2020,,2020,10.1155/2020/8828078,1875905X,"Communicating mobile security threats and best practices has become a central objective due to the ongoing discovery of new vulnerabilities of mobile devices. To cope with this overarching issue, the goal of this paper is to identify and analyze existing threats and best practices in the domain of mobile security. To this extent, we conducted a literature review based on a set of keywords. The obtained results concern recognizable threats and established best practices in the domain ofmobile security. Afterwards, this outcome was put forward for consideration by mobile application users (n = 167) via a survey instrument. To this end, the results show high awareness of the threatsand their countermeasures in the domain of mobile applications. While recognizing the risks associated with physical and social factors, the majority of respondents declared the use of built-in methods to mitigate the negative impact of malicious software and social-engineering scams. The study results contribute to the theory on mobile security through the identification and exploration of a variety of issues, regarding both threats and best practices. Besides this, this bulk of up-to-dateknowledge has practical value which reflects in its applicability at both the individual and enterprise level. Moreover, at this point, we argue that understanding the factors affecting users' intentions and motivations to accept and use particular technologies is crucial to leverage the securityof mobile applications. Therefore, future work will cover identifying and modeling users' perceptions of the security and usability of mobile applications."
Marucci2020,Lucia Marucci and Matteo Barberis and Jonathan Karr and Oliver Ray and Paul R. Race and Miguel de Souza Andrade and Claire Grierson and Stefan Andreas Hoffmann and Sophie Landon and Elibio Rech and Joshua Rees-Garbutt and Richard Seabrook and William Shaw and Christopher Woods,Computer-Aided Whole-Cell Design: Taking a Holistic Approach by Integrating Synthetic With Systems Biology,Frontiers in Bioengineering and Biotechnology,8,,2020,10.3389/fbioe.2020.00942,22964185,"Computer-aided design (CAD) for synthetic biology promises to accelerate the rational and robust engineering of biological systems. It requires both detailed and quantitative mathematical and experimental models of the processes to (re)design biology, and software and tools for genetic engineering and DNA assembly. Ultimately, the increased precision in the design phase will have a dramatic impact on the production of designer cells and organisms with bespoke functions and increased modularity. CAD strategies require quantitative models of cells that can capture multiscale processes and link genotypes to phenotypes. Here, we present a perspective on how whole-cell, multiscale models could transform design-build-test-learn cycles in synthetic biology. We show how these models could significantly aid in the design and learn phases while reducing experimental testing by presenting case studies spanning from genome minimization to cell-free systems. We also discuss several challenges for the realization of our vision. The possibility to describe and build whole-cells in silico offers an opportunity to develop increasingly automatized, precise and accessible CAD tools and strategies."
Vergara2020,Diego Vergara and Jamil Extremera and Manuel Pablo Rubio and Lilian P. Dávila,The technological obsolescence of virtual reality learning environments,Applied Sciences (Switzerland),10,3,2020,10.3390/app10030915,20763417,"The concept of technological obsolescence that affects computer programs is a readily observable phenomenon that has been widely studied over the past half century. The so-called virtual reality learning environments (VRLEs) which are used to support university classes are significantly affected by this technological obsolescence, decreasing their formative effectiveness as the obsolescence process advances. In this study, the technological obsolescence of two VRLEs is analyzed by means of an empirical research based on survey results (N = 135) after using the VRLEs in engineering classes. Several key performance indicators (KPIs) were analyzed during seven academic courses, including motivation, interactivity, ease of use and usefulness. Since both VRLEs were updated during this research work, the influence of these improvements is discussed in detail from a technological obsolescence point of view. Results suggest that the technological obsolescence negatively affects the students' opinion regarding motivation and interactivity, but the other KPIs (ease of use and usefulness) are hardly affected. In contrast, results indicate that the technological obsolescence can be reversed if periodic updates of educational tools are carried out using modern development software."
DiSipio2020,Claudio Di Sipio and Davide Di Ruscio and Phuong T. Nguyen,Democratizing the development of recommender systems by means of low-code platforms,,,,2020,10.1145/3417990.3420202,,"In recent years, recommender systems have gained an increasingly crucial role in software engineering. Such systems allow developers to exploit a plethora of reusable artifacts, including source code and documentation, which can support the development activities. However, recommender systems are complex tools that are difficult to personalize or fine-tune if developers want to improve them for increasing the relevance of the retrievable recommendations. In this paper, we propose a low-code development approach to engineering recommender systems. Low-code platforms enable the creation and deployment of fully functional applications by mainly using visual abstractions and interfaces and requiring little or no procedural code. Thus, we aim to foster a low-code way of building recommender systems by means of a metamodel to represent the peculiar components. Then, dedicated supporting tools are also proposed to help developers easily model and build their custom recommender systems. Preliminary evaluations of the approach have been conducted by reimplementing real recommender systems, confirming the feasibility of developing them in a low-code manner."
Rosso2021,Stefano Rosso and Federico Uriati and Luca Grigolato and Roberto Meneghello and Gianmaria Concheri and Gianpaolo Savio,An optimization workflow in design for additive manufacturing,Applied Sciences (Switzerland),11,6,2021,10.3390/app11062572,20763417,"Additive Manufacturing (AM) brought a revolution in parts design and production. It enables the possibility to obtain objects with complex geometries and to exploit structural optimization algorithms. Nevertheless, AM is far from being a mature technology and advances are still needed from different perspectives. Among these, the literature highlights the need of improving the frameworks that describe the design process and taking full advantage of the possibilities offered by AM. This work aims to propose a workflow for AM guiding the designer during the embodiment design phase, from the engineering requirements to the production of the final part. The main aspects are the optimization of the dimensions and the topology of the parts, to take into consideration functional and manufacturing requirements, and to validate the geometric model by computer-aided engineering software. Moreover, a case study dealing with the redesign of a piston rod is presented, in which the proposed workflow is adopted. Results show the effectiveness of the workflow when applied to cases in which structural optimization could bring an advantage in the design of a part and the pros and cons of the choices made during the design phases were highlighted."
Berselli2020,Giovanni Berselli and Pietro Bilancia and Luca Luzi,Project-based learning of advanced CAD/CAE tools in engineering education,International Journal on Interactive Design and Manufacturing,14,3,2020,10.1007/s12008-020-00687-4,19552505,"The use of integrated Computer Aided Design/Engineering (CAD/CAE) software capable of analyzing mechanical devices in a single parametric environment is becoming an industrial standard. Potential advantages over traditional enduring multi-software design routines can be outlined into time/cost reduction and easier modeling procedures. To meet industrial requirements, the engineering education is constantly revising the courses programs to include the training of modern advanced virtual prototyping technologies. Within this scenario, the present work describes the CAD/CAE project-based learning (PjBL) activity developed at the University of Genova as a part of course named Design of Automatic Machines, taught at the second level degree in mechanical engineering. The PjBL activity provides a detailed overview of an integrated design environment (i.e. PTC Creo). The students, divided into small work groups, interactively gain experience with the tool via the solution of an industrial design problem, provided by an engineer from industry. The considered case study consists of an automatic pushing device implemented in a commercial machine. Starting from a sub-optimal solution, the students, supervised by the lecturers, solve a series of sequential design steps involving both motion and structural analysis. The paper describes each design phase and summarizes the numerical outputs. At last, the results of the PjBL activity are presented and commented by considering the opinions of all the parties involved."
Du2022,Xin Du and Tian Wang and Liuhai Wang and Weifeng Pan and Chunlai Chai and Xinxin Xu and Bo Jiang and Jiale Wang,CoreBug: Improving Effort-Aware Bug Prediction in Software Systems Using Generalized k-Core Decomposition in Class Dependency Networks,Axioms,11,5,2022,10.3390/axioms11050205,20751680,"Complex network theory has been successfully introduced into the field of software engineering. Many works in the literature have built complex networks in software, usually called software networks, to represent software structure. Such software networks and their related graph algorithms have been proved effective in predicting software bugs. However, the software networks used were unweighted and undirected, neglecting the strength and direction of the couplings. Worse still, they ignored many important types of couplings between classes, such as local variable, instantiates, and access. All of these greatly affect the accuracy of the software network in representing the topological detail of software projects and ultimately affect the metrics derived from it. In this work, an improved effort-aware bug prediction approach named CoreBug is proposed. First, CoreBug uses a weighted directed class dependency network (WDCDN) to precisely describe classes and their couplings, including nine coupling types and their different coupling strengths and directions. Second, a generalized k-core decomposition is introduced to compute the coreness of each class in the WDCDN. Third, CoreBug combines the coreness of each class with its relative risk, as returned by the logistic regression, to quantify the risk of a given class being buggy. Empirical results on eighteen Java projects show that CoreBug is superior to the state-of-the-art approaches according to the average ranking of the Friedman test."
Medina-Martnez2020,Juan S. Medina-Martínez and Juan E. Arango-Ossa and Max F. Levine and Yangyu Zhou and Gunes Gundem and Andrew L. Kung and Elli Papaemmanuil,"Isabl Platform, a digital biobank for processing multimodal patient data",BMC Bioinformatics,21,1,2020,10.1186/s12859-020-03879-7,14712105,"Background: The widespread adoption of high throughput technologies has democratized data generation. However, data processing in accordance with best practices remains challenging and the data capital often becomes siloed. This presents an opportunity to consolidate data assets into digital biobanks—ecosystems of readily accessible, structured, and annotated datasets that can be dynamically queried and analysed. Results: We present Isabl, a customizable plug-and-play platform for the processing of multimodal patient-centric data. Isabl's architecture consists of a relational database (Isabl DB), a command line client (Isabl CLI), a RESTful API (Isabl API) and a frontend web application (Isabl Web). Isabl supports automated deployment of user-validated pipelines across the entire data capital. A full audit trail is maintained to secure data provenance, governance and ensuring reproducibility of findings. Conclusions: As a digital biobank, Isabl supports continuous data utilization and automated meta analyses at scale, and serves as a catalyst for research innovation, new discoveries, and clinical translation."
Sohail2023,Shahab Saquib Sohail and Faiza Farhat and Yassine Himeur and Mohammad Nadeem and Dag Øivind Madsen and Yashbir Singh and Shadi Atalla and Wathiq Mansoor,"Decoding ChatGPT: A taxonomy of existing research, current challenges, and possible future directions",Journal of King Saud University - Computer and Information Sciences,35,8,2023,10.1016/j.jksuci.2023.101675,22131248,"Chat Generative Pre-trained Transformer (ChatGPT) has gained significant interest and attention since its launch in November 2022. It has shown impressive performance in various domains, including passing exams and creative writing. However, challenges and concerns related to biases and trust persist. In this work, we present a comprehensive review of over 100 Scopus-indexed publications on ChatGPT, aiming to provide a taxonomy of ChatGPT research and explore its applications. We critically analyze the existing literature, identifying common approaches employed in the studies. Additionally, we investigate diverse application areas where ChatGPT has found utility, such as healthcare, marketing and financial services, software engineering, academic and scientific writing, research and education, environmental science, and natural language processing. Through examining these applications, we gain valuable insights into the potential of ChatGPT in addressing real-world challenges. We also discuss crucial issues related to ChatGPT, including biases and trustworthiness, emphasizing the need for further research and development in these areas. Furthermore, we identify potential future directions for ChatGPT research, proposing solutions to current challenges and speculating on expected advancements. By fully leveraging the capabilities of ChatGPT, we can unlock its potential across various domains, leading to advancements in conversational AI and transformative impacts in society."
Rabiser2021,Rick Rabiser and Alois Zoitl,Towards Mastering Variability in Software-Intensive Cyber-Physical Production Systems,,180,,2021,10.1016/j.procs.2021.01.128,18770509,"Software-intensive Cyber-Physical Production Systems (SiCPPS), like metallurgical plants or manufacturing plants, are highly variable systems of systems that frequently evolve. They typically involve a large number of heterogeneous components (mechanical, electrical, mechatronic, software) that can be configured and combined in different ways. Variability results not only from hardware and software components but also development processes, disciplines (mechanical, electrical, software engineering), methods, and tools. Dealing with variability in industry currently depends too much on mostly tacit domain expert knowledge and custom-built tools focusing on very specific artifacts and software and hardware platforms. Existing research in the area of SiCPPS does not explicitly and systematically deal with variability. Promising software engineering methods and tools, e.g., from the area of Software Product Lines, need to be adapted for the particular challenges in SiCPPS. In this research preview paper, we discuss open research issues, research goals, and propose a research agenda towards mastering variability in SiCPPS."
Li2020,Jinyu Li and Asif Ullah and Jun Li and Shah Nazir and Habib Ullah Khan and Hanif Ur Rehman and Amin Ul Haq and Erkan Celik,Attributes-Based Decision Making for Selection of Requirement Elicitation Techniques Using the Analytic Network Process,Mathematical Problems in Engineering,2020,,2020,10.1155/2020/2156023,15635147,"Requirement engineering is the first phase of software engineering. In requirement engineering, the first phase is requirement elicitation (RE), which is the most critical and error-prone activity. In this phase, the requirements are extracted from various sources; after extraction, they are analyzed and documented for a specific purpose of software development. In RE, process requirements from stakeholders are gathered, upon which the entire software product failure and success are dependent. In order to accomplish the goal of requirement elicitation, various techniques are used. However, the selection of these techniques is a very challenging task, as one technique may suit a situation but may not be suited for other situations. Besides this, project attributes such as documentation culture of organization, degree of relationship among stakeholders, and familiarity to domain also have a great impact on the process of technique selection. The reason is that there is no empirical value of the techniques that provide help in techniques selection to analyze the basis software project attributes. This study proposed the analytic network process, which is one of the multicriteria decision making processes for the elicitation technique selection process with respect to criterion attributes of project. The motivation toward the use of the ANP approach for the selection of requirement selection technique is that there are dependencies existing among attributes of the project elements. So, the ANP approach is capable of dealing with such situations where dependencies and complexity occur. Results of the proposed study demonstrate that the technique helps in complex situations where decision making is difficult based on the alternatives."
Krupitzer2020,Christian Krupitzer and Timur Temizer and Thomas Prantl and Claudia Raibulet,An overview of design patterns for self-adaptive systems in the context of the internet of things,IEEE Access,8,,2020,10.1109/ACCESS.2020.3031189,21693536,"The Internet of Things (IoT) requires the integration of all available, highly specialized, and heterogeneous devices, ranging from embedded sensor nodes to servers in the cloud. The self-adaptive research domain provides adaptive capabilities that can support the integration in IoT systems. However, developing such systems is a challenging, error-prone, and time-consuming task. In this context, design patterns propose already used and optimized solutions to specific problems in various contexts. Applying design patterns might help to reuse existing knowledge about similar development issues. However, so far, there is a lack of taxonomies on design patterns for self-adaptive systems. To tackle this issue, in this paper, we provide a taxonomy on design patterns for self-adaptive systems that can be transferred to support adaptivity in IoT systems. Besides describing the taxonomy and the design patterns, we discuss their applicability in an Industrial IoT case study."
Kovaevi2022,Aleksandar Kovačević and Jelena Slivka and Dragan Vidaković and Katarina Glorija Grujić and Nikola Luburić and Simona Prokić and Goran Sladić,Automatic detection of Long Method and God Class code smells through neural source code embeddings,Expert Systems with Applications,204,,2022,10.1016/j.eswa.2022.117607,09574174,"Code smells are structures in code that often harm its quality. Manually detecting code smells is challenging, so researchers proposed many automatic detectors. Traditional code smell detectors employ metric-based heuristics, but researchers have recently adopted a Machine-Learning (ML) based approach. This paper compares the performance of multiple ML-based code smell detection models against multiple metric-based heuristics for detection of God Class and Long Method code smells. We assess the effectiveness of different source code representations for ML: we evaluate the effectiveness of traditionally used code metrics against code embeddings (code2vec, code2seq, and CuBERT). This study is the first to evaluate the effectiveness of pre-trained neural source code embeddings for code smell detection to the best of our knowledge. This approach helped us leverage the power of transfer learning – our study is the first to explore whether the knowledge mined from code understanding models can be transferred to code smell detection. A secondary contribution of our research is the systematic evaluation of the effectiveness of code smell detection approaches on the same large-scale, manually labeled MLCQ dataset. Almost every study that proposes a detection approach tests this approach on the dataset unique for the study. Consequently, we cannot directly compare the reported performances to derive the best-performing approach."
Moustafa2021,Ahmed A. Moustafa and Abubakar Bello and Alana Maurushat,The Role of User Behaviour in Improving Cyber Security Management,Frontiers in Psychology,12,,2021,10.3389/fpsyg.2021.561011,16641078,"Information security has for long time been a field of study in computer science, software engineering, and information communications technology. The term ‘information security’ has recently been replaced with the more generic term cybersecurity. The goal of this paper is to show that, in addition to computer science studies, behavioural sciences focused on user behaviour can provide key techniques to help increase cyber security and mitigate the impact of attackers’ social engineering and cognitive hacking methods (i.e., spreading false information). Accordingly, in this paper, we identify current research on psychological traits and individual differences among computer system users that explain vulnerabilities to cyber security attacks and crimes. Our review shows that computer system users possess different cognitive capabilities which determine their ability to counter information security threats. We identify gaps in the existing research and provide possible psychological methods to help computer system users comply with security policies and thus increase network and information security."
Wessel2021,Mairieli Wessel and Igor Wiese and Igor Steinmacher and Marco Aurelio Gerosa,Don't Disturb Me: Challenges of Interacting with Software Bots on Open Source Software Projects,Proceedings of the ACM on Human-Computer Interaction,5,CSCW2,2021,10.1145/3476042,25730142,"Software bots are used to streamline tasks in Open Source Software (OSS) projects' pull requests, saving development cost, time, and effort. However, their presence can be disruptive to the community. We identified several challenges caused by bots in pull request interactions by interviewing 21 practitioners, including project maintainers, contributors, and bot developers. In particular, our findings indicate noise as a recurrent and central problem. Noise affects both human communication and development workflow by overwhelming and distracting developers. Our main contribution is a theory of how human developers perceive annoying bot behaviors as noise on social coding platforms. This contribution may help practitioners understand the effects of adopting a bot, and researchers and tool designers may leverage our results to better support human-bot interaction on social coding platforms."
Mkitalo2020,Niko Mäkitalo and Antero Taivalsaari and Arto Kiviluoto and Tommi Mikkonen and Rafael Capilla,On opportunistic software reuse,Computing,102,11,2020,10.1007/s00607-020-00833-6,14365057,"The availability of open source assets for almost all imaginable domains has led the software industry to opportunistic design—an approach in which people develop new software systems in an ad hoc fashion by reusing and combining components that were not designed to be used together. In this paper we investigate this emerging approach. We demonstrate the approach with an industrial example in which Node.js modules and various subsystems are used in an opportunistic way. Furthermore, to study opportunistic reuse as a phenomenon, we present the results of three contextual interviews and a survey with reuse practitioners to understand to what extent opportunistic reuse offers improvements over traditional systematic reuse approaches."
Rodrguez2021,José L. Rodríguez and Isabel Romero and Antonio Codina,The influence of neotrie vr’s immersive virtual reality on the teaching and learning of geometry,Mathematics,9,19,2021,10.3390/math9192411,22277390,"The use of dynamic, three-dimensional software with virtual reality offers new possibili-ties for the teaching and learning of geometry. We explore the effects of introducing the immersive virtual reality software NeoTrie VR in real classes. Within a Design Research framework, we present qualitative observational data to report how the collaboration among a software development com-pany, university researchers, and schools produces improvements in the design and updating of the software; the geometrical content, representations, and mathematical activity that students have access to as well as the way teachers conceive and manage the teaching of geometry."
Calero2020,Coral Calero and Javier Mancebo and Felix Garcia and Maria Angeles Moraga and Jose Alberto Garcia Berna and Jose Luis Fernandez-Aleman and Ambrosio Toval,5Ws of green and sustainable software,Tsinghua Science and Technology,25,3,2020,10.26599/TST.2019.9010006,18787606,"Green and Sustainable Software has emerged as a new and highly active area in the software community. After several years of research and work, we believe that it is now necessary to obtain a general snapshot of how the research in this area is evolving. To do so, we have applied the 5Ws (why, when, who, where, and what), a formula for getting the complete story on a subject. We have therefore carried out a study, using 542 publications related to Green and Sustainable Software research; these were recovered using SCOPUS. The results obtained allow us to conclude that it is important to identify key elements of the research to allow researchers be fully aware of the state of the research on Green and Sustainable Software (why); the study uses papers published between 2000 and the beginning of November 2018 (when); the most prolific authors are mainly from Europe, although the USA is the most active country, Green and Sustainable Software being a very interactive area with a good number of multinational publications (who); the top five keywords related to sustainable aspects are Green Software, Green IT, Software Sustainability, Energy Consumption, and Energy Efficiency (what); finally, as regards the places authors prefer to publish in, there is almost a complete balance between conferences and journals, with a trend towards an increase in the number of publications (where)."
Subramanya2022,Rakshith Subramanya and Seppo Sierla and Valeriy Vyatkin,From DevOps to MLOps: Overview and Application to Electricity Market Forecasting,Applied Sciences (Switzerland),12,19,2022,10.3390/app12199851,20763417,"In the Software Development Life Cycle (SDLC), Development and Operations (DevOps) has been proven to deliver reliable, scalable software within a shorter time. Due to the explosion of Machine Learning (ML) applications, the term Machine Learning Operations (MLOps) has gained significant interest among ML practitioners. This paper explains the DevOps and MLOps processes relevant to the implementation of MLOps. The contribution of this paper towards the MLOps framework is threefold: First, we review the state of the art in MLOps by analyzing the related work in MLOps. Second, we present an overview of the leading DevOps principles relevant to MLOps. Third, we derive an MLOps framework from the MLOps theory and apply it to a time-series forecasting application in the hourly day-ahead electricity market. The paper concludes with how MLOps could be generalized and applied to two more use cases with minor changes."
Rasheed2021,Aqsa Rasheed and Bushra Zafar and Tehmina Shehryar and Naila Aiman Aslam and Muhammad Sajid and Nouman Ali and Saadat Hanif Dar and Samina Khalid,Requirement Engineering Challenges in Agile Software Development,Mathematical Problems in Engineering,2021,,2021,10.1155/2021/6696695,15635147,"Agile software development has large success rate due to its benefits and promising nature but natively where the size of the project is small. Requirement engineering (RE) is crucial as in each software development life cycle, ""Requirements""play a vital role. Though agile provides values to customer's business needs, changing requirement, and interaction, we also have to face impediments in agile, many of which are related to requirement challenges. This article aims to find out the challenges being faced during requirement engineering of agile projects. Many research studies have been conducted on requirement challenges which are somehow biased, no suggestions are given to improve the agile development process, and the research does not highlight large-scale agile development challenges. Hence, this article covers all the challenges discussed above and presents a comprehensive overview of agile models from requirement engineering perspective. The findings and results can be very helpful for software industry to improve development process as well as for researchers who want to work further in this direction."
Gupta2020,Varun Gupta and Jose Maria Fernandez-Crehuet and Thomas Hanne,Fostering continuous value proposition innovation through freelancer involvement in software startups: Insights from multiple case studies,Sustainability (Switzerland),12,21,2020,10.3390/su12218922,20711050,"[Context] The software startups could continuously innovate business model value proposition by involving freelancers as a source of innovative ideas (that enhance customer perceived value) and as experts for implementing the innovative ideas (by undertaking software engineering tasks). Startups employ one of three strategies for associating with freelancers i.e., task based (association ends with completion of the outsourced task), panel based (outsourcing task to a panel of freelancers associated with startup), or hybrid. Uncertainties, terminology issues, high technical debt, lack of documentation, lack of systematic decision making processes, lack of resources, lack of brand values, need for the continuous involvement of the freelancer to incorporate continuous validated learnings, merging freelancer perspectives, and deciding the level of their involvement in individual requirement engineering (or value proposition innovation) activities, are the main inhibitors for associations with freelancers. The availability of good freelancers and their long term and continuous commitments are necessary requirements for value proposition innovation. The theory about freelancer association with the software startups is extended by studying the real practices of startups, which successfully involved freelancers for value proposition innovation by capturing innovative ideas and acquiring the freelancer’s skills to implement those ideas. [Objectives] The objective of this paper is to explain the strategies adopted by the software startups to foster value proposition innovation by continuously involving the freelancers and the way they overcome the challenges arising because of the associations. The findings are driven by the study of real practices of startups that proved to be successful in the market by involving freelancers and continuous innovations leading to increased market shares. [Method] This paper performs empirical studies through case studies of three software startups located in Italy, France, and India, which are at the verge of being transforming into big companies, with large market share. The current practices highlighting the successful way of executing freelancing association strategies for value proposition innovation and the way to overcome the arising challenges are reported. The findings are also compared with those of two young startups based in Switzerland and India, to bring useful lessons for the young startups. The case study results are validated by employees from the studied startups (both those who participated in data collection and those who did not). [Results] The results indicate that freelancer involvement during value proposition activities, which is the core business operation, is beneficial to the both freelancers and the startups. Startup teams could reduce the development costs, shorten time to market, and increase customer satisfaction (by providing features addressing real market needs) by correctly involving the freelancers uniformly across value proposition activities. The startups could manage innovation with small teams (compared to human resources in companies) if done jointly with the freelancers, that helps the team members to learn new skills, upgrade their skills, and learn new perspectives about their markets. Business impacts due to freelancer involvement are stronger if the level of freelancer involvement across various value proposition activities is higher compared to their involvement across few activities only. The studied startups are not completely dependent on the freelancers but the freelancer’s perspectives and skills are valued as a rich source of market success. Freelancer involvement is taken as an opportunity to gain access to global market perspectives, which otherwise would be effortful for in-house teams to collect. In addition, they resolve technical debt, are a source of upgrading skills for undertaking future innovation, and help reaching customers for marketing (promoting product and gaining access to the feedbacks). Overall, the value proposition innovation in the studied startups have different levels of involvement of the freelancers but these startups have reported positive impacts on the business in terms of development cost reductions, shorten time to market, and high customer satisfaction (measured on early attainment of product/market fit and fast growth thereafter). The case study results are validated by the startup employees (member checking). The responses collected are analysed using box plots, which shows a higher level of result agreements among the employees."
Jiang2021,Shihui Jiang and Zhaoxia Yu and Lanrui Zhang and Guanhua Wang and Xiaohua Dai and Xiaoli Lian and Yan Yan and Linpu Zhang and Yue Wang and Ruixin Li and Huiru Zou,Effects of different aperture-sized type I collagen/silk fibroin scaffolds on the proliferation and differentiation of human dental pulp cells,Regenerative Biomaterials,8,4,2021,10.1093/rb/rbab028,20563426,"This study aimed at evaluate the effects of different aperture-sized type I collagen/silk fibroin (CSF) scaffolds on the proliferation and differentiation of human dental pulp cells (HDPCs). The CSF scaffolds were designed with 3D mapping software Solidworks. Three different aperture-sized scaffolds (CSF1-CSF3) were prepared by low-temperature deposition 3D printing technology. The morphology was observed by scanning electron microscope (SEM) and optical coherence tomography. The porosity, hydrophilicity and mechanical capacity of the scaffold were detected, respectively. HDPCs (third passage, 1 × 105 cells) were seeded into each scaffold and investigated by SEM, CCK-8, alkaline phosphatase (ALP) activity and HE staining. The CSF scaffolds had porous structures with macropores and micropores. The macropore size of CSF1 to CSF3 was 421 ± 27 μm, 579 ± 36 μm and 707 ± 43 μm, respectively. The porosity was 69.8 ± 2.2%, 80.1 ± 2.8% and 86.5 ± 3.3%, respectively. All these scaffolds enhanced the adhesion and proliferation of HDPCs. The ALP activity in the CSF1 group was higher than that in the CSF3 groups (P < 0.01). HE staining showed HDPCs grew in multilayer within the scaffolds. CSF scaffolds significantly improved the adhesion and ALP activity of HDPCs. CSF scaffolds were promising candidates in dentine-pulp complex regeneration."
Yuan2021,Changfeng Yuan and Zhenhui Hu and Zhen Zhu and Zijin Yuan and Yanxiang Fan and Hui Guan and Liang Li,Numerical Simulation of Seepage and Deformation in Excavation of a Deep Foundation Pit under Water-Rich Fractured Intrusive Rock,Geofluids,2021,,2021,10.1155/2021/6628882,14688123,"Water is one of the major risk sources in the excavation of deep-large foundation pits in a water-rich area. The presence of intrusive broken diorite porphyrite in the stratum aggravates the risk level of deep foundation pits. Based on a geological survey report and design documents of parameter information, MIDAS/GTS software was used to perform the numerical simulation of an engineering example of a deep foundation pit project of ultradeep and water-rich intrusion into the broken rock station of subway line 4 in a city. The simulation results show the characteristics of seepage path evolution, seepage aggregation areas and points, and the effect of seepage on the deformation of a deep foundation pit during the whole construction of this deep foundation pit. The results show that with the precipitation-excavation of the deep foundation pit, the pore water pressure at the bottom of the foundation pit follows a distribution of three ""concave""shapes. High-permeability pressure zones are found around the foundation pit, intruding broken diorite porphyrite zones, and middle coarse sand zones. With further excavation of the foundation pit, the seepage pressure in the middle part of the foundation pit gradually decreases, and the two ""concave""distributions in the middle gradually merge together. After excavation to the bottom of the pit, the pore water pressure at the bottom is distributed in two asymmetrical ""concave""shapes, and the maximum peak of pore water pressure is found at the intrusion of fractured porphyrites prone to water inrush. The four corners of the foundation pit are prone to form seepage accumulation zones; therefore, suffosion and piping zones are formed. The surface settlement caused by excavation is found to be the largest along the longitudinal axis of the deep foundation pit, whereas the largest deformation is found near the foundation pit side in the horizontal axis direction of the foundation pit. With the excavation of the deep foundation pit, the diaphragm wall converges to the foundation pit with the maximum deformation reaching about 25 mm. After the first precipitation-excavation of the deep foundation pit to the silty clay and the bottom of the pit with the largest uplift, with further precipitation-excavation of the deep foundation pit, the uplift at the bottom of the deep foundation pit changes only slightly."
Njim2021,Emad Kadum Njim and Sadeq H. Bakhy and Muhannad Al-Waily,Analytical and numerical investigation of free vibration behavior for sandwich plate with functionally graded porous metal core,Pertanika Journal of Science and Technology,29,3,2021,10.47836/pjst.29.3.39,22318526,"The current work presents a free vibration analysis of a simply supported rectangular functionally graded sandwich plate using a new analytical model. The core of the sandwich plate is made up of porous metal, and the top and bottom faces are made up of homogenous materials. The core metal properties are assumed to be porosity dependent and graded in the thickness direction according to a simple power-law distribution in terms of the volume fractions of the constituents. The contribution of this paper is to evaluate the performance of functionally graded porous materials (FGPMs) as it is used for many biomedical applications, particularly in tissue engineering. Theoretical formulations are based on the classical plate theory to find the free vibration characteristics of the imperfect FGM sandwich plate and include different parameters. Parameters included are graded distributions of porosity, power-law index, core metal type, and aspect ratios. A numerical investigation using finite element analysis (FEA) and the modal analysis was conducted with the assistance of the commercial ANSYS-2020-R2 software to validate the analytical solution. To detect the various parameters influencing the fundamental frequencies of sandwich plate comprehensive numerical results are presented in dimensionless tabular and graphical forms. The results reveal that the frequency parameter of the sandwich plate increases with the increase of the porosity parameter and number of the constraints in the boundary conditions. Furthermore, the increase in the number of layers leads to an increase in the accuracy of the results for the same FGM core thickness. An accepted agreement can be observed between the proposed analytical solution and numerical results with a maximum error discrepancy of 8%."
Gong2023,Jianhu Gong and Amin Rezaeipanah,A fuzzy delay-bandwidth guaranteed routing algorithm for video conferencing services over SDN networks,Multimedia Tools and Applications,82,17,2023,10.1007/s11042-023-14349-6,15737721,"Video conferencing is one of the advanced technologies for users that allows online communication despite long distances. High quality communication and ongoing support for the principles of video conferencing service that can be achieved through Software-Defined Networking (SDN). SDN is a new architecture for computer networks that separates the control plane from the data plane to improve network resources and reduce operating costs. All routing decisions and control mechanisms are made by a device called a controller. Traffic engineering can be well implemented in SDN because the entire network topology is known to the controller. Considering SDN features, user requests can be dynamically routed according to current network status and Quality of Service (QoS) requirements. In general, the purpose of SDN routing algorithms is to maximize the acceptance rate of user requests by considering QoS requirements. In this literature, most routing studies to provide satisfactory video conferencing services have focused solely on bandwidth. Nevertheless, some studies have considered both delay and bandwidth constraints. In this paper, a Fuzzy Delay-Bandwidth Guaranteed Routing (FDBGR) algorithm is proposed that considers both delay and bandwidth constraints in routing. The proposed fuzzy system is based on rules that can postpone requests with high resource demands. Also, the purpose of the FDBGR is to distribute the network workload evenly for all requests, where this is done by maintaining the capacity to accept future requests. The combination of conventional routing algorithms and SDN provides remarkable improvements in mobility, scalability and the overall performance of the networks. Simulations are performed on different scenarios to evaluate the performance of the FDBGR compared to state-of-the-art methods. Besides, FDBGR has been compared with a number of most related previous works such as H-MCOP, MH-MCOP, QoMRA, QROUTE and REDO based on criteria such as number of accepted requests, average path length, energy consumption, load balancing, and average delay. The simulation results clearly prove the superiority of the proposed algorithm with an average delay of 48 ms in different topologies for video conferencing applications."
Fedushko2020,Solomiia Fedushko and Taras Ustyianovych and Yuriy Syerov and Tomas Peracek,User-engagement score and SLIs/SLOs/SLAs measurements correlation of e-business projects through big data analysis,Applied Sciences (Switzerland),10,24,2020,10.3390/app10249112,20763417,"The Covid-19 crisis lockdown caused rapid transformation to remote working/learning modes and the need for e-commerce-, web-education-related projects development, and maintenance. However, an increase in internet traffic has a direct impact on infrastructure and software performance. We study the problem of accurate and quick web-project infrastructure issues/bottleneck/overload identification. The research aims to achieve and ensure the reliability and availability of a commerce/ educational web project by providing system observability and Site Reliability Engineering (SRE) methods. In this research, we propose methods for technical condition assessment by applying the correlation of user-engagement score and Service Level Indicators (SLIs)/Service Level Objectives (SLOs)/Service Level Agreements (SLAs) measurements to identify user satisfaction types along with the infrastructure state. Our solution helps to improve content quality and, mainly, detect abnormal system behavior and poor infrastructure conditions. A straightforward interpretation of potential performance bottlenecks and vulnerabilities is achieved with the developed contingency table and correlation matrix for that purpose. We identify big data and system logs and metrics as the central sources that have performance issues during web-project usage. Throughout the analysis of an educational platform dataset, we found the main features of web-project content that have high user-engagement and provide value to services’ customers. According to our study, the usage and correlation of SLOs/SLAs with other critical metrics, such as user satisfaction or engagement improves early indication of potential system issues and avoids having users face them. These findings correspond to the concepts of SRE that focus on maintaining high service availability."
Herrero2022,María J. Herrero and A. Patricia Pérez-Fortes and José I. Escavy and Juan M. Insua-Arévalo and Raúl De la Horra and Francisco López-Acevedo and Laura Trigos,3D model generated from UAV photogrammetry and semi-automated rock mass characterization,Computers and Geosciences,163,,2022,10.1016/j.cageo.2022.105121,00983004,"This work develops a rock mass characterization of a limestone quarry in northern Spain based on a 3D model obtained by using photographs taken from an unmanned aerial vehicle (UAV) flight and structure-from-motion algorithms. This methodology permits to obtain photogrammetric information in a rapid and low-cost way. Geological planar facets (stratification, faults) are related to the tectonic history of a geological formation and permit assessing slope stability. The spatial orientation of planar features is usually measured with a compass and clinometer, which requires experience and knowledge, it is space limited, and sometimes hazardous. Geological 3D models can mitigate these limitations. A 3D point cloud generated from a series of images obtained using an UAV is included in the open-access applications DSE and FACET, which serve to determine the main discontinuity planes within a limestone quarry. Comparison of the results from software analyses with data hand-measured directly in the field reveals the effectiveness of the use of UAV to develop a virtual outcrop model that permits to obtain accurate measurements. The resultant quarry 3D model has been included in Sketchfab, an open access platform that permits easy and quick access and availability for a wide audience. This study shows that the use of UAV combined with structure-from-motion algorithms is of great interest for geosciences as well as other related disciplines such as mining or civil engineering, and can facilitate decision-making for policy makers and authorities. In addition, it is a technique of great use to develop rock mass characterization in a low-cost, rapid, and easy way, and permits to reach areas with difficult accessibility. This way, this methodology can also be very useful for geosciences teaching purposes, as a complement to traditional field lectures, or to develop virtual field trips and laboratories."
Qiu2022,Zhi Qiu and Zuoxi Zhao and Shaoji Chen and Junyuan Zeng and Yuan Huang and Borui Xiang,Application of an Improved YOLOv5 Algorithm in Real-Time Detection of Foreign Objects by Ground Penetrating Radar,Remote Sensing,14,8,2022,10.3390/rs14081895,20724292,"Ground penetrating radar (GPR) detection is a popular technology in civil engineering. Because of its advantages of non-destructive testing (NDT) and high work efficiency, GPR is widely used to detect hard foreign objects in soil. However, the interpretation of GPR images relies heavily on the work experience of researchers, which may lead to problems of low detection efficiency and a high false recognition rate. Therefore, this paper proposes a real-time detection technology of GPR based on deep learning for the application of soil foreign object detection. In this study, the GPR image signal is obtained in real time by the GPR instrument and software, and the image signals are preprocessed to improve the signal-to-noise ratio of the GPR image signals and improve the image quality. Then, in view of the problem that YOLOv5 poorly detects small targets, this study improves the problems of false detection and missed detection in real-time GPR detection by improving the network structure of YOLOv5, adding an attention mechanism, data enhancement, and other means. Finally, by establishing a regression equation for the position information of the ground penetrating radar, the precise localization of the foreign matter in the underground soil is realized."
Lu2021,Xufei Lu and Michele Chiumenti and Miguel Cervera and Hua Tan and Xin Lin and Song Wang,Warpage analysis and control of thin-walled structures manufactured by laser powder bed fusion,Metals,11,5,2021,10.3390/met11050686,20754701,"Thin-walled structures are of great interest because of their use as lightweight components in aeronautical and aerospace engineering. The fabrication of these components by additive manufacturing (AM) often produces undesired warpage because of the thermal stresses induced by the manufacturing process and the components’ reduced structural stiffness. The objective of this study is to analyze the distortion of several thin-walled components fabricated by Laser Powder Bed Fusion (LPBF). Experiments are performed to investigate the sensitivity of the warpage of thin-walled structures fabricated by LPBF to different design parameters such as the wall thickness and the component height in several open and closed shapes. A 3D-scanner is used to measure the residual distortions in terms of the out-of-plane displacement. Moreover, an in-house finite element software is firstly calibrated and then used to enhance the original design in order to minimize the warpage induced by the LPBF printing process. The outcome of this shows that open geometries are more prone to warping than closed ones, as well as how vertical stiffeners can mitigate component warpage by increasing stiffness."
Liang2020,Dongfang Liang and Xuanyu Zhao and Kenichi Soga,Simulation of overtopping and seepage induced dike failure using two-point MPM,Soils and Foundations,60,4,2020,10.1016/j.sandf.2020.06.004,00380806,"Fluvial dikes are important engineering works for protecting river valleys from flooding, so the stability of them is of great importance. In this paper, we apply a two-point two-phase formulation of the Material Point Method (MPM) to investigate the dike stability problem under the action of overtopping flows. Such a method has been incorporated in the Anura3D software (www.anura3d.com). In the model, the behaviours of soil and water are analysed in a single framework, so the interactions between the two phases are fully dynamic. The computational results agree well with the laboratory experiments. Parametric studies have been carried out to examine the effectiveness of various dike stability measures. The two-point MPM shows encouraging capabilities for studying a broad range of phenomena involving strong soil/water interactions."
Njah2021,Yosra Njah and Mohamed Cheriet,Parallel Route Optimization and Service Assurance in Energy-Efficient Software-Defined Industrial IoT Networks,IEEE Access,9,,2021,10.1109/ACCESS.2021.3056931,21693536,"In recent years, the Industrial world has been embracing new digital technology, including the internet of things (IoT) paradigm that promises revolutionizing-prospects in numerous industrial applications. However, many deployment challenges related to real-time big data analytics, service assurance, resource optimization, energy consumption, and security awareness are raised. In this work, we focus on service assurance and resource optimization, including energy consumption challenges over Industrial Internet of Things (IIoT)-based environments since the existing network routing algorithms cannot meet the strict heterogeneous quality of service (QoS) requirements of industrial communications while optimizing resources. We take advantage of the flexibility and programmability offered by the promising software-defined networking paradigm, and we propose a centralized route optimization and service assurance scheme, named ROSA, over a multi-layer programmable industrial architecture. The proposed solution supports a wide range of heterogeneous flows, such as ultra-reliable low-latency communications (URLLC) and bandwidth-sensitive services. The routing optimization problems are formulated as multi-constrained shortest path problems. The Lagrangian Relaxation approach is used to solve the NP-hard complexity. Hence, we deploy a pair of parallel routing algorithms run according to the flow type to ensure QoS requirements, efficiently allocate constrained resources, and enhance the overall network energy consumption. We conduct extensive simulations to validate the proposed ROSA scheme. The experimental results show promising performance in terms of reducing bandwidth utilization by up to 22%, end-to-end delay at least by 21%, packet loss by more than 19%, flow violation by about 16%, and energy consumption up to 14% as compared to well-known benchmarks in QoS provisioning and energy-aware routing problem."
Ferrari2020,Alessio Ferrari and Franco Mazzanti and Davide Basile and Maurice H. ter Beek and Alessandro Fantechi,Comparing formal tools for system design: A judgment study,,,,2020,10.1145/3377811.3380373,02705257,"Formal methods and tools have a long history of successful applications in the design of safety-critical railway products. However, most of the experiences focused on the application of a single method at once, and little work has been performed to compare the applicability of the dierent available frameworks to the railway context. As a result, companies willing to introduce formal methods in their development process have little guidance on the selection of tools that could t their needs. To address this goal, this paper presents a comparison between 9 dierent formal tools, namely Atelier B, CADP, FDR4, NuSMV, ProB, Simulink, SPIN, UMC, and UPPAAL SMC. We performed a judgment study, involving 17 experts with experience in formal methods applied to railways. In the study, part of the experts were required to model a railway signaling problem (a moving-block train distancing system) with the dierent tools, and to provide feedback on their experience. The information produced was then synthesized, and the results were validated by the remaining experts. Based on the outcome of this process, we provide a synthesis that describes when to use a certain tool, and what are the problems that may be faced by modelers. Our experience shows that the dierent tools serve dierent purposes, and multiple formal methods are required to fully cover the needs of the railway system design process."
Guo2020,Hui Guo and Munindar P. Singh,Caspar: Extracting and synthesizing user stories of problems from app reviews,,,,2020,10.1145/3377811.3380924,02705257,"A user's review of an app often describes the user's interactions with the app. These interactions, which we interpret as mini stories, are prominent in reviews with negative ratings. In general, a story in an app review would contain at least two types of events: user actions and associated app behaviors. Being able to identify such stories would enable an app's developer in better maintaining and improving the app's functionality and enhancing user experience. We present Caspar, a method for extracting and synthesizing user-reported mini stories regarding app problems from reviews. By extending and applying natural language processing techniques, Caspar extracts ordered events from app reviews, classifies them as user actions or app problems, and synthesizes action-problem pairs. Our evaluation shows that Caspar is effective in finding actionproblem pairs from reviews. First, Caspar classifies the events with an accuracy of 82.0% on manually labeled data. Second, relative to human evaluators, Caspar extracts event pairs with 92.9% precision and 34.2% recall. In addition, we train an inference model on the extracted action-problem pairs that automatically predicts possible app problems for different use cases. Preliminary evaluation shows that our method yields promising results. Caspar illustrates the potential for a deeper understanding of app reviews and possibly other natural language artifacts arising in software engineering."
Amosov2021,A. G. Amosov,Special software application for antenna modelling in mechanical engineering,,1889,4,2021,10.1088/1742-6596/1889/4/042031,17426596,"The article is devoted to one of the best today computer program for antenna modelling. No special computer simulation skills are required to use this software. In this study, based on foreign and domestic sources, the influence of the MMANA-GAL program on the design of various types of civil antennas is considered. The paper specifically describes this simulation program. The description includes its history of creation, application, comparison with previous programs and algorithms, the interface of the MMANA-GAL itself is also described. It tells about how useful this program is, how it simplifies life and saves time for developers and designers, since the built-in library contains examples of more than two hundred antennas. Standing out advantages are: its free-of-charge basis, and from the operational characteristics - display of 2D and 3D antenna radiation patterns and its frequency response."
Mustafa2021,Nur Syahirah Mustafa and Nor Hasrul Akhmal and Sudin Izman and Mat Hussin Ab Talib and Ashrul Ishak Mohamad Shaiful and Mohd Nazri Bin Omar and Nor Zaiazmin Yahaya and Suhaimi Illias,Application of computational method in designing a unit cell of bone tissue engineering scaffold: A review,Polymers,13,10,2021,10.3390/polym13101584,20734360,"The design of a scaffold of bone tissue engineering plays an important role in ensuring cell viability and cell growth. Therefore, it is a necessity to produce an ideal scaffold by predicting and simulating the properties of the scaffold. Hence, the computational method should be adopted since it has a huge potential to be used in the implementation of the scaffold of bone tissue engineering. To explore the field of computational method in the area of bone tissue engineering, this paper provides an overview of the usage of a computational method in designing a unit cell of bone tissue engineering scaffold. In order to design a unit cell of the scaffold, we discussed two categories of unit cells that can be used to design a feasible scaffold, which are non-parametric and parametric designs. These designs were later described and being categorised into multiple types according to their characteristics, such as circular structures and Triply Periodic Minimal Surface (TPMS) structures. The advantages and disadvantages of these designs were discussed. Moreover, this paper also represents some software that was used in simulating and designing the bone tissue scaffold. The challenges and future work recommendations had also been included in this paper."
Desai2021,Utkarsh Desai and Sambaran Bandyopadhyay and Srikanth Tamilselvam,Graph Neural Network to Dilute Outliers for Refactoring Monolith Application,,1,,2021,10.1609/aaai.v35i1.16079,2159-5399,"Microservices are becoming the defacto design choice for software architecture. It involves partitioning the software components into finer modules such that the development can happen independently. It also provides natural benefits when deployed on the cloud since resources can be allocated dynamically to necessary components based on demand. Therefore, enterprises as part of their journey to cloud, are increasingly looking to refactor their monolith application into one or more candidate microservices; wherein each service contains a group of software entities (e.g., classes) that are responsible for a common functionality. Graphs are a natural choice to represent a software system. Each software entity can be represented as nodes and its dependencies with other entities as links. Therefore, this problem of refactoring can be viewed as a graph based clustering task. In this work, we propose a novel method to adapt the recent advancements in graph neural networks in the context of code to better understand the software and apply them in the clustering task. In that process, we also identify the outliers in the graph which can be directly mapped to top refactor candidates in the software. Our solution is able to improve state-of-the-art performance compared to works from both software engineering and existing graph representation based techniques."
Liu2020,Zhibo Liu and Shuai Wang,How far we have come: Testing decompilation correctness of C decompilers,,,,2020,10.1145/3395363.3397370,,"A C decompiler converts an executable (the output from a C compiler) into source code. The recovered C source code, once recompiled, will produce an executable with the same functionality as the original executable. With over twenty years of development, C decompilers have been widely used in production to support reverse engineering applications, including legacy software migration, security retrofitting, software comprehension, and to act as the first step in launching adversarial software exploitations. As the paramount component and the trust base in numerous cybersecurity tasks, C decompilers have enabled the analysis of malware, ransomware, and promoted cybersecurity professionals' understanding of vulnerabilities in real-world systems. In contrast to this flourishing market, our observation is that in academia, outputs of C decompilers (i.e., recovered C source code) are still not extensively used. Instead, the intermediate representations are often more desired for usage when developing applications such as binary security retrofitting. We acknowledge that such conservative approaches in academia are a result of widespread and pessimistic views on the decompilation correctness. However, in conventional software engineering and security research, how much of a problem is, for instance, reusing a piece of simple legacy code by taking the output of modern C decompilers? In this work, we test decompilation correctness to present an up-to-date understanding regarding modern C decompilers. We detected a total of 1,423 inputs that can trigger decompilation errors from four popular decompilers, and with extensive manual effort, we identified 13 bugs in two open-source decompilers. Our findings show that the overly pessimistic view of decompilation correctness leads researchers to underestimate the potential of modern decompilers; the state-of-the-art decompilers certainly care about the functional correctness, and they are making promising progress. However, some tasks that have been studied for years in academia, such as type inference and optimization, still impede C decompilers from generating quality outputs more than is reflected in the literature. These issues rarely receive enough attention and can lead to great confusion that misleads users."
Ahadi2022,Alireza Ahadi and Abhay Singh and Matt Bower and Michael Garrett,Text Mining in Education—A Bibliometrics-Based Systematic Review,Education Sciences,12,3,2022,10.3390/educsci12030210,22277102,"Advances in Information Technology (IT) and computer science have without a doubt had a significant impact on our daily lives. The past few decades have witnessed the advancement of IT enabled processes in generating actionable insights in various fields, encouraging research based applications of modern Data Science methods. Among many other fields, education research has also been adopting different analytical approaches to advance the state of education systems. Moreover, developments in software engineering and web-based applications have made collection of education data possible at large scales. This systematic review aims to explore the 21st century’s state of the art applications of text mining methods used in the field of education. We analyse the metadata of all publications that use text mining or natural language processing in educational settings to report on the key themes of application of text mining methods in educational studies providing an overview of the current state of the art and the future directions for research and applications."
Lee2020,Dong Gun Lee and Yeong Seok Seo,Improving bug report triage performance using artificial intelligence based document generation model,Human-centric Computing and Information Sciences,10,1,2020,10.1186/s13673-020-00229-7,21921962,"Artificial intelligence is one of the key technologies for progression to the fourth industrial revolution. This technology also has a significant impact on software professionals who are continuously striving to achieve high-quality software development by fixing various types of software bugs. During the software development and maintenance stages, software bugs are the major factor that can affect the cost and time of software delivery. To efficiently fix a software bug, open bug repositories are used for identifying bug reports and for classifying and prioritizing the reports for assignment to the most appropriate software developers based on their level of interest and expertise. Owing to a lack of resources such as time and manpower, this bug report triage process is extremely important in software development. To improve the bug report triage performance, numerous studies have focused on a latent Dirichlet allocation (LDA) using the k-nearest neighbors or a support vector machine. Although the existing approaches have improved the accuracy of a bug triage, they often cause conflicts between the combined techniques and generate incorrect triage results. In this study, we propose a method for improving the bug report triage performance using multiple LDA-based topic sets by improving the LDA. The proposed method improves the existing topic sets of the LDA by building two adjunct topic sets. In our experiment, we collected bug reports from a popular bug tracking system, Bugzilla, as well as Android bug reports, to evaluate the proposed method and demonstrate the achievement of the following two goals: increase the bug report triage accuracy, and satisfy the compatibility with other state-of-the-art approaches."
Iqbal2020,Javed Iqbal and Rodina B. Ahmad and Muzafar Khan and Fazal-E-Amin and Sultan Alyahya and Mohd Hairul Nizam Nasir and Adnan Akhunzada and Muhammad Shoaib,Requirements engineering issues causing software development outsourcing failure,PLoS ONE,15,4,2020,10.1371/journal.pone.0229785,19326203,"Software development outsourcing is becoming more and more famous because of the advantages like cost abatement, process enhancement, and coping with the scarcity of needed resources. Studies confirm that unfortunately a large proportion of the software development outsourcing projects fails to realize anticipated benefits. Investigations into the failures of such projects divulge that in several cases software development outsourcing projects are failed because of the issues that are associated with requirements engineering process. The objective of this study is the identification and the ranking of the commonly occurring issues of the requirements engineering process in the case of software development outsourcing. For this purpose, contemporary literature has been assessed rigorously, issues faced by practitioners have been identified and three questionnaire surveys have been organized by involving experienced software development outsourcing practitioners. The Delphi technique, cut-off value method and 50% rule have also been employed. The study explores 150 issues (129 issues from literature and 21 from industry) of requirements engineering process for software development outsourcing, groups the 150 issues into 7 identified categories and then extricates 43 customarily or commonly arising issues from the 150 issues. Founded on 'frequency of occurrence' the 43 customarily arising issues have been ranked with respect to respective categories (category-wise ranking) and with respect to all the categories (overall ranking). Categories of the customarily arising issues have also been ranked. The issues' identification and ranking contribute to design proactive software project management plan for dealing with software development outsourcing failures and attaining conjectured benefits of the software development outsourcing."
Bucaioni2022,Alessio Bucaioni and Antonio Cicchetti and Federico Ciccozzi,Modelling in low-code development: a multi-vocal systematic review,Software and Systems Modeling,21,5,2022,10.1007/s10270-021-00964-0,16191374,"In 2014, a new software development approach started to get a foothold: low-code development. Already from its early days, practitioners in software engineering have been showing a rapidly growing interest in low-code development. In 2021 only, the revenue of low-code development technologies reached 13.8 billion USD. Moreover, the business success of low-code development has been sided by a growing interest from the software engineering research community. The model-driven engineering community has shown a particular interest in low-code development due to certain similarities between the two. In this article, we report on the planning, execution, and results of a multi-vocal systematic review on low-code development, with special focus to its relation to model-driven engineering. The review is intended to provide a structured and comprehensive snapshot of low-code development in its peak of inflated expectations technology adoption phase. From an initial set of potentially relevant 720 peer-reviewed publications and 199 grey literature sources, we selected 58 primary studies, which we analysed according to a meticulous data extraction, analysis, and synthesis process. Based on our results, we tend to frame low-code development as a set of methods and/or tools in the context of a broader methodology, often being identified as model-driven engineering."
Gupta2020,Varun Gupta and Jose Maria Fernandez-Crehuet and Thomas Hanne and Rainer Telesko,Fostering product innovations in software startups through freelancer supported requirement engineering,Results in Engineering,8,,2020,10.1016/j.rineng.2020.100175,25901230,"This research paper explores the involvement of freelancers in requirement engineering activities to continuously innovating the value propositions and utilizing their expertise in various requirement engineering tasks. This paper reports the case study conducted with the startups that involve freelancers for the requirement engineering activities. The findings are then compared with the literature to explore the freelancer supported requirement engineering domain. Results indicate that the freelancers could help innovate value proposition by providing different perspectives of the global segments and also expertise in executing requirement engineering activities. The freelancers have varying levels of involvement in requirement engineering activities depending on on startup contexts and is highly challenged by various inhibitors. The inhibitors include difficulty to select freelancers optimally, ensuring their long term association for continuous rework arising because of continuous learnings in the market, building trust, mechanism to integrate their perspective, establishing communication, negotiations and strategic pricings. However, there is a need to optimally establish the freelancer involvement from beginning of the startup life cycle with a promise for long term benefits in exchange for their trustworthy and accurate perspectives, which is harder to get by involving crowds of customers due to resource limitations. Further research is required to investigate how freelancers could represent the samples of globally distributed customer segments as input source of information on one side and on another side become startup team representatives to establish direct interactions with global customer segments."
Islam2020,Hashinur Islam and Saumya Das and Tanushree Bose and Tanweer Ali,Diode based reconfigurable microwave filters for cognitive radio applications: A review,IEEE Access,8,,2020,10.1109/ACCESS.2020.3030020,21693536,"The cognitive radio paradigm for developing next-century wireless communication systems is rapidly entering the mainstream, and various aspects of it are currently being applied in 5G technology, aeronautical engineering, military communications, emergency, and public safety applications, satellite communication, and healthcare. Cognitive radio focuses on the existence of software defined radio architectures that allow dynamic reconfiguration. Many researchers have taken initiatives in the last decade to achieve the reconfiguration ability in cognitive radio systems to support the concept of dynamic spectrum access. As cognitive radio adapts dynamic spectrum allocation for its users, the physical implementation requires reconfigurable filters that can alter the carrier frequencies and bandwidth. Although there are many ways to reconfigure filter operation, diode based reconfiguration has received utmost attention among researchers because of its shorter response delay and easy implementation. In the last decade, researchers have reported several diode-based reconfigurable filters, including their characteristics such as filter function, filter combination, tuning range, variation in bandwidth, isolation, and resonance. However, to examine the potential of these filters in the application of cognitive radio, a comprehensive review needs to be pursued. In this review article, the descriptions of several diode based reconfigurable filters are illustrated with their exhibiting characteristics. The detailed information provided in this article has disclosed that primarily three different types diode based reconfigurable filters have been reported by researchers: Tunable, Switchable, and Hybrid (Both Tunable and Switchable). It is also found that each type of reconfiguration can further be segregated in terms of filter function, centre frequency variation, and bandwidth variation. The detailed categorization of the reconfiguration presented in this paper provides a systematic approach to select the correct reconfigurable filter for the desired frequency reconfiguration in cognitive radio."
Wei2020,Hongxu Wei and Richard J. Hauer and Xuquan Zhai,The relationship between the facial expression of people in university campus and host-city variables,Applied Sciences (Switzerland),10,4,2020,10.3390/app10041474,20763417,"Public attitudes towards local university matters for the resource investment to sustainable science and technology. The application of machine learning techniques enables the evaluation of resource investments more precisely even at the national scale. In this study, a total number of 4327 selfies were collected from the social network services (SNS) platform of Sina Micro-Blog for check-in records of 92 211-Project university campuses from 82 cities of 31 Provinces across mainland China. Photos were analyzed by the FireFACETM-V1.0 software to obtain scores of happy and sad facial expressions and a positive response index (PRI) was calculated (happy-sad). One-way analysis of variance indicated that both happy and PRI scores were highest in Shandong University and lowest in Harbin Engineering University. The national distribution of positive expression scores was highest in Changchun, Jinan, and Guangzhou cities. The maximum likelihood estimates from general linear regression indicated that the city-variable of the number of regular institutions of higher learning had the positive contribution to the happy score. The number of internet accesses and area of residential housing contributed to the negative expression scores. Therefore, people tend to show positive expression at campuses in cities with more education infrastructures but fewer residences and internet users. The geospatial analysis of facial expression data can be one approach to supply theoretical evidence for the resource arrangement of sustainable science and technology from universities."
Hoang2021,Anh Tuan Hoang and Xuan Phuong Nguyen and Osamah Ibrahim Khalaf and Thi Xuan Tran and Minh Quang Chau and Thi Minh Hao Dong and Duong Nam Nguyen,Thermodynamic simulation on the change in phase for carburizing process,"Computers, Materials and Continua",68,1,2021,10.32604/cmc.2021.015349,15462226,"The type of technology used to strengthen the surface structure of machine parts, typically by carbon-permeation, has made a great contribution to the mechanical engineering industry because of its outstanding advantages in corrosion resistance and enhanced mechanical and physical properties. Furthermore, carbon permeation is considered as an optimal method of heat treatment through the diffusion of carbon atoms into the surface of alloy steel. This study presented research results on the thermodynamic calculation and simulation of the carbon permeability process. Applying Fick's law, the paper calculated the distribution of carbon concentration in the alloy steel after it is absorbed from the surface into the internal of the sample. Using the SYSWELD software, an analysis was performed on the carbon permeability process to determine the distribution of carbon concentrations in 20CrMo steel that was then followed by a detailed analysis of the microstructure of the sample post the carburizing process. According to the calculation results, the surface carbon content was 0.9% and steadily decreased into the core. After 3 hours, the depth of the absorbent layer was measured at 0.5 mm for both the cylindrical and cubic samples. By analyzing the phase, the distribution of martensite phases such as ferrite/pearlite and residual austenite was also determined after the carburizing process."
Ourloglou2020,Olga Ourloglou and Konstantinos Stefanidis and Elias Dimitriou,Assessing nature-based and classical engineering solutions for flood-risk reduction in urban streams,Journal of Ecological Engineering,21,2,2020,10.12911/22998993/116349,22998993,"Urbanization of stream ecosystems with the purpose of managing the flash-flood events is nowadays considered responsible for habitat loss and alteration of the natural flow regime with severe implications for the ecosystem functioning. Unsurprisingly, the river scientists have started seeking alternative options inspired from nature for mitigating the flood-risk and maintaining the stream at its natural state. With this article the authors demonstrate the effects of a nature-based solution (NBS) for managing an urban stream based on the use of bioengineering materials (e.g. plants) and the implementation of the actions that restore the stream to its natural form (e.g channel widening). The HEC-RAS software was employed to simulate the flow and hydraulic components of an approximately 800m long reach of an urban stream under three different scenarios of flood risk management with a design flow set to 400 m3/s. The first scenario was based on the current situation of the stream, the second scenario concerned the stream restoration by following the nature-based solutions, while the third scenario was based on the classical ""grey"" engineering approach of concrete channelization. Unmanned Aerial Vehicle (UAV) photogrammetry methods and the Pix4Dmapper software were used in order to develop a detailed 3D model of the studied reach that accurately captured the current geomorphology. The obtained results showed that with concrete channelization, the average and maximum flow of the stream increases significantly in relation to the current situation, from 2.48 and 4.88m/s to 9.82 and 11.22 m/s, respectively, while the average Froude number raises from 0.36 to 1.69 implying super-critical flows. In contrast, the NBS scenario retained lower flow velocities and average Froude number similar to those under the current conditions. In addition, a cost estimation analysis for both stream management techniques revealed that the NBS is much cheaper than the traditional channelization (1.1 mil € vs 5.6 mil €). In conclusion, our findings suggest that the future restoration of urban streams should consider the nature-based solutions since i) they can be effective with regard to the reduction of flood-risk, ii) are cheaper than the traditional ""grey"" techniques and, most importantly, iii) maintain the natural state of the ecosystem which improves not only the ecosystem functioning but also the aesthetic value within the urban context."
Morsy2020,Karim M. Morsy and Mohamed K. Mostafa and Khaled Z. Abdalla and Mona M. Galal,Life Cycle Assessment of Upgrading Primary Wastewater Treatment Plants to Secondary Treatment Including a Circular Economy Approach,"Air, Soil and Water Research",13,,2020,10.1177/1178622120935857,11786221,"Although significant progress has been achieved in the field of environmental impact assessment in many engineering disciplines, the impact of wastewater treatment plants has not yet been well integrated. In light of this remarkable scientific progress, the outputs of the plants as treated water and clean sludge have become potential sources of irrigation and energy, not a waste. The aim of this study is to assess the environmental impacts of upgrading the wastewater treatment plants from primary to secondary treatment. The Lifecycle Assessment Framework (ISO 14040 and 14044) was applied using GaBi Software. Abu Rawash wastewater treatment plant (WWTP) has been taken as a case study. Two scenarios were studied, Scenario 1 is the current situation of the WWTP using the primary treatment units and Scenario 2 is upgrading the WWTP by adding secondary treatment units. The study highlighted the influence and cumulative impact of upgrading all the primary WWTPs in Egypt to secondary treatment. With the high amount of energy consumed in the aeration process, energy recovery methods were proposed to boost the circular economy concept in Abu Rawash WWTP in order to achieve optimal results from environmental and economic perspectives."
Nienhuis2020,Kyndylan Nienhuis and Alexandre Joannou and Thomas Bauereiss and Anthony Fox and Michael Roe and Brian Campbell and Matthew Naylor and Robert M. Norton and Simon W. Moore and Peter G. Neumann and Ian Stark and Robert N.M. Watson and Peter Sewell,Rigorous engineering for hardware security: Formal modelling and proof in the CHERI design and implementation process,,2020-May,,2020,10.1109/SP40000.2020.00055,10816011,"The root causes of many security vulnerabilities include a pernicious combination of two problems, often regarded as inescapable aspects of computing. First, the protection mechanisms provided by the mainstream processor architecture and C/C++ language abstractions, dating back to the 1970s and before, provide only coarse-grain virtual-memory-based protection. Second, mainstream system engineering relies almost exclusively on test-and-debug methods, with (at best) prose specifications. These methods have historically sufficed commercially for much of the computer industry, but they fail to prevent large numbers of exploitable bugs, and the security problems that this causes are becoming ever more acute.In this paper we show how more rigorous engineering methods can be applied to the development of a new security-enhanced processor architecture, with its accompanying hardware implementation and software stack. We use formal models of the complete instruction-set architecture (ISA) at the heart of the design and engineering process, both in lightweight ways that support and improve normal engineering practice - as documentation, in emulators used as a test oracle for hardware and for running software, and for test generation - and for formal verification. We formalise key intended security properties of the design, and establish that these hold with mechanised proof. This is for the same complete ISA models (complete enough to boot operating systems), without idealisation.We do this for CHERI, an architecture with hardware capabilities that supports fine-grained memory protection and scalable secure compartmentalisation, while offering a smooth adoption path for existing software. CHERI is a maturing research architecture, developed since 2010, with work now underway on an Arm industrial prototype to explore its possible adoption in mass-market commercial processors. The rigorous engineering work described here has been an integral part of its development to date, enabling more rapid and confident experimentation, and boosting confidence in the design."
Li2021,Jingyi Li and Sonia Hashim and Jennifer Jacobs,Whatwe can learn from visual artists about sofware development,,,,2021,10.1145/3411764.3445682,,"This paper explores software's role in visual art production by examining how artists use and develop software. We conducted interviews with professional artists who were collaborating with software developers, learning software development, and building and maintaining software.We found artists were motivated to learn software development for intellectual growth and access to technical communities. Artists valued efcient workfows through skilled manual execution and personal software development, but avoided high-level forms of software automation. Artists identifed conficts between their priorities and those of professional developers and computational art communities, which infuenced how they used computational aesthetics in their work. These fndings contribute to eforts in systems engineering research to integrate end-user programming and creativity support across software and physical media, suggesting opportunities for artists as collaborators. Artists' experiences writing software can guide technical implementations of domain-specifc representations, and their experiences in interdisciplinary production can aid inclusive community building around computational tools."
Khaleel2021,Omar J. Khaleel and Thamir Khalil Ibrahim and Firas Basim Ismail and Ahmed T. Al-Sammarraie,Developing an analytical model to predict the energy and exergy based performances of a coal-fired thermal power plant,Case Studies in Thermal Engineering,28,,2021,10.1016/j.csite.2021.101519,2214157X,"In the present study, an analytical model has been developed to predict the effectiveness of the closed feedwater heaters based on the measured data available from a coal-fired thermal power plant. Invoking this model, a ‘Predictive Model’ has been developed to anticipate the energy and exergy-based behavior of the coal-fired power plants. In this model, all the components have been assumed to be installed, and hence they could not be changed. Based on this model, a numerical code has been developed to analyze an existing coal-fired power plant using Engineering Equation Solver software. Invoking this code, the impact of the operational conditions on the performance of the power plant is examined. The effects of superheater outlet pressure, superheater outlet temperature, and reheater inlet pressure upon energy and exergy-based parameters of the cycle have been examined. It has been observed that by increasing the superheater pressure by 100%, the net power delivered by the cycle increases more than 8%. While increasing the superheated steam temperature from 539.8 to 580 °C, the net power delivered by the cycle increases more than 6%. Further, by increasing the reheater pressure from 30 to 34 bar, the net power delivered by the cycle reduces 1.57%."
ElHajAssad2021,Mamdouh El Haj Assad and Milad Sadeghzadeh and Mohammad Hossein Ahmadi and Mohammad Al-Shabi and Mona Albawab and Amjad Anvari-Moghaddam and Ehab Bani Hani,Space cooling using geothermal single-effect water/lithium bromide absorption chiller,Energy Science and Engineering,9,10,2021,10.1002/ese3.946,20500505,"This research is proposed to fully investigate the performance of a single-effect water/lithium bromide absorption chiller driven by geothermal energy. Since absorption cycles are considered as low-grade energy cycles, this innovative idea of rejecting fluid from a single-flash geothermal power plant with low-grade energy would serve as efficient, economical, and promising technology. In order to examine the feasibility of this approach, a residential building which is located in Sharjah, UAE, considered to evaluate its cooling capacity of 39 kW which is calculated using MATLAB software. Based on the obtained cooling load, modeling of the required water/lithium bromide single-effect absorption chiller machine is implemented and discussed. A detailed performance analysis of the proposed model under different conditions is performed using Engineering Equation Solver software (EES). Based on the obtained results, the major factors in the design of the proposed system are the size of the heat exchangers and the input heat source temperature. The results are presented graphically to find out the geofluid temperature and mass flow and solution heat exchanger effectiveness effects on the chiller thermal performance. Moreover, the effects of the size of all components of the absorption chiller on the cooling load to meet the space heating are presented. The thermal efficiency of the single-flash geothermal power plant is about 13% when the power plant is at production well temperature 250℃, separator pressure 0.24 MPa, and condenser pressure 7.5 kPa. The results show that the coefficient of performance (COP) reaches about 0.87 at solution heat exchanger effectiveness of 0.9, when the geofluid temperature is 120℃."
Alnafessah2021,Ahmad Alnafessah and Alim Ul Gias and Runan Wang and Lulai Zhu and Giuliano Casale and Antonio Filieri,Quality-Aware DevOps Research: Where Do We Stand?,IEEE Access,9,,2021,10.1109/ACCESS.2021.3064867,21693536,"DevOps is an emerging paradigm that reduces the barriers between developers and operations teams to offer continuous fast delivery and enable quick responses to changing requirements within the software life cycle. A significant volume of activity has been carried out in recent years with the aim of coupling DevOps stages with tools and methods to improve the quality of the produced software and the underpinning delivery methodology. While the research community has produced a sustained effort by conducting numerous studies and innovative development tools to support quality analyses within DevOps, there is still a limited cohesion between the research themes in this domain and a shortage of surveys that holistically examine quality engineering work within DevOps. In this paper, we address the gap by comprehensively surveying existing efforts in this area, categorizing them according to the stage of the DevOps lifecycle to which they primarily contribute. The survey holistically spans across all the DevOps stages, identify research efforts to improve architectural design, modeling and infrastructure-as-code, continuous-integration/continuous-delivery (CI/CD), testing and verification, and runtime management. Our analysis also outlines possible directions for future work in quality-aware DevOps, looking in particular at AI for DevOps and DevOps for AI software."
Freire2020,Sávio Freire and Nicolli Rios and Manoel Mendonça and Davide Falessi and Carolyn Seaman and Clemente Izurieta and Rodrigo O. Spínola,Actions and impediments for technical debt prevention: Results from a global family of industrial surveys,,,,2020,10.1145/3341105.3373912,,"Background: Preventing the occurrence of technical debt (TD) in software projects can be cheaper than its payment. Prevention practices also help in catching inexperienced developers' 'not-so-good' solutions. However, little is known on how to prevent the occurrence of TD. Aims: To investigate, from the point of view of software practitioners, preventive actions that can be used to curb the occurrence of TD and the impediments that hamper the use of those actions. Method: We use data from the InsighTD Project, a family of industrial surveys specifically designed to study software engineering TD. We use a corpus of answers from 207 practitioners across different geographic locations to identify and analyze - both quantitatively and qualitatively - the TD preventive actions most used in practice. Results: We found that project planning, adoption of good practices, well-defined requirements, creating tests, and training are the most cited preventive actions that curb TD in software projects. We also identified seven preventive action categories and defined relationships among them and TD types. On the other hand, the main impediments to prevent TD are related to inappropriate project planning and lack of expertise of the team. Conclusions: Our list of preventive actions and impediments can help practitioners to implement policies for the sector and guide TD researches in a problem-driven way."
Raman2020,Naveen Raman and Minxuan Cao and Yulia Tsvetkov and Christian Kastner and Bogdan Vasilescu,"Stress and Burnout in Open Source: Toward Finding, Understanding, and Mitigating Unhealthy Interactions",,,,2020,10.1145/3377816.3381732,02705257,"Developers from open-source communities have reported high stress levels from frequent demands for features and bug fixes and from the sometimes aggressive tone of these demands. Toxic conversations may demotivate and burn out developers, creating challenges for sustaining open source. We outline a path toward finding, understanding, and possibly mitigating such unhealthy interactions. We take a first step toward finding them, by developing and demonstrating a measurement instrument (an SVM classifier tailored for software engineering) to detect toxic discussions in GITHUB issues. We used our classifier to analyze trends over time and in different GITHUB communities, finding that toxicity varies by community and that toxicity decreased between 2012 and 2018."
Rahman2020,Md Mostafizer Rahman and Yutaka Watanobe and Keita Nakamura and Miroslav Bures,A Neural Network Based Intelligent Support Model for Program Code Completion,Scientific Programming,2020,,2020,10.1155/2020/7426461,10589244,"In recent years, millions of source codes are generated in different languages on a daily basis all over the world. A deep neural network-based intelligent support model for source code completion would be a great advantage in software engineering and programming education fields. Vast numbers of syntax, logical, and other critical errors that cannot be detected by normal compilers continue to exist in source codes, and the development of an intelligent evaluation methodology that does not rely on manual compilation has become essential. Even experienced programmers often find it necessary to analyze an entire program in order to find a single error and are thus being forced to waste valuable time debugging their source codes. With this point in mind, we proposed an intelligent model that is based on long short-term memory (LSTM) and combined it with an attention mechanism for source code completion. Thus, the proposed model can detect source code errors with locations and then predict the correct words. In addition, the proposed model can classify the source codes as to whether they are erroneous or not. We trained our proposed model using the source code and then evaluated the performance. All of the data used in our experiments were extracted from Aizu Online Judge (AOJ) system. The experimental results obtained show that the accuracy in terms of error detection and prediction of our proposed model approximately is 62% and source code classification accuracy is approximately 96% which outperformed a standard LSTM and other state-of-the-art models. Moreover, in comparison to state-of-the-art models, our proposed model achieved an interesting level of success in terms of error detection, prediction, and classification when applied to long source code sequences. Overall, these experimental results indicate the usefulness of our proposed model in software engineering and programming education arena."
Zhang2020,Jin Zhang and Jingyue Li,Testing and verification of neural-network-based safety-critical control software: A systematic literature review,Information and Software Technology,123,,2020,10.1016/j.infsof.2020.106296,09505849,"Context: Neural Network (NN) algorithms have been successfully adopted in a number of Safety-Critical Cyber-Physical Systems (SCCPSs). Testing and Verification (T&V) of NN-based control software in safety-critical domains are gaining interest and attention from both software engineering and safety engineering researchers and practitioners. Objective: With the increase in studies on the T&V of NN-based control software in safety-critical domains, it is important to systematically review the state-of-the-art T&V methodologies, to classify approaches and tools that are invented, and to identify challenges and gaps for future studies. Method: By searching the six most relevant digital libraries, we retrieved 950 papers on the T&V of NN-based Safety-Critical Control Software (SCCS). Then we filtered the papers based on the predefined inclusion and exclusion criteria and applied snowballing to identify new relevant papers. Results: To reach our result, we selected 83 primary papers published between 2011 and 2018, applied the thematic analysis approach for analyzing the data extracted from the selected papers, presented the classification of approaches, and identified challenges. Conclusion: The approaches were categorized into five high-order themes, namely, assuring robustness of NNs, improving the failure resilience of NNs, measuring and ensuring test completeness, assuring safety properties of NN-based control software, and improving the interpretability of NNs. From the industry perspective, improving the interpretability of NNs is a crucial need in safety-critical applications. We also investigated nine safety integrity properties within four major safety lifecycle phases to investigate the achievement level of T&V goals in IEC 61508-3. Results show that correctness, completeness, freedom from intrinsic faults, and fault tolerance have drawn most attention from the research community. However, little effort has been invested in achieving repeatability, and no reviewed study focused on precisely defined testing configuration or defense against common cause failure."
Chen2021,Jianqiang Chen,Advances in the key technologies of Chinese national numerical windtunnel project,Zhongguo Kexue Jishu Kexue/Scientia Sinica Technologica,51,11,2021,10.1360/SST-2020-0334,2095946X,"The rapid advancements in computational fluid dynamics (CFD) and high-performance computers (HPC) have led to the terminology of numerical windtunnel (NW) which first appeared in late 1980s in Japan. The concept of NW has been further developed in the United States of America. It is considered that NW is the product of the cross integration of fluid mechanics, mathematics, computer science and aeronautical engineering, and plays an increasingly important role in the development of aerospace vehicles. In 2018, China launched the national numerical windtunnel (NNW) project. The project aims to unite domestic superior partners, take advantage of the whole national forces, carry out collaborative researches, independently develop a CFD software system with advanced functions and complete categories, build world-leading CFD dedicated HPC, develop a domestic-open/shared aerodynamic numerical simulation platform with independent intellectual property rights in China. In this paper, we first introduce the system components and the key goals of the NNW project. Then, we summarize the latest advances on key technologies including the software framework, the parallel techniques, the grid generation methods, the visualization techniques, the verification and validation processes, the algorithms and models, etc. Finally, the next technological development plan of the NNW project is prospected."
Leman2020,Julia Koehler Leman and Brian D. Weitzner and P. Douglas Renfrew and Steven M. Lewis and Rocco Moretti and Andrew M. Watkins and Vikram Khipple Mulligan and Sergey Lyskov and Jared Adolf-Bryfogle and Jason W. Labonte and Justyna Krys and Christopher Bystroff and William Schief and Dominik Gront and Ora Schueler-Furman and David Baker and Philip Bradley and Roland Dunbrack and Tanja Kortemme and Andrew Leaver-Fay and Charlie E.M. Strauss and Jens Meiler and Brian Kuhlman and Jeffrey J. Gray and Richard Bonneau,Better together: Elements of successful scientific software development in a distributed collaborative community,PLoS Computational Biology,16,5,2020,10.1371/journal.pcbi.1007507,15537358,"Many scientific disciplines rely on computational methods for data analysis, model generation, and prediction. Implementing these methods is often accomplished by researchers with domain expertise but without formal training in software engineering or computer science. This arrangement has led to underappreciation of sustainability and maintainability of scientific software tools developed in academic environments. Some software tools have avoided this fate, including the scientific library Rosetta. We use this software and its community as a case study to show how modern software development can be accomplished successfully, irrespective of subject area. Rosetta is one of the largest software suites for macromolecular modeling, with 3.1 million lines of code and many state-of-the-art applications. Since the mid 1990s, the software has been developed collaboratively by the Rosetta- Commons, a community of academics from over 60 institutions worldwide with diverse backgrounds including chemistry, biology, physiology, physics, engineering, mathematics, and computer science. Developing this software suite has provided us with more than two decades of experience in how to effectively develop advanced scientific software in a global community with hundreds of contributors. Here we illustrate the functioning of this development community by addressing technical aspects (like version control, testing, and maintenance), community-building strategies, diversity efforts, software dissemination, and user support. We demonstrate how modern computational research can thrive in a distributed collaborative community. The practices described here are independent of subject area and can be readily adopted by other software development communities."
Paiva2020,Joseane O.V. Paiva and Rossana M.C. Andrade and Pedro Almir M. de Oliveira and Paulo Duarte and Ismayle S. Santos and Aline L.P. de Evangelista and Rebecca L. Theophilo and Luiz Odorico M. de Andrade and Ivana Cristina H.C. de Barreto,Mobile applications for elderly healthcare: A systematic mapping,PLoS ONE,15,7 July,2020,10.1371/journal.pone.0236091,19326203,"Context: The increase in the population aging has brought more significant concern about how proper care will be provided to the elderly in the future. Thus, the development of technological solutions for the health domain has gained more prominence. Joining this scenario to the growing use of mobile devices for daily activities, several mobile applications focused on the elderly healthcare have been developed with healthcare and software engineer professionals involved. However, there is no survey to help both professionals to take decisions on the target of application, elderly profile, empirical validation techniques, among others. Thus, the following question arises: how have mobile applications for elderly healthcare been addressed in the literature in the past years? Objective: To identify the state of the art in the literature concerned with the development of mobile applications for elderly healthcare, considering healthcare and software Engineering viewpoints. Method: We performed a systematic mapping conducted by health and software engineering researchers to provide an interdisciplinary investigation of the papers that address mobile applications for elderly healthcare, summarizing the data collected under the following classification: target of application, older adult profile, spatial-temporal distribution, techniques for empirical validation and type of software engineering research. Results: We found a total of 2533 papers and, after applying our eligibility criteria, we got 149. We observed aspects related to the digital health initiative type, using the classification proposed by the World Health Organization (WHO), the elderly profile prioritized by the application, the spatial-temporal distribution of the studies, the empirical validation type, and the research contribution of each analyzed paper to the software engineering area. Conclusions: Regarding the WHO classification, we noticed that two categories were more frequently found, Clients and Data Services, and that none of the mobile apps were classified in the Health System Manager category. The data extraction result also reveals that most of the applications found in the literature focused on the independent elderly. Moreover, we observed that most of the studies were proposals of solutions for elderly health and the validation process of these solutions generally consisted of controlled experiments and usability evaluations. At last, the research focused on mobile applications for elderly healthcare has been performed mostly by developed countries."
Verdecchia2021,Roberto Verdecchia and Philippe Kruchten and Patricia Lago and Ivano Malavolta,Building and evaluating a theory of architectural technical debt in software-intensive systems,Journal of Systems and Software,176,,2021,10.1016/j.jss.2021.110925,01641212,"Architectural technical debt in software-intensive systems is a metaphor used to describe the “big” design decisions (e.g., choices regarding structure, frameworks, technologies, languages, etc.) that, while being suitable or even optimal when made, significantly hinder progress in the future. While other types of debt, such as code-level technical debt, can be readily detected by static analyzers, and often be refactored with minimal or only incremental efforts, architectural debt is hard to be identified, of wide-ranging remediation cost, daunting, and often avoided. In this study, we aim at developing a better understanding of how software development organizations conceptualize architectural debt, and how they deal with it. In order to do so, in this investigation we apply a mixed empirical method, constituted by a grounded theory study followed by focus groups. With the grounded theory method we construct a theory on architectural technical debt by eliciting qualitative data from software architects and senior technical staff from a wide range of heterogeneous software development organizations. We applied the focus group method to evaluate the emerging theory and refine it according to the new data collected. The result of the study, i.e., a theory emerging from the gathered data, constitutes an encompassing conceptual model of architectural technical debt, identifying and relating concepts such as its symptoms, causes, consequences, management strategies, and communication problems. From the conducted focus groups, we assessed that the theory adheres to the four evaluation criteria of classic grounded theory, i.e., the theory fits its underlying data, is able to work, has relevance, and is modifiable as new data appears. By grounding the findings in empirical evidence, the theory provides researchers and practitioners with novel knowledge on the crucial factors of architectural technical debt experienced in industrial contexts."
Berg2020,Vebjørn Berg and Jørgen Birkeland and Anh Nguyen-Duc and Ilias O. Pappas and Letizia Jaccheri,Achieving agility and quality in product development - an empirical study of hardware startups,Journal of Systems and Software,167,,2020,10.1016/j.jss.2020.110599,01641212,"Context: Startups aim at scaling their business, often by developing innovative products with limited human and financial resources. The development of software products in the startup context is known as opportunistic, agility-driven, and with high tolerance for technical debt. The special context of hardware startups calls for a better understanding of state-of-the-practice of hardware startups’ activities. Objective: This study aimed to identify whether and how startups can achieve product quality while maintaining focus on agility. Method: We conducted an exploratory study with 13 hardware startups, collecting data through semi-structured interviews and analysis of documentation. We proposed an integrative model of agility and quality in hardware startups. Results: Agility in hardware startups is complex and not achieved through adoption of fast-paced development practices alone. Hardware startups follow a quality-driven approach for development of core components, where frequent user testing is a measure for early debt management. Hardware startups often lack mindset and strategies for achieving long-term quality in early stages. Conclusions: Hardware startups need attention to hardware quality to allow for evolutionary prototyping and speed. Future research should focus on defining quality-driven practices that contribute to agility, and strategies and mindsets to support long-term quality in the hardware startup context."
Bouich2022,Amal Bouich and Julia Marí-Guaita and Bernabé Marí Soucase and Pablo Palacios,Manufacture of High-Efficiency and Stable Lead-Free Solar Cells through Antisolvent Quenching Engineering,Nanomaterials,12,17,2022,10.3390/nano12172901,20794991,"Antisolvent quenching has shown to significantly enhance several perovskite films used in solar cells; however, no studies have been conducted on its impact on MASnI3. Here, we investigated the role that different antisolvents, i.e., diethyl ether, toluene, and chlorobenzene, have on the growth of MASnI3 films. The crystallinity, morphology, topography, and optical properties of the obtained thin films were characterized by X-ray diffraction (XRD), scanning electron microscopy (SEM), photoluminescence (PL) measurements, and UV–visible spectroscopy. The impact of the different antisolvent treatments was evaluated based on the surface homogeneity as well as the structure of the MASnI3 thin films. In addition, thermal annealing was optimized to control the crystallization process. The applied antisolvent was modified to better manage the supersaturation process. The obtained results support the use of chlorobenzene and toluene to reduce pinholes and increase the grain size. Toluene was found to further improve the morphology and stability of thin films, as it showed less degradation after four weeks under dark with 60% humidity. Furthermore, we performed a simulation using SCAPS-1D software to observe the effect of these antisolvents on the performance of MASnI3-based solar cells. We also produced the device FTO/TiO2/MASnI3/Spiro-OMeTAD/Au, obtaining a remarkable photoconversion efficiency (PCE) improvement of 5.11% when using the MASnI3 device treated with chlorobenzene. A PCE improvement of 9.44% was obtained for the MASnI3 device treated with toluene, which also showed better stability. Our results support antisolvent quenching as a reproducible method to improve perovskite devices under ambient conditions."
Wang2021,Jingyu Wang and Shangshang Wei and Qiuwang Wang and Bengt Sundén,Transient numerical modeling and model predictive control of an industrial-scale steam methane reforming reactor,International Journal of Hydrogen Energy,46,29,2021,10.1016/j.ijhydene.2021.02.123,03603199,"A steam methane reforming reactor is a key equipment in hydrogen production, and numerical analysis and process control can provide a critical insight into its reforming mechanisms and flexible operation in real engineering applications. The present paper firstly studies the transport phenomena in an industrial-scale steam methane reforming reactor by transient numerical simulations. Wall effect and local non thermal equilibrium is considered in the simulations. A temperature profile of the tube outer wall is given by user defined functions integrated into the ANSYS FLUENT software. Dynamic simulations show that the species distribution is closely related to the temperature distribution which makes the temperature of the reactor tube wall an important factor for the hydrogen production of the reformer and the thermal conductivity of the catalyst network is crucial in the heat transfer in the reactor. Besides, there exists a delay of the reformer's hydrogen production when the temperature profile of the tube wall changes. Among inlet temperature, inlet mass flow rate and inlet steam-to-carbon (S/C) ratio, the mass flow rate is the most influencing factor for the hydrogen production. The dynamic matrix control (DMC) scheme is subsequently designed to manipulate the mole fraction of hydrogen of the outlet to the target value by setting the temperature profile trajectory of the reforming tube with time. The proportional-integral control strategy is also studied for comparison. The closed-loop simulation results show that the proposed DMC control strategy can reduce the overshoot and have a small change of the input variable. In addition, the disturbances of feed disturbance can also be well rejected to assure the tracking performance, indicating the superiority of the DMC controller. All the results give insight to the theoretical analysis and controller design of a steam methane reformer and demonstrate the potential of the CFD modeling in study the transport mechanism and the idea of combining CFD modeling with controller design for the real application."
Wang2022,Kai Wang and Guodong Zhang and Yanhai Wang and Xiang Zhang and Kangnan Li and Wei Guo and Feng Du,A numerical investigation of hydraulic fracturing on coal seam permeability based on PFC-COMSOL coupling method,International Journal of Coal Science and Technology,9,1,2022,10.1007/s40789-022-00484-2,21987823,"Hydraulic fracturing and permeability enhancement are effective methods to improve low-permeability coal seams. However, few studies focused on methods to increase permeability, and there are no suitable prediction methods for engineering applications. In this work, PFC2D software was used to simulate coal seam hydraulic fracturing. The results were used in a coupled mathematical model of the interaction between coal seam deformation and gas flow. The results show that the displacement and velocity of particles increase in the direction of minimum principal stress, and the cracks propagate in the direction of maximum principal stress. The gas pressure drop rate and permeability increase rate of the fracture model are higher than that of the non-fracture model. Both parameters decrease rapidly with an increase in the drainage time and approach 0. The longer the hydraulic fracturing time, the more complex the fracture network is, and the faster the gas pressure drops. However, the impact of fracturing on the gas drainage effect declines over time. As the fracturing time increases, the difference between the horizontal and vertical permeability increases. However, this difference decreases as the gas drainage time increases. The higher the initial void pressure, the faster the gas pressure drops, and the greater the permeability increase is. However, the influence of the initial void pressure on the permeability declines over time. The research results provide guidance for predicting the anti-reflection effect of hydraulic fracturing in underground coal mines."
Wu2020,Y. H. Wu and C. Y. Dong and H. S. Yang,Isogeometric indirect boundary element method for solving the 3D acoustic problems,Journal of Computational and Applied Mathematics,363,,2020,10.1016/j.cam.2019.06.013,03770427,"This paper provides an Isogeometric Indirect Boundary Element Method (IGIBEM) based on NURBS (Non-Uniform Rational B-Splines) and PHT-splines (polynomial splines over hierarchical T-meshes) for analyzing the three-dimensional (3D) acoustic problems. In the classical procedure, the geometries are discretized by Lagrange polynomials elements, which leads to both substantial geometrical error and time-consuming meshing steps. However, these deficiencies can be eliminated by the isogeometric analysis (IGA) directly incorporating the geometry description generated from the CAD (Computer Aided Design) software into CAE (Computer Aided Engineering) analysis. Unlike the DBEM (direct boundary element method), the IBEM (indirect boundary element method) allows different types of boundary conditions on the two sides of a surface. Moreover, the hypersingular integrals in IBEM can be transformed to a weakly singular form. In addition, the PHT-based IBEM is used to investigate the influence of local refinement on the accuracy of solutions. Finally, the non-uniqueness problem is solved, which is a fatal defect in the acoustic BEM for the exterior problem. Four different methods to handle the non-uniqueness problem are discussed and compared. The results obtained by the proposed method were compared with analytical solutions and the results computed by Lagrange-based IBEM. Several benchmark examples demonstrate: (1) the present method, i.e. IGIBEM, has super accuracy over conventional IBEM for the acoustic problems; (2) local refinement has a significant influence on the convergence rate of the solutions, and the numerical accuracy is relevant to the distance to the boundary where the local refinement acted; (3) as for the non-uniqueness problem, the imposition of specific interior boundary conditions not only obtains the best calculation result over the entire range of frequencies, but also has a simple integral formulation."
Ali2022,Shahid Ali and Aziz Khan and Kamal Shah and Manar A. Alqudah and Thabet Abdeljawad and Siraj-ul-Islam,On computational analysis of highly nonlinear model addressing real world applications,Results in Physics,36,,2022,10.1016/j.rinp.2022.105431,22113797,"This paper presents a numerical strategy for solving boundary value problems (BVPs) that is based on the Haar wavelets method (HWM). BVPs having high Prandtl numbers are discussed, Because they are very important in many practical problems of science and engineering. By using group-theoretic method, the considered model of partial differential equations (PDEs) are converted to system of nonlinear ordinary differential equations. By using HWM, the numerical results are established. Further, solutions obtained on a coarse resolution with low accuracy is refined towards higher accuracy by increasing the level of resolution. Superiority of the HWM has been established over the commercial software NDSolve and available numerical and approximated methods."
Clark2022,Renee M. Clark and Autar K. Kaw and Rafael Braga Gomes,Adaptive learning: Helpful to the flipped classroom in the online environment of COVID?,Computer Applications in Engineering Education,30,2,2022,10.1002/cae.22470,10990542,"Flipped instruction in an undergraduate numerical methods course in the online, remote environment during the COVID-19 pandemic was conducted with and without the use of adaptive-learning lessons for pre-class preparation. This comparison was made to explore potential differences with and without adaptive software relative to exam and concept inventory performance and student perceptions of the classroom environment, learning and motivation, and benefits and drawbacks. Student perceptions were gathered via the College and University Classroom Environment Inventory (CUCEI) and a survey designed to capture feedback specific to flipped instruction. The analysis was made possible by a current NSF grant to study adaptive learning in the flipped classroom at three universities and extensive prior research with the flipped classroom and adaptive learning by the authors. Results gathered in the online flipped classroom with adaptive learning suggested positive changes in the following: classroom environmental perceptions, preference for flipped instruction, perceived responsibility imposed, motivation for independent learning, and perceived learning. Furthermore, based on an open-ended question, there was a significant decrease in the proportion of students who experienced load, burden, or stressors in the online flipped classroom when adaptive learning was available versus not. Multiple-choice exam and concept-inventory results were slightly higher with adaptive lessons (although not significantly so), with the most promising results occurring for Pell grant recipients. The emerging medical education literature has suggested that adaptive learning and flipped instruction will be key to post-pandemic education. The present article begins advocacy for adaptive learning with flipped instruction in engineering education."
Pepe2022,Massimiliano Pepe and Vincenzo Saverio Alfio and Domenica Costantino,UAV Platforms and the SfM-MVS Approach in the 3D Surveys and Modelling: A Review in the Cultural Heritage Field,Applied Sciences (Switzerland),12,24,2022,10.3390/app122412886,20763417,"In recent years, structure from motion (SfM) and multi-view stereo (MVS) algorithms have been successfully applied to stereo images generated by cameras mounted on unmanned aerial vehicle (UAV) platforms to build 3D models. Indeed, the approach based on the combination of SfM-MVS and UAV-generated images allows for cost-effective acquisition, fast and automated processing, and detailed and accurate reconstruction of 3D models. As a consequence, this approach has become very popular for representation, management, and conservation in the field of cultural heritage (CH). Therefore, this review paper discusses the use of UAV photogrammetry in CH environments with a focus on state of the art trends and best practices in image acquisition technologies and 3D model-building software. In particular, this paper intends to emphasise the different techniques of image acquisition and processing in relation to the different platforms and navigation systems available, as well as to analyse and deepen the aspects of 3D reconstruction that efficiently describe the entire photogrammetric process, providing further insights for new applications in different fields, such as structural engineering and conservation and maintenance restoration of sites and structures belonging to the CH field."
Ahrendt2020,Dustin Ahrendt and Arturo Romero Karam,Development of a computer-aided engineering–supported process for the manufacturing of customized orthopaedic devices by three-dimensional printing onto textile surfaces,Journal of Engineered Fibers and Fabrics,15,,2020,10.1177/1558925020917627,15589250,"Today, additive manufacturing, also called three-dimensional printing, is used for producing prototypes as well as other products for various industrial sectors. Although this technology is already well established in the automotive, aviation and space travel, building, dental and medical sectors, its integration in the textile and ready-made industry is still in progress. At present, there is a lack of specific application scenarios for the combination of three-dimensional printing and textile materials, apart from fashion and shoe design. Hence, this article presents a digital computer-aided engineering–supported process to manufacture customized orthopaedic devices by three-dimensional printing directly onto a textile fabric. State-of-the-art fabrication methods for orthoses are typically labour intensive. The combination of three-dimensional scanning, computer-aided design modelling and three-dimensional printing onto textile materials open up new possibilities for producing custom-made products. After three-dimensional scanning of a patient’s individual body shape, the surface is prepared for constructing the textile pattern cuts by reverse engineering. The transformation of the designed three-dimensional patterns into two-dimensional is software supported. Additional positioning lines in accordance with specific body measurements are transferred onto the two-dimensional pattern cuts, which are then used as the basis for the design of the three-dimensional printed functional elements. Subsequently, the design is saved in STL (Standard Triangulation/Tessellation Language) file format, prepared by slicing and directly printed onto textile pattern cuts by means of fused deposition modelling. The last manufacturing step involves the assembly of the textile fabric. The proposed process is demonstrated by an example application scenario, thus proving its potential for industrial use in the textile and ready-made industry."
Shende2021,Pravin Shende and Riddhi Trivedi,3D Printed Bioconstructs: Regenerative Modulation for Genetic Expression,Stem Cell Reviews and Reports,17,4,2021,10.1007/s12015-021-10120-2,26293277,"Layer-by-layer deposition of cells, tissues and similar molecules provided by additive manufacturing techniques such as 3D bioprinting offers safe, biocompatible, effective and inert methods for the production of biological structures and biomimetic scaffolds. 3D bioprinting assisted through computer programmes and software develops mutli-modal nano- or micro-particulate systems such as biosensors, dosage forms or delivery systems and other biological scaffolds like pharmaceutical implants, prosthetics, etc. This review article focuses on the implementation of 3D bioprinting techniques in the gene expression, in gene editing or therapy and in delivery of genes. The applications of 3D printing are extensive and include gene therapy, modulation and expression in cancers, tissue engineering, osteogenesis, skin and vascular regeneration. Inclusion of nanotechnology with genomic bioprinting parameters such as gene conjugated or gene encapsulated 3D printed nanostructures may offer new avenues in the future for efficient and controlled treatment and help in overcoming the limitations faced in conventional methods. Moreover, expansion of the benefits from such techniques is advantageous in real-time delivery or in-situ production of nucleic acids into the host cells. Graphical abstract: [Figure not available: see fulltext.]"
Chen2021,Yimin Chen and Wenzhuo Zhang and Lu Dong and Korhan Cengiz and Amit Sharma,Study on vibration and noise influence for optimization of garden mower,Nonlinear Engineering,10,1,2021,10.1515/nleng-2021-0034,21928029,"Advancement in engineering provides various improvement in quality life while taking consideration of important factors for safety and environment. The use of mower food maintenance of land it is very common across several parts of the world with some frequent noise generated through its operation. This article is an attempt to study the noise and frequency generated through the vibrations of mower blade. In this study, an integrated design for designing, testing and developing mower blade that generates less noise is presented. For designing efficient blade that produces less noise, we have implemented various engineering approaches such as rapid product design, process of re-engineering and reverse engineering. The simulation of the designed blade is carried out through CAD software where the design prototype is analysed for its performance. The outcomes of the prototype are tested through simulation and its performance is compared for the determination of success of proposed design at different variations in frequency level. It is observed through the experimentation that the noise and vibration differences are generated through load carrying vehicles, mowers with riding capacity and simple mowers. From the analysis, mower with riding capacity is observed as safest among all other types of machines."
Bui2021,Nghi D.Q. Bui and Yijun Yu and Lingxiao Jiang,TreeCaps: Tree-Based Capsule Networks for Source Code Processing,,1,,2021,10.1609/aaai.v35i1.16074,2159-5399,"Recently program learning techniques have been proposed to process source code based on syntactical structures (e.g., abstract syntax trees) and/or semantic information (e.g., dependency graphs). While graphs may be better than trees at capturing code semantics, constructing the graphs from code inputs through the semantic analysis of multiple viewpoints can lead to inaccurate noises for a specific software engineering task. Compared to graphs, syntax trees are more precisely defined on the grammar and easier to parse; unfortunately, previous tree-based learning techniques have not been able to learn semantic information from trees to achieve better accuracy than graph-based techniques. We have proposed a new learning technique, named TreeCaps, by fusing together capsule networks with tree-based convolutional neural networks to achieve a learning accuracy higher than some existing graph-based techniques while it is based only on trees. TreeCaps introduces novel variable-to-static routing algorithms into the capsule networks to compensate for the loss of previous routing algorithms. Aside from accuracy, we also find that TreeCaps is the most robust to withstand those semantic-preserving program transformations that change code syntax without modifying the semantics. Evaluated on a large number of Java and C/C++ programs, TreeCaps models outperform prior deep learning models of program source code, in terms of both accuracy and robustness for program comprehension tasks such as code functionality classification and function name prediction. Our implementation is publicly available at: https://github.com/bdqnghi/treecaps."
Stojanovic2021,Ljiljana Stojanovic and Thomas Usländer and Friedrich Volz and Christian Weißenbacher and Jens Müller and Michael Jacoby and Tino Bischoff,Methodology and Tools for Digital Twin Management—The FA3ST Approach,Internet of Things,2,4,2021,10.3390/iot2040036,2624831X,"The concept of digital twins (DT) has already been discussed some decades ago. Digital representations of physical assets are key components in industrial applications as they are the basis for decision making. What is new is the conceptual approach to consider DT as well-defined software entities themselves that follow the whole lifecycle of their physical counterparts from the engineering, operation up to the discharge, and hence, have their own type description, identity, and lifecycle. This paper elaborates on this idea and argues the need for systematic DT engineering and management. After a conceptual description of DT, the paper proposes a DT lifecycle model and presents methodologies and tools for DT management, also in the context of Industrie 4.0 concepts, such as the asset administration shell (AAS), the international data spaces (IDS), and IEC standards (such as OPC UA and AML). As a tool example for the support of DT engineering and management, the Fraunhofer-advanced AAS tools for digital twins (FA3ST) are presented in more detail."
Biancolini2020,Marco Evangelos Biancolini and Katia Capellini and Emiliano Costa and Corrado Groth and Simona Celi,Fast interactive CFD evaluation of hemodynamics assisted by RBF mesh morphing and reduced order models: the case of aTAA modelling,International Journal on Interactive Design and Manufacturing,14,4,2020,10.1007/s12008-020-00694-5,19552505,"The medical digital twin is emerging as a viable opportunity to provide patient-specific information useful for treatment, prevention and surgical planning. A bottleneck toward its effective use when computational fluid dynamics (CFD) techniques and tools are adopted for the high fidelity prediction of blood flow, is the significant computing cost required. Reduced order models (ROM) looks to be a promising solution for facing the aforementioned limit. In fact, once ROM data processing is accomplished, the consumption stage can be performed outside the computer-aided engineering software adopted for simulation and, in addition, it could be also implemented on interactive software visualization interfaces that are commonly employed in the medical context. In this paper we demonstrate the soundness of such a concept by numerically investigating the effect of the bulge shape for the ascending thoracic aorta aneurysm case. Radial basis functions (RBF) based mesh morphing enables the implementation of a parametric shape, which is used to build up the ROM framework and data. The final result is an inspection tool capable to visualize, interactively and almost in real-time, the effect of shape parameters on the entire flow field. The approach is first verified considering a morphing action representing the progression from an average healthy patient to an average aneurismatic one (Capellini et al. in Proceedings VII Meeting Italian Chapter of the European Society of Biomechanics (ESB-ITA 2017), 2017; Capellini et al. in J. Biomech. Eng. 140(11):111007-1–111007-10, 2018). Then, a set of shape parameters, suitable to consistently represent a widespread number of possible bulge configurations, are defined and accordingly generated. The concept is showcased taking into account the steady flow field at systolic peak conditions, using ANSYS®Fluent®and its ROM environment for CFD and ROM calculations respectively, and the RBF MorphTM software for shape parametrization."
Okagbue2021,Hilary I. Okagbue and Pelumi E. Oguntunde and Emmanuela C.M. Obasi and Elvir M. Akhmetshin,Trends and usage pattern of SPSS and Minitab Software in Scientific research,,1734,1,2021,10.1088/1742-6596/1734/1/012017,17426596,"Most scientific research generates data. Analysis of the data from scientific research helps create new knowledge or a deep understanding of natural phenomena. Statistical software is used mainly in data analysis. SPSS and Minitab appear to be most popular, especially for those that could neither code nor mathematical inclined to handle advanced software such as R, MATLAB, Maple, etc. Trends and usage pattern of SPSS and Minitab Software in Scientific research was studied in this paper with the data obtained from the Scopus database. In their abstracts or keywords, documents that have mentioned SPSS were extracted for the years 2010 to 2019. Frequency analysis showed that the trend of using SPSS and Minitab is steadily increasing, although the use of Minitab is a fraction of SPSS. Minitab is mostly used in engineering, materials science, and computer science, while SPSS is mainly used in medicine, social science, and engineering. Analysis of the document type showed that SPSS and Minitab are mostly stated in abstracts or keywords of research articles, conference papers, review papers, and books indexed in Scopus."
Bruce2020,Bobby R. Bruce and Tianyi Zhang and Jaspreet Arora and Guoqing Harry Xu and Miryung Kim,JShrink: In-depth investigation into debloating modern Java applications,,,,2020,10.1145/3368089.3409738,,"Modern software is bloated. Demand for new functionality has led developers to include more and more features, many of which become unneeded or unused as software evolves. This phenomenon, known as software bloat, results in software consuming more resources than it otherwise needs to. How to effectively and automatically debloat software is a long-standing problem in software engineering. Various debloating techniques have been proposed since the late 1990s. However, many of these techniques are built upon pure static analysis and have yet to be extended and evaluated in the context of modern Java applications where dynamic language features are prevalent. To this end, we develop an end-to-end bytecode debloating framework called JShrink. It augments traditional static reachability analysis with dynamic profiling and type dependency analysis and renovates existing bytecode transformations to account for new language features in modern Java. We highlight several nuanced technical challenges that must be handled properly and examine behavior preservation of debloated software via regression testing. We find that (1) JShrink is able to debloat our real-world Java benchmark suite by up to 47% (14% on average); (2) accounting for dynamic language features is indeed crucial to ensure behavior preservation - -reducing 98% of test failures incurred by a purely static equivalent, Jax, and 84% for ProGuard; and (3) compared with purely dynamic approaches, integrating static analysis with dynamic profiling makes the debloated software more robust to unseen test executions - -in 22 out of 26 projects, the debloated software ran successfully under new tests."
Schwartz-Chassidim2020,Hadas Schwartz-Chassidim and Oshrat Ayalon and Tamir Mendel and Ron Hirschprung and Eran Toch,"Selectivity in posting on social networks: the role of privacy concerns, social capital, and technical literacy",Heliyon,6,2,2020,10.1016/j.heliyon.2020.e03298,24058440,"People's posting behaviors in social networks was perceived as ambiguous, with concerns misaligned with people's public postings. To address this gap, we suggest a model that offers new insights into the relationship between perceptions and actual behaviors. We define a quantitative marker for agility, the frequency in which people update their audience selection when posting information in online social networks, and evaluate the factors that contribute to the variability of agility between different users. We analyzed the posting behavior of Facebook 181 participants, as well as their answers to open and close questions. We find that frequent changes in privacy settings are correlated with high social privacy and with institutional privacy concerns, whereas social concerns were found to be more prominent. Agility was negatively correlated with low public sharing. Our findings show that users use privacy settings to effectively mitigate privacy concerns and desires for creating and strengthening social connections. We discuss how agility can be used to design and to evaluate new user interfaces for managing privacy in social settings."
Mohseni-Bonab2020,Seyed Masoud Mohseni-Bonab and Ali Hajebrahimi and Innocent Kamwa and Ali Moeini,Transmission and distribution co-simulation: A review and propositions,"IET Generation, Transmission and Distribution",14,21,2020,10.1049/iet-gtd.2020.0244,17518687,"With the growing trend of emerging new technologies in distribution networks, such as wind turbines, solar panels, electric vehicles, and distributed generations, the passive distribution systems may become 'active' which requires more study in the area of integrated transmission and distribution systems (ITDSs) and corresponding bilateral interactions. To solve this problem, most of the studies connect distinct simulators to create a novel co-simulation framework for ITDS. In this study, the authors present a literature survey of existing ITDS co-simulation frameworks along co-optimisation in ITDS. These frameworks are categorised on multiple characteristics, such as simulation tools, synchronisation methods, and research topics. Furthermore, they propose a software platform that is focused on the integrated generation, transmission, distribution, and customer systems (IGTDCSs). The proposed framework also comprises several technological dimensions such as stochastic optimisation, high-performance computing, and high-level design software architecture for planning integrated and flexible power networks and optimising their technological trajectories and operational functioning considering uncertainties. By developing a prototype informed with software engineering and complex system design approaches, they will demonstrate the relevance of a unified vision of IGTDCS simulation in a minute-by-minute horizon, a vision that may later benefit electromagnetic transient simulation or stability co-simulation tools, in horizons from the microsecond to the second."
Best2020,Natalie Best and Jordan Ott and Erik J. Linstead,Exploring the efficacy of transfer learning in mining image-based software artifacts,Journal of Big Data,7,1,2020,10.1186/s40537-020-00335-4,21961115,"Background: Transfer learning allows us to train deep architectures requiring a large number of learned parameters, even if the amount of available data is limited, by leveraging existing models previously trained for another task. In previous attempts to classify image-based software artifacts in the absence of big data, it was noted that standard off-the-shelf deep architectures such as VGG could not be utilized due to their large parameter space and therefore had to be replaced by customized architectures with fewer layers. This proves to be challenging to empirical software engineers who would like to make use of existing architectures without the need for customization. Findings: Here we explore the applicability of transfer learning utilizing models pre-trained on non-software engineering data applied to the problem of classifying software unified modeling language (UML) diagrams. Our experimental results show training reacts positively to transfer learning as related to sample size, even though the pre-trained model was not exposed to training instances from the software domain. We contrast the transferred network with other networks to show its advantage on different sized training sets, which indicates that transfer learning is equally effective to custom deep architectures in respect to classification accuracy when large amounts of training data is not available. Conclusion: Our findings suggest that transfer learning, even when based on models that do not contain software engineering artifacts, can provide a pathway for using off-the-shelf deep architectures without customization. This provides an alternative to practitioners who want to apply deep learning to image-based classification but do not have the expertise or comfort to define their own network architectures."
Mkhinini2020,Meriem Mejhed Mkhinini and Ouassila Labbani-Narsis and Christophe Nicolle,Combining UML and ontology: An exploratory survey,Computer Science Review,35,,2020,10.1016/j.cosrev.2019.100223,15740137,"UML models and ontologies are two knowledge representations with different strengths and weaknesses. Until recently, they were considered unrelated research domains. However, studies investigating their underlying paradigms and the approaches combining these two are increasingly emerging. Nevertheless, the state of the art research covering the relationship between the two is still under exploration. In this paper, we aim to provide a comprehensive overview of both domains by conducting a literature review of the relevant research work. In this survey, the relationship between UML and ontology is investigated from both the theoretical and practical perspectives. We present a detailed classification of the existing work based on the considered issues and their practical use cases. Finally, we provide an evaluation of the existing work according to the criteria we identified."
Lavin2022,Alexander Lavin and Ciarán M. Gilligan-Lee and Alessya Visnjic and Siddha Ganju and Dava Newman and Sujoy Ganguly and Danny Lange and Atílím Güneş Baydin and Amit Sharma and Adam Gibson and Stephan Zheng and Eric P. Xing and Chris Mattmann and James Parr and Yarin Gal,Technology readiness levels for machine learning systems,Nature Communications,13,1,2022,10.1038/s41467-022-33128-9,20411723,"The development and deployment of machine learning systems can be executed easily with modern tools, but the process is typically rushed and means-to-an-end. Lack of diligence can lead to technical debt, scope creep and misaligned objectives, model misuse and failures, and expensive consequences. Engineering systems, on the other hand, follow well-defined processes and testing standards to streamline development for high-quality, reliable results. The extreme is spacecraft systems, with mission critical measures and robustness throughout the process. Drawing on experience in both spacecraft engineering and machine learning (research through product across domain areas), we’ve developed a proven systems engineering approach for machine learning and artificial intelligence: the Machine Learning Technology Readiness Levels framework defines a principled process to ensure robust, reliable, and responsible systems while being streamlined for machine learning workflows, including key distinctions from traditional software engineering, and a lingua franca for people across teams and organizations to work collaboratively on machine learning and artificial intelligence technologies. Here we describe the framework and elucidate with use-cases from physics research to computer vision apps to medical diagnostics."
Azarafza2020,Mohammad Azarafza and Haluk Akgün and Mohammad Reza Feizi-Derakhshi and Mehdi Azarafza and Jafar Rahnamarad and Reza Derakhshani,Discontinuous rock slope stability analysis under blocky structural sliding by fuzzy key-block analysis method,Heliyon,6,5,2020,10.1016/j.heliyon.2020.e03907,24058440,"This study presents a fuzzy logical decision-making algorithm based on block theory to effectively determine discontinuous rock slope reliability under various wedge and planar slip scenarios. The algorithm was developed to provide rapid response operations without the need for extensive quantitative stability evaluations based on the rock slope sustainability ratio. The fuzzy key-block analysis method utilises a weighted rational decision (multi-criteria decision-making) function to prepare the ‘degree of reliability (degree of stability-instability contingency)’ for slopes as implemented through the Mathematica software package. The central and analyst core of the proposed algorithm is provided as based on discontinuity network geometrical uncertainties and hierarchical decision-making. This algorithm uses block theory principles to proceed to rock block classification, movable blocks and key-block identifications under ambiguous terms which investigates the sustainability ratio with accurate, quick and appropriate decisions especially for novice engineers in the context of discontinuous rock slope stability analysis. The method with very high precision and speed has particular matches with the existing procedures and has the potential to be utilised as a continuous decision-making system for discrete parameters and to minimise the need to apply common practises. In order to justify the algorithm, a number of discontinuous rock mass slopes were considered as examples. In addition, the SWedge, RocPlane softwares and expert assignments (25-member specialist team) were utilised for verification of the applied algorithm which led to a conclusion that the algorithm was successful in providing rational decision-making."
ZakaUllah2020,Malik Zaka Ullah and T. S. Jang,An efficient numerical scheme for analyzing bioconvection in von-Kármán flow of third-grade nanofluid with motile microorganisms,Alexandria Engineering Journal,59,4,2020,10.1016/j.aej.2020.05.017,11100168,"This article addresses the numerical approximation of third-grade nanoliquid flow over a stretchable rotating disk in the existence of nano-sized particles and gyrotactic motile microorganisms. The behavior of bioconvection is contemplated. The comportment of mixed convection, activation energy and Joule heating effects are also considered. Further the nano-characteristics for Brownian dispersion of fluid particles and thermophoresis aspect are also taken into account. Utilizing appropriate scaling group of transformation variables, the partial differential equations are converted into governing ordinary differential system. Then coupled ordinary differential equations are tackled numerically by using shooting technique via built-in function bvp4c solver with the help of computational software MATLAB. Graphical impact of miscellaneous arising sundry parameters on distribution, volumetric concentration of nano-particles, velocity field and rescaled density of motile micro-organisms are scrutinized and deliberated. The present model plays a crucial role in sectors of industry and engineering. This model is suitable for heating and cooling including engine cooling, microelectronics, biotechnology, bio-microsystems, cancer therapy, fertilizers and biofuel etc. Furthermore, the temperature distribution is enhanced for larger values of thermal radiation parameter."
Premkumar2021,M. Premkumar and R. Sowmya and Pradeep Jangir and Kottakkaran Sooppy Nisar and Mujahed Aldhaifallah,A New Metaheuristic Optimization Algorithms for Brushless Direct Current Wheel Motor Design Problem,"Computers, Materials and Continua",67,2,2021,10.32604/cmc.2021.015565,15462226,"The Equilibrium Optimizer (EO), Grey Wolf Optimizer (GWO), and Whale Optimizer (WO) algorithms are being recently developed for engineering optimization problems. In this paper, the EO, GWO, and WO algorithms are applied individually for a brushless direct current (BLDC) design optimization problem. The EO algorithm is inspired by the models utilized to find the system’s dynamic state and equilibrium state. The GWO and WO algorithms are inspired by the hunting behavior of the wolf and the whale, respectively. The primary purpose of any optimization technique is to find the optimal configuration by maximizing motor efficiency and/or minimizing the total mass. Therefore, two objective functions are being used to achieve these objectives. The first refers to a design with high power output and efficiency. The second is a constraint imposed by the reality that the motor is built into the wheel of the vehicle and, therefore, a lightweight is needed. The EO, GWO, and WOA algorithms are then utilized to optimize the BLDC motor’s design variables to minimize the motor’s total mass or maximize the motor efficiency by simultaneously satisfying the six inequality constraints. The simulation is carried out using MATLAB simulation software, and the simulation results prove the dominance of the proposed algorithms. This paper also suggests an efficient method from the proposed three methods for the BLDC motor design optimization problem."
Bornholt2021,James Bornholt and Rajeev Joshi and Vytautas Astrauskas and Brendan Cully and Bernhard Kragl and Seth Markle and Kyle Sauri and Drew Schleit and Grant Slatton and Serdar Tasiran and Jacob Van Geffen and Andrew Warfield,Using Lightweight Formal Methods to Validate a Key-Value Storage Node in Amazon S3,,,,2021,10.1145/3477132.3483540,,"This paper reports our experience applying lightweight formal methods to validate the correctness of ShardStore, a new key-value storage node implementation for the Amazon S3 cloud object storage service. By ""lightweight formal methods""we mean a pragmatic approach to verifying the correctness of a production storage node that is under ongoing feature development by a full-time engineering team. We do not aim to achieve full formal verification, but instead emphasize automation, usability, and the ability to continually ensure correctness as both software and its specification evolve over time. Our approach decomposes correctness into independent properties, each checked by the most appropriate tool, and develops executable reference models as specifications to be checked against the implementation. Our work has prevented 16 issues from reaching production, including subtle crash consistency and concurrency problems, and has been extended by non-formal-methods experts to check new features and properties as ShardStore has evolved."
Tsige2020,Damtew Tsige and Sanjaya Senadheera and Ayalew Talema,Stability analysis of plant-root-reinforced shallow slopes along mountainous road corridors based on numerical modeling,Geosciences (Switzerland),10,1,2020,10.3390/geosciences10010019,20763263,"Engineering methods such as soil nails, geosynthetic reinforcement, retaining structures, gabions, and shotcrete are implemented to stabilize road cut slopes along mountainous areas. However, these structures are not environmentally friendly and, particularly in Ethiopia, it is impossible to address all road problems due to financial limitations. Nowadays, soil reinforcement with plant roots is recognized as an environmentally sustainable alternative to improve shallow slope failure along mountainous transportation corridors. The aims of this study was, therefore, to conduct slope stability analysis along a road corridor by incorporating the effect of plant roots. Five plant species were selected for the analysis based on their mechanical characteristics. Namely, Eucalyptus globules (tree), Psidium guajava (shrub), Salix subserrata (shrub), Chrysopogon zizanioides, and Pennisetum macrourum (grasses). The roots’ tensile strength and soil parameters were determined through tensile strength testing and triaxial compression tests, respectively. The factor of safety of the slope was calculated by the PLAXIS-2D software. The study showed that when the slope was reinforced with plant roots, the factor of safety (FOS) improved from 22–34%. The decreasing effect of vegetation on slope stability was observed when soil moisture increased. The sensitivity analysis also indicated that: (1) as the spacing between plants decreased, the effect of vegetation on the slope increased. (2) Slope angle modification with a combination of plant roots had a significant impact on slope stabilization. Of the five-selected plant species, Salix subserrata was the promising plant species for slope stabilization as it exhibited better root mechanical properties among selected plant species."
Mamun2021,Abdulla Al Mamun and Samsun Nahar Ananna and Tianqing An and Nur Hasan Mahmud Shahen and Foyjonnesa,Periodic and solitary wave solutions to a family of new 3D fractional WBBM equations using the two-variable method,Partial Differential Equations in Applied Mathematics,3,,2021,10.1016/j.padiff.2021.100033,26668181,"For the newly implemented 3D fractional Wazwaz–Benjamin–Bona–Mahony​ (WBBM) equation family, the present study explores exact singular, solitary, and periodic singular wave solutions via the (G′∕G,1∕G)-expansion process. In the sense of conformable derivatives, the equations considered are transformed into ordinary differential equations. In spite of many trigonometric, complex hyperbolic, and rational functions, some fresh exact singular, solitary, and periodic wave solutions to the deliberated equations in fractional systems are attained by the implementation of the (G′∕G,1∕G)-expansion technique through the computational software Mathematica. The unique solutions derived by the process defined are articulated with the arrangement of the functions tanh, sech; tan, sec; coth, csch, and cot, csc. With three-dimensional graphics, some of the latest solutions created have been envisaged, by selecting appropriate arbitrary constraints to illustrate their physical representation. The outcomes which are obtained to show the power of the computational technique for the WBBM equations can be applied to other nonlinear water model equations in ocean and coastal engineering. All the obtained solutions have been verified by the computational software Mathematica."
Fasano2021,Andrew Fasano and Tiemoko Ballo and Marius Muench and Tim Leek and Alexander Bulekov and Brendan Dolan-Gavitt and Manuel Egele and Aurélien Francillon and Long Lu and Nick Gregory and Davide Balzarotti and William Robertson,SoK: Enabling Security Analyses of Embedded Systems via Rehosting,,,,2021,10.1145/3433210.3453093,,"Closely monitoring the behavior of a software system during its execution enables developers and analysts to observe, and ultimately understand, how it works. This kind of dynamic analysis can be instrumental to reverse engineering, vulnerability discovery, exploit development, and debugging. While these analyses are typically well-supported for homogeneous desktop platforms (e.g., x86 desktop PCs), they can rarely be applied in the heterogeneous world of embedded systems. One approach to enable dynamic analyses of embedded systems is to move software stacks from physical systems into virtual environments that sufficiently model hardware behavior. This process which we call ""rehosting""poses a significant research challenge with major implications for security analyses. Although rehosting has traditionally been an unscientific and ad-hoc endeavor undertaken by domain experts with varying time and resources at their disposal, researchers are beginning to address rehosting challenges systematically and in earnest. In this paper, we establish that emulation is insufficient to conduct large-scale dynamic analysis of real-world hardware systems and present rehosting as a firmware-centric alternative. Furthermore, we taxonomize preliminary rehosting efforts, identify the fundamental components of the rehosting process, and propose directions for future research."
Sofos2022,Filippos Sofos and Christos Stavrogiannis and Kalliopi K. Exarchou‐kouveli and Daniel Akabua and George Charilas and Theodoros E. Karakasidis,Current Trends in Fluid Research in the Era of Artificial Intelligence: A Review,Fluids,7,3,2022,10.3390/fluids7030116,23115521,"Computational methods in fluid research have been progressing during the past few years, driven by the incorporation of massive amounts of data, either in textual or graphical form, generated from multi‐scale simulations, laboratory experiments, and real data from the field. Artificial Intelligence (AI) and its adjacent field, Machine Learning (ML), are about to reach standardization in most fields of computational science and engineering, as they provide multiple ways for extracting information from data that turn into knowledge, with the aid of portable software implementations that are easy to adopt. There is ample information on the historical and mathematical background of all aspects of AI/ML in the literature. Thus, this review article focuses mainly on their impact on fluid research at present, highlighting advances and opportunities, recognizing techniques and methods having been proposed, tabulating, and testing some of the most popular algorithms that have shown significant accuracy and performance on fluid applications. We also investigate algorithmic accuracy on several fluid datasets that correspond to simulation results for the transport properties of fluids and suggest that non‐linear, decision tree‐based methods have shown remarkable performance on reproducing fluid properties."
Pavn2020,Rubén Muñoz Pavón and Antonio A.Arcos Alvarez and Marcos G. Alberti,Possibilities of bim-fm for the management of covid in public buildings,Sustainability (Switzerland),12,23,2020,10.3390/su12239974,20711050,"The COVID-19 pandemic, with more than 49.7 million reported cases and over 1.2 million deaths globally confirmed deaths at the time of writing, demands global action to counteract this virus. It is widely accepted that COVID-19 is a long-term pandemic that will require a constant and innovative range of mitigation approaches to protect public health. This paper provides infrastructure facility management (FM) systems based on Building Information Modeling (BIM) to reduce the likelihood of COVID-19 infections indoors. Although there are several factors for dealing with COVID-19, the sole focus of this project is to reduce crowding and facilitate social distancing between occupants. The significance of this research relies on the use of mathematical methods, BIM, programming as well as FM tools and databases to achieve safer management of large and populated public buildings during the COVID-19 pandemic. The infrastructure management example refers to the Civil Engineering School at the Universidad Politécnica de Madrid. It is based on mathematical applications to find the paths of people paths inside the infrastructure and is synchronized with in-house developed software and the Internet domain as source and input data."
Butenko2020,Konstantin Butenko and Christian Bahls and Max Schröder and Rüdiger Köhling and Ursula Van Rienen,OSS-DBS: Open-source simulation platform for deep brain stimulation with a comprehensive automated modeling,PLoS Computational Biology,16,7,2020,10.1371/journal.pcbi.1008023,15537358,"In this study, we propose a new open-source simulation platform that comprises computeraided design and computer-aided engineering tools for highly automated evaluation of electric field distribution and neural activation during Deep Brain Stimulation (DBS). It will be shown how a Volume Conductor Model (VCM) is constructed and examined using Pythoncontrolled algorithms for generation, discretization and adaptive mesh refinement of the computational domain, as well as for incorporation of heterogeneous and anisotropic properties of the tissue and allocation of neuron models. The utilization of the platform is facilitated by a collection of predefined input setups and quick visualization routines. The accuracy of a VCM, created and optimized by the platform, was estimated by comparison with a commercial software. The results demonstrate no significant deviation between the models in the electric potential distribution. A qualitative estimation of different physics for the VCM shows an agreement with previous computational studies. The proposed computational platform is suitable for an accurate estimation of electric fields during DBS in scientific modeling studies. In future, we intend to acquire SDA and EMA approval. Successful incorporation of open-source software, controlled by in-house developed algorithms, provides a highly automated solution. The platform allows for optimization and uncertainty quantification (UQ) studies, while employment of the open-source software facilitates accessibility and reproducibility of simulations. Copyright:"
Zaidel2021,Yuval Zaidel and Albert Shalumov and Alex Volinski and Lazar Supic and Elishai Ezra Tsur,Neuromorphic NEF-Based Inverse Kinematics and PID Control,Frontiers in Neurorobotics,15,,2021,10.3389/fnbot.2021.631159,16625218,"Neuromorphic implementation of robotic control has been shown to outperform conventional control paradigms in terms of robustness to perturbations and adaptation to varying conditions. Two main ingredients of robotics are inverse kinematic and Proportional–Integral–Derivative (PID) control. Inverse kinematics is used to compute an appropriate state in a robot's configuration space, given a target position in task space. PID control applies responsive correction signals to a robot's actuators, allowing it to reach its target accurately. The Neural Engineering Framework (NEF) offers a theoretical framework for a neuromorphic encoding of mathematical constructs with spiking neurons for the implementation of functional large-scale neural networks. In this work, we developed NEF-based neuromorphic algorithms for inverse kinematics and PID control, which we used to manipulate 6 degrees of freedom robotic arm. We used online learning for inverse kinematics and signal integration and differentiation for PID, offering high performing and energy-efficient neuromorphic control. Algorithms were evaluated in simulation as well as on Intel's Loihi neuromorphic hardware."
Chakraborty2022,Saikat Chakraborty and Toufique Ahmed and Yangruibo Ding and Premkumar T. Devanbu and Baishakhi Ray,"NatGen: generative pre-training by ""naturalizing"" source code",,,,2022,10.1145/3540250.3549162,,"Pre-trained Generative Language models (e.g., PLBART, CodeT5, SPT-Code) for source code yielded strong results on several tasks in the past few years, including code generation and translation. These models have adopted varying pre-training objectives to learn statistics of code construction from very large-scale corpora in a self-supervised fashion; the success of pre-trained models largely hinges on these pre-training objectives. This paper proposes a new pre-training objective, ""Naturalizing""of source code, exploiting code's bimodal, dual-channel (formal & natural channels) nature. Unlike natural language, code's bimodal, dual-channel nature allows us to generate semantically equivalent code at scale. We introduce six classes of semantic preserving transformations to introduce unnatural forms of code, and then force our model to produce more natural original programs written by developers. Learning to generate equivalent, but more natural code, at scale, over large corpora of open-source code, without explicit manual supervision, helps the model learn to both ingest & generate code. We fine-tune our model in three generative Software Engineering tasks: code generation, code translation, and code refinement with limited human-curated labeled data and achieve state-of-the-art performance rivaling CodeT5. We show that our pre-trained model is especially competitive at zero-shot and few-shot learning, and better at learning code properties (e.g., syntax, data flow)"
Kavadias2021,Kosmas A. Kavadias and Panagiotis Triantafyllou,Hybrid renewable energy systems’ optimisation. A review and extended comparison of the most-used software tools,Energies,14,24,2021,10.3390/en14248268,19961073,"To help stakeholders plan, research, and develop Hybrid Renewable Energy Systems (HRES), the elaboration of numerous modelling techniques and software simulation tools has been reported. The thorough analysis of these undoubtedly complex systems is strongly correlated with the efficient utilisation of the potential of renewable energy and the meticulous development of pertinent designs. In this context, various optimisation constraints/targets have also been utilised. This specific work initially carries out a thorough review of the modelling techniques and simulation software developed in an attempt to define a commonly accepted categorisation methodology for the various existing HRES simulation methods. Moreover, the widely utilised optimisation targets are analysed in detail. Finally, it identifies the sensitivity of two commercial software tools (HOMER Pro and iHOGA) by examining nine case studies based on different wind and solar potential combinations. The results obtained by the two commercial tools are compared with the ESA Microgrid Simulator, a software developed by the Soft Energy Applications and Environmental Protection Laboratory of the Mechanical Engineering Department of the University of West Attica. The evaluation of the results, based on the diversification of the renewable energy potential used as input, has led to an in-depth assessment of the deviances detected in the software tools selected."
Rezaei2021,Farnoush Sadat Rezaei and Ayeh Khorshidian and Farzaneh Mahmoudi Beram and Atefeh Derakhshani and Javad Esmaeili and Aboulfazl Barati,3D printed chitosan/polycaprolactone scaffold for lung tissue engineering: hope to be useful for COVID-19 studies,RSC Advances,11,32,2021,10.1039/d1ra03410c,20462069,"To prevent or reduce mortality from lung diseases, new biological materials and scaffolds are needed to conduct more accurate research and support lung tissue regeneration. On the other hand, the outbreak of the COVID-19 virus and its targeting of the human lung has caused many deaths worldwide. The main aim of this study was to provide a biologically and mechanically suitable 3D printed scaffold using chitosan/polycaprolactone bioink for lung tissue engineering. Design-Expert software was employed for studying various compositions for 3D printing. The selected scaffolds underwent physiochemical, biological and mechanical studies to evaluate if they are capable of MRC-5 cell line growth, proliferation, and migration. Based on the results, the average diameter of the chitosan/polycaprolactone strands was measured at 360 μm. Chitosan concentration controlled the printability, while changes in polycaprolactone content did not affect printability. The scaffolds showed excellent potential in swelling, degradation, and mechanical behavior, although they can be modified by adjusting the polycaprolactone content. The scaffolds also revealed notable cell adhesion, nontoxicity, low apoptosis, high proliferation, and cell biocompatibilityin vitro. To sum up, scaffold 3 (chitosan/polycaprolactone ratio: 4 : 1) revealed better activity for MRC-5 cell culture. Thereby, this scaffold can be a good candidate for lung tissue engineering and may be applicable for more studies on the COVID-19 virus."
Ross2023,Steven I. Ross and Fernando Martinez and Stephanie Houde and Michael Muller and Justin D. Weisz,The Programmer's Assistant: Conversational Interaction with a Large Language Model for Software Development,,,,2023,10.1145/3581641.3584037,,"Large language models (LLMs) have recently been applied in software engineering to perform tasks such as translating code between programming languages, generating code from natural language, and autocompleting code as it is being written. When used within development tools, these systems typically treat each model invocation independently from all previous invocations, and only a specific limited functionality is exposed within the user interface. This approach to user interaction misses an opportunity for users to more deeply engage with the model by having the context of their previous interactions, as well as the context of their code, inform the model's responses. We developed a prototype system - the Programmer's Assistant - in order to explore the utility of conversational interactions grounded in code, as well as software engineers' receptiveness to the idea of conversing with, rather than invoking, a code-fluent LLM. Through an evaluation with 42 participants with varied levels of programming experience, we found that our system was capable of conducting extended, multi-turn discussions, and that it enabled additional knowledge and capabilities beyond code generation to emerge from the LLM. Despite skeptical initial expectations for conversational programming assistance, participants were impressed by the breadth of the assistant's capabilities, the quality of its responses, and its potential for improving their productivity. Our work demonstrates the unique potential of conversational interactions with LLMs for co-creative processes like software development."
Sobania2022,Dominik Sobania and Martin Briesch and Franz Rothlauf,Choose Your Programming Copilot: A Comparison of the Program Synthesis Performance of GitHub Copilot and Genetic Programming,,,,2022,10.1145/3512290.3528700,,"GitHub Copilot, an extension for the Visual Studio Code development environment powered by the large-scale language model Codex, makes automatic program synthesis available for software developers. This model has been extensively studied in the field of deep learning, however, a comparison to genetic programming, which is also known for its performance in automatic program synthesis, has not yet been carried out. In this paper, we evaluate GitHub Copilot on standard program synthesis benchmark problems and compare the achieved results with those from the genetic programming literature. In addition, we discuss the performance of both approaches. We find that the performance of the two approaches on the benchmark problems is quite similar, however, in comparison to GitHub Copilot, the program synthesis approaches based on genetic programming are not yet mature enough to support programmers in practical software development. Genetic programming usually needs a huge amount of expensive hand-labeled training cases and takes too much time to generate solutions. Furthermore, source code generated by genetic programming approaches is often bloated and difficult to understand. For future work on program synthesis with genetic programming, we suggest researchers to focus on improving the execution time, readability, and usability."
DeNicola2020,Rocco De Nicola and Stefan Jähnichen and Martin Wirsing,Rigorous engineering of collective adaptive systems: special section,International Journal on Software Tools for Technology Transfer,22,4,2020,10.1007/s10009-020-00565-0,14332787,"An adaptive system is able to adapt at runtime to dynamically changing environments and to new requirements. Adaptive systems can be single adaptive entities or collective ones that consist of several collaborating entities. Rigorous engineering requires appropriate methods and tools that help guaranteeing that an adaptive system lives up to its intended purpose. This paper introduces the special section on “Rigorous Engineering of Collective Adaptive Systems.” It presents the seven contributions of the section and gives a short overview of the field of rigorously engineering collective adaptive systems by structuring it according to three topics: systematic development, methods and theories for modelling and analysis, and techniques for programming and operating collective adaptive systems."
Choras2020,Michal Choras and Tomasz Springer and Rafal Kozik and Lidia Lopez and Silverio Martinez-Fernandez and Prabhat Ram and Pilar Rodriguez and Xavier Franch,Measuring and improving agile processes in a small-size software development company,IEEE Access,8,,2020,10.1109/ACCESS.2020.2990117,21693536,"Context: Agile software development has become commonplace in software development companies due to the numerous benefits it provides. However, conducting Agile projects is demanding in Small and Medium Enterprises (SMEs), because projects start and end quickly, but still have to fulfil customers' quality requirements. Objective: This paper aims at reporting a practical experience on the use of metrics related to the software development process as a means supporting SMEs in the development of software following an Agile methodology. Method: We followed Action-Research principles in a Polish small-size software development company. We developed and executed a study protocol suited to the needs of the company, using a pilot case. Results: A catalogue of Agile development process metrics practically validated in the context of a small-size software development company, adopted by the company in their Agile projects. Conclusions: Practitioners may adopt these metrics in their Agile projects, especially if working in an SME, and customise them to their own needs and tools. Academics may use the findings as a baseline for new research work, including new empirical studies."
Wang2020,Ruyun Wang and Hanwen Zhang and Guoliang Lu and Lei Lyu and Chen Lyu,Fret: Functional Reinforced Transformer with BERT for Code Summarization,IEEE Access,8,,2020,10.1109/ACCESS.2020.3011744,21693536,"Code summarization has long been viewed as a challenge in software engineering because of the difficulties of understanding source code and generating natural language. Some mainstream methods combine abstract syntax trees with language models to capture the structural information of the source code and generate relatively satisfactory comments. However, these methods are still deficient in code understanding and limited by the long dependency problem. In this paper, we propose a novel model called Fret, which stands for Functional REinforced Transformer with BERT. The model provides a new way to generate code comments by learning code functionalities and deepening code understanding while alleviating the problem of long dependency. For this purpose, a novel reinforcer is proposed for learning the functional contents of code so that more accurate summaries to describe the code functionalities can be generated. In addition, a more efficient algorithm is newly designed to capture the source code structure. The experimental results show that the effectiveness of our model is remarkable. Fret significantly outperforms all the state-of-the-art methods we examine. It pushes the BLEU-4 score to 24.32 for Java code summarization (14.23% absolute improvement) and the ROUGE-L score to 40.12 for Python. An ablation test is also conducted to further explore the impact of each component of our method."
Khan2021,Shan Ali Khan and Hassan Waqas and Syed Muhammad Raza Shah Naqvi and Metib Alghamdi and Qasem Al-Mdallal,Cattaneo-Christov double diffusions theories with bio-convection in nanofluid flow to enhance the efficiency of nanoparticles diffusion,Case Studies in Thermal Engineering,26,,2021,10.1016/j.csite.2021.101017,2214157X,"Purpose The current mathematical model is developed to scrutinize the consequence of bioconvective cross diffusion flow of magnetized viscous nanofluid past multiple geometries (cone, wedge and plate) with convective boundary conditions. Together the nanoparticles and motile microorganism are incorporated into the dimensionless nonlinear differential expressions. The behavior of Cattaneo-Christov heat and mass flux is accounted for energy and concentration expressions. The influence of activation energy and thermal radiation are considered. The mathematical model is reduced into an ordinary one by using adequate similarity transformation. Buongiorno model is utilized for nanofluid (nanoliquids) analysis. Methodology/approach The renovated dimensionless self-similarity systems are then solved numerically by utilizing shooting technique built-in function bvp4c solver with the help of commercial software Matlab. The obtained results are verified and an outstanding agreement has been found. Engineering quantities of interest are observed physically. Findings The features of various emerging parameters against velocity distribution, thermal distribution, and solutal field of species, microorganism concentration as well as skin friction coefficient, gradient of temperature, local Sherwood number and density number of motile microorganisms are interpreted and deliberated in tabulated and graphical form. Results The results indicate that velocity field is raises via larger Grashof number. The resultant velocity is decline via larger magnetic parameter. Larger estimation of thermal Biot number increases the heat transfer. Larger thermal relaxation parameter reduces the temperature of fluid. The concentration of nanoparticles is declines via concentration relaxation parameter. The microorganism's field is declines by varying the variations of Peclet number."
Suwartha2020,Nyoman Suwartha and Destrianti Syamzida and Cindy Rianti Priadi and Setyo Sarwanto Moersidik and Firdaus Ali,Effect of size variation on microbubble mass transfer coefficient in flotation and aeration processes,Heliyon,6,4,2020,10.1016/j.heliyon.2020.e03748,24058440,"Microbubble technology dramatically raises the efficiency of the flotation and aeration processes of water treatment plants (WTPs), which see extensive use in developed countries. A local institution, Indonesia Water Institute, has tried to investigate microbubble technology intended for lab-scale WTP. However, the current reactor system does not yet meet the microbubble criteria, especially as it has had few investigations of its abilities in flotation and aeration. This study aims to analyze the effect of size variations that affect the rising velocity and mass transfer coefficient (kLa) of aeration contact time. Three local spargers were used to produce microbubbles. Bubble diameters were measured optically and analyzed using ImageJ software. The dissolved oxygen (DO) concentration was measured every minute using an automated sensor so that the kLa could be determined. Of the three spargers, the smallest bubble size was produced by the vortex type with an average bubble diameter of 89 μm and the slowest rising velocity of 17.67 m/h. It also yielded the highest kLa of 0.297/min, which gave an aeration contact time of 3.64 minutes. The experimental uses of three local spargers revealed that the smaller the microbubble diameter, the higher the mass transfer coefficient in flotation and aeration processes. This research can be the basis for developing microbubble technology for WTP in Indonesia."
Wang2020,Ziyi Wang and Debin Ma and Ru Pang and Fan Xie and Jingxiang Zhang and Dongqi Sun,Research progress and development trend of social media big data (SMBD): Knowledge mapping analysis based on CiteSpace,ISPRS International Journal of Geo-Information,9,11,2020,10.3390/ijgi9110632,22209964,"Social Media Big Data (SMBD) is widely used to serve the economic and social development of human beings. However, as a young research and practice field, the understanding of SMBD in academia is not enough and needs to be supplemented. This paper took Web of Science (WoS) core collection as the data source, and used traditional statistical methods and CiteSpace software to carry out the scientometrics analysis of SMBD, which showed the research status, hotspots and trends in this field. The results showed that: (1) More and more attention has been paid to SMBD research in academia, and the number of journals published has been increased in recent years, mainly in subjects such as Computer Science Engineering and Telecommunications. The results were published primarily in IEEE Access Sustainability and Future Generation Computer Systems the International Journal of eScience and so on; (2) In terms of contributions, China, the United States, the United Kingdom and other countries (regions) have published the most papers in SMBD, high-yield institutions also mainly from these countries (regions). There were already some excellent teams in the field, such as the Wanggen Wan team at Shanghai University and Haoran Xie team from City University of Hong Kong; (3) we studied the hotspots of SMBD in recent years, and realized the summary of the frontier of SMBD based on the keywords and co-citation literature, including the deep excavation and construction of social media technology, the reflection and concerns about the rapid development of social media, and the role of SMBD in solving human social development problems. These studies could provide values and references for SMBD researchers to understand the research status, hotspots and trends in this field."
Kastridis2020,Aristeidis Kastridis and Dimitrios Stathis,Evaluation of hydrological and hydraulic models applied in typical mediterranean ungauged watersheds using post-flash-flood measurements,Hydrology,7,1,2020,10.3390/hydrology7010012,23065338,"In this paper, three different flash floods episodes were analyzed, which occurred in October 2006, February 2010, and June 2018 in the Chalkidiki peninsula (North Greece). The Soil Conservation Service (SCS) model and a revised assessment of the CN parameter were applied to estimate the flood hydrographs, and Hydrologic Engineering Centers-River Analysis System (HECRAS) software was used for the flood simulations. Initially, hydrological and hydraulic models were calibrated at Vatonias watershed (240.90 km2, North Greece), where three rain gauges and one water level station are located. Vatonias is located very close to the Stavros ungauged watersheds and presents similar geomorphology and land use conditions. The effectiveness and accuracy of the methodology were validated using post-flash-flood measurements. The root mean square error goodness of fit was used to compare the observed and simulated flood depths. Critical success index was calculated for the assessment of the accuracy of observed and modeled flooded areas. The results showed that the dense forest vegetation was not capable of preventing the flash flood generation or reducing the peak discharge, especially in small watersheds characterized by short concentration times. The main cause of flash flood generation was the human interference that influenced the hydraulic characteristics of streams and floodplains. The revised assessment of the CN parameter enhanced the estimation and spatial distribution of CN over the entire watershed. The results revealed that the proposed methodology could be a very useful tool to researchers and policy makers for flood risk assessment of higher accuracy and effectiveness in ungauged Mediterranean watersheds."
Wang2021,Luqi Wang and Jiahao Wu and Wengang Zhang and Lin Wang and Wei Cui,Efficient Seismic Stability Analysis of Embankment Slopes Subjected to Water Level Changes Using Gradient Boosting Algorithms,Frontiers in Earth Science,9,,2021,10.3389/feart.2021.807317,22966463,"Embankments are widespread throughout the world and their safety under seismic conditions is a primary concern in the geotechnical engineering community since the failure events may lead to disastrous consequences. This study proposes an efficient seismic slope stability analysis approach by introducing advanced gradient boosting algorithms, namely Categorical Boosting (CatBoost), Light Gradient Boosting Machine (LightGBM), and Extreme Gradient Boosting (XGBoost). A database consisting of 600 datasets is prepared for model calibration and evaluation, where the factor of safety (FS) is regarded as the output and four influential factors are selected as the inputs. For each dataset, the FS corresponding to the four inputs is evaluated using the commercial geotechnical software of Slide2. As an illustration, the proposed approach is applied to the seismic stability analysis of a hypothetical embankment example subjected to water level changes. For comparison, the predictive performance of CatBoost, LightGBM, and XGBoost is investigated. Moreover, the Shapley additive explanations (SHAP) method is used in this study to explore the relative importance of the four features. Results show that all the three gradient boosting algorithms (i.e., CatBoost, LightGBM, and XGBoost) perform well in the prediction of FS for both the training dataset and testing dataset. Among the four influencing factors, the friction angle φ is the most important feature variable, followed by horizontal seismic coefficient Kh, cohesion c, and saturated permeability ks."
Zhang2021,Liang Zhang and Matt Leach and Yeonjin Bae and Borui Cui and Saptarshi Bhattacharya and Seungjae Lee and Piljae Im and Veronica Adetola and Draguna Vrabie and Teja Kuruganti,Sensor impact evaluation and verification for fault detection and diagnostics in building energy systems: A review,Advances in Applied Energy,3,,2021,10.1016/j.adapen.2021.100055,26667924,"Sensors are the key information source for fault detection and diagnostics (FDD) in buildings. However, sensors are often not properly designed, installed, calibrated, located, and maintained, which negatively impacts FDD performance. Several sensor-related FDD topics have been widely studied, covering a wide range of fault types and applications. However, it is difficult to get a clear picture of the technical development of sensor-related topics in FDD. A systematic review of sensor topics is needed to summarize the existing research in a logical way, draw conclusions on the current development, and predict the future development of sensors in building FDD. To address this gap, we conducted a comprehensive literature review of more than 100 FDD-sensor-related papers. In this article, we subdivide the FDD tasks into building-level, system-level, and component-level FDD, and review sensor-related topics in each category. Our major conclusions are: (a) current data-driven FDD research focuses more on FDD algorithms than sensors, (b) sensor “hardware” research topics are less studied than sensor “software” topics, (c) very few papers focus on sensor engineering as an integral aspect of FDD development, and (d) some important sensor topics, such as sensor cost-effectiveness and sensor schema/layout/location, are not well studied. Finally, we discuss the need for a systematic framework of FDD sensors and models to integrate sensor design/selection, sensor data analysis/mining, feature selection, physics-based or data-driven algorithm development, sensor fault detection, sensor calibration, and sensor maintenance. Finally, expert interviews are conducted to validate the above findings and conclusions."
Graessler2020,Iris Graessler and Christian Oleff and Philipp Scholle,Method for systematic assessment of requirement change risk in industrial practice,Applied Sciences (Switzerland),10,23,2020,10.3390/app10238697,20763417,"Requirement changes and cascading effects of change propagation are major sources of inefficiencies in product development and increase the risk of project failure. Risk management regarding these requirement changes yields the potential to handle such changes efficiently. Currently unlocked, a systematic approach is required for risk management to assess the risk of a requirement change with appropriate effort in industrial application. Within the paper at hand, a novel method for systematic assessment of requirement change risk is presented. It is developed in a multiple case study approach with three product development projects from different industrial branches. The change risk is assessed by combining change likelihood and change impact. Propagation effects are considered by analyzing requirement interrelations. To limit application effort, a tailorable approach towards assessment of change causes based on generalized influence factors and a pre‐defined rule set for semi‐automatized assessment of requirements interrelations is used. A software prototype is developed and implemented to enable evaluation and transfer to industrial application. The approach is evaluated using a combination of case study projects, stakeholder workshops, questionnaires and semi‐structured interviews. Applying the method, the risks of requirement changes are assessed systematically, and subsequent risk management is enabled. The contribution at hand opens up the research space of risk management in handling requirement changes which is currently almost unexploited. At the same time, it enables more efficient product development."
Dennis2020,Louise A. Dennis and Michael Fisher,Verifiable Self-Aware Agent-Based Autonomous Systems,Proceedings of the IEEE,108,7,2020,10.1109/JPROC.2020.2991262,15582256,"In this article, we describe an approach to autonomous system construction that not only supports self-awareness but also formal verification. This is based on modular construction where the key autonomous decision making is captured within a symbolically described 'agent.' So, this article leads us from traditional systems architectures, via agent-based computing, to explainability, reconfigurability, and verifiability, and on to applications in robotics, autonomous vehicles, and machine ethics. Fundamentally, we consider self-awareness from an agent-based perspective. Agents are an important abstraction capturing autonomy, and we are particularly concerned with intentional, or rational, agents that expose the 'intentions' of the autonomous system. Beyond being a useful abstract concept, agents also provide a practical engineering approach for building the core software in autonomous systems such as robots and vehicles. In a modular autonomous system architecture, agents of this form capture important decision making elements. Furthermore, this ability to transparently capture such decision making processes, and especially being able to expose their intentions, within an agent allows us to apply strong (formal) agent verification techniques to these systems."
Sasiain2020,Jorge Sasiain and Ane Sanz and Jasone Astorga and Eduardo Jacob,Towards flexible integration of 5G and IIoT technologies in industry 4.0: A practical use case,Applied Sciences (Switzerland),10,21,2020,10.3390/app10217670,20763417,"The Industry 4.0 revolution envisions fully interconnected scenarios in the manufacturing industry to improve the efficiency, quality, and performance of the manufacturing processes. In parallel, the consolidation of 5G technology is providing substantial advances in the world of communication and information technologies. Furthermore, 5G also presents itself as a key enabler to fulfill Industry 4.0 requirements. In this article, the authors first propose a 5G-enabled architecture for Industry 4.0. Smart Networks for Industry (SN4I) is introduced, an experimental facility based on two 5G key-enabling technologies—Network Functions Virtualization (NFV) and Software-Defined Networking (SDN)—which connects the University of the Basque Country’s Aeronautics Advanced Manufacturing Center and Faculty of Engineering in Bilbao. Then, the authors present the deployment of a Wireless Sensor Network (WSN) with strong access control mechanisms into such architecture, enabling secure and flexible Industrial Internet of Things (IIoT) applications. Additionally, the authors demonstrate the implementation of a use case consisting in the monitoring of a broaching process that makes use of machine tools located in the manufacturing center, and of services from the proposed architecture. The authors finally highlight the benefits achieved regarding flexibility, efficiency, and security within the presented scenario and to the manufacturing industry overall."
VanDeBurgt2020,Yoeri Van De Burgt and Paschalis Gkoupidenis,Organic materials and devices for brain-inspired computing: From artificial implementation to biophysical realism,MRS Bulletin,45,8,2020,10.1557/mrs.2020.194,08837694,"Many of the current artificial intelligence (AI) applications that are rapidly becoming indispensable in our society rely on software-based artificial neural networks or deep learning algorithms that are powerful, but energy-inefficient. The brain in comparison is highly efficient at similar classification and pattern finding tasks. Neuromorphic engineering attempts to take advantage of the efficiency of the brain by mimicking several crucial concepts to efficiently emulate AI tasks. Organic electronic materials have been particularly successful in mimicking both the basic functionality of the brain, including important spiking phenomena, but also in low-power operation of hardware-implemented artificial neural networks as well as interfacing with physiological environments due to their biocompatible nature. This article provides an overview of the basic functional operation of the brain and its artificial counterparts, with a particular focus on organic materials and devices. We highlight efforts to mimic brain functions such as spatiotemporal processing, homeostasis, and functional connectivity and emphasize current challenges for efficient neuromorphic computing applications. Finally, we present our view of future directions in this exciting and rapidly growing field of organic neuromorphic devices."
Kamari2021,Aliakbar Kamari and Ashwin Paari and Henrik Øien Torvund,Bim-enabled virtual reality (Vr) for sustainability life cycle and cost assessment,Sustainability (Switzerland),13,1,2021,10.3390/su13010249,20711050,"Virtual Reality (VR) is receiving ever-increasing attention and is utilized by many construction companies in their current practices. This paper aims at a critical investigation of the impact of VR technology on how sustainability and cost are understood and perceived by the users in building design projects, which could lead to improving and supporting the actual building design processes. The research study focused on evaluating design alternatives using Building Information Modeling (BIM)-enabled VR technology integrated with cost and sustainability life cycle assessment (LCA) software. In doing so, the paper begins with reviewing the relevant literature in the mentioned areas. Thereafter, it adopts an experimental-qualitative-quantitative method to test the research hypothesis and analyze the effects of 360-degree VR on the users (66 participants), while distinguishing between users who have a relevant background in building/construction engineering (i.e., architect engineers and civil engineers), and those who have not (i.e., owners and clients). It is observed that despite their background, the user participants positively embrace the ideas and aspirations of sustainability, and that there is some evidence of respondents preferring the economy over sustainability. Likewise, the participants are not making an effort to measure the emissions of their design options rather than focus on the building’s economic aspects."
Javanshir2020,Nima Javanshir and S. M. Seyed Mahmoudi and M. Akbari Kordlar and Marc A. Rosen,Energy and cost analysis and optimization of a geothermal-based cogeneration cycle using an ammonia-water solution: Thermodynamic and thermoeconomic viewpoints,Sustainability (Switzerland),12,2,2020,10.3390/su12020484,20711050,"A cogeneration cycle for electric power and refrigeration, using an ammonia-water solution as a working fluid and the geothermal hot water as a heat source, is proposed and investigated. The system is a combination of a modified Kalina cycle (KC) which produces power and an absorption refrigeration cycle (ARC) that generates cooling. Geothermal water is supplied to both the KC boiler and the ARC generator. The system is analyzed from thermodynamic and economic viewpoints, utilizing Engineering Equation Solver (EES) software. In addition, a parametric study is carried out to evaluate the effects of decision parameters on the cycle performance. Furthermore, the system performance is optimized for either maximizing the exergy efficiency (EOD case) or minimizing the total product unit cost (COD case). In the EOD case the exergy efficiency and total product unit cost, respectively, are calculated as 34.7% and 15.8$/GJ. In the COD case the exergy efficiency and total product unit cost are calculated as 29.8% and 15.0$/GJ. In this case, the cooling unit cost, cp,cooling, and power unit cost, cp,power, are achieved as 3.9 and 11.1$/GJ. These values are 20.4% and 13.2% less than those obtained when the two products are produced separately by the ARC and KC, respectively. The thermoeconomic analysis identifies the more important components, such as the turbine and absorbers, for modification to improve the cost-effectiveness of the system."
Gallala2022,Abir Gallala and Atal Anil Kumar and Bassem Hichri and Peter Plapper,Digital Twin for Human–Robot Interactions by Means of Industry 4.0 Enabling Technologies,Sensors,22,13,2022,10.3390/s22134950,14248220,"There has been a rapid increase in the use of collaborative robots in manufacturing industries within the context of Industry 4.0 and smart factories. The existing human–robot interactions, simulations, and robot programming methods do not fit into these fast-paced technological advances as they are time-consuming, require engineering expertise, waste a lot of time in programming and the interaction is not trivial for non-expert operators. To tackle these challenges, we propose a digital twin (DT) approach for human–robot interactions (HRIs) in hybrid teams in this paper. We achieved this using Industry 4.0 enabling technologies, such as mixed reality, the Internet of Things, collaborative robots, and artificial intelligence. We present a use case scenario of the proposed method using Microsoft Hololens 2 and KUKA IIWA collaborative robot. The obtained results indicated that it is possible to achieve efficient human–robot interactions using these advanced technologies, even with operators who have not been trained in programming. The proposed method has further benefits, such as real-time simulation in natural environments and flexible system integration to incorporate new devices (e.g., robots or software capabilities)."
Ghodbane2021,Mokhtar Ghodbane and Boussad Boumeddane and Ahmed Kadhim Hussein,"Performance Analysis of A Solar-Driven Ejector Air Conditioning System Under EL-Oued Climatic Conditions, Algeria",Journal of Thermal Engineering,7,1,2021,10.18186/thermal.847334,21487847,"In order to understand the behavior and to determine the effective operational parameters of a solar-driven ejector air conditioning system at low or medium temperature, a dynamic model depends on the principles of conservation, the momentum mass and energy is developed. For this purpose, the thermodynamic characteristics of the liquid and vapor refrigerant were identified using the Engineering Equation Solver (EES) software. Linear Fresnel solar reflector has been used as a tool to convert solar energy into thermal energy. Water (R718) was used as a refrigerant. The operational conditions for the studied solar-driven ejector air conditioning system are as follows: evaporator temperature “Te =283.15 K”, condenser temperature “Tc =305.15 K”, and generator temperature “Tg = 373.15 K”. The performance of the ejector air conditioning system was compared as a function of the operating parameters of the subsystem. The average value of thermal efficiency of the Fresnel linear concentrator has reached 31.60 %, the drive ratio “ω” is 0.4934, the performance value of the ejector air conditioning subsystem “COPejc” is 60.664 % and the average value of the thermal performance of the machine “STR” has touched 19.17 %. The results obtained through this scientific subject are stimulating and encouraging, where this technique can be used for air conditioning in desert areas in southern Algeria, where fossil energy (petroleum, gas, etc.) is extracted and produced in various types."
Deierlein2020,Gregory G. Deierlein and Frank McKenna and Adam Zsarnóczay and Tracy Kijewski-Correa and Ahsan Kareem and Wael Elhaddad and Laura Lowes and Matthew J. Schoettler and Sanjay Govindjee,A Cloud-Enabled Application Framework for Simulating Regional-Scale Impacts of Natural Hazards on the Built Environment,Frontiers in Built Environment,6,,2020,10.3389/fbuil.2020.558706,22973362,"With the goal to facilitate evaluation and mitigation of the risks from natural hazards, the Natural Hazards Engineering Research Infrastructure’s Computational Modeling, and Simulation Center (NHERI SimCenter) is developing computational workflows for regional hazard simulations. These simulations enable research to combine detailed assessments of individual facilities with comprehensive regional-scale simulations of natural hazard effects. By integration of multi-fidelity and multi-resolution models to assess natural hazard impacts on buildings, infrastructure systems and other constructed facilities, the approach enables the engineering analysis of public policies and socio-economic impacts. Effective development of platforms for high-resolution regional simulations requires modular workflows that can integrate state-of-the-art models with information technologies and high-performance computing resources. In this paper, the modular architecture of the computational workflow models is described and illustrated through testbed applications to evaluate regional building damage under an earthquake and a hurricane scenario. Developed and disseminated as open-source software on the NHERI DesignSafe Cyberinfrastructure, the computational models and workflows are enabling multi-disciplinary collaboration on research to mitigate the effects of natural hazard disasters."
Dorweiler2021,Bernhard Dorweiler and Pia Elisabeth Baqué and Rayan Chaban and Ahmed Ghazy and Oroa Salem,Quality control in 3D printing: Accuracy analysis of 3D-printed models of patient-specific anatomy,Materials,14,4,2021,10.3390/ma14041021,19961944,"As comparative data on the precision of 3D-printed anatomical models are sparse, the aim of this study was to evaluate the accuracy of 3D-printed models of vascular anatomy generated by two commonly used printing technologies. Thirty-five 3D models of large (aortic, wall thickness of 2 mm, n = 30) and small (coronary, wall thickness of 1.25 mm, n = 5) vessels printed with fused deposition modeling (FDM) (rigid, n = 20) and PolyJet (flexible, n = 15) technology were subjected to high-resolution CT scans. From the resulting DICOM (Digital Imaging and Communications in Medicine) dataset, an STL file was generated and wall thickness as well as surface congruency were compared with the original STL file using dedicated 3D engineering software. The mean wall thickness for the large-scale aortic models was 2.11 µm (+5%), and 1.26 µm (+0.8%) for the coronary mod-els, resulting in an overall mean wall thickness of +5% for all 35 3D models when compared to the original STL file. The mean surface deviation was found to be +120 µm for all models, with +100 µm for the aortic and +180 µm for the coronary 3D models, respectively. Both printing technologies were found to conform with the currently set standards of accuracy (<1 mm), demonstrating that accurate 3D models of large and small vessel anatomy can be generated by both FDM and PolyJet printing technology using rigid and flexible polymers."
Standage-Beier2021,Kylie Standage-Beier and Stefan J. Tekel and David A. Brafman and Xiao Wang,Prime Editing Guide RNA Design Automation Using PINE-CONE,ACS Synthetic Biology,10,2,2021,10.1021/acssynbio.0c00445,21615063,"CRISPR-based technologies are paramount in genome engineering and synthetic biology. Prime editing (PE) is a technology capable of installing genomic edits without double-stranded DNA breaks (DSBs) or donor DNA. Prime editing guide RNAs (pegRNAs) simultaneously encode both guide and edit template sequences. They are more design intensive than CRISPR single guide RNAs (sgRNAs). As such, application of PE technology is hindered by the limited throughput of manual pegRNA design. To that end, we designed a software tool, Prime Induced Nucleotide Engineering Creator of New Edits (PINE-CONE), that enables high-throughput automated design of pegRNAs and prime editing strategies. PINE-CONE translates edit coordinates and sequences into pegRNA designs, accessory guides, and oligonucleotides for facile cloning workflows. To demonstrate PINE-CONE's utility in studying disease-relevant genotypes, we rapidly design a library of pegRNAs targeting Alzheimer's Disease single nucleotide polymorphisms (SNPs). Overall, PINE-CONE will accelerate the application of PEs in synthetic biology and biomedical research."
Gleirscher2020,Mario Gleirscher and Diego Marmsoler,Formal methods in dependable systems engineering: a survey of professionals from Europe and North America,Empirical Software Engineering,25,6,2020,10.1007/s10664-020-09836-5,15737616,"Context: Formal methods (FMs) have been around for a while, still being unclear how to leverage their benefits, overcome their challenges, and set new directions for their improvement towards a more successful transfer into practice. Objective: We study the use of formal methods in mission-critical software domains, examining industrial and academic views. Method: We perform a cross-sectional on-line survey. Results: Our results indicate an increased intent to apply FMs in industry, suggesting a positively perceived usefulness. But the results also indicate a negatively perceived ease of use. Scalability, skills, and education seem to be among the key challenges to support this intent. Conclusions: We present the largest study of this kind so far (N = 216), and our observations provide valuable insights, highlighting directions for future theoretical and empirical research of formal methods. Our findings are strongly coherent with earlier observations by Austin and Graeme (1993)."
Nicolas2021,Kervins Nicolas and Yi Wang and George C. Giakos and Bingyang Wei and Hongda Shen,Blockchain System Defensive Overview for Double-Spend and Selfish Mining Attacks: A Systematic Approach,IEEE Access,9,,2021,10.1109/ACCESS.2020.3047365,21693536,"Blockchain is a technology that ensures data security by verifying database of records established in a decentralized and distributed network. Blockchain-based approaches have been applied to secure data in the fields of the Internet of Things, software engineering, healthcare systems, financial services, and smart power grids. However, the security of the blockchain system is still a major concern. We took the initiative to present a systematic study which sheds light on what defensive strategies are used to secure the blockchain system effectively. Specifically, we focus on blockchain data security that aims to mitigate the two data consistency attacks: double-spend attack and selfish mining attack. We employed the systematic approach to analyze a total of 40 selected studies using the proposed taxonomy of defensive strategies: monitoring, alert forwarding, alert broadcasting, inform, detection, and conceptual research design. It presents a comparison framework for existing and future research on blockchain security. Finally, some recommendations are proposed for blockchain researchers and developers."
Prez-Delgado2020,Carlos A. Pérez-Delgado and Hector G. Perez-Gonzalez,Towards a Quantum Software Modeling Language,,,,2020,10.1145/3387940.3392183,,"We set down the principles behind a modeling language for quantum software. We present a minimal set of extensions to the well-known Unified Modeling Language (UML) that allows it to effectively model quantum software. These extensions are separate and independent of UML as a whole. As such they can be used to extend any other software modeling language, or as a basis for a completely new language. We argue that these extensions are both necessary and sufficient to model, abstractly, any piece of quantum software. Finally, we provide a small set of examples that showcase the effectiveness of the extension set."
Nahar2022,Nadia Nahar and Shurui Zhou and Grace Lewis and Christian Kastner,"Collaboration Challenges in Building ML-Enabled Systems: Communication, Documentation, Engineering, and Process",,2022-May,,2022,10.1145/3510003.3510209,02705257,"The introduction of machine learning (ML) components in software projects has created the need for software engineers to collabo-rate with data scientists and other specialists. While collaboration can always be challenging, ML introduces additional challenges with its exploratory model development process, additional skills and knowledge needed, difficulties testing ML systems, need for continuous evolution and monitoring, and non-traditional quality requirements such as fairness and explainability. Through inter-views with 45 practitioners from 28 organizations, we identified key collaboration challenges that teams face when building and deploying ML systems into production. We report on common col-laboration points in the development of production ML systems for requirements, data, and integration, as well as corresponding team patterns and challenges. We find that most of these challenges center around communication, documentation, engineering, and process, and collect recommendations to address these challenges."
Babaytsev2020,Arseniy V. Babaytsev and Ye Ko Kyaw and Sergey N. Vakhneev and Thant Zin Hein,Study of the influence of spherical inclusions on mechanical characteristics,Periodico Tche Quimica,17,35,2020,10.52571/ptq.v17.n35.2020.56_babaytsev_pgs_654_662.pdf,21790302,"The relevance of the article is due to the stable growth of the composite industry. Due to its high physical and mechanical characteristics, composite materials (CM) play a vital role in many areas of technology, such as aerospace, aviation, automotive, engineering and instrumentation, and the medical industry. When creating materials with the required mechanical and thermal characteristics, various additives and fillers are often used that affect the strength and elasticity of the samples obtained. In this paper, we study the influence of spherical inclusions in epoxy on the mechanical properties of the material. A composite structure of ED-20 epoxy with inclusions in the form of spherical particles of PBS-50 glass was considered. The characteristic particle size was about 50 μm. Within the framework of this study, batches of samples with a specific volumetric inclusion content of 5%, 10%, 15%, and 20% were produced. Each batch consisted of 6 samples of the same type. To check the distribution of particle volume and inclusion studies, the microstructure of the thin section of the sample and separately the powder were studied. Using methods of modeling the process, the samples were tested for 3-point bending. Based on the results of mechanical tests, the elastic module, strength limit, and ultimate strains were obtained. As part of the study of materials, numerical calculations were performed using Digimat software (MSC Software) to model the structure and obtain effective material properties, as well as strength analysis using the finite element method in Ansys Workbench. The calculation was carried out for the volumetric content of inclusions of 5%, 10%, 15%, and 20% order and in the absence of inclusions. The obtained numerical results were compared with an experimental study. Based on the numerical calculations, the dependence of the percentage of inclusions on the strength limit and elastic module was obtained."
Villarreal2023,Edgar R.Dulce Villarreal and Jose Garcia-Alonso and Enrique Moguel and Julio Ariel Hurtado Alegria,Blockchain for Healthcare Management Systems: A Survey on Interoperability and Security,IEEE Access,11,,2023,10.1109/ACCESS.2023.3236505,21693536,"n recent years it has been shown that the secure exchange of medical information significantly benefits people's life quality, improving their care and treatment. The interoperability of the entire healthcare ecosystem is a constant challenge, and even more, with all the risks posed to the security of healthcare information. Blockchain technology is emerging as one of the main alternatives when it comes to finding a balance in the healthcare ecosystem. However, the constant development of new Blockchain technologies and the evolution of healthcare systems make it difficult to find established proposals. From an architectural point of view, the design of blockchain-based solutions requires trade-offs e.g., security and interoperability. This paper focuses on two main objectives, in the first one, it was carried out a Systematic Literature Review for exploring architectural mechanisms used to support the interoperability and security of Blockchain-based Health Management Systems. Taking into account of results, a series of scenarios were generated where these mechanisms can be used along with their context, issues, and various architectural concerns (interoperability and security). In the second objective, a high-level architecture and its validation were proposed through an experiment for the whole process of developing a Domain Specific Language, using the Model Driven Engineering methodology for specific Smart Contracts."
Liu2021,Hongda Liu and Yuxi Luo and Jiejun Geng and Pinbo Yao,Research hotspots and frontiers of product R&D management under the background of the digital intelligence era-bibliometrics based on citespace and histcite,Applied Sciences (Switzerland),11,15,2021,10.3390/app11156759,20763417,"The rise of “cloud-computing, mobile-Internet, Internet of things, big-data, and smart-data” digital technology has brought a subversive revolution to enterprises and consumers’ traditional patterns. Product research and development has become the main battlefield of enterprise competition, facing an environment where challenges and opportunities coexist. Regarding the concepts and methods of product R&D projects, the domestic start was later than the international ones, and many domestic companies have also used successful foreign cases as benchmarks to innovate their management methods in practice. “Workers must first sharpen their tools if they want to do their jobs well”. This article will start from the relevant concepts of product R&D projects and summarize current R&D management ideas and methods. We combined the bibliometric analysis software Histcite and Citespace to sort out the content of domestic and foreign literature and explore the changing trends of research hotspots. Finally, combined with the analysis of confirmed cases in domestic masters and doctoral dissertations to test the theory, the literature review of the product R&D project management theme was carried out from the dual perspectives of comprehensive theory and practice. This study uses the core collection library of Web of Science as the object of document extraction. Based on the search conditions of “Product development” or “Intergrat* product development”, 8998 sample documents were initially retrieved. The search deadline was June 2019, with a time range from 2000 to June 2019. Then, using the record number of 50 as the critical condition, 5007 analysis samples were deleted, refined, and cleaned. Through the review and measurement of 5007 papers, the analysis showed that: (1) in the last ten years, sustainability, consumer focus, new approaches to product development management, and organizational design have become critical considerations in the product development process stage; (2) at this stage, researchers are paying more attention to the innovation, design, product development, identification, simultaneous engineering, consequence, and stage/gate model aspects of product development; and (3) factors such as long development cycles, high costs, and poor organizational design are now common problems in the product development process."
Khudaykulov2024,Akmal Khudaykulov and Zheng Changjun and Bojan Obrenovic and Danijela Godinic and Hussain Zaid H. Alsharif and Ilimdorjon Jakhongirov,The fear of COVID-19 and job insecurity impact on depression and anxiety: An empirical study in China in the COVID-19 pandemic aftermath,Current Psychology,43,9,2024,10.1007/s12144-022-02883-9,19364733,"The employees' psychological health and resilience in times of emergency and general uncertainty was chosen due to the immense implications for economics, entrepreneurs, psychologists and psychiatrists, and policymakers. This study aims to provide an insight into uncertainty-induced anxiety and depression among Chinese employees in the aftermath of the COVID-19 outbreak. Analysis performed in the context of China in the COVID-19 pandemic aftermath is significant due to the universal nature of external shock impact on psychological welfare, applicable across nations and business sectors and in similar contexts. The statistical analysis was performed with SEM software AMOS version 23. The research model consisting of fear of COVID-19, job insecurity, anxiety, depression, was empirically tested. A purposive sampling technique was applied with the online questionnaire shared with employees in companies located in China. Respondents were working in educational services, information technology, engineering, electronics, and other sectors on white-collar jobs. The data collection was conducted from May to August 2020, in the aftermath of the COVID-19 pandemic in China. The research sample consisting of 283 respondents was used for analysis. Path analysis was performed, and standardized parameter estimates, standard errors, and p-values were calculated. The results indicate a positive and significant impact of job insecurity on depression and anxiety. Furthermore, results indicate that the fear of COVID-19 significantly impacts anxiety and depression but does not impact job insecurity. The findings can be used in a multidisciplinary effort to mitigate the psychological damage. Furthermore, they complement the ongoing epidemiological and scientific discourse on people's personal health and choice of coping."
Wati2020,E. K. Wati and N. Widiansyah,Design of learning media: Modeling & simulation of building thermal comfort optimization system in building physics course,Jurnal Pendidikan IPA Indonesia,9,2,2020,10.15294/jpii.v9i2.23504,20894392,"The use of instructional media is something that can support the teaching and learning process; therefore, a lecturer must have the ability to create and develop learning media. This study aims to improve student learning outcomes in building physics course by using simulation learning media and models to help students understand thermal comfort material. Making modelling and simulation media is done using MATLAB software. The subjects of this study were physics engineering students who took Building Physics course. At the beginning of the study, students are given material and then in groups discuss thermal contents and then given a pretest test with an average score of 70.27, and for an average grade of 71.3 assignments. At the meeting next week, using the Student-Centered Learning (SCL) method and using problem-based learning in groups, students take temperature measurements in several rooms in the Building at the UNAS Physical Engineering Laboratory. The measurement results show that the room does not have thermal requirements (PERGUB No. 38/2012), so students have the task of conducting experiments using models that have been created by researchers to create learning media to improve comfort in using thermal buildings. Simulation results carried out by students, that is, can produce rooms with thermal conditions at 21-25°C (PERGUB No. 38/2012). This simulation is also able to provide the score of building energy efficiency. After students succeed in conducting the test, the assessment test or posttest is carried out with an average score obtained 80.55, and an average score of 80 assignments. The results of the pretest, assignment 1, assignment 2, and posttest show an increase in students’ scores of 14.6% for the Test and Task Score of 12.20%. Based on the hypothesis test, for both variables showed t-count < t-table and significance < 0.05. It shows there are significant differences in student learning outcomes both test scores and assignment scores before and after using a simulated media. Thus, the system and simulation model designed can be used as learning media that can improve student learning outcomes."
Antoniadis2022,Antonis F. Antoniadis and Dimitris Drikakis and Pericles S. Farmakis and Lin Fu and Ioannis Kokkinakis and Xesús Nogueira and Paulo A.S.F. Silva and Martin Skote and Vladimir Titarev and Panagiotis Tsoutsanis,UCNS3D: An open-source high-order finite-volume unstructured CFD solver,Computer Physics Communications,279,,2022,10.1016/j.cpc.2022.108453,00104655,"UCNS3D is an open-source computational solver for compressible flows on unstructured meshes. State-of-the-art high-order methods and their associated benefits can now be implemented for industrial-scale CFD problems due to the flexibility and highly-automated generation offered by unstructured meshes. We present the governing equations of the physical models employed in UCNS3D, and the numerical framework developed for their solution. The code has been designed so that extended to other systems of equations and numerical models is straightforward. The employed methods are validated towards a series of stringent well-established test problems against experimental or analytical solutions, where the full capabilities of UCNS3D in terms of applications spectrum, robustness, efficiency, and accuracy are demonstrated. Program summary: Program title: UCNS3D (Unstructured Compressible Flow Solver) CPC Library link to program files: https://doi.org/10.17632/222zh873kh.1 Developer's repository link: https://github.com/ucns3d-team/UCNS3D Licensing provisions: GNU General Public License 3 Programming language: Fortran2008 Nature of problem: UCNS3D is intended for the simulation of compressible flows in 2D and 3D unstructured meshes, by employing high-resolution, high-order methods capable of providing physically meaningful results in a computational efficient manner. The solver is designed for a broad range of problems encountered in engineering applications such as transitional, fully turbulent, and multicomponent flows with several fidelity level modelling options available. Solution method: The present software includes multiple physical models, numerical methods, and modelling techniques such as iLES, RANS, DES for unstructured meshes. The software has been developed such that the inclusion of additional physical models and numerical methods can be easily accommodated."
Driss2020,Maha Driss and Amani Aljehani and Wadii Boulila and Hamza Ghandorh and Mohammed Al-Sarem,Servicing Your Requirements: An FCA and RCA-Driven Approach for Semantic Web Services Composition,IEEE Access,8,,2020,10.1109/ACCESS.2020.2982592,21693536,"The evolution of Service-Oriented Computing (SOC) provides more efficient software development methods for building and engineering new value-added service-based applications. SOC is a computing paradigm that relies on Web services as fundamental elements. Research and technical advancements in Web services composition have been considered as an effective opportunity to develop new service-based applications satisfying complex requirements rapidly and efficiently. In this paper, we present a novel approach enhancing the composition of semantic Web services. The novelty of our approach, as compared to others reported in the literature, rests on: i) mapping user's/organization's requirements with Business Process Modeling Notation (BPMN) and semantic descriptions using ontologies, ii) considering functional requirements and also different types of non-functional requirements, such as quality of service (QoS), quality of experience (QoE), and quality of business (QoBiz), iii) using Formal Concept Analysis (FCA) technique to select the optimal set of Web services, iv) considering composability levels between sequential Web services using Relational Concept Analysis (RCA) technique to decrease the required adaptation efforts, and finally, v) validating the obtained service-based applications by performing an analytical technique, which is the monitoring. The approach experimented on an extended version of the OWLS-TC dataset, which includes more than 10830 Web services descriptions from various domains. The obtained results demonstrate that our approach allows to successfully and effectively compose Web services satisfying different types of user's functional and non-functional requirements."
Urzic2021,Andrei Urzică and Alin Mihu-Pintilie and Cristian Constantin Stoleriu and Cătălin Ioan Cîmpianu and Elena Huţanu and Claudiu Ionuţ Pricop and Adrian Grozavu,Using 2D HEC-RAS modeling and embankment dam break scenario for assessing the flood control capacity of a multireservoir system (Ne Romania),Water (Switzerland),13,1,2021,10.3390/w13010057,20734441,"Using hydraulic modeling techniques (e.g., one-dimensional/two-dimensional (1D/2D) hydraulic modeling, dam break scenarios) for extracting the flood settings is an important aspect of any action plan for dam failure (APDF) and flood mitigation strategy. For example, the flood hydraulic models and dam break scenario generated based on light detection and ranging (LiDAR)-derived digital elevation models (DEMs) and processed in the dedicated geographic information systems (GIS) and hydraulic modeling software (e.g., HEC-RAS—Hydrologic Engineering Center River Analysis System, developed by USACE HEC, Davis, CA, USA) can improve the flood hazard maps in case of potentially embankment dam failure. In this study, we develop a small-scale conceptual approach using 2D HEC-RAS software according to the three embankment dam break scenarios, LiDAR data (0.5 m spatial resolution), and 2D hydraulic modeling for the Başeu multi-reservoir system which belongs to the Başeu River (NE Romania) including R1—Cal Alb reservoir, R2—Movileni reservoirs, R3—Tătărăşeni reservoirs, R4—Negreni reservoirs, and R5—Hăneşti reservoirs. In order to test the flood control capacity of the Bașeu multi-reservoir system, the Cal Alb (R1) dam break scenario (piping failure) was taken into account. Three 2D stream flow modeling configurations based on R1 inflow rate with a 1% (100 year), 0.5% (500 year), and 0.1% (1000 year) recurrence interval and the water volume which can be accumulated with that specific inflow rate (1% = 10.19 × 106 m3; 0.5% = 12.39 × 106 m3; 0.1% = 17.35 × 106 m3 ) were computed. The potential flood wave impact was achieved on the basis of different flood severity maps (e.g., flood extent, flood depth, flood velocity, flood hazard) generated for each recurrence interval scenario and highlighted within the built-up area of 27 settlements (S1–S27) located downstream of R1. The results showed that the multi-reservoir system of Bașeu River has an important role in flood mitigation and contributes to the APDF in the context of climate change and the intensification of hydrological hazard manifestation in northeastern Romania."
Prada2020,Miguel Angel Prada and Manuel Dominguez and Jose Lopez Vicario and Paulo Alexandre Vara Alves and Marian Barbu and Michal Podpora and Umberto Spagnolini and Maria J.Varanda Pereira and Ramon Vilanova,Educational Data Mining for Tutoring Support in Higher Education: A Web-Based Tool Case Study in Engineering Degrees,IEEE Access,8,,2020,10.1109/ACCESS.2020.3040858,21693536,"This paper presents a web-based software tool for tutoring support of engineering students without any need of data scientist background for usage. This tool is focused on the analysis of students' performance, in terms of the observable scores and of the completion of their studies. For that purpose, it uses a data set that only contains features typically gathered by university administrations about the students, degrees and subjects. The web-based tool provides access to results from different analyses. Clustering and visualization in a low-dimensional representation of students' data help an analyst to discover patterns. The coordinated visualization of aggregated students' performance into histograms, which are automatically updated subject to custom filters set interactively by an analyst, can be used to facilitate the validation of hypotheses about a set of students. Classification of students already graduated over three performance levels using exploratory variables and early performance information is used to understand the degree of course-dependency of students' behavior at different degrees. The analysis of the impact of the student's explanatory variables and early performance in the graduation probability can lead to a better understanding of the causes of dropout. Preliminary experiments on data of the engineering students from the 6 institutions associated to this project were used to define the final implementation of the web-based tool. Preliminary results for classification and drop-out were acceptable since accuracies were higher than 90% in some cases. The usefulness of the tool is discussed with respect to the stated goals, showing its potential for the support of early profiling of students. Real data from engineering degrees of EU Higher Education institutions show the potential of the tool for managing high education and validate its applicability on real scenarios."
Bazn2020,Ángela Moreno Bazán and Marcos G. Alberti and Antonio Arcos Álvarez and Jesús Alonso Trigueros,New perspectives for bim usage in transportation infrastructure projects,Applied Sciences (Switzerland),10,20,2020,10.3390/app10207072,20763417,"Although there is already a great amount of scientific literature dealing with the use of building information modeling (BIM) in engineering activities, the majority refer to successful case studies using the usual methods and technology of building construction but rarely bring up the real problems for implementing BIM methodology to the field of transportation infrastructure. It must be also considered that the construction activity is only a part of the infrastructure life and the stakeholder must consider the works of enlargement, renewal, and maintenance of the infrastructure. The purpose of this paper is not only to show a mere review of the existing literature but also present a rational analysis for the use of BIM in different areas of civil engineering. For that purpose, the gathered experience in the use of BIM in civil engineering projects in the final course of Civil Engineering Master Studies in the Civil Engineering School (ETSICCP) at Universidad Politécnica de Madrid were compared with the reported literature. This way, a complete and updated information regarding tendencies, applications, and practice along with limitations and benefits can be presented. The significance of this research relies on the original insight of BIM for civil engineering applications through four case studies. Two of them were focused on construction possibilities and the other two on the possibilities in the exploitation, rehabilitation, and maintenance. The results showed that despite the lack of previous experiences, the use of BIM methodology is possible for activities such as maintenance, managing, or expansion of infrastructure by applying different specific software packages. Among the main problems needing to be addressed are the following: handling of big data files, the integration of new data non-related with the modeled object, and interchange of data without losing information. That proves the need of new more efficient techniques to overcome the challenge of the full use of BIM in the civil engineering field and obtain the mutual advantage of the co-operation of the academic and industrial worlds."
Yurin2020,Aleksandr Yu Yurin and Nikita O. Dorodnykh,Personal knowledge base designer: Software for expert systems prototyping,SoftwareX,11,,2020,10.1016/j.softx.2020.100411,23527110,"In most cases, the complexity of expert systems engineering depends on the complexity of knowledge base engineering. This process includes formalization and programming tasks. In this connection, the use of visual programming, model transformation and code generation principles are relevant. We present a new software with similar properties. Our software provides the use of a domain-specific notation for rule modeling, namely, Rule Visual Modeling Language (RVML); wizards for creating and editing knowledge base elements; conceptual models and canonical spreadsheet tables as main sources of domain knowledge. The core of the new software is a unified model for representing and editing knowledge in the form of logical rules, as well as its interpretation using the built-in rule engine. This enables the use of conceptual models in the form of UML class diagrams, concept maps, mind maps, Ishikawa diagrams and others as a source of information, and also helps involve non-programming users in the process of knowledge base engineering and to minimize coding errors. Our empirical results demonstrate the ability to use the proposed software for prototyping rule-based knowledge bases by transforming different conceptual models. Two case studies are also presented."
Tissenbaum2021,Mike Tissenbaum and David Weintrop and Nathan Holbert and Tamara Clegg,The case for alternative endpoints in computing education,British Journal of Educational Technology,52,3,2021,10.1111/bjet.13072,14678535,"This paper argues for a re-examination of the nature and goals of broad computing education initiatives. Instead of starting with specific values or goals, we instead begin by considering various desired endpoints of computing instruction and then work backward to reason about what form learning activities might take and what are the underlying values and principles that support learners in reaching these endpoints. The result of this exercise is a push for rethinking the form of contemporary computing education with an eye toward more diverse, equitable and meaningful endpoints. With a focus on the role that constructionist-focused pedagogies and designs can play in supporting these endpoints, we examine four distinct cases and the endpoints they support. This paper is not intended to encompass all the possible alternate endpoints for computer science education; rather, this work seeks to start a conversation around the nature of and need for alternate endpoints, as a means to re-evaluate the current tools and curricula to prepare learners for a future of active and empowered computing-literate citizens. Practitioner notes What is already known about this topic There is a growing call for computing education to be a core educational component. Computing education traditionally has a narrow goal of training people for programming jobs. Computing education fails to connect with students underrepresented in STEM. What this paper adds An argument for why we need more and diverse endpoints to computing education. That many possible endpoints for computing education can be more inclusive, just and equitable than software engineering. Constructionism is a particularly useful paradigm for approaching and supporting alternate endpoints. Implications for practice and policy Helps reframe the goals of computing education, to truly be “for all.” Provides a set of cases for how this reframing can be achieved. Gives policy new lenses for understanding, evaluating and implementing computing education."
Rahimi2020,Nouf Rahimi and Fathy Eassa and Lamiaa Elrefaei,An ensemble machine learning technique for functional requirement classification,Symmetry,12,10,2020,10.3390/sym12101601,20738994,"In Requirement Engineering, software requirements are classified into two main categories: Functional Requirement (FR) and Non‐Functional Requirement (NFR). FR describes user and system goals. NFR includes all constraints on services and functions. Deeper classification of those two categories facilitates the software development process. There are many techniques for classifying FR; some of them are Machine Learning (ML) techniques, and others are traditional. To date, the classification accuracy has not been satisfactory. In this paper, we introduce a new ensemble ML technique for classifying FR statements to improve their accuracy and availability. This technique combines different ML models and uses enhanced accuracy as a weight in the weighted ensemble voting approach. The five combined models are Naïve Bayes, Support Vector Machine (SVM), Decision Tree, Logistic Regression, and Support Vector Classification (SVC). The technique was implemented, trained, and tested using a collected dataset. The accuracy of classifying FR was 99.45%, and the required time was 0.7 s."
Zhai2020,Juan Zhai and Xiangzhe Xu and Yu Shi and Guanhong Tao and Minxue Pan and Shiqing Ma and Lei Xu and Weifeng Zhang and Lin Tan and Xiangyu Zhang,CPC: Automatically classifying and propagating natural language comments via program analysis,,,,2020,10.1145/3377811.3380427,02705257,"Code comments provide abundant information that have been leveraged to help perform various software engineering tasks, such as bug detection, speciication inference, and code synthesis. However, developers are less motivated to write and update comments, making it infeasible and error-prone to leverage comments to facilitate software engineering tasks. In this paper, we propose to leverage program analysis to systematically derive, reine, and propagate comments. For example, by propagation via program analysis, comments can be passed on to code entities that are not commented such that code bugs can be detected leveraging the propagated comments. Developers usually comment on diferent aspects of code elements like methods, and use comments to describe various contents, such as functionalities and properties. To more efectively utilize comments, a ine-grained and elaborated taxonomy of comments and a reliable classiier to automatically categorize a comment are needed. In this paper, we build a comprehensive taxonomy and propose using program analysis to propagate comments. We develop a prototype CPC, and evaluate it on 5 projects. The evaluation results demonstrate 41573 new comments can be derived by propagation from other code locations with 88% accuracy. Among them, we can derive precise functional comments for 87 native methods that have neither existing comments nor source code. Leveraging the propagated comments, we detect 37 new bugs in open source large projects, 30 of which have been conirmed and ixed by developers, and 304 defects in existing comments (by looking at inconsistencies between existing and propagated comments), including 12 incomplete comments and 292 wrong comments. This demonstrates the efectiveness of our approach. Our user study conirms propagated comments align well with existing comments in terms of quality."
Ferko2022,Enxhi Ferko and Alessio Bucaioni and Moris Behnam,Architecting Digital Twins,IEEE Access,10,,2022,10.1109/ACCESS.2022.3172964,21693536,"In 2002, Grieves defined the concept of the digital twin as a virtual instance of physical assets capable of continuously mirroring them. Ever since then, driven by remarkable industrial attention, digital twins flourished and ripened in several sectors. The notable industrial adoption has been sided by a growing interest from the software engineering community in general and the software architecture community in particular as demonstrated by the growing number of published peer-reviewed publications and proposed software architectural solutions for digital twins. In this paper, we report on the planning, execution, and results of a systematic mapping study on architecting digital twins. The study captures crucial aspects of software architectures for digital twins as types of architectural solutions, quality attributes, and architectural patterns. It supports practitioners in creating digital twins tailored to their specific needs and researchers in identifying trends and open challenges. Starting from an initial set of potentially relevant 1630 peer-reviewed publications, we selected 140 primary studies. We analysed the set of primary studies using thorough data extraction, analysis, and synthesis process. To compensate for single method limitations and reduce possible threats to conclusion validity, we discussed the results of our study with experts in the software architecture community. Based on our results, the field of software architecture for digital twins is lively and an increasing number of architectural solutions are being proposed. Although there is a lack of widely accepted reference architectural solutions for digital twins, most of them are built using a combination of the layered and service-oriented patterns and address maintainability, performance efficiency, and compatibility quality attributes."
Kassaymeh2022,Sofian Kassaymeh and Salwani Abdullah and Mohammed Azmi Al-Betar and Mohammed Alweshah,Salp swarm optimizer for modeling the software fault prediction problem,Journal of King Saud University - Computer and Information Sciences,34,6,2022,10.1016/j.jksuci.2021.01.015,22131248,"This paper proposes the salp swarm algorithm (SSA) combined with a backpropagation neural network (BPNN) to solve the software fault prediction (SFP) problem. The SFP problem is one of the well-known software engineering problems that are concerned with anticipating the software defects that are likely to appear during a software project or thereafter. In order to find the optimal BPNN parameters, a combination of SSA optimizer and BPNN named (SSA-BPNN) is proposed, so as to enhance prediction accuracy. The proposed method is evaluated against several datasets for the SFP problem. These datasets vary in both size and complexity. The results obtained are evaluated using a variety of performance measures (i.e., the AUC, Confusion Matrix, Sensitivity, Specificity, Accuracy, and Error Rate). The results obtained by SSA-BPNN are better than those obtained by the conventional BPNN over all of the datasets. The proposed method also has the ability to outperform several state-of-the-art methods over the same datasets in respect of most of the aforementioned performance measures. Therefore, the hybridization of SSA with BPNN is a valuable addition to the software engineering issues and can be utilized to achieve higher prediction accuracy for a variety of prediction problems."
Snchez-Gmez2020,Nicolás Sánchez-Gómez and Jesus Torres-Valderrama and J. A. García-García and Javier J. Gutiérrez and M. J. Escalona,Model-based software design and testing in blockchain smart contracts: A systematic literature review,IEEE Access,8,,2020,10.1109/ACCESS.2020.3021502,21693536,"Blockchain technology promises to spark a real revolution. One of most important concepts associated with this technology is smart contracts, which enable the automatic execution of agreements and augur a world without intermediaries. The conditions and rules of ""contracts"" are established in a computer codes and trust is enforced by consensus among the participants. One relevant feature associated with smart contract is the immutability property, which establishes the non-Alteration of blockchain network data after the clauses of the contract are been approved by all parties or entities involved. For this reason, smart contract development requires more effort and care than the development of other common programs. They require systematic mechanisms to collect requirements and functional specifications. In addition, it is necessary to verify and validate the agreed functionality and the implemented code before they are deployed in the blockchain platform. This article presents a systematic literature review of primary studies in the field of Software Development Life Cycle, focusing on model-based software design and testing in the blockchain domain of smart contracts. This research aims to identify gaps and/or opportunities for further research. After carried out this review, it was observed that no clear methodology exists for evaluating and validating the quality either of this software or the overall development process. This means that software developers may implement smart contract code in which bugs and serious security vulnerabilities appear when the software is delivered to their customers."
Manjunatha2022,M. Manjunatha and Dinesh Seth and Balaji KVGD and Bharath A,Engineering properties and environmental impact assessment of green concrete prepared with PVC waste powder: A step towards sustainable approach,Case Studies in Construction Materials,17,,2022,10.1016/j.cscm.2022.e01404,22145095,"The primary objective of this study is to offer a comparative picture to assess the potential environmental impact and to reduce the energy required for concrete preparation. For this investigation, concrete is prepared with industrial wastes like PVC waste powder (PWP) and ground granulated blast furnace slag (GGBS) vis-a-vis conventional concrete. This investigation mainly focuses on the experimental study and environmental impacts of green versus conventional concrete using the life cycle assessment (LCA) approach. LCA models are prepared for green concrete by considering industrial waste by-products materials like GGBS and PWP. For this study, green concrete is considered by using 30% constant GGBS for all the mixes. In contrast, PWP is considered in varying percentages like 0%, 5%, 10%, 15%, 20%, 25%, and 30% by weight of cement and compared with conventional concrete prepared. To compare the environmental impacts of green and conventional concrete, initially, authors compared the physical and chemical properties of materials, fresh and hardened properties of concrete for all the mixes. Based on test results, LCA models are prepared by using SimaPro software and the Ecoinvent database. The originality of this study lies in offering a comparative picture for researchers and practitioners. Experimental investigation of this study exhibits that concrete prepared with PWP (15–20%) and fixed contents of GGBS helps to improve the structural performance and durability properties of green concrete. This investigation also indicates that green concrete prepared with PWP and GGBS helps to reduce environmental damage substantially."
Gatto2021,Maria Laura Gatto and Riccardo Groppo and Nora Bloise and Lorenzo Fassina and Livia Visai and Manuela Galati and Luca Iuliano and Paolo Mengucci,"Topological, mechanical and biological properties of Ti6Al4V scaffolds for bone tissue regeneration fabricated with reused powders via electron beam melting",Materials,14,1,2021,10.3390/ma14010224,19961944,"Cellularized scaffold is emerging as the preferred solution for tissue regeneration and restoration of damaged functionalities. However, the high cost of preclinical studies creates a gap between investigation and the device market for the biomedical industry. In this work, bone-tailored scaffolds based on the Ti6Al4V alloy manufactured by electron beam melting (EBM) technology with reused powder were investigated, aiming to overcome issues connected to the high cost of preclinical studies. Two different elementary unit cell scaffold geometries, namely diamond (DO) and rhombic dodecahedron (RD), were adopted, while surface functionalization was performed by coating scaffolds with single layers of polycaprolactone (PCL) or with mixture of polycaprolactone and 20 wt.% hydroxyapatite (PCL/HA). The mechanical and biological performances of the produced scaffolds were investigated, and the results were compared to software simulation and experimental evidence available in literature. Good mechanical properties and a favorable environment for cell growth were obtained for all combinations of scaffold geometry and surface functionalization. In conclusion, powder recycling provides a viable practice for the biomedical industry to strongly reduce preclinical costs without altering biomechanical performance."
Jawad2022,Muhammad Jawad and Ziad Khan and Ebenezer Bonyah and Rashid Jan,Analysis of Hybrid Nanofluid Stagnation Point Flow over a Stretching Surface with Melting Heat Transfer,Mathematical Problems in Engineering,2022,,2022,10.1155/2022/9469164,15635147,"The behavior of hybrid nanofluid and stagnation point flow toward a stretched surface along with melting heat transfer, second-order slip, electric field, and magnetic field effect is investigated in this study. Hybrid nanoparticles alumina Al2O3 and copper (Cu) are considered with the base fluids water H2O. The PDEs with corresponding boundary constraints are transformed into a set of nonlinear ODEs using similarities transformation. The set of nonlinear ODEs are analyzed analytically using semianalytical method HAM in Mathematica software. Dual solution is determined which relaying on the emerging parameters included magnetic field, volume fractions, electric field, dimensionless surface velocity slip factors, Eckert number, and Prandtl number. The results are shown in the velocity and temperature curves as well as skin friction coefficient and local Nusselt number. The analysis shows that velocity profile is an increasing function of slip parameter, electric field, while reducing function of magnetic field. Temperature profile is an increasing function of magnetic field parameter, electric field parameter, volume fraction parameter, and Eckert number, while reducing function of Prandtl number. The main outcomes are as follows that hybrid nanofluids are higher thermal properties as compared to conventional fluids. As a result, hybrid nanofluid has numerous uses in engineering cosmetics, automotive industry, home industry, for cancer treatment, food packaging, pharmaceuticals, fabrics, paper plastics, paints, ceramics, food colorants, and soaps as well."
Flood2021,Matthew W. Flood and Bernd Grimm,EntropyHub: An open-source toolkit for entropic time series analysis,PLoS ONE,16,11 November,2021,10.1371/journal.pone.0259448,19326203,"An increasing number of studies across many research fields from biomedical engineering to finance are employing measures of entropy to quantify the regularity, variability or randomness of time series and image data. Entropy, as it relates to information theory and dynamical systems theory, can be estimated in many ways, with newly developed methods being continuously introduced in the scientific literature. Despite the growing interest in entropic time series and image analysis, there is a shortage of validated, open-source software tools that enable researchers to apply these methods. To date, packages for performing entropy analysis are often run using graphical user interfaces, lack the necessary supporting documentation, or do not include functions for more advanced entropy methods, such as cross-entropy, multiscale cross-entropy or bidimensional entropy. In light of this, this paper introduces EntropyHub, an open-source toolkit for performing entropic time series analysis in MATLAB, Python and Julia. EntropyHub (version 0.1) provides an extensive range of more than forty functions for estimating cross-, multiscale, multiscale cross-, and bidimensional entropy, each including a number of keyword arguments that allows the user to specify multiple parameters in the entropy calculation. Instructions for installation, descriptions of function syntax, and examples of use are fully detailed in the supporting documentation, available on the EntropyHub website– www.EntropyHub.xyz. Compatible with Windows, Mac and Linux operating systems, EntropyHub is hosted on GitHub, as well as the native package repository for MATLAB, Python and Julia, respectively. The goal of EntropyHub is to integrate the many established entropy methods into one complete resource, providing tools that make advanced entropic time series analysis straightforward and reproducible."
Quartier2021,Nicolas Quartier and Alejandro J.C. Crespo and José M. Domínguez and Vasiliki Stratigaki and Peter Troch,Efficient response of an onshore Oscillating Water Column Wave Energy Converter using a one-phase SPH model coupled with a multiphysics library,Applied Ocean Research,115,,2021,10.1016/j.apor.2021.102856,01411187,"In this paper the numerical modelling of an Oscillating Water Column (OWC) Wave Energy Converter (WEC) is studied using DualSPHysics, a software that applies the Smoothed Particle Hydrodynamics (SPH) method. SPH is a Lagrangian meshless method used in a growing range of applications within the field of Computational Fluid Dynamics (CFD). The power take-off (PTO) system of the OWC WEC is numerically modelled by adding a force on a plate floating on top of the free surface inside the OWC chamber. That force is implemented in the multiphysics library Project Chrono, which avoids the need of simulating the air phase that is computationally expensive in the SPH methods. Validation is carried out with experimental data received from the Korea Research Institute of Ship and Ocean Engineering (KRISO) and Ocean Energy Systems (OES) of the International Energy Agency (IEA) Task 10. The numerical and experimental water surface elevation at the centre of the OWC WEC chamber and the airflow speed through the orifice are compared for different wave conditions and different PTO systems (different orifice diameters at the top part of the chamber of the OWC WEC). Results show that DualSPHysics is a valid tool to model an OWC WEC with and without PTO system, even though no air phase is included."
Pasko2021,Oleh Pasko and Fuli Chen and Alvina Oriekhova and Alina Brychko and Iryna Shalyhina,Mapping the literature on sustainability reporting: A bibliometric analysis grounded in scopus and web of science core collection,European Journal of Sustainable Development,10,1,2021,10.14207/ejsd.2021.v10n1p303,22396101,"Sustainability reporting has become an increasingly common practice among companies around the globe as around 90% of the world’s 250 largest companies from Fortune 500 prepare and publish its sustainability reporting. Aiming to help researchers to grasp the intellectual landscape of global research on sustainable reporting, we conducted a bibliometric analysis using CiteSpace software by applying evaluative and relational techniques to 928 articles published in 480 different journals in Scopus and 698 articles published in 374 different journals in Web of Science Core Collection from 1981 to 2020. Our findings indicate that the number of articles published in the field has increased rapidly, especially since 2009. We identified the leading countries (the United States, Australia, the United Kingdom, Germany, Spain, Canada, the Netherlands and Italy), the most prolific journals (Journal of Cleaner Production, Business Strategy and the Environment, Journal of Business Ethics), main journals categories (Business, Economics, Management and Finance, Environmental and Ecology and Science, Technology and Engineering), and the major research directions in the near future (sustainability reporting, corporate social responsibility, sustainable development, disclosure). From our findings we infer that the sustainability reporting research has just recently (2013-2019) gained traction in the literature. Moreover, our findings testify that a kind of bifurcation point has occurred is 2011 that manifests the maturity of the field of sustainability reporting. All this provides the reader with a high-view look at sustainable reporting as these quantitative findings complementing qualitative and providing valuable insights into the field."
Aggarwal2021,Veena Aggarwal and Christina Maslen and Richard L. Abel and Pinaki Bhattacharya and Paul A. Bromiley and Emma M. Clark and Juliet E. Compston and Nicola Crabtree and Jennifer S. Gregory and Eleni P. Kariki and Nicholas C. Harvey and Kate A. Ward and Kenneth E.S. Poole,"Opportunistic diagnosis of osteoporosis, fragile bone strength and vertebral fractures from routine CT scans; a review of approved technology systems and pathways to implementation",Therapeutic Advances in Musculoskeletal Disease,13,,2021,10.1177/1759720X211024029,17597218,"Osteoporosis causes bones to become weak, porous and fracture more easily. While a vertebral fracture is the archetypal fracture of osteoporosis, it is also the most difficult to diagnose clinically. Patients often suffer further spine or other fractures, deformity, height loss and pain before diagnosis. There were an estimated 520,000 fragility fractures in the United Kingdom (UK) in 2017 (costing £4.5 billion), a figure set to increase 30% by 2030. One way to improve both vertebral fracture identification and the diagnosis of osteoporosis is to assess a patient’s spine or hips during routine computed tomography (CT) scans. Patients attend routine CT for diagnosis and monitoring of various medical conditions, but the skeleton can be overlooked as radiologists concentrate on the primary reason for scanning. More than half a million CT scans done each year in the National Health Service (NHS) could potentially be screened for osteoporosis (increasing 5% annually). If CT-based screening became embedded in practice, then the technique could have a positive clinical impact in the identification of fragility fracture and/or low bone density. Several companies have developed software methods to diagnose osteoporosis/fragile bone strength and/or identify vertebral fractures in CT datasets, using various methods that include image processing, computational modelling, artificial intelligence and biomechanical engineering concepts. Technology to evaluate Hounsfield units is used to calculate bone density, but not necessarily bone strength. In this rapid evidence review, we summarise the current literature underpinning approved technologies for opportunistic screening of routine CT images to identify fractures, bone density or strength information. We highlight how other new software technologies have become embedded in NHS clinical practice (having overcome barriers to implementation) and highlight how the novel osteoporosis technologies could follow suit. We define the key unanswered questions where further research is needed to enable the adoption of these technologies for maximal patient benefit."
Mamun2021,Abdulla Al Mamun and Nur Hasan Mahmud Shahen and Samsun Nahar Ananna and Md Asaduzzaman and Foyjonnesa,Solitary and periodic wave solutions to the family of new 3D fractional WBBM equations in mathematical physics,Heliyon,7,7,2021,10.1016/j.heliyon.2021.e07483,24058440,"For the newly implemented 3D fractional Wazwaz-Benjamin-Bona-Mahony (WBBM) equation family, the present study explores the exact singular, solitary, and periodic singular wave solutions via the (G′/G2)-expansion process. In the sense of conformable derivatives, the equations considered are translated into ordinary differential equations. In spite with many trigonometric, complex hyperbolic, and rational functions, some fresh exact singular, solitary, and periodic wave solutions to the deliberated equations in fractional systems are attained by the implementation of the (G′/G2)-expansion technique through the computational software Mathematica. The unique solutions derived by the process defined are articulated with the arrangement of the functions tanh, sech; tan, sec; coth, cosech, and cot, cosec. With three-dimensional (3D), two dimensional (2D) and contour graphics, some of the latest solutions created have been envisaged, selecting appropriate arbitrary constraints to illustrate their physical representation. The outcomes were obtained to determine the power of the completed technique to calculate the exact solutions of the equations of the WBBM that can be used to apply the nonlinear water model in the ocean and coastal engineering. All the solutions given have been certified by replacing their corresponding equations with the computational software Mathematica."
Ahmad2020,Arshad Ahmad and Chong Feng and Muzammil Khan and Asif Khan and Ayaz Ullah and Shah Nazir and Adnan Tahir,A Systematic Literature Review on Using Machine Learning Algorithms for Software Requirements Identification on Stack Overflow,Security and Communication Networks,2020,,2020,10.1155/2020/8830683,19390122,"Context. The improvements made in the last couple of decades in the requirements engineering (RE) processes and methods have witnessed a rapid rise in effectively using diverse machine learning (ML) techniques to resolve several multifaceted RE issues. One such challenging issue is the effective identification and classification of the software requirements on Stack Overflow (SO) for building quality systems. The appropriateness of ML-based techniques to tackle this issue has revealed quite substantial results, much effective than those produced by the usual available natural language processing (NLP) techniques. Nonetheless, a complete, systematic, and detailed comprehension of these ML based techniques is considerably scarce. Objective. To identify or recognize and classify the kinds of ML algorithms used for software requirements identification primarily on SO. Method. This paper reports a systematic literature review (SLR) collecting empirical evidence published up to May 2020. Results. This SLR study found 2,484 published papers related to RE and SO. The data extraction process of the SLR showed that (1) Latent Dirichlet Allocation (LDA) topic modeling is among the widely used ML algorithm in the selected studies and (2) precision and recall are amongst the most commonly utilized evaluation methods for measuring the performance of these ML algorithms. Conclusion. Our SLR study revealed that while ML algorithms have phenomenal capabilities of identifying the software requirements on SO, they still are confronted with various open problems/issues that will eventually limit their practical applications and performances. Our SLR study calls for the need of close collaboration venture between the RE and ML communities/researchers to handle the open issues confronted in the development of some real world machine learning-based quality systems."
DeGaetani2020,Carlo Iapige De Gaetani and Mertkan Mert and Federica Migliaccio,Interoperability analyses of BIM platforms for construction management,Applied Sciences (Switzerland),10,13,2020,10.3390/app10134437,20763417,"It is incontrovertible that an exchange of files is essentially required at several stages of the workflow in the architecture, engineering, and construction (AEC) industry. Therefore, investigating and detecting the capabilities/inabilities of building information modeling (BIM) software packages with respect to interoperability can be informative to stakeholders who exchange data between various BIM packages. The work presented in this paper includes a discussion on the interoperability of different software platforms commonly used in the AEC industry. Although, in theory, flawless interoperability of some types of files between different BIM platforms is ensured, in practical applications, this is not always the case. Hence, this research aims to identify faults in data exchange by assessing different possible scenarios where a sample Industry Foundation Classes (IFC) four-dimensions (4D) BIM model and related Gantt charts are exchanged. Throughout the interoperability analysis of both IFC file and Gantt charts, the following checks were carried out: geometrical and nongeometrical information exchange through IFC files, 4D information correct readability, and presence of missing schedule information in Gantt charts after their import/export procedure. The results show that interoperability between the analyzed platforms is not always ensured, providing useful insight into realistic scenarios."
Choi2020,Seoyun Choi and Jong Hyouk Lee,Blockchain-Based Distributed Firmware Update Architecture for IoT Devices,IEEE Access,8,,2020,10.1109/ACCESS.2020.2975920,21693536,"The Internet of Things (IoT) which creates a hyper-connected society is playing a major role in the 4th industrial revolution. The IoT is being leveraged across various fields of business globally and the number of IoT devices is causing serious security concerns. Since the firmware update of an IoT device is necessary for its lifecycle, secure firmware update of the IoT device is being brought as the first step in IoT security. The Internet Engineering Task Force (IETF) Software Updates for Internet of Things (SUIT) working group has started to specify a software update architecture for IoT devices. However, the current SUIT working group adopts a traditional client-server model to distribute firmware images, which potentially causes security risks. The current approach of the SUIT working group is unable to solve a targeting issue and an author-disappearing issue, which is suggested in this paper. Therefore, in this work, we introduce a distributed firmware update architecture based on the SUIT firmware update architecture applying blockchain. Our update architecture can prevent the issues we concern through the characteristics of blockchain, such as decentralization, transparency, and irreversibility. The blockchain network has registration nodes that process registration of manifest and firmware image files from authors, and retrieval nodes that process downloading manifest and firmware image files. The firmware image files are stored in a distributed file system and the hash values of firmware image chunks are stored on the blockchain with manifest files. The proposed architecture in this paper enables the irreversible downloads even in the author-disappearing state and tolerant to a single point of failure."
Karalekas2020,Georgios Karalekas and Stavros Vologiannidis and John Kalomiros,"Europa: A case study for teaching sensors, data acquisition and robotics via a ROS-based educational robot",Sensors (Switzerland),20,9,2020,10.3390/s20092469,14248220,"Robots have become a popular educational tool in secondary education, introducing scientific, technological, engineering and mathematical concepts to students all around the globe. In this paper EUROPA, an extensible, open software and open hardware robotic platform is presented focusing on teaching physics, sensors, data acquisition and robotics. EUROPA’s software infrastructure is based οn Robot Operating System (ROS). It includes easy to use interfaces for robot control and interaction with users and thus can easily be incorporated in Science, Technology, Engineering and Mathematics (STEM) and robotics classes. EUROPA was designed taking into account current trends in educational robotics. An overview of widespread robotic platforms is presented, documenting several critical parameters of interest such as their architecture, sensors, actuators and controllers, their approximate cost, etc. Finally, an introductory STEM curriculum developed for EUROPA and applied in a class of high school students is presented."
Li2020,Ke Li and Zilin Xiang and Tao Chen and Shuo Wang and Kay Chen Tan,Understanding the automated parameter optimization on transfer learning for cross-project defect prediction: An empirical study,,,,2020,10.1145/3377811.3380360,02705257,"Data-driven defect prediction has become increasingly important in software engineering process. Since it is not uncommon that data from a software project is insufficient for training a reliable defect prediction model, transfer learning that borrows data/konwledge from other projects to facilitate the model building at the current project, namely cross-project defect prediction (CPDP), is naturally plausible. Most CPDP techniques involve two major steps, i.e., transfer learning and classification, each of which has at least one parameter to be tuned to achieve their optimal performance. This practice fits well with the purpose of automated parameter optimization. However, there is a lack of thorough understanding about what are the impacts of automated parameter optimization on various CPDP techniques. In this paper, we present the first empirical study that looks into such impacts on 62 CPDP techniques, 13 of which are chosen from the existing CPDP literature while the other 49 ones have not been explored before. We build defect prediction models over 20 real-world software projects that are of different scales and characteristics. Our findings demonstrate that: (1) Automated parameter optimization substantially improves the defect prediction performance of 77% CPDP techniques with a manageable computational cost. Thus more efforts on this aspect are required in future CPDP studies. (2) Transfer learning is of ultimate importance in CPDP. Given a tight computational budget, it is more cost-effective to focus on optimizing the parameter configuration of transfer learning algorithms (3) The research on CPDP is far from mature where it is not difficult' to find a better alternative by making a combination of existing transfer learning and classification techniques. This finding provides important insights about the future design of CPDP techniques."
Gain2021,Ulla Gain and Virpi Hotti,Low-code AutoML-augmented data pipeline – A review and experiments,,1828,1,2021,10.1088/1742-6596/1828/1/012015,17426596,"There is a lack of knowledge concerning the low-code autoML (automated machine learning) frameworks that can be used to enrich data for several purposes concerning either data engineering or software engineering. In this paper, 34 autoML frameworks have been reviewed based on the latest commits and augmentation properties of their GitHub content. The PyCaret framework was the result of the review due to requirements concerning adaptability by Google Colaboratory (Colab) and the BI (business intelligence) tool. Finally, the low-code autoML-augmented data pipeline from raw data to dashboards and low-code apps has been drawn based on the experiments concerned classifications of the ""Census Income"" dataset. The constructed pipeline preferred the same data to be a ground for different reports, dashboards, and applications. However, the constructed low-code autoML-augmented data pipeline contains changeable building blocks such as libraries and visualisations."
Pierro2020,Giuseppe Antonio Pierro and Roberto Tonelli and Michele Marchesi,An organized repository of ethereum smart contracts’ source codes and metrics,Future Internet,12,11,2020,10.3390/fi12110197,19995903,"Many empirical software engineering studies show that there is a need for repositories where source codes are acquired, filtered and classified. During the last few years, Ethereum block explorer services have emerged as a popular project to explore and search for Ethereum blockchain data such as transactions, addresses, tokens, smart contracts’ source codes, prices and other activities taking place on the Ethereum blockchain. Despite the availability of this kind of service, retrieving specific information useful to empirical software engineering studies, such as the study of smart contracts’ software metrics, might require many subtasks, such as searching for specific transactions in a block, parsing files in HTML format, and filtering the smart contracts to remove duplicated code or unused smart contracts. In this paper, we afford this problem by creating Smart Corpus, a corpus of smart contracts in an organized, reasoned and up-to-date repository where Solidity source code and other metadata about Ethereum smart contracts can easily and systematically be retrieved. We present Smart Corpus’s design and its initial implementation, and we show how the data set of smart contracts’ source codes in a variety of programming languages can be queried and processed to get useful information on smart contracts and their software metrics. Smart Corpus aims to create a smart-contract repository where smart-contract data (source code, application binary interface (ABI) and byte code) are freely and immediately available and are classified based on the main software metrics identified in the scientific literature. Smart contracts’ source codes have been validated by EtherScan, and each contract comes with its own associated software metrics as computed by the freely available software PASO. Moreover, Smart Corpus can be easily extended as the number of new smart contracts increases day by day."
Malik2020,Bilal Tariq Malik and Viktor Doychinov and Ali Mohammad Hayajneh and Syed Ali Raza Zaidi and Ian D. Robertson and Nutapong Somjit,Wireless power transfer system for battery-less sensor nodes,IEEE Access,8,,2020,10.1109/ACCESS.2020.2995783,21693536,"For the first time, the design and implementation of a fully-integrated wireless information and power transfer system, operating at 24 GHz and enabling battery-less sensor nodes, is presented in this paper. The system consists of an RF power source, a receiver antenna array, a rectifier, and a battery-less sensor node which communicates via backscatter modulation at 868 MHz. The rectifier circuits use commercially available Schottky diodes to convert the RF power to DC with a measured efficiency of up to 35%, an improvement of ten percentage points compared with previously reported results. The rectifiers and the receive antenna arrays were jointly designed and optimised, thereby reducing the overall circuit size. The battery-less sensor transmitted data to a base station realised as a GNU Radio flow running on a bladeRF Software Defined Radio module. The whole system was tested in free-space in laboratory conditions and was capable of providing sufficient energy to the sensor node in order to enable operation and wireless communication at a distance of 0.15 metres."
Sestras2020,Paul Sestras and Sanda Roșca and Ștefan Bilașco and Sanda Naș and Stefan M. Buru and Leontina Kovacs and Velibor Spalević and Adriana F. Sestras,"Feasibility assessments using unmanned aerial vehicle technology in heritage buildings: Rehabilitation-restoration, spatial analysis and tourism potential analysis",Sensors (Switzerland),20,7,2020,10.3390/s20072054,14248220,"The Transylvanian region of Romania is a place of rich history since ancient times, where the original natural environment around architectural heritage sites or buildings has not been severely altered by urban development. Unfortunately, many such places are left by the authorities to degrade or totally collapse for lack of funds, vision or initiatives. The current paper addresses the potential of Unmanned Aerial Vehicles (UAVs) in the assessment of a viable and feasible prospect of restoration on a 19th century mansion that belonged to a nobiliary family. UAV use is rising in many industries and has become very popular in the last decade, but for survey engineering and related domains they represent a quantum leap in technology. Integrating UAV-acquired data and structure from motion software, has enabled modern techniques to obtain useful metrics from the field, accurate photorealistic 3D models for visual inspection, structural damage analyses, architectural rehabilitation-restoration, conservation and spatial analysis of the surrounding area. In this work a socio-cultural planning and design process is explored and presented to improve the local community and inclusion in a tourist circuit based on the regional potential, as well as an evaluation of accessibility derived from a vector-raster database that highlights the central position of the cultural heritage in regards to the axis of circulation between the important metropolitan areas and the local tourist attractions. This established workflow of modern topographic and construction measurements is fully integrable into the architectural process, building information modelling, heritage conservation and reconstruction."
Grechi2021,Guglielmo Grechi and Matteo Fiorucci and Gian Marco Marmoni and Salvatore Martino,3d thermal monitoring of jointed rock masses through infrared thermography and photogrammetry,Remote Sensing,13,5,2021,10.3390/rs13050957,20724292,"The study of strain effects in thermally-forced rock masses has gathered growing interest from engineering geology researchers in the last decade. In this framework, digital photogrammetry and infrared thermography have become two of the most exploited remote surveying techniques in engineering geology applications because they can provide useful information concerning ge-omechanical and thermal conditions of these complex natural systems where the mechanical role of joints cannot be neglected. In this paper, a methodology is proposed for generating point clouds of rock masses prone to failure, combining the high geometric accuracy of RGB optical images and the thermal information derived by infrared thermography surveys. Multiple 3D thermal point clouds and a high-resolution RGB point cloud were separately generated and co-registered by acquiring thermograms at different times of the day and in different seasons using commercial software for Structure from Motion and point cloud analysis. Temperature attributes of thermal point clouds were merged with the reference high-resolution optical point cloud to obtain a composite 3D model storing accurate geometric information and multitemporal surface temperature distributions. The quality of merged point clouds was evaluated by comparing temperature distributions derived by 2D thermograms and 3D thermal models, with a view to estimating their accuracy in describing surface thermal fields. Moreover, a preliminary attempt was made to test the feasibility of this approach in investigating the thermal behavior of complex natural systems such as jointed rock masses by analyzing the spatial distribution and temporal evolution of surface temperature ranges under different climatic conditions. The obtained results show that despite the low resolution of the IR sensor, the geometric accuracy and the correspondence between 2D and 3D temperature measurements are high enough to consider 3D thermal point clouds suitable to describe surface temperature distributions and adequate for monitoring purposes of jointed rock mass."
DurnAcevedo2021,Cristhian Manuel Durán Acevedo and Jeniffer Katerine Carrillo Gómez and Camilo Andrés Albarracín Rojas,Academic stress detection on university students during COVID-19 outbreak by using an electronic nose and the galvanic skin response,Biomedical Signal Processing and Control,68,,2021,10.1016/j.bspc.2021.102756,17468108,"Academic stress is an emotion that students experience during their time at the university, sometimes causing physical and mental health effects. Because of the COVID-19 pandemic, universities worldwide have left the classroom to provide the method of teaching virtually, generating challenges, adaptations, and more stress in students. In this pilot study, a methodology for academic stress detection in engineering students at the University of Pamplona (Colombia) is proposed by developing and implementing an artificial electronic nose system and the galvanic skin response. For the study, the student's stress state and characteristics were taken into account to make the data analysis where a set of measurements were acquired when the students were presenting a virtual exam. Likewise, for the non-stress state, a set of measurements were obtained in a relaxation state after the exam date. To carry out the pre-processing and data processing from the measurements obtained previously by both systems, a set of algorithms developed in Python software were used to perform the data analysis. Linear Discriminant Analysis (LDA), K-Nearest Neighbors (K-NN), and Support Vector Machine (SVM) classification methods were applied for the data classification, where a 96 % success rate of classification was obtained with the E-nose, and 100 % classification was achieved by using the Galvanic Skin Response."
Wang2022,Hao Wang and Wenjie Qu and Gilad Katz and Wenyu Zhu and Zeyu Gao and Han Qiu and Jianwei Zhuge and Chao Zhang,JTrans: Jump-aware transformer for binary code similarity detection,,,,2022,10.1145/3533767.3534367,,"Binary code similarity detection (BCSD) has important applications in various fields such as vulnerabilities detection, software component analysis, and reverse engineering. Recent studies have shown that deep neural networks (DNNs) can comprehend instructions or control-flow graphs (CFG) of binary code and support BCSD. In this study, we propose a novel Transformer-based approach, namely jTrans, to learn representations of binary code. It is the first solution that embeds control flow information of binary code into Transformer-based language models, by using a novel jump-aware representation of the analyzed binaries and a newly-designed pre-training task. Additionally, we release to the community a newly-created large dataset of binaries, BinaryCorp, which is the most diverse to date. Evaluation results show that jTrans outperforms state-of-the-art (SOTA) approaches on this more challenging dataset by 30.5% (i.e., from 32.0% to 62.5%). In a real-world task of known vulnerability searching, jTrans achieves a recall that is 2X higher than existing SOTA baselines."
Gopinath2020,Rahul Gopinath and Björn Mathis and Andreas Zeller,Mining input grammars from dynamic control flow,,,,2020,10.1145/3368089.3409679,,"One of the key properties of a program is its input specification. Having a formal input specification can be critical in fields such as vulnerability analysis, reverse engineering, software testing, clone detection, or refactoring. Unfortunately, accurate input specifications for typical programs are often unavailable or out of date. In this paper, we present a general algorithm that takes a program and a small set of sample inputs and automatically infers a readable context-free grammar capturing the input language of the program. We infer the syntactic input structure only by observing access of input characters at different locations of the input parser. This works on all stack based recursive descent input parsers, including parser combinators, and works entirely without program specific heuristics. Our Mimid prototype produced accurate and readable grammars for a variety of evaluation subjects, including complex languages such as JSON, TinyC, and JavaScript."
Rawat2022,Shisrut Rawat and Aishwarya Srinivasan and Vinayakumar Ravi and Uttam Ghosh,Intrusion detection systems using classical machine learning techniques vs integrated unsupervised feature learning and deep neural network,Internet Technology Letters,5,1,2022,10.1002/itl2.232,24761508,"Security analysts and administrators face a lot of challenges to detect and prevent network intrusions in their organizations, and to prevent network breaches, detecting the breach on time is crucial. Challenges arise while detecting unforeseen attacks. This work includes a performance comparison of classical machine learning approaches that require vast feature engineering, vs integrated unsupervised feature learning and deep neural networks on the NSL-KDD dataset. Various trials of experiments were run to identify suitable hyperparameters and network configurations of machine learning models. The DNN using 15 features extracted using Principal Component analysis (PCA) was the most effective modeling method. The further analysis using the Software Defined Networking features also presented a good accuracy using Deep Neural network."
Gregory2020,Joe Gregory and Lucy Berthoud and Theo Tryfonas and Alain Rossignol and Ludovic Faure,The long and winding road: MBSE adoption for functional avionics of spacecraft,Journal of Systems and Software,160,,2020,10.1016/j.jss.2019.110453,01641212,"Model-Based Systems Engineering (MBSE) represents a move away from the traditional approach of Document-Based Systems Engineering (DBSE). It is claimed that MBSE promotes consistency, communication, clarity and maintainability within systems engineering projects and addresses issues associated with cost, complexity and safety. While these potential benefits of MBSE are generally agreed upon by would-be practitioners, its implementation is challenging and many organisations struggle to overcome the cultural and technical hurdles along the long and winding road to MBSE adoption. In this paper, we aim to ease the process of implementation by investigating where the current issues with the existing systems engineering processes lie, and where a model-based approach may be able to help, from the perspective of engineers working on spacecraft functional avionics in Airbus. A repeatable process has been developed to elicit this information. Semi-structured interviews have been conducted with 25 Airbus engineers working in Operations, Software and Failure, Detection, Isolation and Recovery. The acquired data has been thematically analysed to extract common themes from the responses. The results presented in this paper have yielded four recommended application areas to consider when applying MBSE to Functional Avionics: organisation modelling; early functional validation; communication and consistency; template model framework development."
Li2022,Bowen Li and Xin Peng and Qilin Xiang and Hanzhang Wang and Tao Xie and Jun Sun and Xuanzhe Liu,Enjoy your observability: an industrial survey of microservice tracing and analysis,Empirical Software Engineering,27,1,2022,10.1007/s10664-021-10063-9,15737616,"Microservice systems are often deployed in complex cloud-based environments and may involve a large number of service instances being dynamically created and destroyed. It is thus essential to ensure observability to understand these microservice systems’ behaviors and troubleshoot their problems. As an important means to achieve the observability, distributed tracing and analysis is known to be challenging. While many companies have started implementing distributed tracing and analysis for microservice systems, it is not clear whether existing approaches fulfill the required observability. In this article, we present our industrial survey on microservice tracing and analysis through interviewing developers and operation engineers of microservice systems from ten companies. Our survey results offer a number of findings. For example, large microservice systems commonly adopt a tracing and analysis pipeline, and the implementations of the pipeline in different companies reflect different tradeoffs among a variety of concerns. Visualization and statistic-based metrics are the most common means for trace analysis, while more advanced analysis techniques such as machine learning and data mining are seldom used. Microservice tracing and analysis is a new big data problem for software engineering, and its practices breed new challenges and opportunities."
Razzaq2020,Abdul Razzaq,A Systematic Review on Software Architectures for IoT Systems and Future Direction to the Adoption of Microservices Architecture,SN Computer Science,1,6,2020,10.1007/s42979-020-00359-w,26618907,"The Internet of Things-based systems and software allow computations anywhere at any time by interconnecting individuals, networks, services, computers and artefacts that allow autonomous systems to form digitized communities. As the blueprint for software-intensive applications, and software architecture that precise the complexity of a network’s planning, development, and changing phases to effectively and efficiently build complex IoT-driven applications. In any case, there exists no comprehensive analysis in the state of the research on the adoption of MSA for IoT systems. This study effort is needed to explore architectural concepts and practices for designing and developing IoT software to excel state-of-the-art for IoTs along with suggestions and recommendations for IoT software to the adoption of MSA to fulfil the identified gaps. A systematic analysis was coordinated, covering up the literature on existing IoT solutions by studying 140 qualitatively selected articles performed between 2005 and Jan 2020. One hundred forty articles were comprised in this SLR. The findings of this study demonstrated different research topics including software architectural styles, patterns, and models to build IoT software. This research presents cloud-based computing environments, autonomous, software-defined networking, and responsive applications, and IoT-driven agent-based systems, (1) thirteen MSA architectural and design patterns for IoTs and classification of patterns, (2) classification of software architectures for IoTs into nine main categories and their sub-categories, (3) twenty-three most investigated IoT challenges, and (4) mapping of IoT challenges with software architectural solutions. The study revealed the innovative work on IoT software architecture and trends that help in the creation and dynamic adaptation of IoT software for reusability, automation and human decision-making. The outputs of this SLR are useful in revealing many recommendations to the software industry, software engineering community, and computer sciences community with over the past 15 years of research into the adoption of MSA. This study reflects a distilled awareness of architectural practices and principles to assist researchers and practitioners in promoting information sharing software architectural roles and responsibilities for the Internet of Things software."
Rodrguez2021,Marta Videras Rodríguez and Sergio Gómez Melgar and Antonio Sánchez Cordero and José Manuel Andújar Márquez,A critical review of unmanned aerial vehicles (Uavs) use in architecture and urbanism: Scientometric and bibliometric analysis,Applied Sciences (Switzerland),11,21,2021,10.3390/app11219966,20763417,"In recent years the use of UAVs (Unmanned aerial vehicles) have proliferated in the civil sector for purposes such as search and rescue, remote sensing or real-time monitoring of road traffic, among others. In the architecture, engineering and construction fields (AEC) UAVs have demonstrated to be an ideal technology due to their optimal performance in terms of time, precision, safety and cost. Given the rapid growth of interest in this technology, this research presents a critical review of the literature on the use of UAVs in architecture and urbanism to define the most widely used techniques and delimit the fields of application based on the experimentation published by the scientific community. A scientific mapping was carried out in two stages using the VOSviewer™ software: a scientometric and a bibliometric analysis. This technique allowed us to analyse a large body of literature and bibliographic data to obtain trends, patterns and directions of this domain of knowledge. Then, a literature review was presented, highlighting the relevant information identified in the previous analysis. The fields of application of UAVs were delimited and the most commonly used payload types and the most appropriate post-processing techniques were specified, depending on the aerial mission objective. The fields of application identified included different techniques related to the generation of 3D models, land mapping, construction site monitoring, building surveying to detect structural damage and energy losses and urban remote sensing. The literature review showed that UAVs provide a useful multi-tasking tool at any stage of an architectural project. These techniques can be applied to buildings or public spaces from the design and construction processes when the project is initiated to the later stages of maintenance and inspection of the building during its life cycle."
Yao2022,Shao Wen Yao and Sidheawar Behera and Mustafa Inc and Hadi Rezazadeh and Jasvinder Pal Singh Virdi and W. Mahmoud and Omar Abu Arqub and M. S. Osman,Analytical solutions of conformable Drinfel'd–Sokolov–Wilson and Boiti Leon Pempinelli equations via sine–cosine method,Results in Physics,42,,2022,10.1016/j.rinp.2022.105990,22113797,"In this paper, we studied the Drinfel'd–Sokolov–Wilson equation (DSWE) and Boiti Leon Pempinelli equation (BLPE) in the conformable sense. The sine–cosine method is utilized to achieve various traveling wave solutions to the suggested nonlinear systems. It is an easy approach to use and does not require sophisticated mathematical software or a knowledgeable coder. It can also be used for various linear and nonlinear fractional issues, making it pervasive. The obtained solutions in the form of solitons emerge with the necessary constraints to ensure their existence. The obtained results hold significant role in elucidating some important nonlinear problems in applied sciences and engineering."
Mohtaram2021,Soheil Mohtaram and Yonghui Sun and Mohammad Omidi and Ji Lin,Energy-exergy efficiencies analyses of a waste-to-power generation system combined with an ammonia-water dilution Rankine cycle,Case Studies in Thermal Engineering,25,,2021,10.1016/j.csite.2021.100909,2214157X,"This research investigates a waste-to-energy system combined with an ammonia dilution Rankine cycle. It uses cooling-energy recycling of dissolved natural-gas to reduce the working-fluid temperature. This project includes providing a solution for calculating the calorific value of waste by proposing a novel waste incineration power generation combined cycle. The energy-exergy analyses is utilized to study the effects of critical parameters on the efficiency and determines the points with higher efficiencies than the conventional steam Rankine cycle. Engineering Equation Solver (EES) software is used for delivering such accurate outlines. The ammonia dilution distillation temperature and the turbine's inlet and outlet pressures are considered vital parameters. The results revealed that mutually energy and exergy efficiencies rise as the ammonia solution's distillation temperature decreases. Besides, by increasing the inlet turbine pressure, the energy and exergy efficiencies increase. In addition, as the output pressure of the turbine increases, the combined cycle's energy efficiency decreases while the exergy efficiency increases."
Borrego-Carazo2020,Juan Borrego-Carazo and David Castells-Rufas and Ernesto Biempica and Jordi Carrabina,Resource-Constrained Machine Learning for ADAS: A Systematic Review,IEEE Access,8,,2020,10.1109/ACCESS.2020.2976513,21693536,"The advent of machine learning (ML) methods for the industry has opened new possibilities in the automotive domain, especially for Advanced Driver Assistance Systems (ADAS). These methods mainly focus on specific problems ranging from traffic sign and light recognition to pedestrian detection. In most cases, the computational resources and power budget found in ADAS systems are constrained while most machine learning methods are computationally intensive. The usual solution consists in adapting the ML models to comply with the memory and real-time (RT) requirements for inference. Some models are easily adapted to resource-constrained hardware, such as Support Vector Machines, while others, like Neural Networks, need more complex processes to fit into the desired hardware. The ADAS hardware (HW platforms) are diverse, from complex MPSoC CPUs down to classical MCUs, DPSs and application-specific FPGAs and ASICs or specific GPU platforms (such as the NVIDIA families Tegra or Jetson). Therefore, there is a tradeoff between the complexity of the ML model implemented and the selected platform that impacts the performance metrics: function results, energy consumption and speed (latency and throughput). In this paper, a survey in the form of systematic review is conducted to analyze the scope of the published research works that embed ML models into resource-constrained implementations for ADAS applications and what are the achievements regarding the ML performance, energy and speed trade-off."
Fang2022,Jielun Fang and Yanfeng Zhuang and Kailang Liu and Zhuo Chen and Zhou Liu and Tiantian Kong and Jianhong Xu and Cheng Qi,A Shift from Efficiency to Adaptability: Recent Progress in Biomimetic Interactive Soft Robotics in Wet Environments,Advanced Science,9,8,2022,10.1002/advs.202104347,21983844,"Research field of soft robotics develops exponentially since it opens up many imaginations, such as human-interactive robot, wearable robots, and transformable robots in unpredictable environments. Wet environments such as sea and in vivo represent dynamic and unstructured environments that adaptive soft robots can reach their potentials. Recent progresses in soft hybridized robotics performing tasks underwater herald a diversity of interactive soft robotics in wet environments. Here, the development of soft robots in wet environments is reviewed. The authors recapitulate biomimetic inspirations, recent advances in soft matter materials, representative fabrication techniques, system integration, and exemplary functions for underwater soft robots. The authors consider the key challenges the field faces in engineering material, software, and hardware that can bring highly intelligent soft robots into real world."
Xie2020,Xiaoyuan Xie and Zhiyi Zhang and Tsong Yueh Chen and Yang Liu and Pak Lok Poon and Baowen Xu,METTLE: A METamorphic Testing Approach to Assessing and Validating Unsupervised Machine Learning Systems,IEEE Transactions on Reliability,69,4,2020,10.1109/TR.2020.2972266,15581721,"Unsupervised machine learning is the training of an artificial intelligence system using information that is neither classified nor labeled, with a view to modeling the underlying structure or distribution in a dataset. Since unsupervised machine learning systems are widely used in many real-world applications, assessing the appropriateness of these systems and validating their implementations with respect to individual users' requirements and specific application scenarios/contexts are indisputably two important tasks. Such assessments and validation tasks, however, are fairly challenging due to the absence of a priori knowledge of the data. In view of this challenge, in this article, we develop a METamorphic Testing approach to assessing and validating unsupervised machine LEarning systems, abbreviated as mettle. Our approach provides a new way to unveil the (possibly latent) characteristics of various machine learning systems, by explicitly considering the specific expectations and requirements of these systems from individual users' perspectives. To support mettle, we have further formulated 11 generic metamorphic relations (MRs), covering users' generally expected characteristics that should be possessed by machine learning systems. We have performed an experiment and a user evaluation study to evaluate the viability and effectiveness of mettle. Our experiment and user evaluation study have shown that, guided by user-defined MR-based adequacy criteria, end users are able to assess, validate, and select appropriate clustering systems in accordance with their own specific needs. Our investigation has also yielded insightful understanding and interpretation of the behavior of the machine learning systems from an end-user software engineering's perspective, rather than a designer's or implementor's perspective, who normally adopts a theoretical approach."
Bejjanki2020,Kiran Kumar Bejjanki and Jayadev Gyani and Narsimha Gugulothu,Class imbalance reduction (CIR): A novel approach to software defect prediction in the presence of class imbalance,Symmetry,12,3,2020,10.3390/sym12030407,20738994,"Software defect prediction (SDP) is the technique used to predict the occurrences of defects in the early stages of software development process. Early prediction of defects will reduce the overall cost of software and also increase its reliability. Most of the defect prediction methods proposed in the literature suffer from the class imbalance problem. In this paper, a novel class imbalance reduction (CIR) algorithm is proposed to create a symmetry between the defect and non-defect records in the imbalance datasets by considering distribution properties of the datasets and is compared with SMOTE (synthetic minority oversampling technique), a built-in package of many machine learning tools that is considered a benchmark in handling class imbalance problems, and with K-Means SMOTE. We conducted the experiment on forty open source software defect datasets from PRedict or Models in Software Engineering (PROMISE) repository using eight different classifiers and evaluated with six performance measures. The results show that the proposed CIR method shows improved performance over SMOTE and K-Means SMOTE."
Jaubert2020,Jean Noël Jaubert and Yohann Le Guennec and Andrés Piña-Martinez and Nicolas Ramirez-Velez and Silvia Lasala and Bastian Schmid and Ilias K. Nikolaidis and Ioannis G. Economou and Romain Privat,Benchmark Database Containing Binary-System-High-Quality-Certified Data for Cross-Comparing Thermodynamic Models and Assessing Their Accuracy,Industrial and Engineering Chemistry Research,59,33,2020,10.1021/acs.iecr.0c01734,15205045,"In the last two centuries, equations of state (EoSs) have become a key tool for the correlation and prediction of thermodynamic properties of fluids. They not only can be applied to pure substances as well as to mixtures but also constitute the heart of commercially available computer-aided-process-design software. In the last 20 years, thousands of publications have been devoted to the development of sophisticated models or to the improvement of already existing EoSs. Chemical engineering thermodynamics is thus a field under steady development, and to assess the accuracy of a thermodynamic model or to cross-compare two models, it is necessary to confront model predictions with experimental data. In this context, the importance of a reliable free-to-access benchmark database is pivotal and becomes absolutely necessary. The goal of this paper is thus to present a database, specifically designed to assess the accuracy of a thermodynamic model or cross-compare models, to explain how it was developed and to enlighten how to use it. A total of 200 nonelectrolytic binary systems have been selected and divided into nine groups according to the associating character of the components, i.e., their ability to be involved in a hydrogen bond (the nature and strength of the association phenomena are indeed considered a measure of the complexity to model the thermodynamic properties of mixtures). The methodology for assessing the performance of a given model is then described. As an illustration, the Peng-Robinson EoS with classical van der Waals mixing rules and a temperature-dependent binary interaction parameter (kij) have been used to correlate the numerous data included in the proposed database, and its performance has been assessed following the proposed methodology."
Mimendi2022,Leonel Mimendi and Rodolfo Lorenzo and Haitao Li,"An innovative digital workflow to design, build and manage bamboo structures",Sustainable Structures,2,1,2022,10.54113/j.sust.2022.000011,2789312X,"At current rates, the building industry is the major contributor to gas emissions and energy consumption in the world, placing unprecedented pressure to find alternative and sustainable construction materials, particularly in regions where urbanization and population growth are expected to rise. Coincidentally, bamboo culms are a sustainable and abundant resource with the potential to be used as a structural element in those regions, however, their organic nature and inherent incompatibility with modern design and construction procedures have hampered their formal utilization. This article presents the details of an innovative workflow based on the philosophy that the quality and reliability of bamboo structures can be computationally managed through the digitization of individual structural bamboo elements. The workflow relies on reverse-engineering processes that integrate and make bamboo culms compatible with modern data-management platforms such as Building Information Modelling. A case study based on a reconstruction project of bamboo houses in Lombok, Indonesia is presented to illustrate the proposed workflow. This work showed that digitization and management are not just to represent shapes and information regarding bamboo culms through computer software, but can also control the quality, sustainability, and structural behavior of a bamboo structure during its entire service life."
Haque2020,Mubin Ul Haque and Leonardo Horn Iwaya and M. Ali Babar,Challenges in docker development: A large-scale study using stack overflow,,,,2020,10.1145/3382494.3410693,19493789,"Background: Docker technology has been increasingly used among software developers in a multitude of projects. This growing interest is due to the fact that Docker technology supports a convenient process for creating and building containers, promoting close cooperation between developer and operations teams, and enabling continuous software delivery. As a fast-growing technology, it is important to identify the Docker-related topics that are most popular as well as existing challenges and difficulties that developers face. Aims: This paper presents a large-scale empirical study identifying practitioners' perspectives on Docker technology by mining posts from the Stack Overflow (SoF) community. Method: A dataset of 113, 922 Docker-related posts was created based on a set of relevant tags and contents. The dataset was cleaned and prepared. Topic modelling was conducted using Latent Dirichlet Allocation (LDA), allowing the identification of dominant topics in the domain. Results: Our results show that most developers use SoF to ask about a broad spectrum of Docker topics including framework development, application deployment, continuous integration, web-server configuration and many more. We determined that 30 topics that developers discuss can be grouped into 13 main categories. Most of the posts belong to categories of application development, configuration, and networking. On the other hand, we find that the posts on monitoring status, transferring data, and authenticating users are more popular among developers compared to the other topics. Specifically, developers face challenges in web browser issues, networking error and memory management. Besides, there is a lack of experts in this domain. Conclusion: Our research findings will guide future work on the development of new tools and techniques, helping the community to focus efforts and understand existing trade-offs on Docker topics."
Khan2021,Muhammad Sufyan Khan and Farhana Jabeen and Sanaa Ghouzali and Zobia Rehman and Sheneela Naz and Wadood Abdul,Metaheuristic Algorithms in Optimizing Deep Neural Network Model for Software Effort Estimation,IEEE Access,9,,2021,10.1109/ACCESS.2021.3072380,21693536,"Effort estimation is the most critical activity for the success of overall solution delivery in software engineering projects. In this context, the paper's main contributions to the literature on software effort estimation are twofold. First, this paper examines the application of meta-heuristic algorithms to have a logical and acceptable parametric model for software effort estimation. Secondly, to unravel the benefits of nature-inspired meta-heuristic algorithms usage in optimizing Deep Learning (DL) architectures for software effort estimation, this paper presents a Deep Neural Network (DNN) model for software effort estimation based on meta-heuristic algorithms. In this paper, Grey Wolf Optimizer (GWO) and StrawBerry (SB) meta-heuristic algorithms are applied for having a logical and acceptable parametric model for software effort estimation. To validate the performances of these two algorithms, a set of nine benchmark functions having wide dimensions is applied. Results from GWO and SB algorithms are compared with five other meta-heuristic algorithms used in literature for software effort estimation. Experimental results showed that the GWO has comprehensive superiority in terms of accuracy in estimation. The proposed DNN model (GWDNNSB) using meta-heuristic algorithms for initial weights and learning rate selection, produced better results compared to existing work on using DNN for software effort estimation."
Haakman2021,Mark Haakman and Luís Cruz and Hennie Huijgens and Arie van Deursen,AI lifecycle models need to be revised: An exploratory study in Fintech,Empirical Software Engineering,26,5,2021,10.1007/s10664-021-09993-1,15737616,"Tech-leading organizations are embracing the forthcoming artificial intelligence revolution. Intelligent systems are replacing and cooperating with traditional software components. Thus, the same development processes and standards in software engineering ought to be complied in artificial intelligence systems. This study aims to understand the processes by which artificial intelligence-based systems are developed and how state-of-the-art lifecycle models fit the current needs of the industry. We conducted an exploratory case study at ING, a global bank with a strong European base. We interviewed 17 people with different roles and from different departments within the organization. We have found that the following stages have been overlooked by previous lifecycle models: data collection, feasibility study, documentation, model monitoring, and model risk assessment. Our work shows that the real challenges of applying Machine Learning go much beyond sophisticated learning algorithms – more focus is needed on the entire lifecycle. In particular, regardless of the existing development tools for Machine Learning, we observe that they are still not meeting the particularities of this field."
Panichella2021,Annibale Panichella,A Systematic Comparison of search-Based approaches for LDA hyperparameter tuning,Information and Software Technology,130,,2021,10.1016/j.infsof.2020.106411,09505849,"Context:Latent Dirichlet Allocation (LDA) has been successfully used in the literature to extract topics from software documents and support developers in various software engineering tasks. While LDA has been mostly used with default settings, previous studies showed that default hyperparameter values generate sub-optimal topics from software documents. Objective: Recent studies applied meta-heuristic search (mostly evolutionary algorithms) to configure LDA in an unsupervised and automated fashion. However, previous work advocated for different meta-heuristics and surrogate metrics to optimize. The objective of this paper is to shed light on the influence of these two factors when tuning LDA for SE tasks. Method:We empirically evaluated and compared seven state-of-the-art meta-heuristics and three alternative surrogate metrics (i.e., fitness functions) to solve the problem of identifying duplicate bug reports with LDA. The benchmark consists of ten real-world and open-source projects from the Bench4BL dataset. Results:Our results indicate that (1) meta-heuristics are mostly comparable to one another (except for random search and CMA-ES), and (2) the choice of the surrogate metric impacts the quality of the generated topics and the tuning overhead. Furthermore, calibrating LDA helps identify twice as many duplicates than untuned LDA when inspecting the top five past similar reports. Conclusion:No meta-heuristic and/or fitness function outperforms all the others, as advocated in prior studies. However, we can make recommendations for some combinations of meta-heuristics and fitness functions over others for practical use. Future work should focus on improving the surrogate metrics used to calibrate/tune LDA in an unsupervised fashion."
Yaseen2022,Moh Yaseen and Sawan Kumar Rawat and Anum Shafiq and Manoj Kumar and Kamsing Nonlaopon,Analysis of Heat Transfer of Mono and Hybrid Nanofluid Flow between Two Parallel Plates in a Darcy Porous Medium with Thermal Radiation and Heat Generation/Absorption,Symmetry,14,9,2022,10.3390/sym14091943,20738994,"In the last two decades, academicians have concentrated on the nanofluid squeezing flow between parallel plates. The increasing energy demands and their applications have seen the focus shifted to the hybrid nanofluid flows, but so much is still left to be investigated. This analysis is executed to explore the symmetry of the MHD squeezing nanofluid (MoS2/H2O) flow and the hybrid nanofluid (MoS2–SiO2/H2O–C2H6O2) flow between the parallel plates and their heat transport property. The heat transport phenomenon is analyzed with the magnetic field, thermal radiation, heat source/sink, suction/injection effect, and porous medium. In the present model, the plate situated above is in the movement towards the lower plate, and the latter is stretching with a linear velocity. The prevailing PDEs depicting the modeled problem with the aforementioned effects are transformed via similarity transformations and solved via the “bvp4c” function, which is an inbuilt function in MATLAB software. The control of the factors on the fields of velocity and temperature, heat transfer rate, velocity boundary layer patterns, and streamlines is investigated. The solution profiles are visually shown and explained. Furthermore, the Nusselt number at the bottom plate is larger for the (MoS2–SiO2/H2O–C2H6O2) hybrid nanofluid than for the (MoS2/H2O) nanofluid flow. In the presence of suction/injection, the streamlines appear to be denser. In addition, the magnetic field has a thinning consequence on the velocity boundary layer region. The results of this study apply to several thermal systems, engineering, and industrial processes, which utilize nanofluid and hybrid nanofluid for cooling and heating processes."
Ren2020,Yu Xuan Ren and Jianglai Wu and Queenie T.K. Lai and Hei Ming Lai and Dickson M.D. Siu and Wutian Wu and Kenneth K.Y. Wong and Kevin K. Tsia,Parallelized volumetric fluorescence microscopy with a reconfigurable coded incoherent light-sheet array,Light: Science and Applications,9,1,2020,10.1038/s41377-020-0245-8,20477538,"Parallelized fluorescence imaging has been a long-standing pursuit that can address the unmet need for a comprehensive three-dimensional (3D) visualization of dynamical biological processes with minimal photodamage. However, the available approaches are limited to incomplete parallelization in only two dimensions or sparse sampling in three dimensions. We hereby develop a novel fluorescence imaging approach, called coded light-sheet array microscopy (CLAM), which allows complete parallelized 3D imaging without mechanical scanning. Harnessing the concept of an “infinity mirror”, CLAM generates a light-sheet array with controllable sheet density and degree of coherence. Thus, CLAM circumvents the common complications of multiple coherent light-sheet generation in terms of dedicated wavefront engineering and mechanical dithering/scanning. Moreover, the encoding of multiplexed optical sections in CLAM allows the synchronous capture of all sectioned images within the imaged volume. We demonstrate the utility of CLAM in different imaging scenarios, including a light-scattering medium, an optically cleared tissue, and microparticles in fluidic flow. CLAM can maximize the signal-to-noise ratio and the spatial duty cycle, and also provides a further reduction in photobleaching compared to the major scanning-based 3D imaging systems. The flexible implementation of CLAM regarding both hardware and software ensures compatibility with any light-sheet imaging modality and could thus be instrumental in a multitude of areas in biological research."
Li2022,Piyu Li and A. Abbasi and Essam Roshdy El-Zahar and Waseh Farooq and Zahid Hussain and Sami Ullah Khan and M. Ijaz Khan and Shahid Farooq and M. Y. Malik and Fuzhang Wang,Hall effects and viscous dissipation applications in peristaltic transport of Jeffrey nanofluid due to wave frame,Colloids and Interface Science Communications,47,,2022,10.1016/j.colcom.2022.100593,22150382,"The thermal aspect of nanofluid is significantly improved the heat and mass transportation phenomenon which complies importance in various engineering and industrial processes. The peristaltic transport of nanofluid is also important research area with applications of bioengineering and biotechnology. This research presents the thermal aspect of Jeffrey nanofluid in curved geometry channel with applications of Hall effects. The viscous dissipation and thermal radiation are also incorporated. The curvilinear coordinates are followed to model the problem. The long wavelength approximations are used for the formulation of problem. The numerical shooting procedure is implemented by using the MATHEMTICA software. The velocity magnitude declines in upper half and boundary layer phenomenon develops in lower half of the channel with enlarge variation of Hall parameter. A decreasing trend in temperature profile is observed with curvature parameter. For all types of waves, the magnetic parameter enhanced the heat transfer coefficient."
Alshehri2022,Ahmed Alshehri and Zahir Shah,Computational analysis of viscous dissipation and Darcy-Forchheimer porous medium on radioactive hybrid nanofluid,Case Studies in Thermal Engineering,30,,2022,10.1016/j.csite.2021.101728,2214157X,"In the disciplines of engineering and research, the most demanding uses of nanofluids are of great interest. The use of nanotechnology in current science encouraged scholars to investigate nanofluid models from several perspectives. The main idea of the existing research is to describe the impacts of Darcy-Forchheimer flow of hybrid nanofluid and single nanofluid fluid through a slippery nonlinear, non-uniform starching surface. These nanofluids are developed by submerging different nanoparticles Zirconium dioxide and Aluminum in Kerosene Oil base fluid. The effects of Cattaneo-Christov (C-C) heat flow and radiative flux are examined in this work. Furthermore, radiative, heat source/sink, and dissipation of viscosity impacts are taken into account. This research use mathematical modelling to create and renovate a system of PDEs for fluid flow based on a few standard ones. The produced nonlinear framework is numerically calculated within the shooting approach using the bvp4c solver provided in the computational MATLAB software. When compared to typical profiles, the nature of a range of criteria is visually presented and analyzed in depth."
Latimer2021,Jessica M. Latimer and Shogo Maekawa and Yao Yao and David T. Wu and Michael Chen and William V. Giannobile,"Regenerative Medicine Technologies to Treat Dental, Oral, and Craniofacial Defects",Frontiers in Bioengineering and Biotechnology,9,,2021,10.3389/fbioe.2021.704048,22964185,"Additive manufacturing (AM) is the automated production of three-dimensional (3D) structures through successive layer-by-layer deposition of materials directed by computer-aided-design (CAD) software. While current clinical procedures that aim to reconstruct hard and soft tissue defects resulting from periodontal disease, congenital or acquired pathology, and maxillofacial trauma often utilize mass-produced biomaterials created for a variety of surgical indications, AM represents a paradigm shift in manufacturing at the individual patient level. Computer-aided systems employ algorithms to design customized, image-based scaffolds with high external shape complexity and spatial patterning of internal architecture guided by topology optimization. 3D bioprinting and surface modification techniques further enhance scaffold functionalization and osteogenic potential through the incorporation of viable cells, bioactive molecules, biomimetic materials and vectors for transgene expression within the layered architecture. These computational design features enable fabrication of tissue engineering constructs with highly tailored mechanical, structural, and biochemical properties for bone. This review examines key properties of scaffold design, bioresorbable bone scaffolds produced by AM processes, and clinical applications of these regenerative technologies. AM is transforming the field of personalized dental medicine and has great potential to improve regenerative outcomes in patient care."
Bonatti2021,Amedeo Franco Bonatti and Irene Chiesa and Giovanni Vozzi and Carmelo De Maria,Open-source CAD-CAM simulator of the extrusion-based bioprinting process,Bioprinting,24,,2021,10.1016/j.bprint.2021.e00172,24058866,"Extrusion-based Bioprinting (EBB) represents one of the most used Bioprinting technologies among researchers, thanks to its ease of use, wide variety of available materials, and affordable cost. Even though the technique has successfully been applied in bioprinting constructs for Tissue Engineering applications, there is still no consensus on which parameters have more influence of the accuracy of bioprinted scaffolds. Moreover, the literature lacks a rapid and robust method to consistently set the printing parameters before the actual printing phase, thus minimizing the trial-and-error process. In this context, we present a mathematical model for understanding the printability of a defined structure by depositing a given biomaterial ink through a specific EBB apparatus. The model takes into account different steps of the printing process, including extrusion, line formation and scaffold stabilization over time. The model was experimentally validated and implemented in an open-source software to guide the user in setting the correct printing parameters (i.e., printing speed, layer height and flow) based on scaffold dimensions, material properties (including rheological and mechanical ones) and printer set-up. To encourage the model use, we also propose a set of experiments to extract the relevant material properties for our model, and the software is available both as a stand-alone program (available at https://github.com/CentroEPiaggio/Rheology-GUI), as well as a webpage (available at https://www.prin-vision.it/printability-assessment)."
Hu2022,Yunwei Hu and Tarannom Parhizkar and Ali Mosleh,"Guided simulation for dynamic probabilistic risk assessment of complex systems: Concept, method, and application",Reliability Engineering and System Safety,217,,2022,10.1016/j.ress.2021.108047,09518320,"Probabilistic risk assessment (PRA) is a systematic process of examining how engineered systems work to ensure safety. With the growth of the size of dynamic systems and the complexity of the interactions between hardware, software, and humans, it is extremely difficult to enumerate risky scenarios by the traditional PRA methods. In this study, a new dynamic probabilistic risk assessment methodology is proposed that employs a new exploration strategy to generate risky scenarios. The proposed methodology consists of three main modules, including simulation, planner, and scheduler. In this methodology, the engineering knowledge of the system is explicitly used to guide the simulation module to achieve higher efficiency and accuracy. The engineering knowledge is reflected in the planner module which is responsible for generating plans as a high-level map to guide the simulation. The scheduler module is responsible for guiding the simulation by controlling the timing and occurrence of the random events. In this paper, modules of the proposed methodology, and their interactions are explained in detail. The developed methodology is used to perform risk assessment of a Space Shuttle ascent phase, and results show the effectiveness of the proposed platform."
Huang2022,M. Q. Huang and H. M. Zhu and J. Ninić and Q. B. Zhang,Multi-LOD BIM for underground metro station: Interoperability and design-to-design enhancement,Tunnelling and Underground Space Technology,119,,2022,10.1016/j.tust.2021.104232,08867798,"Underground metro stations as essential large-scale infrastructures that ease the traffic congestion on the overcrowded urban surface should be thoroughly designed, constructed and maintained. Building information modelling (BIM) has been increasingly employed for project design authoring, construction monitoring and operation management to promote digital engineering. Geotechnical design engaging numerical modelling techniques is an indispensable process for underground construction projects. These cross-disciplinary processes employing heterogeneous applications are yet to be coordinated in an efficient way, and thus improved interoperability can streamline the processes and optimise engineering design. This paper proposes an extension to the primary standard for openBIM data exchange – the Industry Foundation Classes (IFC) to facilitate wider adoption of BIM in underground infrastructure design, construction and management. Then a multiple level-of-detail (LOD) metro station BIM model is introduced to represent information at distinct levels of geometric and semantic richness required for varying application scenarios. Finally, we suggest a workflow underpinned by heuristic techniques, as an intermediate solution, to enhance the interoperability between a BIM authoring tool and a numerical modelling program for geotechnical analysis, and exemplify the workflow with the State Library Station of Melbourne. The proposed workflow offers an automated error-free, design-to-design solution, and hence enables the efficient exploration of design scenarios and construction optimisation before IFC is adopted by geotechnical modelling software."
Camposano2021,Jose Carlos Camposano and Kari Smolander and Tuomas Ruippo,Seven Metaphors to Understand Digital Twins of Built Assets,IEEE Access,9,,2021,10.1109/ACCESS.2021.3058009,21693536,"Digital twins have raised the attention of practitioners in the fields of Architecture, Engineering and Construction, and Facilities Management (AEC/FM). The term broadly refers to the cyber part of cyber-physical systems used for representing and managing real-world assets. This qualitative study explores how Finnish AEC/FM practitioners describe digital twins of assets in the built environment. Our findings are primarily derived from the interpretive analysis of semi-structured interviews with project managers and C-level executives during 2018 and 2019. The results of this analysis are discussed within the existing literature about digital twins, complex software ecosystems, and Service-Dominant (S-D) logic. We observed that digital twins were often explained using simple metaphors that could be easily understood by practitioners. We identified seven of such metaphors, each associated with a key attribute of digital twins. We argue that digital twins are the basis of complex software ecosystems, resulting from the increased expectations of AEC/FM stakeholders about the role of Building Information Modeling and other software solutions in their daily operations. Under an S-D logic perspective, digital twins are a resource applied by multiple interdependent actors to integrate information, co-create value for their entire network, and jointly deliver new products or services."
Kianifar2020,Mohammed Reza Kianifar and Felician Campean,Performance evaluation of metamodelling methods for engineering problems: towards a practitioner guide,Structural and Multidisciplinary Optimization,61,1,2020,10.1007/s00158-019-02352-1,16151488,"Metamodelling or surrogate modelling techniques are frequently used across the engineering disciplines in conjunction with expensive simulation models or physical experiments. With the proliferation of metamodeling techniques developed to provide enhanced performance for specific problems, and the wide availability of a diverse choice of tools in engineering software packages, the engineering task of selecting a robust metamodeling technique for practical problems is still a challenge. This research introduces a framework for describing the typology of engineering problems, in terms of dimensionality and complexity, and the modelling conditions, reflecting the noisiness of the signals and the affordability of sample sizes, and on this basis presents a systematic evaluation of the performance of frequently used metamodeling techniques. A set of metamodeling techniques, selected based on their reported use for engineering problems (i.e. Polynomial, Radial Basis Function, and Kriging), were systematically evaluated in terms of accuracy and robustness against a carefully assembled set of 18 test functions covering different types of problems, sampling conditions and noise conditions. A set of four real-world engineering case studies covering both computer simulation and physical experiments were also analysed as validation tests for the proposed guidelines. The main conclusions drawn from the study are that Kriging model with Matérn 5/2 correlation function performs consistently well across different problem types with smooth (i.e. not noisy) data, while Kriging model with Matérn 3/2 correlation function provides robust performance under noisy conditions, except for the very high noise conditions, where the Kriging model with nugget appears to provide better models. These results provide engineering practitioners with a guide for the choice of a metamodeling technique for problem types and modelling conditions represented in the study, whereas the evaluation framework and benchmarking problems set will be useful for researchers conducting similar studies."
Zohdinasab2021,Tahereh Zohdinasab and Vincenzo Riccio and Alessio Gambi and Paolo Tonella,DeepHyperion: Exploring the feature space of deep learning-based systems through illumination search,,,,2021,10.1145/3460319.3464811,,"Deep Learning (DL) has been successfully applied to a wide range of application domains, including safety-critical ones. Several DL testing approaches have been recently proposed in the literature but none of them aims to assess how different interpretable features of the generated inputs affect the system's behaviour. In this paper, we resort to Illumination Search to find the highest-performing test cases (i.e., misbehaving and closest to misbehaving), spread across the cells of a map representing the feature space of the system. We introduce a methodology that guides the users of our approach in the tasks of identifying and quantifying the dimensions of the feature space for a given domain. We developed DeepHyperion, a search-based tool for DL systems that illuminates, i.e., explores at large, the feature space, by providing developers with an interpretable feature map where automatically generated inputs are placed along with information about the exposed behaviours."
Balayn2021,Agathe Balayn and Christoph Lofi and Geert Jan Houben,Managing bias and unfairness in data for decision support: a survey of machine learning and data engineering approaches to identify and mitigate bias and unfairness within data management and analytics systems,VLDB Journal,30,5,2021,10.1007/s00778-021-00671-8,0949877X,"The increasing use of data-driven decision support systems in industry and governments is accompanied by the discovery of a plethora of bias and unfairness issues in the outputs of these systems. Multiple computer science communities, and especially machine learning, have started to tackle this problem, often developing algorithmic solutions to mitigate biases to obtain fairer outputs. However, one of the core underlying causes for unfairness is bias in training data which is not fully covered by such approaches. Especially, bias in data is not yet a central topic in data engineering and management research. We survey research on bias and unfairness in several computer science domains, distinguishing between data management publications and other domains. This covers the creation of fairness metrics, fairness identification, and mitigation methods, software engineering approaches and biases in crowdsourcing activities. We identify relevant research gaps and show which data management activities could be repurposed to handle biases and which ones might reinforce such biases. In the second part, we argue for a novel data-centered approach overcoming the limitations of current algorithmic-centered methods. This approach focuses on eliciting and enforcing fairness requirements and constraints on data that systems are trained, validated, and used on. We argue for the need to extend database management systems to handle such constraints and mitigation methods. We discuss the associated future research directions regarding algorithms, formalization, modelling, users, and systems."
Rahmadian2022,E. Rahmadian and D. Feitosa and A. Zwitter,A systematic literature review on the use of big data for sustainable tourism,Current Issues in Tourism,25,11,2022,10.1080/13683500.2021.1974358,13683500,"Sustainable tourism research focuses on mitigating or remediating environmental, social and economic impacts on tourism. In the past years, Big Data approaches have been applied to the field of tourism allowing for remarkable progress. However, there seems to be little evidence to support that such approaches are an inspiration to sustainable tourism and are being implemented. In this context, we aim to obtain a comprehensive overview of the use of Big Data in sustainable tourism to address various issues and understand how Big Data can support decision-making in such scenarios. To that end, this paper reports on the results of a literature review via a combination of a Systematic Literature Review (SLR) in Software Engineering, and the use of the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) method. In summary, we investigated four facets: (a) sources of big data, (b) approaches, (c) purposes, and (d) contexts of application. The results suggest that the use of various approaches have impacted practices in sustainable tourism. The findings provide a thorough understanding of the state of the art of Big Data application in sustainable tourism and provide valuable insights to foster growth both in terms of research and practice."
Gupta2020,Varun Gupta and Jose Maria Fernandez-Crehuet and Thomas Hanne and Rainer Telesko,Requirements engineering in software startups: A systematic mapping study,Applied Sciences (Switzerland),10,17,2020,10.3390/app10176125,20763417,"Startups have high failure rates due to their inability to attain a sufficient product/market fit, i.e., delivering a solution that best matches the user needs in the market. Requirement engineering is the activity that could help startup teams identify the value proposition that provides high value to the users and continuously innovate it. The objective of the study is to analyse the state of art of the requirement engineering research in the context of startups, as available in the literature. The analysis of the research area highlights the research trends to achieve two things i.e., (a) predict how much support the startups can get from the literature for enhancing their success rates and (b) identify the research gaps to motivate researchers to conduct future research that could be adoptable in startup contexts. Systematic mapping is conducted on studies extracted from the four bibliographic databases (IEEExplore, ACM, Springerlink and ScienceDirect) and studies extracted by using a forward snowballing approach. Individual studies are coded to yield the classification scheme. Formulated schemes and those already available in literature, were populated with information extracted from the abstracts of the studies. The research is mostly focused on generic requirement engineering and product validation activities. The research is conducted mostly as evaluations (empirical studies) with the outcome of providing theory to the research community. Major underlying motivation of the research is to attain the product/market fit. However, research studies focusing on requirement documentation, prioritization and elicitation are losing focus from 2017, 2018 and 2019, respectively. The literature lacks the studies that reports research solutions which are validated in laboratory settings or in real contexts, experience reports, opinion papers and philosophical papers. The positive side of the finding is that the number of requirement engineering research studies in a startup context have increased in the past five years. At this instant, unfortunately the literature has limited ability to support startups by providing solutions (for instance, research solutions, evidence to support decision makings, best practices, experiences etc.) that are adoptable in their real context. Uniform focus of the researchers across all sub-activities of requirement engineering is required with effort distributed across different research types that supports startups, not only by providing validated solutions but experience reports, opinions, new conceptual frameworks and empirical evidence that can aid their decision making."
Novielli2020,Nicole Novielli and Fabio Calefato and Davide Dongiovanni and Daniela Girardi and Filippo Lanubile,Can We Use SE-specific Sentiment Analysis Tools in a Cross-Platform Setting?,,,,2020,10.1145/3379597.3387446,,"In this paper, we address the problem of using sentiment analysis tools 'off-the-shelf', that is when a gold standard is not available for retraining. We evaluate the performance of four SE-specific tools in a cross-platform setting, i.e., on a test set collected from data sources different from the one used for training. We find that (i) the lexicon-based tools outperform the supervised approaches retrained in a cross-platform setting and (ii) retraining can be beneficial in within-platform settings in the presence of robust gold standard datasets, even using a minimal training set. Based on our empirical findings, we derive guidelines for reliable use of sentiment analysis tools in software engineering."
Bhatti2020,Muhammad Mubashir Bhatti and Asmaa F. Elelamy and Sadiq M. Sait and Rahmat Ellahi,Hydrodynamics interactions of metachronalwaves on particulate-liquid motion through a ciliated annulus: Application of bio-engineering in blood clotting and endoscopy,Symmetry,12,4,2020,10.3390/SYM12040532,20738994,"This study deals with the mass transport phenomena on the particle-fluid motion through an annulus. The non-Newtonian fluid propagates through a ciliated annulus in the presence of two phenomenon, namely (i) endoscopy, and (ii) blood clot. The outer tube is ciliated. To examine the flow behavior we consider the bi-viscosity fluid model. The mathematical modeling has been formulated for small Reynolds number to examine the inertia free flow. The purpose of this assumption is that wavelength-to-diameter is maximal, and the pressure could be considerably uniform throughout the entire cross-section. The resulting equations are analytically solved, and exact solutions are given for particle-and fluid-phase profiles. Computational software Mathematica has been used to evaluate both the closed-form and numerical results. The graphical behavior across each parameter has been discussed in detail and presented with graphs. The trapping mechanism is also shown across each parameter. It is noticed clearly that particle volume fraction and the blood clot reveal converse behavior on fluid velocity; however, the velocity of the fluid reduced significantly when the fluid behaves as a Newtonian fluid. Schmidt and Soret numbers enhance the concentration mechanism. Furthermore, more pressure is required to pass the fluid when the blood clot appears."
Gerasimou2020,Simos Gerasimou and Hasan Ferit Eniser and Alper Sen and Alper Cakan,Importance-Driven Deep Learning System Testing,,,,2020,10.1145/3377812.3390793,02705257,"Deep Learning (DL) systems are key enablers for engineering intelligent applications. Nevertheless, using DL systems in safety- A nd security-critical applications requires to provide testing evidence for their dependable operation. We introduce DeepImportance, a systematic testing methodology accompanied by an Importance-Driven (IDC) test adequacy criterion for DL systems. Applying IDC enables to establish a layer-wise functional understanding of the importance of DL system components and use this information to assess the semantic diversity of a test set. Our empirical evaluation on several DL systems and across multiple DL datasets demonstrates the usefulness and effectiveness of DeepImportance."
Zhong2021,Jing Zhong and Li Chen and Lijun Zhang,Automation of diffusion database development in multicomponent alloys from large number of experimental composition profiles,npj Computational Materials,7,1,2021,10.1038/s41524-021-00500-0,20573960,"Nowadays, the urgency for the high-quality interdiffusion coefficients and atomic mobilities with quantified uncertainties in multicomponent/multi-principal element alloys, which are indispensable for comprehensive understanding of the diffusion-controlled processes during their preparation and service periods, is merging as a momentous trending in materials community. However, the traditional exploration approach for database development relies heavily on expertize and labor-intensive computation, and is thus intractable for complex systems. In this paper, we augmented the HitDIC (high-throughput determination of interdiffusion coefficients, https://hitdic.com) software into a computation framework for automatic and efficient extraction of interdiffusion coefficients and development of atomic mobility database directly from large number of experimental composition profiles. Such an efficient framework proceeds in a workflow of automation concerning techniques of data-cleaning, feature engineering, regularization, uncertainty quantification and parallelism, for sake of agilely establishing high-quality kinetic database for target alloy. Demonstration of the developed infrastructures was finally conducted in fcc CoCrFeMnNi high-entropy alloys with a dataset of 170 diffusion couples and 34,000 composition points for verifying their reliability and efficiency. Thorough investigation over the obtained kinetic descriptions indicated that the sluggish diffusion is merely unilateral interpretation over specific composition and temperature ranges affiliated to limited dataset. It is inferred that data-mining over large number of experimental data with the combinatorial infrastructures are superior to reveal extremely complex composition- and temperature-dependent thermal–physical properties."
Xie2023,Jiamiao Xie and Xingyu Wei and Xiqiao Bo and Peng Zhang and Pengyun Chen and Wenqian Hao and Meini Yuan,State of charge estimation of lithium-ion battery based on extended Kalman filter algorithm,Frontiers in Energy Research,11,,2023,10.3389/fenrg.2023.1180881,2296598X,"Due to excellent power and energy density, low self-discharge and long life, lithium-ion battery plays an important role in many fields. Directed against the complexity of above noises and the strong sensitivity of the common Kalman filter algorithm to noises, the state of charge estimation of lithium-ion battery based on extended Kalman filter algorithm is investigated in this paper. Based on the second-order resistor-capacitance equivalent circuit model, the battery model parameters are identified using the MATLAB/Simulink software. A battery parameter test platform is built to test the charge-discharge efficiency, open-circuit voltage and state of charge relationship curve, internal resistance and capacitance of the individual battery are tested. The simulation and experimental results of terminal voltage for lithium-ion battery is compared to verify the effectiveness of this method. In addition, the general applicability of state of charge estimation algorithm for the battery pack is explored. The ampere-hour integral method combined with the battery modeling is used to estimate the state of charge of lithium-ion battery. The comparison of extended Kalman filter algorithm between experimental results and simulation estimated results is obtained to verify the accuracy. The extended Kalman filter algorithm proposed in this study not only establishes the theoretical basis for the condition monitoring but also provides the safe guarantee for the engineering application of lithium-ion battery."
Sikandar2021,Huma Sikandar and Yamunah Vaicondam and Nohman Khan and Muhammad Imran Qureshi and Abrar Ullah,Scientific Mapping of Industry 4.0 Research: A Bibliometric Analysis,International Journal of Interactive Mobile Technologies,15,18,2021,10.3991/ijim.v15i18.25535,18657923,"The fourth industrial revolution is progressing very rapidly. This research aims to investigate the research patterns and trends of industry 4.0 research with a focus on manufacturing. This bibliometric analysis is performed on data of the past five years (2016 to 2020) retrieved from the Scopus database. This research is conducted on 1426 articles in which the top productive countries, authors, institutions, and most cited articles were investigated. Findings demonstrated that Italy, the United States, and China are the most active countries in terms of research publications. South China University of Technology (China) has been identified as the most productive institution. Wan, J., Li, D., Rauch, E. were found to be the most productive authors. Industry 4.0 is primarily focused on the fields of engineering and computer science and sustainability is the most prolific journal. Co-occurrence analysis of keywords, co-authorship analysis of authors and countries were carried out along with bibliographic coupling of documents using VoS viewer which is the most common information visualisation software. This article summarises the growth of Industry 4.0 in the past five years and gives a short overview of the related works and applications of Industry 4.0."
Spolaor2020,Simone Spolaor and Caro Fuchs and Paolo Cazzaniga and Uzay Kaymak and Daniela Besozzi and Marco S. Nobile,Simpful: A user-friendly python library for fuzzy logic,International Journal of Computational Intelligence Systems,13,1,2020,10.2991/ijcis.d.201012.002,18756883,"Many researchers have used fuzzy set theory and fuzzy logic in a variety of applications related to computer science and engineering, given the capability of fuzzy inference systems to deal with uncertainty, represent vague concepts, and connect human language to numerical data. In this work we propose Simpful, a general-purpose and user-friendly Python library designed to facilitate the definition, analysis, and interpretation of fuzzy inference systems. Simpful provides a lightweight Application Programming Interface that allows to intuitively define fuzzy sets and fuzzy rules, and to perform fuzzy inference. Worthy of note, in Simpful the fuzzy rules are specified by means of strings of text written in natural language. We provide here some practical examples to show that Simpful represents a valuable addition to the open-source software that supports fuzzy reasoning."
Campaner2021,Larissa Mendes Campaner and Marcos Paulo Motta Silveira and Guilherme Schmitt de Andrade and Alexandre Luiz Souto Borges and Marco Antonio Bottino and Amanda Maria de Oliveira Dal Piva and Roberto Lo Giudice and Pietro Ausiello and João Paulo Mendes Tribst,Influence of polymeric restorative materials on the stress distribution in posterior fixed partial dentures: 3D finite element analysis,Polymers,13,5,2021,10.3390/polym13050758,20734360,"Background: This study evaluated the effect of interim restorative materials (acrylic resin (AR), resin composite (RC) or polyetheretherketone (PEEK) for dental computer-aided design/computeraided manufacturing (CAD/CAM)) on the stress distribution of a posterior three-unit fixed partial denture. Methods: The abutment teeth (first molar and first premolar) were modeled using the BioCAD protocol containing 1.5 mm of axial reduction and converging axial walls. A static structural analysis was performed in the computer-aided engineering software, and the Maximum Principal Stress criterion was used to analyze the prosthesis and the cement layers of both abutment teeth. The materials were considered isotropic, linearly elastic, homogeneous and with bonded contacts. An axial load (600 N) was applied to the occlusal surface of the second premolar. Results: Regardless of the restorative material, the region of the prosthetic connectors showed the highest tensile stress magnitude. The highest stress peak was observed with the use of RC (129 MPa) compared to PEEK and AR. For the cement layers, RC showed the lowest values in the occlusal region (7 MPa) and the highest values for the cervical margin (14 MPa) compared to PEEK (21 and 12 MPa) and AR (21 and 13 MPa). Conclusions: Different interim restorative materials for posterior fixed partial dentures present different biomechanical behavior. The use of resin composite can attenuate the stress magnitude on the cement layer, and the use of acrylic resin can attenuate the stress magnitude on the connector region."
Torkura2020,Kennedy A. Torkura and Muhammad I.H. Sukmana and Feng Cheng and Christoph Meinel,CloudStrike: Chaos Engineering for Security and Resiliency in Cloud Infrastructure,IEEE Access,8,,2020,10.1109/ACCESS.2020.3007338,21693536,"Most cyber-attacks and data breaches in cloud infrastructure are due to human errors and misconfiguration vulnerabilities. Cloud customer-centric tools are imperative for mitigating these issues, however existing cloud security models are largely unable to tackle these security challenges. Therefore, novel security mechanisms are imperative, we propose Risk-driven Fault Injection (RDFI) techniques to address these challenges. RDFI applies the principles of chaos engineering to cloud security and leverages feedback loops to execute, monitor, analyze and plan security fault injection campaigns, based on a knowledge-base. The knowledge-base consists of fault models designed from secure baselines, cloud security best practices and observations derived during iterative fault injection campaigns. These observations are helpful for identifying vulnerabilities while verifying the correctness of security attributes (integrity, confidentiality and availability). Furthermore, RDFI proactively supports risk analysis and security hardening efforts by sharing security information with security mechanisms. We have designed and implemented the RDFI strategies including various chaos engineering algorithms as a software tool: CloudStrike. Several evaluations have been conducted with CloudStrike against infrastructure deployed on two major public cloud infrastructure: Amazon Web Services and Google Cloud Platform. The time performance linearly increases, proportional to increasing attack rates. Also, the analysis of vulnerabilities detected via security fault injection has been used to harden the security of cloud resources to demonstrate the effectiveness of the security information provided by CloudStrike. Therefore, we opine that our approaches are suitable for overcoming contemporary cloud security issues."
Guan2020,Tianmin Guan and Yufang Zhang and Adeel Anwar and Yufen Zhang and Lina Wang,Determination of Three-Dimensional Corrective Force in Adolescent Idiopathic Scoliosis and Biomechanical Finite Element Analysis,Frontiers in Bioengineering and Biotechnology,8,,2020,10.3389/fbioe.2020.00963,22964185,"Aims: In this study we have considered the three dimensional corrective forces for correction of scoliosis by using a patient specific finite element model. Materials and Methods: An objective function of corrective forces in three-dimensional space was defined. Computed tomography images were used to reconstruct three dimensional model of scoliotic trunk. Computer aided engineering software Abaqus was used to establish finite element model of deformed spine and its biomechanical characteristics were analyzed. By adjusting magnitude and position of corrective forces, objective function was minimized to achieve best orthopedic effect. The proposed corrective conditions were divided into three groups: (1) thoracic deformity; (2) lumbar deformity; (3) both thoracic and lumbar deformities were considered. Results: In all three cases, the objective function was reduced by 58, 52, and 63%, respectively. The best correction forces point was located on convex side of maximum displacement of vertebral body. Conclusion: Using minimum objective function method, spinal deformity in three-dimensional space can be sufficiently reduced. This study provides scientific basis for design of a new corrective brace for treatment of scoliosis."
Cattari2022,Serena Cattari and Bruno Calderoni and Ivo Caliò and Guido Camata and Stefano de Miranda and Guido Magenes and Gabriele Milani and Anna Saetta,Nonlinear modeling of the seismic response of masonry structures: critical review and open issues towards engineering practice,Bulletin of Earthquake Engineering,20,4,2022,10.1007/s10518-021-01263-1,15731456,"This paper provides a comphrensive review of the critical aspects of nonlinear modeling for evaluating the seismic response of masonry structures, emphasizing the issues relevant to engineering practice. Currently, the specialized technical community shares the opinion that, for a performance-based approach, numerical models are the only tools sufficiently effective to support the seismic assessment of existing buildings. However, their potential often falls short when attempting to accurately describe the behavior of masonry structures. In fact, these structures feature highly complex architectural configurations, different masonry types, and various structural solutions, meaning that extra care is required in numerical modeling. This is especially true when the modelers do not have a solid background in the software chosen and may not be practiced using the vast variety of options offered by the software houses. They are often unaware of the consequences that questionable modeling choices may have on the results obtained by the models. These extremely complex topics are treated in the paper from an engineering practice perspective, providing an in-depth overview of the challenging issues related to the use of different modeling strategies. The paper covers strategies ranging from the Equivalent Frame approach (widely used in common engineering practice) to more refined techniques like 2D and 3D Finite Element procedures based on continuous, discrete, and micro-mechanical approaches. Critical aspects in the modeling of both in- and out-of-plane responses of masonry, as well as the critical issues in wall-to-wall connections and diaphragm roles are investigated. All the examined issues are clarified through numerical examples highlighting also how a consistent and integrated use of different procedures may be beneficial. Finally, some of most relevant challenging issues concerning the use of numerical models in seismic assessment with the nonlinear static approach are presented and discussed."
Garavel2020,Hubert Garavel and Maurice H. Ter Beek and Jaco Van De Pol,The 2020 Expert Survey on Formal Methods,,12327 LNCS,,2020,10.1007/978-3-030-58298-2_1,16113349,"Organised to celebrate the 25th anniversary of the FMICS international conference, the present survey addresses 30 questions on the past, present, and future of formal methods in research, industry, and education. Not less than 130 high-profile experts in formal methods (among whom three Turing award winners and many recipients of other prizes and distinctions) accepted to participate in this survey. We analyse their answers and comments, and present a collection of 111 position statements provided by these experts. The survey is both an exercise in collective thinking and a family picture of key actors in formal methods."
Jadhav2022,Anil Jadhav and Mandeep Kaur and Farzana Akter,Evolution of Software Development Effort and Cost Estimation Techniques: Five Decades Study Using Automated Text Mining Approach,Mathematical Problems in Engineering,2022,,2022,10.1155/2022/5782587,15635147,"Software development effort and cost estimation (SDECE) is one of the most important tasks in the field of software engineering. A large number of research papers have been published on this topic in the last five decades. Investigating research trends using a systematic literature review when such a large number of research papers are published is a very tedious and time-consuming task. Therefore, in this research paper, we propose a generic automated text mining framework to investigate research trends by analyzing the title, author's keywords, and abstract of the research papers. The proposed framework is used to investigate research trends by analyzing the title, keywords, and abstract of select 1015 research papers published on SDECE in the last five decades. We have identified the most popular SDECE techniques in each decade to understand how SDECE has evolved in the past five decades. It is found that artificial neural network, fuzzy logic, regression, analogy-based approach, and COCOMO methods are the most used techniques for SDECE followed by optimization, use case point, machine learning, and function point analysis. The NASA and ISBSG are the most used dataset for SDECE. The MMRE, MRE, and PRED are the most used accuracy measures for SDECE. Results of the proposed framework are validated by comparing it with the outcome of the previously published review work and we found that the results are consistent. We have also carried out a detailed bibliometric analysis and metareview of the review and survey papers published on SDECE. This research study is significant for the development of new models for cost and effort estimations."
Kukkar2020,Ashima Kukkar and Rajni Mohana and Yugal Kumar and Anand Nayyar and Muhammad Bilal and Kyung Sup Kwak,Duplicate Bug Report Detection and Classification System Based on Deep Learning Technique,IEEE Access,8,,2020,10.1109/ACCESS.2020.3033045,21693536,"Duplicate bug report detection is a process of finding a duplicate bug report in the bug tracking system. This process is essential to avoid unnecessary work and rediscovery. In typical bug tracking systems, more than thousands of duplicate bug reports are reported every day. In turn, human cost, effort and time are increased. This makes it an important problem in the software management process. The solution is to automate the duplicate bug report detection system for reducing the manual effort, thus the productivity of triager's and developer's is increased. It also speeds up the process of software management as a result software maintenance cost is also reduced. However, existing systems are not quite accurate yet, in spite of these systems used various machine learning approaches. In this work, an automatic bug report detection and classification model is proposed using deep learning technique. The proposed system has three modules i.e. Preprocessing, Deep Learning Model and Duplicate Bug report Detection and Classification. Further, the proposed model used Convolutional Neural Network based deep learning model to extract relevant feature. These relevant features are used to determine the similar features of bug reports. Hence, the bug reports similarity is computers through these similar features. The performance of the proposed system is evaluated on six publicly available datasets using six performance metrics. It is noticed that the proposed system outperforms the existing systems by achieving an accuracy rate in the range of 85% to 99 % and recall@k rate in between 79%-94%. Moreover, the effectiveness of the proposed system is also measured on the cross training datasets of same and different domain. The proposed system achieves a good high accuracy rate for same domain data sets and low accuracy rate for different domain datasets."
Pan2021,Cong Pan and Minyan Lu and Biao Xu,An empirical study on software defect prediction using codebert model,Applied Sciences (Switzerland),11,11,2021,10.3390/app11114793,20763417,"Deep learnisng-based software defect prediction has been popular these days. Recently, the publishing of the CodeBERT model has made it possible to perform many software engineering tasks. We propose various CodeBERT models targeting software defect prediction, including CodeBERT-NT, CodeBERT-PS, CodeBERT-PK, and CodeBERT-PT. We perform empirical studies using such models in cross-version and cross-project software defect prediction to investigate if using a neural language model like CodeBERT could improve prediction performance. We also investigate the effects of different prediction patterns in software defect prediction using CodeBERT models. The empirical results are further discussed."
LariosVargas2020,Enrique Larios Vargas and Maurício Aniche and Christoph Treude and Magiel Bruntink and Georgios Gousios,Selecting third-party libraries: The practitioners' perspective,,,,2020,10.1145/3368089.3409711,,"The selection of third-party libraries is an essential element of virtually any software development project. However, deciding which libraries to choose is a challenging practical problem. Selecting the wrong library can severely impact a software project in terms of cost, time, and development effort, with the severity of the impact depending on the role of the library in the software architecture, among others. Despite the importance of following a careful library selection process, in practice, the selection of third-party libraries is still conducted in an ad-hoc manner, where dozens of factors play an influential role in the decision. In this paper, we study the factors that influence the selection process of libraries, as perceived by industry developers. To that aim, we perform a cross-sectional interview study with 16 developers from 11 different businesses and survey 115 developers that are involved in the selection of libraries. We systematically devised a comprehensive set of 26 technical, human, and economic factors that developers take into consideration when selecting a software library. Eight of these factors are new to the literature. We explain each of these factors and how they play a role in the decision. Finally, we discuss the implications of our work to library maintainers, potential library users, package manager developers, and empirical software engineering researchers."
Niyonteze2020,Jean De Dieu Niyonteze and Fumin Zou and Godwin Norense Osarumwense Asemota and Samuel Bimenyimana and Gilbert Shyirambere,Key technology development needs and applicability analysis of renewable energy hybrid technologies in off-grid areas for the Rwanda power sector,Heliyon,6,1,2020,10.1016/j.heliyon.2020.e03300,24058440,"Until recently, the Rwanda power sector increased rapidly to double the 2010 installed capacity. The energy consumption in Rwanda experienced a steady rise correspondingly with the population and modern socio-economic life. Consequently, Rwanda household access to electricity increased to 53% by September 2019. Not only does 47% of Rwanda's population lack electricity access, there are persistent power failures and the grid is also unstable. Using renewable energy hybrid technologies in off-grid areas might be a solution to this problem. However, the high cost of renewable energy hybrid systems has led to its slow adoption in many developing countries. Hence, it is important to find the most appropriate hybrid combinations that reduce energy cost and access electricity generation that maximizes the available renewable energy resources. This paper examines some new technology development needs related to the power sector in Rwanda. Secondly, four different 100% renewable energy hybrid systems were designed and simulated to support rural and remote areas considering an average load demand of 158.1 kWh/day with a peak load of 18 kW. The hybrid systems simulation and optimization were obtained using HOMER (hybrid optimization model for electric renewables) software. The input data were obtained from National Aeronautics and Space Administration (NASA) for solar and wind resources, and hydro resources were from real-time field data for selected study site. The simulation results indicate hydro/solar/battery hybrid is the most cost-effective and environmentally viable alternative for off-grid rural electrification because of low net present cost (NPC) and least greenhouse gas emissions. The proposed hybrid combination could apply to other rural areas in the region and elsewhere in the world especially where climate conditions are similar."
Nguyen2020,V. H. Nguyen and T. N. Huynh and T. P. Nguyen and T. T. Tran,Single and multi-objective optimisation of processing parameters for fused deposition modelling in 3D printing technology,International Journal of Automotive and Mechanical Engineering,17,1,2020,10.15282/IJAME.17.1.2020.03.0558,21801606,"This paper presents practice and application of design of experiment techniques and genetic algorithm in single and multi-objective optimisation with low cost, robustness, and high effectiveness through 3D printing case studies. 3D printing brings many benefits for engineering design, product development, and production process. However, it faces many challenges related to parameters control. The wrong parameter setup can result in excessive time, high production cost, waste material, and low-quality printing. This study was conducted to optimise the parameter sets for 3D fused deposition modelling (FDM) products. The parameter sets are layer height, infill percentage, printing temperature, printing speed with different levels were experimented and analysed to build mathematic models. The objectives are to describe the relationship between the inputs (parameter values) and the outputs (printing quality in term of weight, printing time and tensile strength of products). Single-objective and multi-objective models, according to the user's desire, are constructed and studied to identify the optimal set, optimal trade-off set of parameters. The paper illustrates Taguchi parameter design that could yield accurate results with a minimal number of experiments to be performed compared with other design of experiment methods. This method is a simple and systematic methodology that is highly effective in optimising the process parameters with low cost. Besides, the paper proposed an approach which is a combination of the response surface methodology and genetic algorithm to solve the multi-objective optimisation problem. This method can fast identify overall Pareto-optimal solutions which define the best trade-off between competing objectives. 3D printer, testing machines, and quality tools were used for doing experiments, measurement and collecting data. Minitab and Matlab software aid for analysis and decision-making. Proposed solutions for handling multi-objective optimisation through 3D fused deposition modelling product printing case study are practical and can extend for other case studies."
David2020,Thamyres Machado David and Paloma Maria Silva Rocha Rizol and Marcela Aparecida Guerreiro Machado and Gilberto Paschoal Buccieri,"Future research tendencies for solar energy management using a bibliometric analysis, 2000–2019",Heliyon,6,7,2020,10.1016/j.heliyon.2020.e04452,24058440,"Using the Scopus database between the years of 2000 and 2019, a bibliometric study was done to analyze the scientific publications in the area of photovoltaic solar energy management. From the preliminary analysis of future research tendencies, ten possibilities of study topics were developed; and due to that it was possible to assume that even though many studies of technological development are found, some insights can still be approached in a way that the practical implementation of solar systems photovoltaic is better used. This data was validated by the analysis performed with the Scimat scientific mapping software under a longitudinal structure, verifying the future tendencies researches mentioned previously."
Rajabli2021,Nijat Rajabli and Francesco Flammini and Roberto Nardone and Valeria Vittorini,Software Verification and Validation of Safe Autonomous Cars: A Systematic Literature Review,IEEE Access,9,,2021,10.1109/ACCESS.2020.3048047,21693536,"Autonomous, or self-driving, cars are emerging as the solution to several problems primarily caused by humans on roads, such as accidents and traffic congestion. However, those benefits come with great challenges in the verification and validation (V&V) for safety assessment. In fact, due to the possibly unpredictable nature of Artificial Intelligence (AI), its use in autonomous cars creates concerns that need to be addressed using appropriate V&V processes that can address trustworthy AI and safe autonomy. In this study, the relevant research literature in recent years has been systematically reviewed and classified in order to investigate the state-of-the-art in the software V&V of autonomous cars. By appropriate criteria, a subset of primary studies has been selected for more in-depth analysis. The first part of the review addresses certification issues against reference standards, challenges in assessing machine learning, as well as general V&V methodologies. The second part investigates more specific approaches, including simulation environments and mutation testing, corner cases and adversarial examples, fault injection, software safety cages, techniques for cyber-physical systems, and formal methods. Relevant approaches and related tools have been discussed and compared in order to highlight open issues and opportunities."
Yao2021,Qi Yao and Mohammad Shabaz and Raj Karan Singh and Tarun Kumar Lohani and Mohammed Wasim Bhatt and Gurpreet Singh Panesar,3D modelling and visualization for Vision-based Vibration Signal Processing and Measurement,Journal of Intelligent Systems,30,1,2021,10.1515/jisys-2020-0123,03341860,"With the technological evolutionary advent, a vision-based approach presents the remote measuring approach for the analysis of vibration. The structure vibration test and model parameter identification in the detection of the structure of the bridge evaluation occupies the important position. The bridge structure to operate safely and reliably is ensured, according to the geological data of qixiashan lead-zinc mine and engineering actual situation, with the aid of international mining software Surpac. To build the 3D visualization model of the application of visualization in mine production are discussed. The results show that the final solid model of -425 stope can accurately display the spatial form of each layer of stope through rotation, amplification and movement. The proposed system is effectually able to perform cutting, volume calculation and roaming in any direction, which has certain guiding significance for mine production management. An accuracy value of 98.75%, the sensitivity of 99%, specificity of 99.64% and PPV of 99.89% are achieved using the proposed 3D modelling and visualization algorithm for vibration signal processing and management."
Sun2021,Hailing Sun and Miao Fan and Ashutosh Sharma,Design and implementation of construction prediction and management platform based on building information modelling and three-dimensional simulation technology in Industry 4.0,IET Collaborative Intelligent Manufacturing,3,3,2021,10.1049/cim2.12019,25168398,"In competitive growth and Industry 4.0, construction prediction and management have a key role. To find a way to provide a simulation method for the damage assessment of buildings and Industry 4.0, building information modelling technology is the most suitable choice. This work presents and analyses the building material from design modelling to model information extraction, virtual construction, and an imported virtual simulation engine. A simulation system has been built to understand the force and material collision detection of buildings, and a three-dimensional (3D) simulation platform is developed based on the Unity3D engine. A 3D display of building model and simulation data is realized in this work based on the simulation software platform. The results show that the building 3D simulation images constructed by the designed system are high definition, take little time, and have excellent performance. The outcomes are realized in terms of the engineering cost ratio and have energy consumption and efficiency values of 20% and 40%, respectively, which are much better than the traditional method. Efficiency has also improved to 76% from the traditional method using the proposed method, which makes it a robust platform for construction prediction and management in industries. The virtual simulation technology is applied to solve problems of building design and damage assessment. The influence of this technology on the overall design of the building is discussed, followed by future development directions for industrial automation."
Wang2020,Bin Wang and Liuming Jing,A Protection Method for Inverter-based Microgrid Using Current-only Polarity Comparison,Journal of Modern Power Systems and Clean Energy,8,3,2020,10.35833/MPCE.2018.000722,21965420,"The design of an effective protection system for inverter-based microgrids is a complicated engineering challenge. This is due to the fact that inverters have limited fault current capabilities, and that the conventional overcurrent protection is not suitable for inverter-based microgrids. This paper introduces a novel protection method for inverter-based microgrid using a current-only polarity comparison. The proposed method is based on the phase difference between the pre-fault and fault current components. The method responds to faults in both grid-connected and autonomous operation modes and provides a new way to identify faulted sections. Simulations of an inverter-based microgrid with a relay model are conducted using PSCAD/EMTDC software. The results show that the proposed method can detect faults in inverter-based microgrids."
Pagoli2022,Amir Pagoli and Frédéric Chapelle and Juan Antonio Corrales-Ramon and Youcef Mezouar and Yuri Lapusta,Review of soft fluidic actuators: Classification and materials modeling analysis,Smart Materials and Structures,31,1,2022,10.1088/1361-665X/ac383a,1361665X,"Soft actuators can be classified into five categories: tendon-driven actuators, electroactive polymers, shape-memory materials, soft fluidic actuators (SFAs), and hybrid actuators. The characteristics and potential challenges of each class are explained at the beginning of this review. Furthermore, recent advances especially focusing on SFAs are illustrated. There are already some impressive SFA designs to be found in the literature, constituting a fundamental basis for design and inspiration. The goal of this review is to address the latest innovative designs for SFAs and their challenges and improvements with respect to previous generations, and to help researchers to select appropriate materials for their application. We suggest seven influential designs: pneumatic artificial muscle, PneuNet, continuum arm, universal granular gripper, origami soft structure, vacuum-actuated muscle-inspired pneumatic, and hydraulically amplified self-healing electrostatic. The hybrid design of SFAs for improved functionality and shape controllability is also considered. Modeling SFAs, based on previous research, can be classified into three main groups: analytical methods, numerical methods, and model-free methods. We demonstrate the latest advances and potential challenges in each category. Regarding the fact that the performance of soft actuators is dependent on material selection, we then focus on the behaviors and mechanical properties of the various types of silicone that can be found in the SFA literature. For a better comparison of the different constitutive models of silicone materials proposed and tested in the literature, ABAQUS software is here employed to generate the engineering and true strain-stress data from the constitutive models, and compare them with standard uniaxial tensile test data based on ASTM412. Although the figures presented show that in a small range of stress-strain data, most of these models can predict the material model acceptably, few of them predict it accurately for large strain-stress values. Sensor technology integrated into SFAs is also being developed, and has the potential to increase controllability and observability by detecting a wide variety of data such as curvature, tactile contacts, produced force, and pressure values."
Dosta2020,Maksym Dosta and Vasyl Skorych,MUSEN: An open-source framework for GPU-accelerated DEM simulations,SoftwareX,12,,2020,10.1016/j.softx.2020.100618,23527110,"The conceptual design, implementation aspects and main features of an open-source DEM simulation framework MUSEN have been described. MUSEN has been developed for efficient calculations that can be performed on personal computers equipped with general-purpose graphics processing units (GPUs). A very intuitive graphical user interface significantly simplifies usage of the system and reduces learning time. It makes this framework an ideal software tool for educational purposes and for solution of classical problems of solids process engineering. Users without significant experience can easily specify initial scenes, perform simulations and analyse results. In the same time, the modular-based structure of the system allows its extension with new components, calculation algorithms or contact models."
Copeland2020,Samuel Copeland and Melissa Bilec,Buildings as material banks using RFID and building information modeling in a circular economy,,90,,2020,10.1016/j.procir.2020.02.122,22128271,"Currently, the Architecture, Engineering, and Construction community operates in an environmentally problematic linear economy by consuming non-renewable virgin materials, producing landfill waste, and increasing the overall environmental impact. A solution to mitigate these problems is the implementation of a circular economy, an economic model that decouples economic growth and depletion of resources by minimizing waste. Technology opportunities explored to foster these relationships include building as material banks (BAMB), building information modeling (BIM) software, radio-frequency identification (RFID) tags, and blockchain. This paper presents a conceptual diagram and framework for implementing a circular economy utilizing the promising technologies and concept."
Bharti2021,Bharti and Ashwani Kumar and Gulzar Ahmed and Meenal Gupta and Patrizia Bocchetta and Ravikant Adalati and Ramesh Chandra and Yogesh Kumar,Theories and models of supercapacitors with recent advancements: impact and interpretations,Nano Express,2,2,2021,10.1088/2632-959X/abf8c2,2632959X,"Supercapacitors provide remarkable eco-friendly advancement in energy conversion and storage with a huge potential to control the future economy of the entire world. Currently, industries focus on the design and engineering aspects of supercapacitors with high performance (high energy), flexibility (by the use of composite polymer based electrolytes), high voltage (ionic liquid) and low cost. The paper reviews the modelling techniques like Empirical modelling, Dissipation transmission line models, Continuum models, Atomistic models, Quantum models, Simplified analytical models etc. proposed for the theoretical study of Supercapacitors and discusses their limitations in studying all the aspects of Supercapacitors. It also reviews the various software packages available for Supercapacitor (SC) modelling and discusses their advantages and disadvantages. The paper also reviews the Experimental advancements in the field of electric double layer capacitors (EDLCs), pseudo capacitors and hybrid/asymmetric supercapacitors and discusses the commercial progress of supercapacitors as well."
Peterson2020,Victoria Peterson and Catalina Galván and Hugo Hernández and Ruben Spies,A feasibility study of a complete low-cost consumer-grade brain-computer interface system,Heliyon,6,3,2020,10.1016/j.heliyon.2020.e03425,24058440,"Brain-computer interfaces (BCIs) are technologies that provide the user with an alternative way of communication. A BCI measures brain activity (e.g. EEG) and converts it into output commands. Motor imagery (MI), the mental simulation of movements, can be used as a BCI paradigm, where the movement intention of the user can be translated into a real movement, helping patients in motor recovery rehabilitation. One of the main limitations for the broad use of such devices is the high cost associated with the high-quality equipment used for capturing the biomedical signals. Different low-cost consumer-grade alternatives have emerged with the objective of bringing these systems closer to the final users. The quality of the signals obtained with such equipments has already been evaluated and found to be competitive with those obtained with well-known clinical-grade devices. However, how these consumer-grade technologies can be integrated and used for practical MI-BCIs has not yet been explored. In this work, we provide a detailed description of the advantages and disadvantages of using OpenBCI boards, low-cost sensors and open-source software for constructing an entirely consumer-grade MI-BCI system. An analysis of the quality of the signals acquired and the MI detection ability is performed. Even though communication between the computer and the OpenBCI board is not always stable and the signal quality is sometimes affected by ambient noise, we find that by means of a filter-bank based method, similar classification performances can be achieved with an MI-BCI built under low-cost consumer-grade devices as compared to when clinical-grade systems are used. By means of this work we share with the BCI community our experience on working with emerging low-cost technologies, providing evidence that an entirely low-cost MI-BCI can be built. We believe that if communication stability and artifact rejection are improved, these technologies will become a valuable alternative to clinical-grade devices."
Assareh2022,Ehsanolah Assareh and Mostafa Delpisheh and Ehsan Farhadi and Wanli Peng and Hesam Moghadasi,Optimization of geothermal- and solar-driven clean electricity and hydrogen production multi-generation systems to address the energy nexus,Energy Nexus,5,,2022,10.1016/j.nexus.2022.100043,27724271,"Given the limited sources of fossil fuels, mankind should find new ways to meet its energy demands. In this regard, geothermal and solar energy are acknowledged as reliable, safe, promising, and clean means for this purpose. In this research study, a comparative analysis is applied on geothermal- and solar-driven multi-generation systems for clean electricity and hydrogen production through energy and exergy assessments. The systems consist of an organic Rankine cycle, a proton electrolyte membrane electrolyzer, and a thermoelectric generator subsystem. The Engineering Equation Solver software has been utilized in order to model the system and obtain the output contours, sensitivity analysis, and exergy destruction. The results were obtained considering the ambient temperature of Bandar Abbas city as a case study. The geothermal system was performant over the solar system, with 11.21% higher hydrogen production and 0.17 % higher exergy efficiency. According to the sensitivity analysis, the turbine efficiency, evaporator inlet temperature, thermoelectric generator suitability criterion, pump efficiency, and evaporator inlet mass flow rate were the most influential parameters. Also, the exergy analysis showed that the utmost system's exergy destruction is pertinent to the evaporator and the least is related to the pump. In addition, the system produces 352,816 kWh and 174.913 kg of electrical power and hydrogen during one year."
Yang2022,Zhou Yang and Jieke Shi and Junda He and David Lo,Natural Attack for Pre-trained Models of Code,,2022-May,,2022,10.1145/3510003.3510146,02705257,"Pre-trained models of code have achieved success in many important software engineering tasks. However, these powerful models are vulnerable to adversarial attacks that slightly perturb model inputs to make a victim model produce wrong outputs. Current works mainly attack models of code with examples that preserve operational program semantics but ignore a fundamental requirement for adversarial example generation: perturbations should be natural to human judges, which we refer to as naturalness requirement. In this paper, we propose ALERT (Naturalness Aware Attack), a black-box attack that adversarially transforms inputs to make victim models produce wrong outputs. Different from prior works, this paper considers the natural semantic of generated examples at the same time as preserving the operational semantic of original inputs. Our user study demonstrates that human developers consistently consider that adversarial examples generated by ALERT are more natural than those generated by the state-of-the-art work by Zhang et al. that ignores the naturalness requirement. On attacking CodeBERT, our approach can achieve attack success rates of 53.62%, 27.79%, and 35.78% across three downstream tasks: vulnerability prediction, clone detection and code authorship attribution. On GraphCodeBERT, our approach can achieve average success rates of 76.95%, 7.96% and 61.47% on the three tasks. The above outperforms the baseline by 14.07% and 18.56% on the two pretrained models on average. Finally, we investigated the value of the generated adversarial examples to harden victim models through an adversarial fine-tuning procedure and demonstrated the accuracy of CodeBERT and GraphCodeBERT against ALERT-generated adversarial examples increased by 87.59% and 92.32%, respectively."
Sequeiros-Borja2021,Carlos Eduardo Sequeiros-Borja and Bartłomiej Surpeta and Jan Brezovsky,Recent advances in user-friendly computational tools to engineer protein function,Briefings in Bioinformatics,22,3,2021,10.1093/bib/bbaa150,14774054,"Progress in technology and algorithms throughout the past decade has transformed the field of protein design and engineering. Computational approaches have become well-engrained in the processes of tailoring proteins for various biotechnological applications. Many tools and methods are developed and upgraded each year to satisfy the increasing demands and challenges of protein engineering. To help protein engineers and bioinformaticians navigate this emerging wave of dedicated software, we have critically evaluated recent additions to the toolbox regarding their application for semi-rational and rational protein engineering. These newly developed tools identify and prioritize hotspots and analyze the effects of mutations for a variety of properties, comprising ligand binding, protein-protein and protein-nucleic acid interactions, and electrostatic potential. We also discuss notable progress to target elusive protein dynamics and associated properties like ligand-transport processes and allosteric communication. Finally, we discuss several challenges these tools face and provide our perspectives on the further development of readily applicable methods to guide protein engineering efforts."
Saputri2020,Theresia Ratih Dewi Saputri and Seok Won Lee,The application of machine learning in self-adaptive systems: A systematic literature review,IEEE Access,8,,2020,10.1109/ACCESS.2020.3036037,21693536,"Context: Self-adaptive systems have been studied in software engineering over the past few decades attempting to address challenges within the field. There is a continuous significant need to fully understand the behavior and characteristics of the systems that operate in dynamic environments. By learning the behavior pattern of the environment, we can avoid unnecessary adaptations imbalance efforts for adaptation. As such, there exist research in the area of machine learning aimed at understanding dynamic environments regarding self-adaptive systems. Objective: This study aims to help software practitioners to address adaptation concerns by performing a systematic literature review that provides a comprehensive overview of using machine learning (ML) in self-adaptive systems. We summarize state-of-the-art Of the ML approaches used to handle self-adaptation to help software engineers in the proper selection of ML techniques based on the adaptation concern. Method: This review examines research published between 2001 and 2019 on ML implementation in self-adaptive systems, focusing on the adaptation aspects and purposes. The review was conducted by analyzing major scientific databases that resulted in 78 primary studies from 315 papers from an automatic search. Result: Finally, this study recommends three future research directions to enhance the application of machine learning in self-adaptive systems."
Domander2021,Richard Domander and Alessandro A. Felder and Michael Doube,BoneJ2 - refactoring established research software,Wellcome Open Research,6,,2021,10.12688/wellcomeopenres.16619.2,2398502X,"Research software is often developed with expedience as a core development objective because experimental results, but not the software, are specified and resourced as a project output. While such code can help find answers to specific research questions, it may lack longevity and flexibility to make it reusable. We reimplemented BoneJ, our software for skeletal biology image analysis, to address design limitations that put it at risk of becoming unusable. We improved the quality of BoneJ code by following contemporary best programming practices. These include separation of concerns, dependency management, thorough testing, continuous integration and deployment, source code management, code reviews, issue and task ticketing, and user and developer documentation. The resulting BoneJ2 represents a generational shift in development technology and integrates with the ImageJ2 plugin ecosystem."
Girardi2020,Daniela Girardi and Nicole Novielli and Davide Fucci and Filippo Lanubile,Recognizing developers' emotions while programming,,,,2020,10.1145/3377811.3380374,02705257,"Developers experience a wide range of emotions during programming tasks, which may have an impact on job performance. In this paper, we present an empirical study aimed at (i) investigating the link between emotion and progress, (ii) understanding the triggers for developers' emotions and the strategies to deal with negative ones, (iii) identifying the minimal set of non-invasive biometric sensors for emotion recognition during programming tasks. Results confirm previous findings about the relation between emotions and perceived productivity. Furthermore, we show that developers' emotions can be reliably recognized using only a wristband capturing the electrodermal activity and heart-related metrics."
Sonica2020,Krzysztof Sośnica and Radosław Zajdel and Grzegorz Bury and Jarosław Bosy and Michael Moore and Salim Masoumi,Quality assessment of experimental IGS multi-GNSS combined orbits,GPS Solutions,24,2,2020,10.1007/s10291-020-0965-5,15211886,"The International GNSS Service (IGS) Analysis Center Coordinator initiated in 2019 an experimental multi-GNSS orbit combination service by adapting the current combination software that has been used for many years for IGS GPS and GLONASS combinations. The multi-GNSS orbits are based on individual products generated by IGS and multi-GNSS Pilot Project analysis centers. However, the combinations are not yet considered to be the final products at this time. The goal of this research is to provide a quality assessment of the very first IGS experimental multi-GNSS combined orbits based on Satellite Laser Ranging (SLR) observations and the mean position errors from the orbit combinations. The errors available in the combined orbit files provide information about the consistency between orbits from different analysis centers, whereas SLR provides independent orbit validation results even for those satellites which are considered only by one analysis center, and thus, the quality of the combination is not provided in the orbit files. We found that the BeiDou-3 satellites manufactured by China Academy of Space Technology and Shanghai Engineering Center for Microsatellites are characterized by opposite SLR residual dependencies with respect to the position of the sun which means that the orbit models for BeiDou-3 need further improvement. Smallest SLR residuals are obtained for Galileo, GLONASS-K1, and GLONASS-M+. However, the latter is characterized by a bias of + 29 mm. The mean standard deviations of SLR residuals are 23, 29, 87, 51, 40, and 72 mm for Galileo, GLONASS, BeiDou GEO, BeiDou IGSO, BeiDou MEO, and QZSS, respectively. The mean orbit combination errors in the radial direction are three times lower than those from SLR residuals in the case of MEO satellites and vary between 8 and 14 mm, whereas the orbit errors are four times lower than SLR residuals in the case of GEO and IGSO and equal to 11–21 mm."
Li2021,Yang Li and Yu Shen and Wentao Zhang and Yuanwei Chen and Huaijun Jiang and Mingchao Liu and Jiawei Jiang and Jinyang Gao and Wentao Wu and Zhi Yang and Ce Zhang and Bin Cui,OpenBox: A Generalized Black-box Optimization Service,,,,2021,10.1145/3447548.3467061,,"Black-box optimization (BBO) has a broad range of applications, including automatic machine learning, engineering, physics, and experimental design. However, it remains a challenge for users to apply BBO methods to their problems at hand with existing software packages, in terms of applicability, performance, and efficiency. In this paper, we build OpenBox, an open-source and general-purpose BBO service with improved usability. The modular design behind OpenBox also facilitates flexible abstraction and optimization of basic BBO components that are common in other existing systems. OpenBox is distributed, fault-tolerant, and scalable. To improve efficiency, OpenBox further utilizes ""algorithm agnostic""parallelization and transfer learning. Our experimental results demonstrate the effectiveness and efficiency of OpenBox compared to existing systems."
An2021,Pengju An and Kun Fang and Qiangqiang Jiang and Haihua Zhang and Yi Zhang,Measurement of rock joint surfaces by using smartphone structure from motion (SFM) photogrammetry,Sensors (Switzerland),21,3,2021,10.3390/s21030922,14248220,"The measurement of rock joint surfaces is essential for the estimation of the shear strength of the rock discontinuities in rock engineering. Commonly used techniques for the acqui-sition of the morphology of the surfaces, such as profilometers and laser scanners, either have low accuracy or high cost. Therefore, a high-speed, low-cost, and high-accuracy method for obtaining the topography of the joint surfaces is necessary. In this paper, a smartphone structure from motion (SfM) photogrammetric solution for measuring rock joint surfaces is presented and evaluated. Image datasets of two rock joint specimens were taken under two different modes by using an iPhone 6s, a Pixel 2, and a T329t and subsequently processed through SfM-based software to obtain 3D models. The technique for measuring rock joint surfaces was evaluated using the root mean square error (RMSE) of the cloud-to-cloud distance and the mean error of the joint roughness coefficient (JRC). The results show that the RMSEs by using the iPhone 6s and Pixel 2 are both less than 0.08 mm. The mean errors of the JRC are −7.54 and −5.27% with point intervals of 0.25 and 1.0 mm, respectively. The smartphone SfM photogrammetric method has comparable accuracy to a 3D laser scanner approach for reconstructing laboratory-sized rock joint surfaces, and it has the potential to become a popular method for measuring rock joint surfaces."
Chen2020,Chan Chen and Yang Lou and Xin Yi Li and Zheng Tian Lv and Lu Qiu Zhang and Wei Mao,Mapping current research and identifying hotspots on mesenchymal stem cells in cardiovascular disease,Stem Cell Research and Therapy,11,1,2020,10.1186/s13287-020-02009-7,17576512,"Background: Mesenchymal stem cells (MSCs) have important research value and broad application prospects in the cardiovascular disease. This study provides information on the latest progress, evolutionary path, frontier research hotspots, and future research developmental trends in this field. Methods: A knowledge map was generated by CiteSpace and VOSviewer analysis software based on data obtained from the literature on MSCs in the cardiovascular field. Results: The USA and China ranked at the top in terms of the percentage of articles, accounting for 34.306% and 28.550%, respectively. The institution with the highest number of research publications in this field was the University of Miami, followed by the Chinese Academy of Medical Sciences and Harvard University. The research institution with the highest ACI value was Harvard University, followed by the Mayo Clinic and the University of Cincinnati. The top three subjects in terms of the number of published articles were cell biology, cardiovascular system cardiology, and research experimental medicine. The journal with the most publications in this field was Circulation Research, followed by Scientific Reports and Biomaterials. The direction of research on MSCs in the cardiovascular system was divided into four parts: (1) tissue engineering, scaffolds, and extracellular matrix research; (2) cell transplantation, differentiation, proliferation, and signal transduction pathway research; (3) assessment of the efficacy of stem cells from different sources and administration methods in the treatment of acute myocardial infarction, myocardial hypertrophy, and heart failure; and (4) exosomes and extracellular vesicles research. Tissue research is the hotspot and frontier in this field. Conclusion: MSC research has presented a gradual upward trend in the cardiovascular field. Multidisciplinary intersection is a characteristic of this field. Engineering and materials disciplines are particularly valued and have received attention from researchers. The progress in multidisciplinary research will provide motivation and technical support for the development of this field."
Khashiie2022,Najiyah Safwa Khashi'ie and Iskandar Waini and Abdul Rahman Mohd Kasim and Nurul Amira Zainal and Anuar Ishak and Ioan Pop,Magnetohydrodynamic and viscous dissipation effects on radiative heat transfer of non-Newtonian fluid flow past a nonlinearly shrinking sheet: Reiner–Philippoff model,Alexandria Engineering Journal,61,10,2022,10.1016/j.aej.2022.01.014,11100168,"Heat transfer is an important process in many engineering, industrial, residential, and commercial buildings. Thus, this study aims to analyse the effect of MHD and viscous dissipation on radiative heat transfer of Reiner–Philippoff fluid flow over a nonlinearly shrinking sheet. By adopting appropriate similarity transformations, the partial derivatives of multivariable differential equations are transformed into the similarity equations of a particular form. The resulting mathematical model is elucidated in MATLAB software using the bvp4c technique. To determine the impact of physical parameters supplied into the problem, the results are shown in the form of tables and graphs. The findings reveal that the heat transfer rate reduces as the Eckert number and radiation parameter are introduced in the operating fluid. However, increasing the magnetic parameter raises both the skin friction coefficient and the local Nusselt number, which impulsively improves the heat transfer performance. The suction effect has a noticeable influence on the Reiner–Philippoff fluid, since increasing the suction parameter's value is seen to enhance the skin friction coefficient and the heat transfer performance. The dual solutions are established, leading to the stability analysis that supports the first solution's validity."
Samyal2021,Rahul Samyal and Ashok Kumar Bagha and Raman Bedi and Shashi Bahl and Kuldeep K. Saxena and Shankar Sehgal,Predicting the effect of fiber orientations and boundary conditions on the optimal placement of PZT sensor on the composite structures,Materials Research Express,8,7,2021,10.1088/2053-1591/ac0de9,20531591,"In this paper, the modal-model of the composite structure is predicted and viewed to decide the optimal position of the PZT sensors on the composite structures. The novelty of this work is to systematically study the effect of fiber orientations and boundary conditions on the modal-model and the optimal location of the PZT sensors on the composite structures. The glass fibers are reinforced in a polyester matrix at different fiber orientations such as 0°, 30°, 45°, 60° and 90°. It is used for various engineering applications, especially in the aerospace and automobile sector, and it is very important to measure its dynamical response. The PZT patches can be embedded on the composite structures to measure their vibrational response. In this paper, ABAQUS software is used to build the finite element model of the PZT-composite structure. The composite structure is modeled with different boundary conditions. It is observed that the orientation of the fibers as well as the boundary condition directly put their effect on the modal-model of the composite structure and also on the selection of the optimal position of the PZT patches. It is found that the optimal position of the PZT directly depends upon the fiber orientation."
Rademacher2020,Florian Rademacher and Sabine Sachweh and Albert Zündorf,A modeling method for systematic architecture reconstruction of microservice-based software systems,,387 LNBIP,,2020,10.1007/978-3-030-49418-6_21,18651356,"Microservice Architecture (MSA) is an approach to architecting service-based software systems, which aims for decreasing service coupling to enable independent service development and deployment. Consequently, the adoption of MSA is expected to particularly benefit the scalability, maintainability, and reliability of monolithic systems. However, MSA adoption also increases architectural complexity in service design, implementation, and operation. As a result, Software Architecture Reconstruction (SAR) of microservice architectures is aggravated. This paper presents a modeling method that systematizes SAR of microservice architectures with the goal to facilitate its execution. The method yields reconstruction models for certain architecture viewpoints in MSA to enable efficient architecture analysis. We validate the method’s applicability by means of a case study architecture and the assessment of its risk in technical debt using derived reconstruction models."
Holowko2021,Maciej B. Holowko and Emma K. Frow and Janet C. Reid and Michelle Rourke and Claudia E. Vickers,Building a biofoundry,Synthetic Biology,6,1,2021,10.1093/synbio/ysaa026,23977000,"A biofoundry provides automation and analytics infrastructure to support the engineering of biological systems. It allows scientists to perform synthetic biology and aligned experimentation on a high-throughput scale, massively increasing the solution space that can be examined for any given problem or question. However, establishing a biofoundry is a challenging undertaking, with numerous technical and operational considerations that must be addressed. Using collated learnings, here we outline several considerations that should be addressed prior to and during establishment. These include drivers for establishment, institutional models, funding and revenue models, personnel, hardware and software, data management, interoperability, client engagement and biosecurity issues. The high cost of establishment and operation means that developing a long-term business model for biofoundry sustainability in the context of funding frameworks, actual and potential client base, and costing structure is critical. Moreover, since biofoundries are leading a conceptual shift in experimental design for bioengineering, sustained outreach and engagement with the research community are needed to grow the client base. Recognition of the significant, long-term financial investment required and an understanding of the complexities of operationalization is critical for a sustainable biofoundry venture. To ensure state-of-the-art technology is integrated into planning, extensive engagement with existing facilities and community groups, such as the Global Biofoundries Alliance, is recommended."
Caas2020,José M. Cañas and Eduardo Perdices and Lía García-Pérez and Jesús Fernández-Conde,A ROS-based open tool for intelligent robotics education,Applied Sciences (Switzerland),10,21,2020,10.3390/app10217419,20763417,"This paper presents an open-access platform for practical learning of intelligent robotics in engineering degrees: Robotics-Academy. It comprises a collection of exercises including recent service robot applications in real life, with different robots such as autonomous cars, drones or vacuum cleaners. It uses Robot Operating System (ROS) middleware, the de facto standard in robot programming, the 3D Gazebo simulator and the Python programming language. For each exercise, a software template has been developed, performing all the auxiliary tasks such as the graphical interface, connection to the sensors and actuators, timing of the code, etc. This also hosts the student’s code. Using this template, the student just focuses on the robot intelligence (for instance, perception and control algorithms) without wasting time on auxiliary details which have little educational value. The templates are coded as ROS nodes or as Jupyter Notebooks ready to use in the web browser. Reference solutions for illustrative purposes and automatic assessment tools for gamification have also been developed. An introductory course to intelligent robotics has been elaborated and its contents are available and ready to use at Robotics-Academy, including reactive behaviors, path planning, local/global navigation, and self-localization algorithms. Robotics- Academy provides a valuable complement to master classes in blended learning, massive online open courses (MOOCs) and online video courses, devoted to addressing theoretical content. This open educational tool connects that theory with practical robot applications and is suitable to be used in distance education. Robotics-Academy has been successfully used in several subjects on undergraduate and master’s degree engineering courses, in addition to a pre-university pilot course."
Gonzalez2020,Danielle Gonzalez and Thomas Zimmermann and Nachiappan Nagappan,The State of the ML-universe: 10 Years of Artificial Intelligence & Machine Learning Software Development on GitHub,,,,2020,10.1145/3379597.3387473,,"In the last few years, artificial intelligence (AI) and machine learning (ML) have become ubiquitous terms. These powerful techniques have escaped obscurity in academic communities with the recent onslaught of AI & ML tools, frameworks, and libraries that make these techniques accessible to a wider audience of developers. As a result, applying AI & ML to solve existing and emergent problems is an increasingly popular practice. However, little is known about this domain from the software engineering perspective. Many AI & ML tools and applications are open source, hosted on platforms such as GitHub that provide rich tools for large-scale distributed software development. Despite widespread use and popularity, these repositories have never been examined as a community to identify unique properties, development patterns, and trends. In this paper, we conducted a large-scale empirical study of AI & ML Tool (700) and Application (4,524) repositories hosted on GitHub to develop such a characterization. While not the only platform hosting AI & ML development, GitHub facilitates collecting a rich data set for each repository with high traceability between issues, commits, pull requests and users. To compare the AI & ML community to the wider population of repositories, we also analyzed a set of 4,101 unrelated repositories. We enhance this characterization with an elaborate study of developer workflow that measures collaboration and autonomy within a repository. We've captured key insights of this community's 10 year history such as it's primary language (Python) and most popular repositories (Tensorflow, Tesseract). Our findings show the AI & ML community has unique characteristics that should be accounted for in future research."
Postorino2020,Maria Nadia Postorino and Giuseppe M.L. Sarné,Reinventing mobility paradigms: Flying car scenarios and challenges for urban mobility,Sustainability (Switzerland),12,9,2020,10.3390/SU12093581,20711050,"Flying vehicles are receiving more and more attention and are becoming an opportunity to start a new urban mobility paradigm. The most interesting feature of flying cars is the expected opportunity they could offer to reduce congestion, traffic jams and the loss of time to move between origin/destination pairs in urban contexts. In this perspective, urban air mobility might meet the concept of ""sustainable mobility"", intended as the ideal model of a transport system that minimizes the environmental impacts by maximizing efficiency and travel speed. For transport engineering planning issues, further knowledge is required in this field to understand the effects that a possible urban air mobility system, including the ground traffic component, could have in terms of sustainable mobility in the above meaning. This paper contributes to this topic by providing an analysis of different urban flying car scenarios by using an agent-based approach with different traffic conditions. The preliminary results obtained on some test networks and focusing on travel cost effects suggest that the expected advantages the flying car will depend on trip origin/destination points, average distances travelled in the urban contexts and the location of transition nodes, which are introduced as interchange nodes between aerial and ground mode."
Fomin2020,Oleksij Fomin and Alyona Lovska,Improvements in passenger car body for higher stability of train ferry,"Engineering Science and Technology, an International Journal",23,6,2020,10.1016/j.jestch.2020.08.010,22150986,"The authors suggest that the strength of passenger car bodies under train ferry transportation can be provided by mounting fastening elements of chain binders on the body bolster beams. The principle of such an element is based on the hydraulic damper operation Therefore, the authors developed a mathematical model which considered displacements of a train ferry loaded with passenger cars under rolling motion. The model was solved in Mathcad software. The mathematic modeling was conducted in order to determine the dynamic loading on a passenger car body under sea transportation. The study established that the improvements mentioned made it possible to reduce the dynamic loading on the body under sea transportation by 30% in comparison with that of a typical fastening scheme. The strength of an improved passenger car body was calculated. The maximum equivalent stresses in the body structure accounted for about 120 MPa, i.e., they did not exceed the admissible values. Besides, the study presents the computer modelling of dynamic loads of the carrying structure of a passenger car body under train ferry transportation. Numerical values of the accelerations and their distribution fields relative to the carrying structure of a car were determined. The adequacy of the designed models was checked with an F-test. The research and proposed engineering solutions can ensure the adequate strength of passenger car bodies under their transportation by train ferries, and also conduct efficient international rail/ferry transportation."
Saidy2022,Navid Toosi Saidy and Alicia Fernández-Colino and Behzad Shiroud Heidari and Ross Kent and Michael Vernon and Onur Bas and Shane Mulderrig and Andreas Lubig and José Carlos Rodríguez-Cabello and Barry Doyle and Dietmar W. Hutmacher and Elena M. De-Juan-Pardo and Petra Mela,Spatially Heterogeneous Tubular Scaffolds for In Situ Heart Valve Tissue Engineering Using Melt Electrowriting,Advanced Functional Materials,32,21,2022,10.1002/adfm.202110716,16163028,"Heart valve tissue engineering (HVTE) aims to provide living autologous heart valve implants endowed with regenerative capabilities and life-long durability. However, fabrication of biomimetic scaffolds capable of providing the required functionality in terms of mechanical performance and tunable porosity to enable cellular infiltration remains a major challenge. Here, the additive manufacturing of bioinspired, spatially heterogeneous, tubular scaffolds enclosing the leaflets, inter-leaflet triangles, and their interface for in situ HVTE using melt electrowriting (MEW) is demonstrated. The innovative platform enables the digital fabrication of scaffolds with ad hoc architecture (e.g., tunable location, specific fiber pattern, and orientation) and customizable geometry via a custom-made control software. The user-friendly interface allows for the definition of areas of the scaffold with specific patterns to obtain properties such as tunable J-shaped stress–stain curve and anisotropy typical of the heart valve leaflet, compliant inter-leaflet triangles, and reinforced curvilinear boundary between them. Heterogeneous, tubular, heart valve MEW scaffolds are then embedded with a microporous elastin-like recombinamer (ELR) hydrogel to develop a soft-network composite favoring cell infiltration and ensuring hemocompatibility. The acute systolic hemodynamic functionality of the MEW/ELR composite satisfies the ISO 5840 requirements, under aortic and pulmonary conditions."
Diadamo2021,Stephen Diadamo and Janis Nötzel and Benjamin Zanger and Mehmet Mert Beşe,QuNetSim: A Software Framework for Quantum Networks,IEEE Transactions on Quantum Engineering,2,,2021,10.1109/TQE.2021.3092395,26891808,"As quantum network technologies develop, the need for teaching and engineering tools such as simulators and emulators rises. QuNetSim addresses this need. QuNetSim is a Python software framework that delivers an easy-to-use interface for simulating quantum networks at the network layer, which can be extended at little effort of the user to implement the corresponding link layer protocols. The goal of QuNetSim is to make it easier to investigate and test quantum networking protocols over various quantum network configurations and parameters. The framework incorporates many known quantum network protocols so that users can quickly build simulations using a quantum-networking toolbox in a few lines of code and so that beginners can easily learn to implement their own quantum networking protocols. Unlike most current tools, QuNetSim simulates with real time and is, therefore, well suited to control laboratory hardware. Here, we present a software design overview of QuNetSim and demonstrate examples of protocols implemented with it. We describe ongoing work, which uses QuNetSim as a library, and describe possible future directions for the development of QuNetSim."
Daramola2020,Olawande Daramola and Darren Thebus,Architecture-centric evaluation of blockchain-based smart contract E-voting for national elections,Informatics,7,2,2020,10.3390/informatics7020016,22279709,"E-voting is one of the valid use cases of blockchain technology with many blockchain e-voting systems already proposed. But efforts that focus on critical analysis of blockchain e-voting architectures for national elections from stakeholders’ perspectives are mostly lacking in the literature. Therefore, government decision-makers and election stakeholders do not yet have a sufficient basis to understand the potential risks, challenges, and prospects that are associated with blockchain e-voting. This paper demonstrates how the use of the Architecture Trade-off Analysis Method (ATAM) can enable stakeholders in national elections to understand the risks, prospects, and challenges that could be associated with a blockchain e-voting system for national elections. By using a study context of South Africa, a proposed blockchain e-voting architecture was used as a basis to aid election stakeholders to reason on the concept of blockchain e-voting to get them to understand the potential risks, security threats, critical requirements attributes, and weaknesses that could be associated with using blockchain e-voting for national elections. The study found that blockchain e-voting can prevent many security attacks, internal vote manipulation, and promote transparency. However, voter validation and the security of the blockchain architecture are potential weaknesses that will need significant attention."
Ahmad2022,Shahnawaz Ahmad and Shabana Mehfuz and Fateh Mebarek-Oudina and Javed Beg,RSM analysis based cloud access security broker: a systematic literature review,Cluster Computing,25,5,2022,10.1007/s10586-022-03598-z,15737543,"A Cloud Access Security Broker (CASB) is a security enforcement point or cloud-based software that is placed between cloud service users and cloud applications of cloud computing (CC) which is used to run the dimensionality, heterogeneity, and ambiguity correlated with cloud services. They permit the organization to amplify the reach of their security approaches past their claim framework to third-party computer programs and storage. In contrast to other systematic literature reviews (SLR), this one is directed at the client setting. To identify and evaluate methods to understand CASB, the SLR discusses the literature, citing a comprehension of the state-of-the-art and innovative characterization to describe. An SLR was performed to compile CASB related experiments and analyze how CASBs are designed and formed. These studies are then analyzed from different contexts, like motivation, usefulness, building approach, and decision method. The SLR has discussed the contrasts present between the studies and implementations, with planning accomplishments conducted with combinations of market-based courses of action, simulation tools, middleware’s, etc. Search words with the keywords, which were extracted from the Research Questions (RQs), were utilized to recognize the essential consideration from the journal papers, conference papers, workshops, and symposiums. This SLR has distinguished 20 particular studies distributed from 2011 to 2021. Chosen studies were evaluated concurring to the defined RQs for their eminence and scope to particular CASB in this way recognizing a few gaps within the literature. Unlike other studies, this one concentrates on the customer's viewpoint. The survey uses a systematic analysis of the literature to discover and classify techniques for realizing CASB, resulting in a comprehensive grasp of the state-of-the-art and a novel taxonomy to describe CASBs. To assemble studies relating to CASB and investigate how CASB are engineered, a systematic literature review was done. These investigations are then evaluated from a variety of angles, including motivation, functionality, engineering approach, and methodology. Engineering efforts were directed at a combination of “market-based solutions”, “middlewares”, “toolkits”, “algorithms”, “semantic frameworks”, and “conceptual frameworks”, according to the study, which noted disparities in the studies’ implementations. For further understanding, the different independent parameters influencing the CASB are studied using PCA (Principal Component Analysis). The outcome of their analysis was the identification of five parameters influencing the PCA analysis. The experimental results were used as input for Research Surface Methodology (RSM) to obtain an empirical model. For this, five-level coding was employed for developing the model and considered three dependent parameters and four center values. For more understanding of these independent variables' influence, on the CASB study, RSM analysis was employed. It was observed from the CCD (Central Composite Design) model that the actual values show significant influence with R2 = 0.90. This wide investigation reveals that CASB is still in a formative state. Even though vital advancement has been carried out in this zone, obvious challenges stay to be tended to, which have been highlighted in this paper."
Frenz2020,Brandon Frenz and Steven M. Lewis and Indigo King and Frank DiMaio and Hahnbeom Park and Yifan Song,Prediction of Protein Mutational Free Energy: Benchmark and Sampling Improvements Increase Classification Accuracy,Frontiers in Bioengineering and Biotechnology,8,,2020,10.3389/fbioe.2020.558247,22964185,"Software to predict the change in protein stability upon point mutation is a valuable tool for a number of biotechnological and scientific problems. To facilitate the development of such software and provide easy access to the available experimental data, the ProTherm database was created. Biases in the methods and types of information collected has led to disparity in the types of mutations for which experimental data is available. For example, mutations to alanine are hugely overrepresented whereas those involving charged residues, especially from one charged residue to another, are underrepresented. ProTherm subsets created as benchmark sets that do not account for this often underrepresent tense certain mutational types. This issue introduces systematic biases into previously published protocols’ ability to accurately predict the change in folding energy on these classes of mutations. To resolve this issue, we have generated a new benchmark set with these problems corrected. We have then used the benchmark set to test a number of improvements to the point mutation energetics tools in the Rosetta software suite."
Nasello2020,Gabriele Nasello and Pilar Alamán-Díez and Jessica Schiavi and María Ángeles Pérez and Laoise McNamara and José Manuel García-Aznar,Primary Human Osteoblasts Cultured in a 3D Microenvironment Create a Unique Representative Model of Their Differentiation Into Osteocytes,Frontiers in Bioengineering and Biotechnology,8,,2020,10.3389/fbioe.2020.00336,22964185,"Microengineered systems provide an in vitro strategy to explore the variability of individual patient response to tissue engineering products, since they prefer the use of primary cell sources representing the phenotype variability. Traditional in vitro systems already showed that primary human osteoblasts embedded in a 3D fibrous collagen matrix differentiate into osteocytes under specific conditions. Here, we hypothesized that translating this environment to the organ-on-a-chip scale creates a minimal functional unit to recapitulate osteoblast maturation toward osteocytes and matrix mineralization. Primary human osteoblasts were seeded in a type I collagen hydrogel, to establish the role of lower (2.5 × 105 cells/ml) and higher (1 × 106 cells/ml) cell density on their differentiation into osteocytes. A custom semi-automatic image analysis software was used to extract quantitative data on cellular morphology from brightfield images. The results are showing that cells cultured at a high density increase dendrite length over time, stop proliferating, exhibit dendritic morphology, upregulate alkaline phosphatase (ALP) activity, and express the osteocyte marker dental matrix protein 1 (DMP1). On the contrary, cells cultured at lower density proliferate over time, do not upregulate ALP and express the osteoblast marker bone sialoprotein 2 (BSP2) at all timepoints. Our work reveals that microengineered systems create unique conditions to capture the major aspects of osteoblast differentiation into osteocytes with a limited number of cells. We propose that the microengineered approach is a functional strategy to create a patient-specific bone tissue model and investigate the individual osteogenic potential of the patient bone cells."
Harrison2021,James H. Harrison and John R. Gilbertson and Matthew G. Hanna and Niels H. Olson and Jansen N. Seheult and James M. Sorace and Michelle N. Stram,Introduction to artificial intelligence and machine learning for pathology,Archives of Pathology and Laboratory Medicine,145,10,2021,10.5858/arpa.2020-0541-CP,15432165,"Context.-Recent developments in machine learning have stimulated intense interest in software that may augment or replace human experts. Machine learning may impact pathology practice by offering new capabilities in analysis, interpretation, and outcomes prediction using images and other data. The principles of operation and management of machine learning systems are unfamiliar to pathologists, who anticipate a need for additional education to be effective as expert users and managers of the new tools. Objective.-To provide a background on machine learning for practicing pathologists, including an overview of algorithms, model development, and performance evaluation; to examine the current status of machine learning in pathology and consider possible roles and requirements for pathologists in local deployment and management of machine learning systems; and to highlight existing challenges and gaps in deployment methodology and regulation. Data Sources.-Sources include the biomedical and engineering literature, white papers from professional organizations, government reports, electronic resources, and authors' experience in machine learning. References were chosen when possible for accessibility to practicing pathologists without specialized training in mathematics, statistics, or software development. Conclusions.-Machine learning offers an array of techniques that in recent published results show substantial promise. Data suggest that human experts working with machine learning tools outperform humans or machines separately, but the optimal form for this combination in pathology has not been established. Significant questions related to the generalizability of machine learning systems, local site verification, and performance monitoring remain to be resolved before a consensus on best practices and a regulatory environment can be established."
Zito2021,Juliette Zito and Ivan Infante,The Future of Ligand Engineering in Colloidal Semiconductor Nanocrystals,Accounts of Chemical Research,54,7,2021,10.1021/acs.accounts.0c00765,15204898,"ConspectusNext-generation colloidal semiconductor nanocrystals featuring enhanced optoelectronic properties and processability are expected to arise from complete mastering of the nanocrystals' surface characteristics, attained by a rational engineering of the passivating ligands. This aspect is highly challenging, as it underlies a detailed understanding of the critical chemical processes that occur at the nanocrystal-ligand-solvent interface, a task that is prohibitive because of the limited number of nanocrystal syntheses that could be tried in the lab, where only a few dozen of the commercially available starting ligands can actually be explored. However, this challenging goal can be addressed nowadays by combining experiments with atomistic calculations and machine learning algorithms. In the last decades we indeed witnessed major advances in the development and application of computational software dedicated to the solution of the electronic structure problem as well as the expansion of tools to improve the sampling and analysis in classical molecular dynamics simulations. More recently, this progress has also embraced the integration of machine learning in computational chemistry and in the discovery of new drugs. We expect that soon this plethora of computational tools will have a formidable impact also in the field of colloidal semiconductor nanocrystals.In this Account, we present some of the most recent developments in the atomistic description of colloidal nanocrystals. In particular, we show how our group has been developing a set of programs interfaced with available computational chemistry software packages that allow the thermodynamic controlling factors in the nanocrystal surface chemistry to be captured atomistically by including explicit solvent molecules, ligands, and nanocrystal sizes that match the experiments. At the same time, we are also setting up an infrastructure to automate the efficient execution of thousands of calculations that will enable the collection of sufficient data to be processed by machine learning.To fully capture the power of these computational tools in the chemistry of colloidal nanocrystals, we decided to embed the thermodynamics behind the dissolution/precipitation of nanocrystal-ligand complexes in organic solvents and the crucial process of binding/detachment of ligands at the nanocrystal surface into a unique chemical framework. We show that formalizing this mechanism with a computational bird's eye view helps in deducing the critical factors that govern the stabilization of colloidal dispersions of nanocrystals in an organic solvent as well as the definition of those key parameters that need to be calculated to manipulate surface ligands. This approach has the ultimate goal of engineering surface ligands in silico, anticipating and driving the experiments in the lab."
Compton2020,Rhys Compton and Eibe Frank and Panos Patros and Abigail Koay,Embedding Java Classes with code2vec: Improvements from Variable Obfuscation,,,,2020,10.1145/3379597.3387445,,"Automatic source code analysis in key areas of software engineering, such as code security, can benefit from Machine Learning (ML). However, many standard ML approaches require a numeric representation of data and cannot be applied directly to source code. Thus, to enable ML, we need to embed source code into numeric feature vectors while maintaining the semantics of the code as much as possible. code2vec is a recently released embedding approach that uses the proxy task of method name prediction to map Java methods to feature vectors. However, experimentation with code2vec shows that it learns to rely on variable names for prediction, causing it to be easily fooled by typos or adversarial attacks. Moreover, it is only able to embed individual Java methods and cannot embed an entire collection of methods such as those present in a typical Java class, making it difficult to perform predictions at the class level (e.g., for the identification of malicious Java classes). Both shortcomings are addressed in the research presented in this paper. We investigate the effect of obfuscating variable names during training of a code2vec model to force it to rely on the structure of the code rather than specific names and consider a simple approach to creating class-level embeddings by aggregating sets of method embeddings. Our results, obtained on a challenging new collection of source-code classification problems, indicate that obfuscating variable names produces an embedding model that is both impervious to variable naming and more accurately reflects code semantics. The datasets, models, and code are shared1 for further ML research on source code."
deSilva2020,Lavindra de Silva and Felipe Meneguzzi and Brian Logan,BDI agent architectures: A survey,,2021-January,,2020,10.24963/ijcai.2020/684,10450823,"The BDI model forms the basis of much of the research on symbolic models of agency and agent-oriented software engineering. While many variants of the basic BDI model have been proposed in the literature, there has been no systematic review of research on BDI agent architectures in over 10 years. In this paper, we survey the main approaches to each component of the BDI architecture, how these have been realised in agent programming languages, and discuss the trade-offs inherent in each approach."
Rahman2020,Md Mostafizer Rahman and Yutaka Watanobe and Keita Nakamura,Source Code assessment and classification based on estimated error probability using attentive lstm language model and its application in programming education,Applied Sciences (Switzerland),10,8,2020,10.3390/APP10082973,20763417,"The rate of software development has increased dramatically. Conventional compilers cannot assess and detect all source code errors. Software may thus contain errors, negatively affecting end-users. It is also difficult to assess and detect source code logic errors using traditional compilers, resulting in software that contains errors. Amethod that utilizes artificial intelligence for assessing and detecting errors and classifying source code as correct (error-free) or incorrect is thus required. Here, we propose a sequential language model that uses an attention-mechanism-based long short-term memory (LSTM) neural network to assess and classify source code based on the estimated error probability. The attentive mechanism enhances the accuracy of the proposed language model for error assessment and classification. We trained the proposed model using correct source code and then evaluated its performance. The experimental results show that the proposed model has logic and syntax error detection accuracies of 92.2% and 94.8%, respectively, outperforming state-of-the-art models. We also applied the proposed model to the classification of source code with logic and syntax errors. The average precision, recall, and F-measure values for such classification are much better than those of benchmark models. To strengthen the proposed model, we combined the attention mechanism with LSTM to enhance the results of error assessment and detection as well as source code classification. Finally, our proposed model can be effective in programming education and software engineering by improving code writing, debugging, error-correction, and reasoning."
Doube2021,Michael Doube and Richard Domander and Alessandro A. Felder,BoneJ2 - refactoring established research software,Wellcome Open Research,6,,2021,10.12688/wellcomeopenres.16619.1,2398502X,"Research software is often developed with expedience as a core development objective because experimental results, but not the software, are specified and resourced as a project output. While such code can help find answers to specific research questions, it may lack longevity and flexibility to make it reusable. We reimplemented BoneJ, our software for skeletal biology image analysis, to address design limitations that put it at risk of becoming unusable. We improved the quality of BoneJ code by following contemporary best programming practices. These include separation of concerns, dependency management, thorough testing, continuous integration and deployment, source code management, code reviews, issue and task ticketing, and user and developer documentation. The resulting BoneJ2 represents a generational shift in development technology and integrates with the ImageJ2 plugin ecosystem."
Harel2020,David Harel and Assaf Marron and Joseph Sifakis,Autonomics: In search of a foundation for next-generation autonomous systems,Proceedings of the National Academy of Sciences of the United States of America,117,30,2020,10.1073/pnas.2003162117,10916490,"The potential benefits of autonomous systems are obvious. However, there are still major issues to be dealt with before developing such systems becomes a commonplace engineering practice, with accepted and trustworthy deliverables. We argue that a solid, evolving, publicly available, community-controlled foundation for developing next-generation autonomous systems is a must, and term the desired foundation “autonomics.” We focus on three main challenges: 1) how to specify autonomous system behavior in the face of unpredictability; 2) how to carry out faithful analysis of system behavior with respect to rich environments that include humans, physical artifacts, and other systems; and 3) how to build such systems by combining executable modeling techniques from software engineering with artificial intelligence and machine learning."
Meinert2020,Edward Meinert and Madison Milne-Ives and Svitlana Surodina and Ching Lam,Agile Requirements Engineering and Software Planning for a Digital Health Platform to Engage the Effects of Isolation Caused by Social Distancing: Case Study,JMIR Public Health and Surveillance,6,2,2020,10.2196/19297,23692960,"Background: Social distancing and shielding measures have been put in place to reduce social interaction and slow the transmission of the coronavirus disease (COVID-19). For older people, self-isolation presents particular challenges for mental health and social relationships. As time progresses, continued social distancing could have a compounding impact on these concerns. Objective: This project aims to provide a tool for older people and their families and peers to improve their well-being and health during and after regulated social distancing. First, we will evaluate the tool’s feasibility, acceptability, and usability to encourage positive nutrition, enhance physical activity, and enable virtual interaction while social distancing. Second, we will be implementing the app to provide an online community to assist families and peer groups in maintaining contact with older people using goal setting. Anonymized data from the app will be aggregated with other real-world data sources to develop a machine learning algorithm to improve the identification of patients with COVID-19 and track for real time use by health systems. Methods: Development of this project is occurring at the time of publication, and therefore, a case study design was selected to provide a systematic means of capturing software engineering in progress. The app development framework for software design was based on agile methods. The evaluation of the app’s feasibility, acceptability and usability shall be conducted using Public Health England's guidance on evaluating digital health products, Bandura’s model of health promotion, the Reach Effectiveness Adoption Implementation Maintenance (RE-AIM) framework and the Nonadoption, Abandonment and Challenges to the Scale-up, Spread and Suitability (NASSS) framework. Results: Making use of a pre-existing software framework for health behavior change, a proof of concept was developed, and a multistage app development and deployment for the solution was created. Grant submissions to fund the project and study execution have been sought at the time of publication, and prediscovery iteration of the solution has begun. Ethical approval for a feasibility study design is being sought. Conclusions: This case study lays the foundations for future app development to combat mental and societal issues arising from social distancing measures. The app will be tested and evaluated in future studies to allow continuous improvement of the app. This novel contribution will provide an evidence-based exemplar for future app development in the space of social isolation and loneliness."
Vissani2020,Matteo Vissani and Ioannis U. Isaias and Alberto Mazzoni,Deep brain stimulation: A review of the open neural engineering challenges,Journal of Neural Engineering,17,5,2020,10.1088/1741-2552/abb581,17412552,"Objective. Deep brain stimulation (DBS) is an established and valid therapy for a variety of pathological conditions ranging from motor to cognitive disorders. Still, much of the DBS-related mechanism of action is far from being understood, and there are several side effects of DBS whose origin is unclear. In the last years DBS limitations have been tackled by a variety of approaches, including adaptive deep brain stimulation (aDBS), a technique that relies on using chronically implanted electrodes on ‘sensing mode’ to detect the neural markers of specific motor symptoms and to deliver on-demand or modulate the stimulation parameters accordingly. Here we will review the state of the art of the several approaches to improve DBS and summarize the main challenges toward the development of an effective aDBS therapy. Approach. We discuss models of basal ganglia disorders pathogenesis, hardware and software improvements for conventional DBS, and candidate neural and non-neural features and related control strategies for aDBS. Main results. We identify then the main operative challenges toward optimal DBS such as (i) accurate target localization, (ii) increased spatial resolution of stimulation, (iii) development of in silico tests for DBS, (iv) identification of specific motor symptoms biomarkers, in particular (v) assessing how LFP oscillations relate to behavioral disfunctions, and (vi) clarify how stimulation affects the cortico-basal-ganglia-thalamic network to (vii) design optimal stimulation patterns. Significance. This roadmap will lead neural engineers novel to the field toward the most relevant open issues of DBS, while the in-depth readers might find a careful comparison of advantages and drawbacks of the most recent attempts to improve DBS-related neuromodulatory strategies."
Singh2023,Jashanpreet Singh and Simranjit Singh and Amit Verma,Artificial intelligence in use of ZrO2 material in biomedical science,Journal of Electrochemical Science and Engineering,13,1,2023,10.5599/jese.1498,18479286,"The rapidly growing discipline of artificial intelligence (AI) seeks to develop software and computers that can do tasks that have historically required the intelligence of people. Machine learning (ML) is a subfield of AI that makes use of algorithms to ""learn"" from data's innate statistical patterns and structures to extrapolate information that is otherwise hidden. A growing emphasis on cosmetic dentistry has coincided with rise of ZrO2 to prominence as a result of its improved biocompatibility, visually pleasant look, strong oxidation resistance, better mechanical properties, and lack of documented allergic responses. Advances in the field of AI and ML have led to novel applications of ZrO2 in dental devices for biological objectives. Artificial intelligence (AI) technologies have attracted a lot of attention in ZrO2-related research and therapeutic applications due to their ability to analyze data and discover connections between seemingly unrelated events. Specifically, their incorporation into zirconia is largely responsible for this. Versatility of zirconia in the scientific community means that how AI is used in the area varies with the specific directions in which zirconia is utilized. Therefore, this article primarily focuses on the use of AI in the biomedical use of ZrO2 in dentistry."
Paduano2020,Bruno Paduano and Giuseppe Giorgi and Rui P.F. Gomes and Edoardo Pasta and João C.C. Henriques and Luís M.C. Gato and Giuliana Mattiazzo,Experimental validation and comparison of numerical models for the mooring system of a floating wave energy converter,Journal of Marine Science and Engineering,8,8,2020,10.3390/JMSE8080565,20771312,"The mooring system of floating wave energy converters (WECs) has a crucial impact on power generation efficiency, cost of delivered energy, proper operation, reliability and survivability. An effective design, addressing such competing objectives, requires appropriate mathematical models to predict mooring loads and dynamic response. However, conversely to traditional offshore engineering applications, experience in modelling mooring systems for WECs is limited, due to their unique requirement of maximising the motion while minimising loads and costs. Even though modelling approaches and software are available for this application, guidelines and critical comparison are still scarce. This paper proposes a discussion and validation of three mooring-line models: one quasi-static approach (developed in-house) and two dynamic lumped-mass approaches (the open source MoorDyn and the commercial OrcaFlex). The case study is a 1:32-scale prototype of a floating oscillating water column WEC tested in a wave tank, with three mooring lines, each one comprising of a riser and a clump weight. Validation, performed by imposing fairlead displacements and comparing resulting tensions, shows good agreement. The small scale may induce numerical instabilities and uncertainties in the parameter estimation. Finally, likely due to internal resonance of this particular mooring system, high-frequency content in the mooring tension is found, albeit absent in the kinematics of the floater."
Soltani2020,Aref Soltani and Reza Noroozi and Mahdi Bodaghi and Ali Zolfagharian and Reza Hedayati,3D printing on-water sports boards with bio-inspired core designs,Polymers,12,1,2020,10.3390/polym12010249,20734360,"Modeling and analyzing the sports equipment for injury prevention, reduction in cost, and performance enhancement have gained considerable attention in the sports engineering community. In this regard, the structure study of on-water sports board (surfboard, kiteboard, and skimboard) is vital due to its close relation with environmental and human health as well as performance and safety of the board. The aim of this paper is to advance the on-water sports board through various bio-inspired core structure designs such as honeycomb, spiderweb, pinecone, and carbon atom configuration fabricated by three-dimensional (3D) printing technology. Fused deposition modeling was employed to fabricate complex structures from polylactic acid (PLA) materials. A 3D-printed sample board with a uniform honeycomb structure was designed, 3D printed, and tested under three-point bending conditions. A geometrically linear analytical method was developed for the honeycomb core structure using the energy method and considering the equivalent section for honeycombs. A geometrically non-linear finite element method based on the ABAQUS software was also employed to simulate the boards with various core designs. Experiments were conducted to verify the analytical and numerical results. After validation, various patterns were simulated, and it was found that bio-inspired functionally graded honeycomb structure had the best bending performance. Due to the absence of similar designs and results in the literature, this paper is expected to advance the state of the art of on-water sports boards and provide designers with structures that could enhance the performance of sports equipment."
SherAkbar2022,Noreen Sher Akbar and E. N. Maraj and N. F.M. Noor and Muhammad Bilal Habib,Exact solutions of an unsteady thermal conductive pressure driven peristaltic transport with temperature-dependent nanofluid viscosity,Case Studies in Thermal Engineering,35,,2022,10.1016/j.csite.2022.102124,2214157X,"Keeping in view the impact of temperature-dependent nanofluid viscosity on peristaltic transport, the present study is an analytical analysis to scrutinize an unsteady flow saturated with carbon nanotubes (CNT) in an irregular channel of finite measure. The ultimate goal is to obtain an exact solution for the stream function of the pressure-driven peristaltic flow of nanofluid with temperature-dependent nanofluid viscosity. Influences of CNT on temperature, axial and transverse velocities, effective thermal conductivity and on pressure gradient are studied analytically and displayed graphically by varying various flow constraints using a Mathematica software. The key findings of the analysis revealed that SWCNT nanofluids have lower pressure gradient, hence higher axial velocity than that of MWCNT whereas the trapped boluses are growing in size with increasing heat generation and decreasing thermal Grashof number. Since the transverse velocity for MWCNT nanofluid can be improved with higher viscosity, this study outlines details of a micro push in the movement of nanofluids as a supplement to medical applications especially for drugs delivery systems in peristaltic pumping and pharmacological engineering."
Kumar2022,Sachin Kumar and Brij Mohan,A novel and efficient method for obtaining Hirota's bilinear form for the nonlinear evolution equation in (n+1) dimensions,Partial Differential Equations in Applied Mathematics,5,,2022,10.1016/j.padiff.2022.100274,26668181,"Bilinearization of nonlinear partial differential equations (PDEs) is essential in the Hirota method, which is a widely used and robust mathematical tool for finding soliton solutions of nonlinear PDEs in a variety of fields, including nonlinear dynamics, mathematical physics, and engineering sciences. We present a novel systematic computational approach for determining the bilinear form of a class of nonlinear PDEs in this article. It can be easily implemented in symbolic system software like Mathematica, Matlab, and Maple because of its simplicity. The proven results are obtained by using a developed method in Mathematica and applying a logarithmic transformation to the dependent variable. Finally, the findings validate the implemented technique's competence, productivity, and dependability. The approach is a useful, authentic, and simple mathematical tool for calculating multiple soliton solutions to nonlinear evolution equations encountered in nonlinear sciences, plasma physics, ocean engineering, applied mathematics, and fluid dynamics."
Cattari2022,Serena Cattari and Guido Magenes,Benchmarking the software packages to model and assess the seismic response of unreinforced masonry existing buildings through nonlinear static analyses,Bulletin of Earthquake Engineering,20,4,2022,10.1007/s10518-021-01078-0,15731456,"Seismic modelling of unreinforced masonry (URM) buildings is addressed worldwide according to different approaches, not only at research level, but also in the current engineering practice. The analysts have so many different possible choices in interpreting the response of the examined structure and in transferring them into the model for the assessment that the achievable results may turn out in a huge scattering, as also testified by various comparative studies already available in the literature. Within this context, this paper is an overview of a wide research activity addressed to the benchmarking of software packages for the modelling and seismic assessment through nonlinear static analyses of URM buildings. The activity conveyed the effort of many experts from various Italian universities and was funded by the Italian Department of Civil Protection within the context of the ReLUIS projects. The main objective of the research is the critical analysis and the systematic comparison of the results obtained by using several modelling approaches and software package tools on selected benchmark examples in order to provide a useful and qualified reference to the engineering and scientific community. To this aim, different benchmark examples—of increasing complexity, ranging from the single panel to 3D existing buildings—have been specifically designed. While other papers from the teams involved in the research project delve on the specific results achieved on each of these case studies, this paper illustrates an overview on such benchmark structures, their purpose and the standardized criteria adopted to compare the results. Moreover, the whole set of benchmark case-studies is made available in this paper through their detailed input data allowing to be replicated also by other researchers and analysts."
Alarie2021,Stéphane Alarie and Charles Audet and Aïmen E. Gheribi and Michael Kokkolaras and Sébastien Le Digabel,Two decades of blackbox optimization applications,EURO Journal on Computational Optimization,9,,2021,10.1016/j.ejco.2021.100011,21924414,"This article reviews blackbox optimization applications of direct search optimization methods over the past twenty years. Emphasis is placed on the Mesh Adaptive Direct Search (MADS) derivative-free optimization algorithm. The main focus is on applications in three specific fields: energy, materials science, and computational engineering design. Nevertheless, other applications in science and engineering, including patents, are also considered. The breadth of applications demonstrates the versatility of MADS and highlights the evolution of its accompanying software NOMAD as a standard tool for blackbox optimization."
Dllner2020,Jürgen Döllner,Geospatial Artificial Intelligence: Potentials of Machine Learning for 3D Point Clouds and Geospatial Digital Twins,"PFG - Journal of Photogrammetry, Remote Sensing and Geoinformation Science",88,1,2020,10.1007/s41064-020-00102-3,25122819,"Artificial intelligence (AI) is changing fundamentally the way how IT solutions are implemented and operated across all application domains, including the geospatial domain. This contribution outlines AI-based techniques for 3D point clouds and geospatial digital twins as generic components of geospatial AI. First, we briefly reflect on the term “AI” and outline technology developments needed to apply AI to IT solutions, seen from a software engineering perspective. Next, we characterize 3D point clouds as key category of geodata and their role for creating the basis for geospatial digital twins; we explain the feasibility of machine learning (ML) and deep learning (DL) approaches for 3D point clouds. In particular, we argue that 3D point clouds can be seen as a corpus with similar properties as natural language corpora and formulate a “Naturalness Hypothesis” for 3D point clouds. In the main part, we introduce a workflow for interpreting 3D point clouds based on ML/DL approaches that derive domain-specific and application-specific semantics for 3D point clouds without having to create explicit spatial 3D models or explicit rule sets. Finally, examples are shown how ML/DL enables us to efficiently build and maintain base data for geospatial digital twins such as virtual 3D city models, indoor models, or building information models."
Pecorelli2020,Fabiano Pecorelli and Fabio Palomba and Foutse Khomh and Andrea De Lucia,Developer-Driven Code Smell Prioritization,,,,2020,10.1145/3379597.3387457,,"Code smells are symptoms of poor implementation choices applied during software evolution. While previous research has devoted effort in the definition of automated solutions to detect them, still little is known on how to support developers when prioritizing them. Some works attempted to deliver solutions that can rank smell instances based on their severity, computed on the basis of software metrics. However, this may not be enough since it has been shown that the recommendations provided by current approaches do not take the developer's perception of design issues into account. In this paper, we perform a first step toward the concept of developer-driven code smell prioritization and propose an approach based on machine learning able to rank code smells according to the perceived criticality that developers assign to them. We evaluate our technique in an empirical study to investigate its accuracy and the features that are more relevant for classifying the developer's perception. Finally, we compare our approach with a state-of-the-art technique. Key findings show that the our solution has an F-Measure up to 85% and outperforms the baseline approach."
Kuwajima2020,Hiroshi Kuwajima and Hirotoshi Yasuoka and Toshihiro Nakae,Engineering problems in machine learning systems,Machine Learning,109,5,2020,10.1007/s10994-020-05872-w,15730565,"Fatal accidents are a major issue hindering the wide acceptance of safety-critical systems that employ machine learning and deep learning models, such as automated driving vehicles. In order to use machine learning in a safety-critical system, it is necessary to demonstrate the safety and security of the system through engineering processes. However, thus far, no such widely accepted engineering concepts or frameworks have been established for these systems. The key to using a machine learning model in a deductively engineered system is decomposing the data-driven training of machine learning models into requirement, design, and verification, particularly for machine learning models used in safety-critical systems. Simultaneously, open problems and relevant technical fields are not organized in a manner that enables researchers to select a theme and work on it. In this study, we identify, classify, and explore the open problems in engineering (safety-critical) machine learning systems—that is, in terms of requirement, design, and verification of machine learning models and systems—as well as discuss related works and research directions, using automated driving vehicles as an example. Our results show that machine learning models are characterized by a lack of requirements specification, lack of design specification, lack of interpretability, and lack of robustness. We also perform a gap analysis on a conventional system quality standard SQuaRE with the characteristics of machine learning models to study quality models for machine learning systems. We find that a lack of requirements specification and lack of robustness have the greatest impact on conventional quality models."
Coetsee2021,T. Coetsee and R. J. Mostert and P. G.H. Pistorius and P. C. Pistorius,The effect of flux chemistry on element transfer in Submerged Arc Welding: Application of thermochemical modelling,Journal of Materials Research and Technology,11,,2021,10.1016/j.jmrt.2021.02.046,22387854,"Post-weld slag and weld metal analyses were used to interpret the effects of different commercial flux compositions on element transfer between molten flux (slag) and weld metal in Submerged Arc Welding (SAW). Selected fluoride based flux compositions cover a wide range of basicity index (BI) values of 0.5e3.0. Thermochemical modelling in FactSage software is used to simulate the welding process in terms of gas-slag-metal equilibria. The importance of the gas phase in SAW element transfer is illustrated. The model provides improved accuracy in predicted weld metal oxygen values (ppm O) compared to the generally used empirical relationship of weld metal ppm O vs. flux BI. Model predicted oxygen values are within 150 ppm of the analysed values, compared to the empirical relationship values which are within 240 ppm from the analysed values. The model provides resolution in ppm O values at BI > 1.8. This information is lacking in the empirical relationship with constant ppm O of 250 ppm at BI > 1.8. The measured ppm O values follow the FeeFeO equilibrium trend with a positive offset. The relative level of oxygen to deoxidation elements (Ti, Al, Mn, Si) in the weld metal is an important factor in oxide inclusion engineering. This model will aid in the specification of flux formulations to attain specific weld metal compositions for maximum acicular ferrite formation. In this way the weld metal mechanical properties can be improved. This model will reduce the number of welding tests required to develop new flux formulations."
Park2020,Joo Hyun Park and Lifeng Zhang,Kinetic Modeling of Nonmetallic Inclusions Behavior in Molten Steel: A Review,Metallurgical and Materials Transactions B: Process Metallurgy and Materials Processing Science,51,6,2020,10.1007/s11663-020-01954-1,10735615,"The kinetic modeling for the nucleation, size growth, and compositional evolution of nonmetallic inclusions in steel was extensively reviewed in the present article. The nucleation and initial growth of inclusion in molten steel during deoxidation as well as the collision growth, motion, removal, and entrapment of inclusions in the molten steel in continuous casting (CC) tundish and strand were discussed. Moreover, the recent studies on the prediction of inclusion composition in CC semiproducts were introduced. Since the 1990s, the development of thermodynamic model and relevant databases for inclusion engineering has been initiated by the steel industry. Later, the commercial software FACTSAGE employing the FACT database was widely used to predict the gas (atmosphere/bubble)–liquid (steel/slag/inclusion)–solid (refractory/slag/steel/inclusion) multiphase equilibria. With the help of the comprehensive thermodynamic database and solution models in conjunction with the development of user-friendly computing packages, the kinetics of inclusion evolution in molten steel can be successfully predicted based on several kinetic models such as the coupled reaction (CR) model, reaction zone model, and tank series recirculation (TSR) model. However, some parameters are needed to represent the real processes according to the model employed at different operational or experimental conditions. The effect of reoxidation on the evolution of inclusions in the ladle and tundish, which was experimentally confirmed, can be simulated by the effective equilibrium reaction zone (EERZ) model. The complex slag–steel interfacial reaction phenomena have been successfully explained by the interfacial kinetic model based on the dynamic interfacial tension and oxygen adsorption/desorption characteristics at the slag-steel interface."
Mangano2020,Francesco Mangano and Henriette Lerner and Bidzina Margiani and Ivan Solop and Nadezhda Latuta and Oleg Admakin,Congruence between meshes and library files of implant scanbodies: An in vitro study comparing five intraoral scanners,Journal of Clinical Medicine,9,7,2020,10.3390/jcm9072174,20770383,"Purpose. To compare the reliability of ﬁve diﬀerent intraoral scanners (IOSs) in the capture of implant scanbodies (SBs) and to verify the dimensional congruence between the meshes (MEs) of the SBs and the corresponding library ﬁle (LF). Methods. A gypsum cast of a fully edentulous maxilla with six implant analogues and SBs screwed on was scanned with ﬁve diﬀerent IOSs (PRIMESCAN®, CS 3700®, MEDIT i-500®, ITERO ELEMENTS 5D®, and Emerald S®). Ten scans were taken for each IOS. The resulting MEs were imported to reverse engineering software for 3D analysis, consisting of the superimposition of the SB LF onto each SB ME. Then, a quantitative and qualitative evaluation of the deviations between MEs and LF was performed. A careful statistical analysis was performed. Results. PRIMESCAN® showed the highest congruence between SB MEs and LF, with the lowest mean absolute deviation (25.5 ± 5.0 µm), immediately followed by CS 3700® (27.0±4.3 µm); the diﬀerence between them was not signiﬁcant (p = 0.1235). PRIMESCAN® showed a signiﬁcantly higher congruence than MEDIT i-500® (29.8±4.8 µm, p < 0.0001), ITERO ELEMENTS 5D® (34.2±9.3 µm, p < 0.0001), and Emerald S® (38.3±7.8 µm, p < 0.0001). CS 3700® had a signiﬁcantly higher congruence than MEDIT i-500® (p = 0.0004), ITERO ELEMENTS 5D® (p < 0.0001), and Emerald S® (p < 0.0001). Signiﬁcant diﬀerences were also found between MEDIT i-500® and ITERO ELEMENTS 5D® (p < 0.0001), MEDIT i-500® and Emerald S® (p < 0.0001), and ITERO ELEMENTS 5D® and Emerald S® (p < 0.0001). Signiﬁcant diﬀerences were found among diﬀerent SBs when scanned with the same IOS.The deviations of the IOSss howed diﬀerent directions and patterns. WithPRIMESCAN®,ITEROELEMENTS5D®,andEmeraldS®,theMEswereincluded inside the LF; with CS 3700®, the LF was included in the MEs. MEDIT i-500® showed interpolation between the MEs and LF, with no clear direction for the deviation. Conclusions. Statistically diﬀerent levelsof congruence were found between the SB MEs and the corresponding LF when using diﬀerent IOSs. Signiﬁcant diﬀerences were also found between diﬀerent SBs when scanned with the same IOS. Finally, the qualitative evaluation revealed diﬀerent directions and patterns for the ﬁve IOSs."
Fawzy2022,Samer Fawzy and Ahmed I. Osman and Neha Mehta and Donal Moran and Ala'a H. Al-Muhtaseb and David W. Rooney,Atmospheric carbon removal via industrial biochar systems: A techno-economic-environmental study,Journal of Cleaner Production,371,,2022,10.1016/j.jclepro.2022.133660,09596526,"It is critical to develop carbon removal projects that are both effective and financially viable. Herein, we investigated the carbon removal potential of an industrial biochar system in Spain. This study is the first to assess the techno-economic-environmental impact of large-scale olive tree pruning residue pyrolysis for atmospheric carbon removal, using an integrated assessment framework that is based on current market dynamics. Production optimization using response surface methodology (RSM) was carried out, aiming to maximize yield, production throughput and stable carbon content while prioritizing stability. It was determined that optimized biochar production was attained at 650 °C and 15 min residence time. Furthermore, a biochar plant with a biomass processing capacity of 6.5 tonnes-per-hour was designed for further analysis. A thermodynamic model was developed using Advanced System for Process Engineering (ASPEN Plus) software, and the process was determined to be self-sufficient with the availability of surplus energy. Moreover, a life cycle assessment (cradle-to-grave) revealed that approximately 2.68 tCO2e are permanently removed from the atmosphere per tonne of biochar produced, after accounting for the carbon footprint of the entire process. This corresponds to a carbon removal capacity of 3.26 tCO2e per hour and the removal of approximately 24,450 tCO2e annually. The economic assessment revealed that the project is profitable; however, profitability is sensitive to pricing of the carbon removal service and biochar. A project internal rate of return (IRR) of 22.35% is achieved at a price combination of EUR 110/tonne CO2e removal and EUR 350/tonne biochar, and a feedstock cost of 45 EUR/tonne (delivered with 20% moisture content), where service and product pricing are both within the lower bound of market pricing. If the project was exclusively designed to offer a carbon removal service, a minimum price of EUR 206/tonne CO2e removal is required to achieve project profitability, based on the same feedstock cost. The findings of this study demonstrate the viability of immediately deploying large-scale biochar-based carbon removal via pyrolytic conversion of olive tree pruning residues to address the climate crisis."
Spadini2020,Davide Spadini and Martin Schvarcbacher and Ana Maria Oprescu and Magiel Bruntink and Alberto Bacchelli,Investigating Severity Thresholds for Test Smells,,,,2020,10.1145/3379597.3387453,,"Test smells are poor design decisions implemented in test code, which can have an impact on the effectiveness and maintainability of unit tests. Even though test smell detection tools exist, how to rank the severity of the detected smells is an open research topic. In this work, we aim at investigating the severity rating for four test smells and investigate their perceived impact on test suite maintainability by the developers. To accomplish this, we first analyzed some 1,500 open-source projects to elicit severity thresholds for commonly found test smells. Then, we conducted a study with developers to evaluate our thresholds. We found that (1) current detection rules for certain test smells are considered as too strict by the developers and (2) our newly defined severity thresholds are in line with the participants' perception of how test smells have an impact on the maintainability of a test suite. Preprint [https://doi.org/10.5281/zenodo.3744281], data and material [https://doi.org/10.5281/zenodo.3611111]."
Marchesi2020,Lodovica Marchesi and Michele Marchesi and Roberto Tonelli,ABCDE—agile block chain DApp engineering,Blockchain: Research and Applications,1,1-2,2020,10.1016/j.bcra.2020.100002,26669536,"Blockchain software development is becoming more and more important for any modern software developer and IT startup. Nonetheless, blockchain software production still lacks a disciplined, organized and mature development process, as demonstrated by the many and (in)famous failures and frauds occurred in recent years. In this paper we present ABCDE, a complete method addressing blockchain software development. The method considers the software integration among the blockchain components—smart contracts, libraries, data structures—and the out-of-chain components, such as web or mobile applications, which all together constitute a complete DApp system. We advocate for ABCDE the use of agile practices, because these are suited to develop systems whose requirements are not completely understood since the beginning, or tend to change, as it is the case of most blockchain-based applications. ABCDE is based on Scrum, and is therefore iterative and incremental. From Scrum, we kept the requirement gathering with user stories, the iterative-incremental approach, the key roles, and the meetings. The main difference with Scrum is the separation of development activities in two flows—one for smart contracts and the other for out-of-chain software interacting with the blockchain—each performed iteratively, with integration activities every 2–3 iterations. ABCDE makes explicit the activities that must be performed to design, develop, test and integrate smart contracts and out-of-chain software, and documents the smart contracts using formal diagrams to help development, security assessment, and maintenance. A diagram derived from UML class diagram helps to effectively model the data structure of smart contracts, whereas the exchange of messages between the entities of the system is modeled using a modified UML sequence diagram. The proposed method has also specific activities for security assessment and gas optimization, through systematic use of patterns and checklists. ABCDE focuses on Ethereum blockchain and its Solidity language, but preserves generality and with proper modifications might be applied to any blockchain software project. ABCDE method is described in detail, and an example is given to show how to concretely implement the various development steps."
Ferrari2022,Alessio Ferrari and Manlio Bacco and Kirsten Gaber and Andreas Jedlitschka and Steffen Hess and Jouni Kaipainen and Panagiota Koltsida and Eleni Toli and Gianluca Brunori,"Drivers, barriers and impacts of digitalisation in rural areas from the viewpoint of experts",Information and Software Technology,145,,2022,10.1016/j.infsof.2021.106816,09505849,"Context: The domain of rural areas, including rural communities, agriculture, and forestry, is going through a process of deep digital transformation. Digitalisation can have positive impacts on sustainability in terms of greater environmental control, and community prosperity. At the same time, it can also have disruptive effects, with the marginalisation of actors that cannot cope with the change. When developing a novel system for rural areas, requirements engineers should carefully consider the specific socio-economic characteristics of the domain, so that potential positive effects can be maximised, while mitigating negative impacts. Objective: The goal of this paper is to support requirements engineers with a reference catalogue of drivers, barriers and potential impacts associated to the introduction of novel ICT solutions in rural areas. Method: To this end, we interview 30 cross-disciplinary experts in digitalisation of rural areas, and we analyse the transcripts to identify common themes. Results: According to the experts, main drivers are economic, with the possibility of reducing costs, and regulatory, as institutions push for more precise tracing and monitoring of production; barriers are the limited connectivity, but also distrust towards technology and other socio-cultural aspects; positive impacts are socio-economic (e.g., reduction of manual labour, greater productivity), while negative ones include potential dependency from technology, with loss of hands-on expertise, and marginalisation of certain actors (e.g., small farms, subjects with limited education). Conclusion: This paper contributes to the literature with a domain-specific catalogue that characterises digitalisation in rural areas. The catalogue can be used as a reference baseline for requirements elicitation endeavours in rural areas, to support domain analysis prior to the development of novel solutions, as well as fit-gap analysis for the adaptation of existing technologies."
Jiang2020,Lishuai Jiang and Yang Zhao and Naser Golsanami and Lianjun Chen and Weichao Yan,A novel type of neural networks for feature engineering of geological data: Case studies of coal and gas hydrate-bearing sediments,Geoscience Frontiers,11,5,2020,10.1016/j.gsf.2020.04.016,16749871,"The nature of the measured data varies among different disciplines of geosciences. In rock engineering, features of data play a leading role in determining the feasible methods of its proper manipulation. The present study focuses on resolving one of the major deficiencies of conventional neural networks (NNs) in dealing with rock engineering data. Herein, since the samples are obtained from hundreds of meters below the surface with the utmost difficulty, the number of samples is always limited. Meanwhile, the experimental analysis of these samples may result in many repetitive values and 0s. However, conventional neural networks are incapable of making robust models in the presence of such data. On the other hand, these networks strongly depend on the initial weights and bias values for making reliable predictions. With this in mind, the current research introduces a novel kind of neural network processing framework for the geological that does not suffer from the limitations of the conventional NNs. The introduced single-data-based feature engineering network extracts all the information wrapped in every single data point without being affected by the other points. This method, being completely different from the conventional NNs, re-arranges all the basic elements of the neuron model into a new structure. Therefore, its mathematical calculations were performed from the very beginning. Moreover, the corresponding programming codes were developed in MATLAB and Python since they could not be found in any common programming software at the time being. This new kind of network was first evaluated through computer-based simulations of rock cracks in the 3DEC environment. After the model's reliability was confirmed, it was adopted in two case studies for estimating respectively tensile strength and shear strength of real rock samples. These samples were coal core samples from the Southern Qinshui Basin of China, and gas hydrate-bearing sediment (GHBS) samples from the Nankai Trough of Japan. The coal samples used in the experiments underwent nuclear magnetic resonance (NMR) measurements, and Scanning Electron Microscopy (SEM) imaging to investigate their original micro and macro fractures. Once done with these experiments, measurement of the rock mechanical properties, including tensile strength, was performed using a rock mechanical test system. However, the shear strength of GHBS samples was acquired through triaxial and direct shear tests. According to the obtained result, the new network structure outperformed the conventional neural networks in both cases of simulation-based and case study estimations of the tensile and shear strength. Even though the proposed approach of the current study originally aimed at resolving the issue of having a limited dataset, its unique properties would also be applied to larger datasets from other subsurface measurements."
Haleem2020,Abid Haleem and Pawan Gupta and Shashi Bahl and Mohd Javaid and Lalit Kumar,3D scanning of a carburetor body using COMET 3D scanner supported by COLIN 3D software: Issues and solutions,,39,,2020,10.1016/j.matpr.2020.07.427,22147853,"Nowadays, the industry uses 3D Scanners for reverse engineering, new product design, rapid manufacturing, multimedia, architecture, inspection, and quality control. The scanning process converts a real object into a digital format. This paper's essential purpose is to show the use of a 3D blue light Scanner/COMET 3D to redesign a carburetor body. The paper identifies different issues involved in the processes to help future users. COMET 3D does scanning of the carburetor body by which COLIN 3D software is used for measurements, editing, and analyzing of the acquired point clouds data. This paper also identifies the necessary steps to undertake 3D Scanning and part dimensioning for a carburetor body. It also discusses the error/problems that occurred during the process. The applications of non-contact blue light 3D Scanners are many as they can be innovatively used to redesign an existing part, architecture designing, and reducing production cycle time, biomedical and associated applications. This paper's contribution lies in achieving a step-by-step procedure of scanning any three-dimensional object as this helps in understanding the 3D scanning hardware and support software. It provides good knowledge of how to resolve the issues that can cause an error during the measurement of the surfaces and scan objects."
Carvalho2020,José Pedro Carvalho and Luís Bragança and Ricardo Mateus,A systematic review of the role of BIM in building sustainability assessment methods,Applied Sciences (Switzerland),10,13,2020,10.3390/app10134444,20763417,"Building Information Modelling (BIM) is creating new opportunities for the Architecture, Engineering and Construction industry. One of them is the integration of the Building Sustainability Assessment (BSA) during the design process. Currently, an approach for using BIM to foster and optimise the application of BSA methods has not been clearly established yet, creating a knowledge gap on the application of BIM for sustainability assessment purposes. Thus, this paper analyses the current role of BIM to evaluate three BSA methods-LEED, BREEAM and SBTool. The current BIM applicability is assessed by performing a systematic review, where the criteria being assessed and the applied BIM software are identified. A comparison is made to determine which BSA method can currently take more advantage from BIM and to identify the number of assessed criteria from each one. Furthermore, the attractiveness of a BIM-based assessment for SBTool is analysed, facing the actual BIM scenario for LEED and BREEAM. Despite the restrictions, BIM use is increasing for sustainability purposes. Most of the analysed studies and identified software are still focused on the use of LEED for assessing sustainability during the design phase. However, BIM software capabilities can also support the assessment of the other BSA methods so that process replicability can happen. Among the most addressed criteria, the energy and material-related categories are the most eminent. Autodesk Revit is the most-used software. A BIM-based assessment for SBTool will have enough attractiveness. It can assess, at least, the same percentage of criteria as the other schemes, creating new opportunities to enhance building sustainability."
Hse2021,Florian Häse and Matteo Aldeghi and Riley J. Hickman and Loïc M. Roch and Melodie Christensen and Elena Liles and Jason E. Hein and Alán Aspuru-Guzik,Olympus: A benchmarking framework for noisy optimization and experiment planning,Machine Learning: Science and Technology,2,3,2021,10.1088/2632-2153/abedc8,26322153,"Research challenges encountered across science, engineering, and economics can frequently be formulated as optimization tasks. In chemistry and materials science, recent growth in laboratory digitization and automation has sparked interest in optimization-guided autonomous discovery and closed-loop experimentation. Experiment planning strategies based on off-the-shelf optimization algorithms can be employed in fully autonomous research platforms to achieve desired experimentation goals with the minimum number of trials. However, the experiment planning strategy that is most suitable to a scientific discovery task is a priori unknown while rigorous comparisons of different strategies are highly time and resource demanding. As optimization algorithms are typically benchmarked on low-dimensional synthetic functions, it is unclear how their performance would translate to noisy, higher-dimensional experimental tasks encountered in chemistry and materials science. We introduce Olympus, a software package that provides a consistent and easy-to-use framework for benchmarking optimization algorithms against realistic experiments emulated via probabilistic deep-learning models. Olympus includes a collection of experimentally derived benchmark sets from chemistry and materials science and a suite of experiment planning strategies that can be easily accessed via a user-friendly Python interface. Furthermore, Olympus facilitates the integration, testing, and sharing of custom algorithms and user-defined datasets. In brief, Olympus mitigates the barriers associated with benchmarking optimization algorithms on realistic experimental scenarios, promoting data sharing and the creation of a standard framework for evaluating the performance of experiment planning strategies."
Bongard2021,Joshua Bongard and Michael Levin,Living Things Are Not (20th Century) Machines: Updating Mechanism Metaphors in Light of the Modern Science of Machine Behavior,Frontiers in Ecology and Evolution,9,,2021,10.3389/fevo.2021.650726,2296701X,"One of the most useful metaphors for driving scientific and engineering progress has been that of the “machine.” Much controversy exists about the applicability of this concept in the life sciences. Advances in molecular biology have revealed numerous design principles that can be harnessed to understand cells from an engineering perspective, and build novel devices to rationally exploit the laws of chemistry, physics, and computation. At the same time, organicists point to the many unique features of life, especially at larger scales of organization, which have resisted decomposition analysis and artificial implementation. Here, we argue that much of this debate has focused on inessential aspects of machines – classical properties which have been surpassed by advances in modern Machine Behavior and no longer apply. This emerging multidisciplinary field, at the interface of artificial life, machine learning, and synthetic bioengineering, is highlighting the inadequacy of existing definitions. Key terms such as machine, robot, program, software, evolved, designed, etc., need to be revised in light of technological and theoretical advances that have moved past the dated philosophical conceptions that have limited our understanding of both evolved and designed systems. Moving beyond contingent aspects of historical and current machines will enable conceptual tools that embrace inevitable advances in synthetic and hybrid bioengineering and computer science, toward a framework that identifies essential distinctions between fundamental concepts of devices and living agents. Progress in both theory and practical applications requires the establishment of a novel conception of “machines as they could be,” based on the profound lessons of biology at all scales. We sketch a perspective that acknowledges the remarkable, unique aspects of life to help re-define key terms, and identify deep, essential features of concepts for a future in which sharp boundaries between evolved and designed systems will not exist."
Lim2021,Sachiko Lim and Aron Henriksson and Jelena Zdravkovic,Data-Driven Requirements Elicitation: A Systematic Literature Review,SN Computer Science,2,1,2021,10.1007/s42979-020-00416-4,26618907,"Requirements engineering has traditionally been stakeholder-driven. In addition to domain knowledge, widespread digitalization has led to the generation of vast amounts of data (Big Data) from heterogeneous digital sources such as the Internet of Things (IoT), mobile devices, and social networks. The digital transformation has spawned new opportunities to consider such data as potentially valuable sources of requirements, although they are not intentionally created for requirements elicitation. A challenge to data-driven requirements engineering concerns the lack of methods to facilitate seamless and autonomous requirements elicitation from such dynamic and unintended digital sources. There are numerous challenges in processing the data effectively to be fully exploited in organizations. This article, thus, reviews the current state-of-the-art approaches to data-driven requirements elicitation from dynamic data sources and identifies research gaps. We obtained 1848 hits when searching six electronic databases. Through a two-level screening and a complementary forward and backward reference search, 68 papers were selected for final analysis. The results reveal that the existing automated requirements elicitation primarily focuses on utilizing human-sourced data, especially online reviews, as requirements sources, and supervised machine learning for data processing. The outcomes of automated requirements elicitation often result in mere identification and classification of requirements-related information or identification of features, without eliciting requirements in a ready-to-use form. This article highlights the need for developing methods to leverage process-mediated and machine-generated data for requirements elicitation and addressing the issues related to variety, velocity, and volume of Big Data for the efficient and effective software development and evolution."
Biswas2020,Sumon Biswas and Hridesh Rajan,Do the machine learning models on a crowd sourced platform exhibit bias? An empirical study on model fairness,,,,2020,10.1145/3368089.3409704,,"Machine learning models are increasingly being used in important decision-making software such as approving bank loans, recommending criminal sentencing, hiring employees, and so on. It is important to ensure the fairness of these models so that no discrimination is made based on protected attribute (e.g., race, sex, age) while decision making. Algorithms have been developed to measure unfairness and mitigate them to a certain extent. In this paper, we have focused on the empirical evaluation of fairness and mitigations on real-world machine learning models. We have created a benchmark of 40 top-rated models from Kaggle used for 5 different tasks, and then using a comprehensive set of fairness metrics, evaluated their fairness. Then, we have applied 7 mitigation techniques on these models and analyzed the fairness, mitigation results, and impacts on performance. We have found that some model optimization techniques result in inducing unfairness in the models. On the other hand, although there are some fairness control mechanisms in machine learning libraries, they are not documented. The mitigation algorithm also exhibit common patterns such as mitigation in the post-processing is often costly (in terms of performance) and mitigation in the pre-processing stage is preferred in most cases. We have also presented different trade-off choices of fairness mitigation decisions. Our study suggests future research directions to reduce the gap between theoretical fairness aware algorithms and the software engineering methods to leverage them in practice."
Awan2022,Usama Awan and Lea Hannola and Anushree Tandon and Raman Kumar Goyal and Amandeep Dhir,Quantum computing challenges in the software industry. A fuzzy AHP-based approach,Information and Software Technology,147,,2022,10.1016/j.infsof.2022.106896,09505849,"Context: The current technology revolution has posed unexpected challenges for the software industry. In recent years, the field of quantum computing (QC) technologies has continued to grow in influence and maturity, and it is now poised to revolutionise software engineering. However, the evaluation and prioritisation of QC challenges in the software industry remain unexplored, relatively under-identified and fragmented. Objective: The purpose of this study is to identify, examine and prioritise the most critical challenges in the software industry by implementing a fuzzy analytic hierarchy process (F-AHP). Method: First, to identify the key challenges, we conducted a systematic literature review by drawing data from the four relevant digital libraries and supplementing these efforts with a forward and backward snowballing search. Second, we followed the F-AHP approach to evaluate and rank the identified challenges, or barriers. Results: The results show that the key barriers to QC adoption are the lack of technical expertise, information accuracy and organisational interest in adopting the new process. Another critical barrier is the lack of standards of secure communication techniques for implementing QC. Conclusion: By applying F-AHP, we identified institutional barriers as the highest and organisational barriers as the second highest global weight ranked categories among the main QC challenges facing the software industry. We observed that the highest-ranked local barriers facing the software technology industry are the lack of resources for design and initiative while the lack of organisational interest in adopting the new process is the most significant organisational barrier. Our findings, which entail implications for both academicians and practitioners, reveal the emergent nature of QC research and the increasing need for interdisciplinary research to address the identified challenges."
Hand2022,K. P. Hand and C. B. Phillips and A. Murray and J. B. Garvin and E. H. Maize and R. G. Gibbs and G. Reeves and A. M. San Martin and G. H. Tan-Wang and J. Krajewski and K. Hurst and R. Crum and B. A. Kennedy and T. P. McElrath and J. C. Gallon and D. Sabahi and S. W. Thurman and B. Goldstein and P. Estabrook and S. W. Lee and J. A. Dooley and W. B. Brinckerhoff and K. S. Edgett and C. R. German and T. M. Hoehler and S. M. Hörst and J. I. Lunine and C. Paranicas and K. Nealson and D. E. Smith and A. S. Templeton and M. J. Russell and B. Schmidt and B. Christner and B. Ehlmann and A. Hayes and A. Rhoden and P. Willis and R. A. Yingst and K. Craft and M. E. Cameron and T. Nordheim and J. Pitesky and J. Scully and J. Hofgartner and S. W. Sell and K. J. Barltrop and J. Izraelevitz and E. J. Brandon and J. Seong and J. P. Jones and J. Pasalic and K. J. Billings and J. P. Ruiz and R. V. Bugga and D. Graham and L. A. Arenas and D. Takeyama and M. Drummond and H. Aghazarian and A. J. Andersen and K. B. Andersen and E. W. Anderson and A. Babuscia and P. G. Backes and E. S. Bailey and D. Balentine and C. G. Ballard and D. F. Berisford and P. Bhandari and K. Blackwood and G. S. Bolotin and E. A. Bovre and J. Bowkett and K. T. Boykins and M. S. Bramble and T. M. Brice and P. Briggs and A. P. Brinkman and S. M. Brooks and B. B. Buffington and B. Burns and M. L. Cable and S. Campagnola and L. A. Cangahuala and G. A. Carr and J. R. Casani and N. E. Chahat and B. K. Chamberlain-Simon and Y. Cheng and S. A. Chien and B. T. Cook and M. Cooper and M. DiNicola and B. Clement and Z. Dean and E. A. Cullimore and A. G. Curtis and J. P. de la Croix and P. Di Pasquale and E. M. Dodd and L. A. Dubord and J. A. Edlund and R. Ellyin and B. Emanuel and J. T. Foster and A. J. Ganino and G. J. Garner and M. T. Gibson and M. Gildner and K. J. Glazebrook and M. E. Greco and W. M. Green and S. J. Hatch and M. M. Hetzel and W. A. Hoey and A. E. Hofmann and R. Ionasescu and A. Jain and J. D. Jasper and J. R. Johannesen and G. K. Johnson and I. Jun and A. B. Katake and S. Y. Kim-Castet and D. I. Kim and W. Kim and E. F. Klonicki and B. Kobeissi and B. D. Kobie and J. Kochocki and M. Kokorowski and J. A. Kosberg and K. Kriechbaum and T. P. Kulkarni and R. L. Lam and D. F. Landau and M. A. Lattimore and S. L. Laubach and C. R. Lawler and G. Lim and J. Y. Lin and T. E. Litwin and M. W. Lo and C. A. Logan and E. Maghasoudi and L. Mandrake and Y. Marchetti and E. Marteau and K. A. Maxwell and J. B. Mc Namee and O. McIntyre and M. Meacham and J. P. Melko and J. Mueller and D. A. Muliere and A. Mysore and J. Nash and H. Ono and J. M. Parker and R. C. Perkins and A. E. Petropoulos and A. Gaut and M. Y. Piette Gomez and R. P. Casillas and M. Preudhomme and G. Pyrzak and J. Rapinchuk and J. M. Ratliff and T. L. Ray and E. T. Roberts and K. Roffo and D. C. Roth and J. A. Russino and T. M. Schmidt and M. J. Schoppers and J. S. Senent and F. Serricchio and D. J. Sheldon and L. R. Shiraishi and J. Shirvanian and K. J. Siegel and G. Singh and A. R. Sirota and E. D. Skulsky and J. S. Stehly and N. J. Strange and S. U. Stevens and E. T. Sunada and S. P. Tepsuporn and L. P.C. Tosi and N. Trawny and I. Uchenik and V. Verma and R. A. Volpe and C. T. Wagner and D. Wang and R. G. Willson and J. L. Wolff and A. T. Wong and A. K. Zimmer and K. G. Sukhatme and K. A. Bago and Y. Chen and A. M. Deardorff and R. S. Kuch and C. Lim and M. L. Syvertson and G. A. Arakaki and A. Avila and K. J. DeBruin and A. Frick and J. R. Harris and M. C. Heverly and J. M. Kawata and S. K. Kim and D. M. Kipp and J. Murphy and M. W. Smith and M. D. Spaulding and R. Thakker and N. Z. Warner and C. R. Yahnker and M. E. Young and T. Magner and D. Adams and P. Bedini and L. Mehr and C. Sheldon and S. Vernon and V. Bailey and M. Briere and M. Butler and A. Davis and S. Ensor and M. Gannon and A. Haapala-Chalk and T. Hartka and M. Holdridge and A. Hong and J. Hunt and J. Iskow and F. Kahler and K. Murray and D. Napolillo and M. Norkus and R. Pfisterer and J. Porter and D. Roth and P. Schwartz and L. Wolfarth and E. H. Cardiff and A. Davis and E. W. Grob and J. R. Adam and E. Betts and J. Norwood and M. M. Heller and T. Voskuilen and P. Sakievich and L. Gray and D. J. Hansen and K. W. Irick and J. C. Hewson and J. Lamb and S. C. Stacy and C. M. Brotherton and A. S. Tappan and D. Benally and H. Thigpen and E. Ortiz and D. Sandoval and A. M. Ison and M. Warren and P. G. Stromberg and P. M. Thelen and B. Blasy and P. Nandy and A. W. Haddad and L. B. Trujillo and T. H. Wiseley and S. A. Bell and N. P. Teske and C. Post and L. Torres-Castro and C. Grosso and M. Wasiolek,Science Goals and Mission Architecture of the Europa Lander Mission Concept,Planetary Science Journal,3,1,2022,10.3847/PSJ/ac4493,26323338,"Europa is a premier target for advancing both planetary science and astrobiology, as well as for opening a new window into the burgeoning field of comparative oceanography. The potentially habitable subsurface ocean of Europa may harbor life, and the globally young and comparatively thin ice shell of Europa may contain biosignatures that are readily accessible to a surface lander. Europa's icy shell also offers the opportunity to study tectonics and geologic cycles across a range of mechanisms and compositions. Here we detail the goals and mission architecture of the Europa Lander mission concept, as developed from 2015 through 2020. The science was developed by the 2016 Europa Lander Science Definition Team (SDT), and the mission architecture was developed by the preproject engineering team, in close collaboration with the SDT. In 2017 and 2018, the mission concept passed its mission concept review and delta-mission concept review, respectively. Since that time, the preproject has been advancing the technologies, and developing the hardware and software, needed to retire risks associated with technology, science, cost, and schedule."
Javaid2021,Mohd Javaid and Abid Haleem and Ravi Pratap Singh and Rajiv Suman,"Industrial perspectives of 3D scanning: Features, roles and it's analytical applications",Sensors International,2,,2021,10.1016/j.sintl.2021.100114,26663511,"3D scanning is one of the lesser talked about technologies used for designing, inspection, and quality control. This non-contact measuring technology converts a physical model into digital 3D Computer-Aided Design (CAD) with the help of various scanning software's. It is becoming an essential tool for producers who need an accurate dimensional inspection, virtual image, analysis, and even physical prototype manufacturing. This paper aims to discuss the potential of 3D scanning for the Industrial Sphere. It will take up 3D Scanners for practical industrial support and develop the Work-Flow Process of 3D Scanners for Industrial requirements. Further, the paper identifies and discusses sixteen major applications of 3D scanning from an Industrial perspective. 3D scanners use sensors to sense the data of any product. This technology can easily capture the virtual image of a physical part, and the same can be analysed, modified, printed, and stored. It allows for careful preparation of production systems involving machinery placement, facilities, repair and human ergonomic interplay. It is a vital performance measure to ensure the initial vision has been realised as intended. The automobile sector ensures that the produced product fits as per the manufacturer's requirement and for quality control. 3D scanning is helpful for reverse engineering, analysis, designing and measuring complex curved surfaces, education, architecture, survey, healthcare, quality monitoring, prototyping, development of industrial tools and many more. This technology uses advanced software for accurate measurement, storage, analysis, which helps increase the process's flexibility and reliability."
Gupta2022,Munish Kumar Gupta and Mehmet Erdi Korkmaz and Murat Sarıkaya and Grzegorz M. Krolczyk and Mustafa Günay and Szymon Wojciechowski,Cutting forces and temperature measurements in cryogenic assisted turning of AA2024-T351 alloy: An experimentally validated simulation approach,Measurement: Journal of the International Measurement Confederation,188,,2022,10.1016/j.measurement.2021.110594,02632241,"Aluminium alloys are widely used in modern engineering applications such as automobile, aerospace etc because of its characteristics. The machining of aluminium alloys are also considered as difficult because of its sticky and soft nature, low thermal conductivity, strain hardening effect etc. The cooling conditions employed at cutting zone improved the machining performance but the resources, material consumption, skilled labor etc. are also required for performing the machining experiments. Therefore, the simulation of process parameters with the help of Finite Element Modelling (FEM) during machining is highly researched topic these days. In this work, a new practice from measurement science i.e., FEM simulation was performed with AdvantEdge software and the prediction models were developed for evaluating the cutting forces and cutting temperature while machining AA2024-T351 alloy under dry, liquid nitrogen (LN2) and carbon dioxide (CO2) conditions. Initially, the 3D turning model was developed and the results were compared with experimental findings. The results obtained from simulation model are very close with experimental results with minimum standard value of 0.67 (5.7%) for cutting forces and 4.58 (6.16%) for cutting temperature. Thus, it is worthy to mention that the 3D FE model is efficient and effective to predict and measurement results with minimum error."
Novak2021,A. J. Novak and R. W. Carlsen and S. Schunert and P. Balestra and D. Reger and R. N. Slaybaugh and R. C. Martineau,Pronghorn: A Multidimensional Coarse-Mesh Application for Advanced Reactor Thermal Hydraulics,Nuclear Technology,207,7,2021,10.1080/00295450.2020.1825307,19437471,"This paper presents an overview of Pronghorn, a multiscale thermal-hydraulic (T/H) application developed by Idaho National Laboratory and the University of California, Berkeley. Pronghorn, built on the open-source finite element Multiphysics Object-Oriented Simulation Environment (MOOSE), leverages state-of-the-art physical models, numerical methods, and nonlinear solvers to deliver fast-running advanced reactor T/H simulation capabilities within a modern software engineering environment. This work summarizes the physical models, multiphysics and multiscale coupling, and numerical discretization in Pronghorn with emphasis on our initial target application to pebble bed reactors (PBRs). A diverse set of applications are shown to depressurized natural circulation in the SANA experiments, forced convection in the Pebble Bed Modular Reactor, three-dimensional (3-D)/one-dimensional coupling of Pronghorn and RELAP-7 systems T/H for loop analysis in the High Temperature Reactor Power Module, and forced convection in the Mark-1 Pebble Bed Fluoride-Salt-Cooled High-Temperature Reactor. A multiphysics coupling of Pronghorn, RELAP-7, and Griffin deterministic neutronics for a gas-cooled PBR demonstrates the capability of the MOOSE framework for reactor design calculations. These applications highlight the verification and validation underlying Pronghorn’s software development while emphasizing features that improve upon capabilities offered by legacy tools in areas such as 3-D unstructured meshing, physics modeling, and multiphysics coupling."
Al-Hawari2021,Feras Al-Hawari and Hala Barham,A machine learning based help desk system for IT service management,Journal of King Saud University - Computer and Information Sciences,33,6,2021,10.1016/j.jksuci.2019.04.001,22131248,"A help desk system that acts as a single point of contact between users and IT staff is introduced in this paper. It utilizes an accurate ticket classification machine learning model to associate a help desk ticket with its correct service from the start and hence minimize ticket resolution time, save human resources, and enhance user satisfaction. The model is generated according to an empirically developed methodology that is comprised of the following steps: training tickets generation, ticket data preprocessing, words stemming, feature vectorization, and machine learning algorithm tuning. Nevertheless, the experimental results showed that including the ticket comments and description in the training data was one of the main factors that enhanced the model prediction accuracy from 53.8% to 81.4%. Furthermore, the system supports an administrator view that facilitates defining offered services, administering user roles, managing tickets and generating management reports. Also, it offers a user view that allows employees to report issues, request services, and exchange information with the IT staff via help desk tickets. Moreover, it supports automatic email notifications amongst collaborators for further action. Yet, it helps in defining business processes with well-defined activities and measuring KPIs to assess the performance of IT staff and processes."
Mehdi2020,Husain Mehdi and R. S. Mishra,Investigation of mechanical properties and heat transfer of welded joint of AA6061 and AA7075 using TIG+FSP welding approach,Journal of Advanced Joining Processes,1,,2020,10.1016/j.jajp.2020.100003,26663309,"Joining of dissimilar aluminum alloys are required in many engineering applications. The fusion welding often results in defective weld like porosity, micro cracks, coarse grain structure and high residual stress. To avoid these defects the top surface of Tungsten inert gas (TIG) welded joint are processed using friction stir processing (FSP). In this work, The influences of friction stir processing on the TIG welding with filler wire ER4043 and ER5356 for dissimilar aluminum alloy AA6061 and AA7075 were carried out and investigate the residual stresses and mechanical properties of (TIG+FSP) welded joint and analyze the finite element formulation and mathematical equations of heat transfer and material flow of TIG + FSP welded joint using ANSYS fluent software by adjusting the processing parameters of FSP. The results show that the maximum compressive residual stress 64 MPa were obtained at the fusion zone (FZ) of the TIG weldment with filler ER4043, whereas minimum compressive residual stress 39 MPa was obtained at stir zone (SZ) of the TIG+FSP with filler 5356. The maximum heat flux 5.33 × 106 W/m2 and temperature 511 °C were observed at tool rotation 1300 rpm with feed rate 44 mm/min. These results give the satisfactory measure of confidence in the fidelity of simulation."
Bar2020,Neil Bar and Michael Kostadinovski and Michael Tucker and Glen Byng and Rully Rachmatullah and Arturo Maldonado and Markus Pötsch and Andreas Gaich and Alison McQuillan and Thamer Yacoub,Rapid and robust slope failure appraisal using aerial photogrammetry and 3D slope stability models,International Journal of Mining Science and Technology,30,5,2020,10.1016/j.ijmst.2020.05.013,20952686,"Slope failures are an inevitable aspect of economic pit slope designs in the mining industry. Large open pit guidelines and industry standards accept up to 30% of benches in open pits to collapse provided that they are controlled and that no personnel are at risk. Rigorous ground control measures including real time monitoring systems at TARP (trigger-action-response-plan) protocols are widely utilized to prevent personnel from being exposed to slope failure risks. Technology and computing capability are rapidly evolving. Aerial photogrammetry techniques using UAV (unmanned aerial vehicle) enable geotechnical engineers and engineering geologists to work faster and more safely by removing themselves from potential line-of-fire near unstable slopes. Slope stability modelling software using limit equilibrium (LE) and finite element (FE) methods in three dimensions (3D) is also becoming more accessible, user-friendly and faster to operate. These key components enable geotechnical engineers to undertake site investigations, develop geotechnical models and assess slope stability faster and in more detail with less exposure to fall of ground hazards in the field. This paper describes the rapid and robust process utilized at BHP Limited for appraising a slope failure at an iron ore mine site in the Pilbara region of Western Australia using a combination of UAV photogrammetry and 3D slope stability models in less than a shift (i.e. less than 12 h)."
Arif2020,Alaa Uldeen Athil Arif and Mohamed Tarek Sorour and Samia Ahmed Aly,Cost analysis of activated sludge and membrane bioreactor WWTPs using CapdetWorks simulation program: Case study of Tikrit WWTP (middle Iraq),Alexandria Engineering Journal,59,6,2020,10.1016/j.aej.2020.08.023,11100168,"This paper aims to display benefits of applying economic modeling to predict the most economic treatment among three wastewater treatment plants containing conventional activated sludge without denitrification (CAS), conventional activated sludge with pre-denitrification (CAS-N), and membrane bioreactor (MBR) using CapdetWorks (v.3) simulation software. The software was run for each alternative with an average flow of 30,000 m3/d to calculate and evaluate the current worth costs of these WWTPs. Besides, the price per cubic meter of influent flow of the WWTPs has been evaluated. Tikrit WWTP was selected as a case study. Results demonstrated that the current worth cost of the MBR plant is higher than that for the CAS and CAS-N by about 57%, 42% respectively. Furthermore, the price per cubic meter of influent flow of the CAS plant is 0.2 $/m3. Dissimilarity, costs of the CAS-N and MBR plants are 0.27 $/m3 and 0.43 $/m3 respectively. The low cost of the CAS comes with lower removal efficiencies in comparison to the CAS-N and MBR. Accordingly, the economic modeling is helpful to build and compare relative costs for treatment alternatives which lead to better engineering decisions and suitable selection of the appropriate treatment based on either cost or effluent quality."
Ma2021,Wei Ma and Mike Papadakis and Anestis Tsakmalis and Maxime Cordy and Yves Le Traon,Test Selection for Deep Learning Systems,ACM Transactions on Software Engineering and Methodology,30,2,2021,10.1145/3417330,15577392,"Testing of deep learning models is challenging due to the excessive number and complexity of the computations involved. As a result, test data selection is performed manually and in an ad hoc way. This raises the question of how we can automatically select candidate data to test deep learning models. Recent research has focused on defining metrics to measure the thoroughness of a test suite and to rely on such metrics to guide the generation of new tests. However, the problem of selecting/prioritising test inputs (e.g., to be labelled manually by humans) remains open. In this article, we perform an in-depth empirical comparison of a set of test selection metrics based on the notion of model uncertainty (model confidence on specific inputs). Intuitively, the more uncertain we are about a candidate sample, the more likely it is that this sample triggers a misclassification. Similarly, we hypothesise that the samples for which we are the most uncertain are the most informative and should be used in priority to improve the model by retraining. We evaluate these metrics on five models and three widely used image classification problems involving real and artificial (adversarial) data produced by five generation algorithms. We show that uncertainty-based metrics have a strong ability to identify misclassified inputs, being three times stronger than surprise adequacy and outperforming coverage-related metrics. We also show that these metrics lead to faster improvement in classification accuracy during retraining: up to two times faster than random selection and other state-of-the-art metrics on all models we considered."
Dychkovskyi2020,Roman Dychkovskyi and Iaroslav Shavarskyi and Pavlo Saik and Vasyl Lozynskyi and Volodymyr Falshtynskyi and Edgar Cabana,Research into stress-strain state of the rock mass condition in the process of the operation of double-unit longwalls,Mining of Mineral Deposits,14,2,2020,10.33271/mining14.02.085,24153443,"Purpose. To substantiate changes in stress-strain state of rock mass in the process of long-pillar mining with the help of double-unit longwalls while evaluating stress of a mine field in terms of Lvivvuhillia SE mine. Methods. Analysis of the plans of mine workings has become a basis for the evaluation of physical and geometrical parameters of a support pressure area of the double-unit stopes depending upon mining and geological as well as engineering conditions for n7b coal seam extraction. 3D model of the rock mass has been rendered using SolidWorks 2019 software. The geomechanical model of the rock mass is based upon the specified output data concerning actual operating schedule of 1018 and 1019 double-unit longwalls (numbers of the longwalls are changed as it has been required by the authorities of Lvivvuhillia SE) in terms of n7b seam and support patterns of the development mine workings in Lvivvuhillia SE mine. Each component of the support was modeled as a separate part with the relevant geotech data. Behaviour of the expansion of the rock mass stress-strain state within the selected point has been analyzed by means of sections at the specified plane. Findings. Rendering algorithm of 3D model of rock mass in terms of long-pillar mining of a coal seam using double-unit longwalls has been developed. A geomechanical model of the rock mass has been substantiated depending upon the mining and geological mode of occurrence and engineering parameters of coal mining process. Originality. Nature of the support pressure area formation in front of a stope as well as along the extraction pillar length has been analyzed. It has been identified that if stopes are within one and the same plane, interconnection of their frontal support pressure areas as well as walls of the development workings take place. In this context, adjoining entry acts as the extra destressing technogenic cavity in addition to its proper functions. Practical implications. Output data to make recommendations concerning the efficient mining parameters and methods for rock pressure control have been identified relying upon the analysis of stress-strain state of rock mass in the process of the operation of double-unit longwalls. Visualization of the principles of formation of the stress-strain state of support pressure area and evaluation of the rock mass condition have shown that the maximum reduced stresses reach 70 MPa in terms of 18 m width of the support pressure area."
Gras2020,Ben Gras and Cristiano Giuffrida and Michael Kurth and Herbert Bos and Kaveh Razavi,ABSynthe: Automatic Blackbox Side-channel Synthesis on Commodity Microarchitectures,,,,2020,10.14722/ndss.2020.23018,,"The past decade has seen a plethora of side-channel attacks on various CPU components. Each new attack typically follows a whitebox analysis approach, which involves (i) identifying a specific shared CPU component, (ii) reversing its behavior on a specific microarchitecture, and (iii) surgically exploiting such knowledge to leak information (e.g., by actively evicting shared entries to monitor victim accesses). This approach requires lengthy reverse engineering, repeated for every component and microarchitecture, and does not allow for attacking unknown shared resources. In this paper, we present ABSynthe, a system that takes a target program and a microarchitecture as inputs and automatically synthesizes new side channels. The key insight is that by limiting ourselves to (typically on-core) contention-based side channels, we can treat the target CPU microarchitecture as a black box, enabling automation. To make ABSynthe possible, we have automatically generated leakage maps for a variety of x86_64 microarchitectures. These leakage maps show a complex picture of interaction between different x86_64 instructions and justify a black box approach to finding the best sequence of instructions that cause information to leak from a given software target, which we also treat as a black box. To recover the secret information using the optimized sequence of instructions, ABSynthe relies on a recurrent neural network to craft practical side-channel attacks that recover a secret bit stream. Our evaluation shows that ABSynthe can synthesize better attacks by exploiting contention on multiple components at the same time compared to state of the art contention-based attacks that focus on a single component. Furthermore, the automation made possible by ABSynthe allows us to synthesize cross-thread attacks for a variety of microarchitectures (from Intel, AMD and ARM) on four different cryptographic software targets, in both native and virtualized environments. The results show that ABSynthe can recover cryptographic key bit streams with high accuracy. As an example, ABSynthe recovers a full 256-bit EdDSA key from just a single trace capture with 100% success rate on one of our test beds."
Zhu2021,Jun Zhu and Changsong Wu and Yihong Ren,Broadband terahertz metamaterial absorber based on graphene resonators with perfect absorption,Results in Physics,26,,2021,10.1016/j.rinp.2021.104466,22113797,"The applications of terahertz waves and metamaterials in electromagnetic wave absorbers are one of the key focus areas of current interdisciplinary scientific research. In this study, we propose a metamaterial absorber composed of graphene double-open rectangular ring and graphene strip cross structures. The experiment uses numerical analysis software to study the proposed absorber. Transverse electric waves were normally incident on the absorber from the plane port, where resonance coupling was achieved. With an increase in the incidence angle alpha, the trough in the middle of the absorption spectrum continued to deepen. The bandwidth that the spectral absorption maintains above 0.9 is 2.88 GHz (1.260–1.548 THz). The maximum spectral absorption has reached 99.9%, which is approximately perfect absorption. The absorber under transverse magnetic wave incidence also exhibited a bandwidth advantage. As the Fermi energy continued to increase, the absorption bandwidth first increased and then decreased, and reached the maximum at ef = 0.5 eV. Simultaneously, the relative absorption bandwidth also reached its maximum. By adjusting the Fermi level of graphene, dynamic tuning of the metamaterial absorber could be achieved. Adjustment of the Fermi level shifted the absorption range and absorption bandwidth, and helped in controlling the increase in the relative absorption bandwidth. The findings of this study can be of theoretical and engineering significance in the domains of thermal photovoltaics, solar cells, and sensors, among others."
Yan2020,Shenao Yan and Guanhong Tao and Xuwei Liu and Juan Zhai and Shiqing Ma and Lei Xu and Xiangyu Zhang,Correlations between deep neural network model coverage criteria and model quality,,,,2020,10.1145/3368089.3409671,,"Inspired by the great success of using code coverage as guidance in software testing, a lot of neural network coverage criteria have been proposed to guide testing of neural network models (e.g., model accuracy under adversarial attacks). However, while the monotonic relation between code coverage and software quality has been supported by many seminal studies in software engineering, it remains largely unclear whether similar monotonicity exists between neural network model coverage and model quality. This paper sets out to answer this question. Specifically, this paper studies the correlation between DNN model quality and coverage criteria, effects of coverage guided adversarial example generation compared with gradient decent based methods, effectiveness of coverage based retraining compared with existing adversarial training, and the internal relationships among coverage criteria."
Ansari2022,Md Tarique Jamal Ansari and Dhirendra Pandey and Mamdouh Alenezi,STORE: Security Threat Oriented Requirements Engineering Methodology,Journal of King Saud University - Computer and Information Sciences,34,2,2022,10.1016/j.jksuci.2018.12.005,22131248,"As we are continuously depending on information technology applications by adopting electronic channels and software applications for our business, online transaction and communication, software security is increasingly becoming a necessity and more advanced concern. Both the functional and non-functional requirements are important and provide the necessary needs at the early phases of the software development process, specifically in the requirement phase. The aim of this research is to identify security threats early in the software development process to help the requirement engineer elicit appropriate security requirements in a more systematic manner throughout the requirement engineering process to ensure a secure and quality software development. This article proposes the STORE methodology for security requirement elicitation based on security threats analysis, which includes the identification of four points: PoA, PoB, PoC and PoD for effective security attack analysis. Further, the proposed STORE methodology is also validated by a case study of an ERP System. We also compare our STORE methodology with two existing techniques, namely, SQUARE and MOSRE. We have shown that more effective and efficient security requirements can be elicited by the STORE methodology and that it helps the security requirement engineer to elicit security requirements in a more organized manner."
Abdelmalek2020,Zahra Abdelmalek and Sami Ullah Khan and Hassan Waqas and Hossam A. Nabwey and Iskander Tlili,"Utilization of second order slip, activation energy and viscous dissipation consequences in thermally developed flow of third grade nanofluid with gyrotactic microorganisms",Symmetry,12,2,2020,10.3390/sym12020309,20738994,"In recent decades, an interest has been developed towards the thermal consequences of nanofluid because of utilization of nano-materials to improve the thermal conductivity of traditional liquid and subsequently enhance the heat transportation phenomenon. Following this primarily concept, this current work investigates the thermal developed flow of third-grade nanofluid configured by a stretched surface with additional features of activation energy, viscous dissipation and second-order slip. Buongiorno's nanofluid model is used to explore the thermophoresis and Brownian motion features based on symmetry fundamentals. It is further assumed that the nanoparticles contain gyrotactic microorganisms, which are associated with the most fascination bioconvection phenomenon. The flow problem owing to the partial differential equations is renovated into dimensional form, which is numerically simulated with the help of bvp4c, by using MATLAB software. The aspects of various physical parameters associated to the current analysis are graphically examined against nanoparticles' velocity, temperature, concentration and gyrotactic microorganisms' density distributions. Further, the objective of local Nusselt number, local Sherwood number and motile density number are achieved numerically with variation of various parameters. The results presented here may find valuable engineering applications, like cooling liquid metals, solar systems, power production, solar energy, thermal extrusion systems cooling of machine equipment, transformer oil and microelectronics. Further, flow of nanoparticles containing gyrotactic microorganisms has interesting applications in microbial fuel cells, microfluidic devices, bio-technology and enzyme biosensors."
Ferdows2020,Mohammad Ferdows and Khairy Zaimi and Ahmed M. Rashad and Hossam A. Nabwey,MHD bioconvection flow and heat transfer of nanofluid through an exponentially stretchable sheet,Symmetry,12,5,2020,10.3390/SYM12050692,20738994,"Recently, bioconvection phenomenon has gained great importance in research for its use in many engineering and biological applications. Therefore, this work investigates the magnetohydrodynamic flow of a dissipative nanofluid, including gyrotactic microorganisms along an exponentially moving sheet. Since the governing equations that describe the problem are nonlinear and more complicated, similarity transformations are used to get a reduced mathematical model in which all the differential equations are ordinary and asymmetric. The computational analysis for the reduced mathematical model is carried out, employing the spectral relaxation technique (SRM) via software called MATLAB. Comparison results are also validated by using the boundary value problem solver (bvp4c) in MATLAB. The obtained results were compared with previously published researches, and a high degree of compatibility and accuracy were found symmetric. The implications of pertinent parameters on velocity, temperature, nanoparticles volume fraction, and density of the microorganism profiles are graphically presented. A decline was seen in the velocity field with augmentation in the magnetic parameter, but certain enhancement was noticed in the temperature field for augmented values of the magnetic parameter, thermophoresis, and Brownian motion parameters. A significant reduction was also noticed in the behavior of the concentration profile for augmented values of the Brownian motion parameter and Lewis number, while it was enhanced with the boost in the thermophoresis and magnetic parameters. The results also indicated that the density of the motile microorganism decreases with bioconvection Lewis number, Prandtl number, Lewis, and Peclet numbers."
Xie2021,Chengyu Xie and Hoang Nguyen and Xuan Nam Bui and Yosoon Choi and Jian Zhou and Thao Nguyen-Trang,Predicting rock size distribution in mine blasting using various novel soft computing models based on meta-heuristics and machine learning algorithms,Geoscience Frontiers,12,3,2021,10.1016/j.gsf.2020.11.005,16749871,"Blasting is well-known as an effective method for fragmenting or moving rock in open-pit mines. To evaluate the quality of blasting, the size of rock distribution is used as a critical criterion in blasting operations. A high percentage of oversized rocks generated by blasting operations can lead to economic and environmental damage. Therefore, this study proposed four novel intelligent models to predict the size of rock distribution in mine blasting in order to optimize blasting parameters, as well as the efficiency of blasting operation in open mines. Accordingly, a nature-inspired algorithm (i.e., firefly algorithm – FFA) and different machine learning algorithms (i.e., gradient boosting machine (GBM), support vector machine (SVM), Gaussian process (GP), and artificial neural network (ANN)) were combined for this aim, abbreviated as FFA-GBM, FFA-SVM, FFA-GP, and FFA-ANN, respectively. Subsequently, predicted results from the abovementioned models were compared with each other using three statistical indicators (e.g., mean absolute error, root-mean-squared error, and correlation coefficient) and color intensity method. For developing and simulating the size of rock in blasting operations, 136 blasting events with their images were collected and analyzed by the Split-Desktop software. In which, 111 events were randomly selected for the development and optimization of the models. Subsequently, the remaining 25 blasting events were applied to confirm the accuracy of the proposed models. Herein, blast design parameters were regarded as input variables to predict the size of rock in blasting operations. Finally, the obtained results revealed that the FFA is a robust optimization algorithm for estimating rock fragmentation in bench blasting. Among the models developed in this study, FFA-GBM provided the highest accuracy in predicting the size of fragmented rocks. The other techniques (i.e., FFA-SVM, FFA-GP, and FFA-ANN) yielded lower computational stability and efficiency. Hence, the FFA-GBM model can be used as a powerful and precise soft computing tool that can be applied to practical engineering cases aiming to improve the quality of blasting and rock fragmentation."
Asteris2022,Panagiotis G. Asteris and Fariz Iskandar Mohd Rizal and Mohammadreza Koopialipoor and Panayiotis C. Roussis and Maria Ferentinou and Danial Jahed Armaghani and Behrouz Gordan,Slope Stability Classification under Seismic Conditions Using Several Tree‐Based Intelligent Techniques,Applied Sciences (Switzerland),12,3,2022,10.3390/app12031753,20763417,"Slope stability analysis allows engineers to pinpoint risky areas, study trigger mechanisms for slope failures, and design slopes with optimal safety and reliability. Before the widespread usage of computers, slope stability analysis was conducted through semi analytical methods, or stability charts. Presently, engineers have developed many computational tools to perform slope stability analysis more efficiently. The challenge associated with furthering slope stability methods is to create a reliable design solution to perform reliable estimations involving a number of geometric and mechanical variables. The objective of this study was to investigate the application of tree‐based models, including decision tree (DT), random forest (RF), and AdaBoost, in slope stability classification under seismic loading conditions. The input variables used in the modelling were slope height, slope inclination, cohesion, friction angle, and peak ground acceleration to classify safe slopes and unsafe slopes. The training data for the developed computational intelligence models resulted from a series of slope stability analyses performed using a standard geotechnical engineering software commonly used in geotechnical engineering practice. Upon construction of the tree‐based models, the model assessment was performed through the use and calculation of accuracy, F1‐score, recall, and precision indices. All tree‐based models could efficiently classify the slope stability status, with the AdaBoost model providing the highest performance for the classification of slope stability for both model development and model assessment parts. The proposed AdaBoost model can be used as a screening tool during the stage of feasibility studies of related infrastructure projects, to classify slopes according to their expected status of stability under seismic loading conditions."
Dey2020,Tapajit Dey and Sara Mousavi and Eduardo Ponce and Tanner Fry and Bogdan Vasilescu and Anna Filippova and Audris Mockus,Detecting and Characterizing Bots that Commit Code,,,,2020,10.1145/3379597.3387478,,"Background: Some developer activity traditionally performed manually, such as making code commits, opening, managing, or closing issues is increasingly subject to automation in many OSS projects. Specifically, such activity is often performed by tools that react to events or run at specific times. We refer to such automation tools as bots and, in many software mining scenarios related to developer productivity or code quality, it is desirable to identify bots in order to separate their actions from actions of individuals. Aim: Find an automated way of identifying bots and code committed by these bots, and to characterize the types of bots based on their activity patterns. Method and Result: We propose BIMAN, a systematic approach to detect bots using author names, commit messages, files modified by the commit, and projects associated with the commits. For our test data, the value for AUC-ROC was 0.9. We also characterized these bots based on the time patterns of their code commits and the types of files modified, and found that they primarily work with documentation files and web pages, and these files are most prevalent in HTML and JavaScript ecosystems. We have compiled a shareable dataset containing detailed information about 461 bots we found (all of which have more than 1000 commits) and 13,762,430 commits they created."
Poveda-Villaln2022,María Poveda-Villalón and Alba Fernández-Izquierdo and Mariano Fernández-López and Raúl García-Castro,LOT: An industrial oriented ontology engineering framework,Engineering Applications of Artificial Intelligence,111,,2022,10.1016/j.engappai.2022.104755,09521976,"Ontology Engineering has captured much attention during the last decades leading to the proliferation of numerous works regarding methodologies, guidelines, tools, resources, etc. including topics which are still being investigated. Even though, there are still many open questions when addressing a new ontology development project, regarding how to manage the overall project and articulate transitions between activities or which tasks and tools are recommended for each step. In this work we propose the Linked Open Terms (LOT) methodology, an overall and lightweight methodology for building ontologies based on existing methodologies and oriented to semantic web developments and technologies. The LOT methodology focuses on the alignment with industrial development, in addition to academic and research projects, and software development, that is making ontology development part of the software industry. This methodology includes lessons learnt from more than 20 years in ontological engineering and its application on 18 projects is reported."
Getu2020,Dawit Getu and Ramesh Babu Nallamothu and Muluken Masresha and Seshu Kishan Nallamothu and Anantha Kamal Nallamothu,Production and characterization of bamboo and sisal fiber reinforced hybrid composite for interior automotive body application,,38,,2020,10.1016/j.matpr.2020.08.780,22147853,"Composite materials have high strength to weight ratio with low density and high stiffness to weight, high strength ratios, and high fatigue strength to weight ratio compared to traditional engineering materials making them find wide applications in structural constructions. When the lightweight composite materials which are made of lightweight natural fibres are used in automotive application, the fuel economy of the vehicle improves reducing the related harmful emissions. The aim of this research is to develop and characterize the performance of sisal and bamboo reinforced polyester hybrid composite (BSFRHC) with different fibre orientation of sisal and unidirectional (UD) bamboo fibre. Next, BSFRHC was fabricated with 20% total fibre volume fraction. Of this total fibre volume, the composite is fabricated in 3:1 bamboo to sisal fibre ratio using hand lay-up technique. Then tensile, compressive, impact and flexural tests were carried out. In general, it is concluded that as varying fiber orientation, the tensile strength varies. The higher tensile strength is observed with 0° fibre orientation of bamboo/sisal fiber reinforced hybrid composite. From compressive strength of the hybrid composite reinforced with bamboo/sisal fibre, it is observed that the 0°-fibre orientation composite is exhibiting higher compressive strength than 90° fibre orientation composite and bidirectional (0°/90°) fibre orientation composite. Unidirectional 90° fibre orientation was found to have a higher tensile and flexural strength whereas unidirectional 90° and bidirectional (0°/90°) fibre orientation nearly have the same value of tensile strength, whereas bidirectional (0°/90°) was found to be having higher flexural strength than unidirectional 90° fibre orientation. Impact analysis of vehicle internal door panel made of BSFRHC was done using ANSYS Software. Furthermore, it is found that the bamboo and sisal fibre reinforced hybrid composite in unidirectional 0° has the potential to be used for automotive interior part application."
Kondiah2020,Pariksha Jolene Kondiah and Pierre P.D. Kondiah and Yahya E. Choonara and Thashree Marimuthu and Viness Pillay,A 3D bioprinted pseudo-bone drug delivery scaffold for bone tissue engineering,Pharmaceutics,12,2,2020,10.3390/pharmaceutics12020166,19994923,"A 3D bioprinted pseudo-bone drug delivery scaffold was fabricated to display matrix strength, matrix resilience, as well as porous morphology of healthy human bone. Computer-aided design (CAD) software was employed for developing the 3D bioprinted scaffold. Further optimization of the scaffold was undertaken using MATLAB® software and artificial neural networks (ANN). Polymers employed for formulating the 3D scaffold comprised of polypropylene fumarate (PPF), free radical polymerized polyethylene glycol-polycaprolactone (PEG-PCL-PEG), and pluronic (PF127). Simvastatin was incorporated into the 3D bioprinted scaffolds to further promote bone healing and repair properties. The 3D bioprinted scaffold was characterized for its chemical, morphological, mechanical, and in vitro release kinetics for evaluation of its behavior for application as an implantable scaffold at the site of bone fracture. The ANN-optimized 3D bioprinted scaffold displayed significant properties as a controlled release platform, demonstrating drug release over 20 days. The 3D bioprinted scaffold further displayed formation as a pseudo-bone matrix, using a human clavicle bone model, induced with a butterfly fracture. The strength of the pseudo-bone matrix, evaluated for its matrix hardness (MH) and matrix resilience (MR), was evaluated to be as strong as original bone, having a 99% MH and 98% MR property, to healthy human clavicle bones."
Bilal2021,Muhammad Bilal and Hamna Arshad and Muhammad Ramzan and Zahir Shah and Poom Kumam,Unsteady hybrid-nanofluid flow comprising ferrousoxide and CNTs through porous horizontal channel with dilating/squeezing walls,Scientific Reports,11,1,2021,10.1038/s41598-021-91188-1,20452322,"The key objective of the present research is to examine the hybrid magnetohydrodynamics (MHD) nanofluid (Carbon-nanotubes and ferrous oxide–water) CNT–Fe3O4/H2 flow into a horizontal parallel channel with thermal radiation through squeezing and dilating porous walls. The parting motion is triggered by the porous walls of the channel. The fluid flow is time-dependent and laminar. The channel is asymmetric and the upper and lower walls are distinct in temperature and are porous. With the combination of nanoparticles of Fe3O4 and single and multi-wall carbon nanotubes, the hybrid nanofluid principle is exploited. By using the similarity transformation, the set of partial differential equations (PDEs) of this mathematical model, governed by momentum and energy equations, is reduced to corresponding ordinary differential equations (ODEs). A very simple numerical approach called the Runge–Kutta system of order four along with the shooting technique is used to achieve the solutions for regulating ODEs. MATLAB computing software is used to create temperature and velocity profile graphs for various emerging parameters. At the end of the manuscript, the main conclusions are summarized. Through different graphs, it is observed that hybrid-nanofluid has more prominent thermal enhancement than simple nanofluid. Further, the single-wall nanotubes have dominated impact on temperature than the multi-wall carbon nanotubes. From the calculations, it is also noted that Fe2O3–MWCNT–water has an average of 4.84% more rate of heat transfer than the Fe2O3–SWCNT–water. On the other hand, 8.27% more heat flow observed in Fe2O3–SWCNT–water than the simple nanofluid. Such study is very important in coolant circulation, inter-body fluid transportation, aerospace engineering, and industrial cleaning procedures, etc."
Mukhamediev2021,Ravil I. Mukhamediev and Adilkhan Symagulov and Yan Kuchin and Elena Zaitseva and Alma Bekbotayeva and Kirill Yakunin and Ilyas Assanov and Vitaly Levashenko and Yelena Popova and Assel Akzhalova and Sholpan Bastaubayeva and Laila Tabynbaeva,Review of some applications of unmanned aerial vehicles technology in the resource-rich country,Applied Sciences (Switzerland),11,21,2021,10.3390/app112110171,20763417,"The use of unmanned aerial vehicles (UAVs) in various spheres of human activity is a promising direction for countries with very different types of economies. This statement refers to resource-rich economies as well. The peculiarities of such countries are associated with the dependence on resource prices since their economies present low diversification. Therefore, the employment of new technologies is one of the ways of increasing the sustainability of such economy development. In this context, the use of UAVs is a prospect direction, since they are relatively cheap, reliable, and their use does not require a high-tech background. The most common use of UAVs is associated with various types of monitoring tasks. In addition, UAVs can be used for organizing communication, search, cargo delivery, field processing, etc. Using additional elements of artificial intelligence (AI) together with UAVs helps to solve the problems in automatic or semi-automatic mode. Such UAV is named intelligent unmanned aerial vehicle technology (IUAVT), and its employment allows increasing the UAV-based technology efficiency. However, in order to adapt IUAVT in the sectors of economy, it is necessary to overcome a range of limitations. The research is devoted to the analysis of opportunities and obstacles to the adaptation of IUAVT in the economy. The possible economic effect is estimated for Kazakhstan as one of the resource-rich countries. The review consists of three main parts. The first part describes the IUAVT application areas and the tasks it can solve. The following areas of application are considered: precision agriculture, the hazardous geophysical processes monitoring, environmental pollution monitoring, exploration of minerals, wild animals monitoring, technical and engineering structures monitoring, and traffic monitoring. The economic potential is estimated by the areas of application of IUAVT in Kazakhstan. The second part contains the review of the technical, legal, and software-algorithmic limitations of IUAVT and modern approaches aimed at overcoming these limitations. The third part—discussion—comprises the consideration of the impact of these limitations and unsolved tasks of the IUAVT employment in the areas of activity under consideration, and assessment of the overall economic effect."
Shokravi2020,Hoofar Shokravi and Hooman Shokravi and Norhisham Bakhary and Seyed Saeid Rahimian Koloor and Michal Petrů,Health monitoring of civil infrastructures by subspace system identification method: An overview,Applied Sciences (Switzerland),10,8,2020,10.3390/APP10082786,20763417,"Structural health monitoring (SHM) is the main contributor of the future's smart city to deal with the need for safety, lower maintenance costs, and reliable condition assessment of structures. Among the algorithms used for SHM to identify the system parameters of structures, subspace system identification (SSI) is a reliable method in the time-domain that takes advantages of using extended observability matrices. Considerable numbers of studies have specifically concentrated on practical applications of SSI in recent years. To the best of author's knowledge, no study has been undertaken to review and investigate the application of SSI in the monitoring of civil engineering structures. This paper aims to review studies that have used the SSI algorithm for the damage identification and modal analysis of structures. The fundamental focus is on data-driven and covariance-driven SSI algorithms. In this review, we consider the subspace algorithm to resolve the problem of a real-world application for SHM. With regard to performance, a comparison between SSI and other methods is provided in order to investigate its advantages and disadvantages. The applied methods of SHM in civil engineering structures are categorized into three classes, from simple one-dimensional (1D) to very complex structures, and the detectability of the SSI for different damage scenarios are reported. Finally, the available software incorporating SSI as their system identification technique are investigated."
Cabot2020,Jordi Cabot,Positioning of the low-code movement within the field of model-driven engineering,,,,2020,10.1145/3417990.3420210,,Low-code is being promoted as the key infrastructure for the digital transformation of our society. But is there something fundamentally new behind the low-code movement? How does it relate to other concepts like Model-Driven Engineering or Model-Driven development? And what are the implications for researchers in the modeling community?. This position paper tries to shed some light on these issues.
Aboaoja2022,Faitouri A. Aboaoja and Anazida Zainal and Fuad A. Ghaleb and Bander Ali Saleh Al-rimy and Taiseer Abdalla Elfadil Eisa and Asma Abbas Hassan Elnour,"Malware Detection Issues, Challenges, and Future Directions: A Survey",Applied Sciences (Switzerland),12,17,2022,10.3390/app12178482,20763417,"The evolution of recent malicious software with the rising use of digital services has increased the probability of corrupting data, stealing information, or other cybercrimes by malware attacks. Therefore, malicious software must be detected before it impacts a large number of computers. Recently, many malware detection solutions have been proposed by researchers. However, many challenges limit these solutions to effectively detecting several types of malware, especially zero-day attacks due to obfuscation and evasion techniques, as well as the diversity of malicious behavior caused by the rapid rate of new malware and malware variants being produced every day. Several review papers have explored the issues and challenges of malware detection from various viewpoints. However, there is a lack of a deep review article that associates each analysis and detection approach with the data type. Such an association is imperative for the research community as it helps to determine the suitable mitigation approach. In addition, the current survey articles stopped at a generic detection approach taxonomy. Moreover, some review papers presented the feature extraction methods as static, dynamic, and hybrid based on the utilized analysis approach and neglected the feature representation methods taxonomy, which is considered essential in developing the malware detection model. This survey bridges the gap by providing a comprehensive state-of-the-art review of malware detection model research. This survey introduces a feature representation taxonomy in addition to the deeper taxonomy of malware analysis and detection approaches and links each approach with the most commonly used data types. The feature extraction method is introduced according to the techniques used instead of the analysis approach. The survey ends with a discussion of the challenges and future research directions."
Chen2020,E. Chen and Carlos G. Berrocal and Ignasi Fernandez and Ingemar Löfgren and Karin Lundgren,Assessment of the mechanical behaviour of reinforcement bars with localised pitting corrosion by Digital Image Correlation,Engineering Structures,219,,2020,10.1016/j.engstruct.2020.110936,18737323,"Corrosion of reinforcement in concrete impairs the mechanical behaviour of rebars by decreasing their strength and deformation capacity. In this study, uniaxial tensile tests were carried out on 61 rebars taken from 22 pre- and un- cracked reinforced concrete beams subjected to drying and wetting cycles in chloride solution for over three years. A 3D-scanning technique was used to characterise the maximum local corrosion level, μmax, and different pit shape parameters. Digital Image Correlation (DIC) was used to capture the displacement field of the test bars; the engineering strain was measured through the virtual extensometers created in the DIC post-processing software. The proof and ultimate forces showed linear decreasing trends of μmax, while the proof and ultimate strengths (based on the minimum residual cross-sectional area) were not obviously affected by corrosion. The ultimate strain of corroded bars depended on the gauge length due to strain localisation in the pit. Thus, it was emphasised that the ultimate strain may be overestimated if measured based on a short gauge across the pit. It was also observed that when μmax exceeded a critical local corrosion level (μcrit depending on the ratio between the yield and ultimate strengths of the steel), the region outside the pit did not develop yielding. A lower bound of ultimate strain was further derived as a function of the mechanical parameters of uncorroded steel and maximum local corrosion level. This provided a good comparison with the experimental results. Ultimately, a hypothesis for time-dependent assessment of strain capacity is proposed, considering the evolution of corrosion morphology over time."
Vogt2021,Maximilian Vogt and Adrian Rips and Claus Emmelmann,Comparison of iPad Pro®’s LiDAR and TrueDepth Capabilities with an Industrial 3D Scanning Solution,Technologies,9,2,2021,10.3390/technologies9020025,22277080,"Today’s smart devices come equipped with powerful hard- and software-enabling professional use cases. The latest hardware by Apple utilizes LiDAR and TrueDepth, which offer the capability of 3D scanning. Devices equipped with these camera systems allow manufacturers to obtain 3D data from their customers at low costs, which potentially enables time-efficient mass customization and product differentiation strategies. However, the utilization is limited by the scanning accuracy. To determine the potential application of LiDAR and TrueDepth as a 3D scanning solution, in this paper an evaluation was performed. For this purpose, different Lego bricks were scanned with the technologies and an industrial 3D scanner. The results were compared according to shape and position tolerances. Even though the industrial 3D scanner consistently delivered more accurate results, the accuracy of the smart device technologies may already be sufficient, depending on the application."
Dzobo2020,Kevin Dzobo and Sampson Adotey and Nicholas E. Thomford and Witness Dzobo,Integrating Artificial and Human Intelligence: A Partnership for Responsible Innovation in Biomedical Engineering and Medicine,OMICS A Journal of Integrative Biology,24,5,2020,10.1089/omi.2019.0038,15578100,"Historically, the term ""artificial intelligence"" dates to 1956 when it was first used in a conference at Dartmouth College in the US. Since then, the development of artificial intelligence has in part been shaped by the field of neuroscience. By understanding the human brain, scientists have attempted to build new intelligent machines capable of performing complex tasks akin to humans. Indeed, future research into artificial intelligence will continue to benefit from the study of the human brain. While the development of artificial intelligence algorithms has been fast paced, the actual use of most artificial intelligence (AI) algorithms in biomedical engineering and clinical practice is still markedly below its conceivably broader potentials. This is partly because for any algorithm to be incorporated into existing workflows it has to stand the test of scientific validation, clinical and personal utility, application context, and is equitable as well. In this context, there is much to be gained by combining AI and human intelligence (HI). Harnessing Big Data, computing power and storage capacities, and addressing societal issues emergent from algorithm applications, demand deploying HI in tandem with AI. Very few countries, even economically developed states, lack adequate and critical governance frames to best understand and steer the AI innovation trajectories in health care. Drug discovery and translational pharmaceutical research stand to gain from AI technology provided they are also informed by HI. In this expert review, we analyze the ways in which AI applications are likely to traverse the continuum of life from birth to death, and encompassing not only humans but also all animal, plant, and other living organisms that are increasingly touched by AI. Examples of AI applications include digital health, diagnosis of diseases in newborns, remote monitoring of health by smart devices, real-time Big Data analytics for prompt diagnosis of heart attacks, and facial analysis software with consequences on civil liberties. While we underscore the need for integration of AI and HI, we note that AI technology does not have to replace medical specialists or scientists and rather, is in need of such expert HI. Altogether, AI and HI offer synergy for responsible innovation and veritable prospects for improving health care from prevention to diagnosis to therapeutics while unintended consequences of automation emergent from AI and algorithms should be borne in mind on scientific cultures, work force, and society at large."
Mehdi2021,Husain Mehdi and R. S. Mishra,Effect of friction stir processing on mechanical properties and heat transfer of TIG welded joint of AA6061 and AA7075,Defence Technology,17,3,2021,10.1016/j.dt.2020.04.014,22149147,"Tungsten inert gas (TIG) welding is the most commonly used joining process for aluminum alloy for AA6061 and AA7075 which are highly demanded in the aerospace engineering and the automobile sector, but there are some defects occur during TIG welding like micro-crack, coarse grain structure, and porosity. To improve these defects, the TIG welded joint is processed using friction stir processing (FSP). This paper presents the effect of friction stir processing on TIG welding with filler ER4043 and ER 5356 for dissimilar aluminum alloy AA6061 and AA7075. The mechanical characterization, finite element formulation and mathematical equations of heat transfer of TIG + FSP welded joints are investigated using ANSYS Fluent software by adjusting process parameters of FSP. The results show that the maximum compressive residual stress 73 MPa was obtained at the fusion zone (FZ) of the TIG weldment with filler ER4043, whereas minimum compressive residual stress 37 MPa was obtained at stir zone (SZ) of the TIG + FSP with filler 5356. The maximum heat flux 5.33 × 106 W/m2 and temperature 515 °C have observed at tool rotation 1600 rpm with a feed rate of 63 mm/min. These results give a satisfactory measure of confidence in the fidelity of the simulation"
Rui2022,Xiaoting Rui and Jianshu Zhang and Xun Wang and Bao Rong and Bin He and Zhan Jin,"Multibody system transfer matrix method: The past, the present, and the future",International Journal of Mechanical System Dynamics,2,1,2022,10.1002/msd2.12037,27671402,"The multibody system transfer matrix method (MSTMM), a novel dynamics approach developed during the past three decades, has several advantages compared to conventional dynamics methods. Some of these advantages include avoiding global dynamics equations with a system inertia matrix, utilizing low-order matrices independent of system degree of freedom, high computational speed, and simplicity of computer implementation. MSTMM has been widely used in computer modeling, simulations, and performance evaluation of approximately 150 different complex mechanical systems. In this paper, the following aspects regarding MSTMM are reviewed: basic theory, algorithms, simulation and design software, and applications. Future research directions and generalization to more applications in various fields of science, technology, and engineering are discussed."
Phoon2022,Kok Kwang Phoon and Zi Jun Cao and Jian Ji and Yat Fai Leung and Shadi Najjar and Takayuki Shuku and Chong Tang and Zhen Yu Yin and Yoshida Ikumasa and Jianye Ching,"Geotechnical uncertainty, modeling, and decision making",Soils and Foundations,62,5,2022,10.1016/j.sandf.2022.101189,00380806,"Modeling only constitutes one aspect of decision making. The prevailing limitation of applying modeling to practice is the absence of explicit consideration of uncertainties. This review paper covers uncertainty quantification (soil properties, stratification, and model performance) and uncertainty calculation with a focus on how it enhances the role of modeling in decision making (reliability analysis, reliability-based design, and inverse analysis). The key output from a reliability analysis is the probability of failure, where “failure” is defined as any condition that does not meet a performance criterion or a set of criteria. In contrast to the global factor of safety, the probability of failure respects both mechanics and statistics, is sensitive to data (thus opening one potential pathway to digital transformation), and it is meaningful for both system and component failures. Resilience engineering requires system level analysis. As such, geotechnical software can provide better decision support by computing the probability of failure/reliability index as one basic output in addition to stresses, strains, forces, and displacements. It is further shown that more critical non-classical failure mechanisms can emerge from spatially variable soils that can escape notice if the engineer were to restrict analysis to conventional homogeneous or layered soil profiles."
Raikar2020,Meenaxi M. Raikar and S. M. Meena and Mohammed Moin Mulla and Nagashree S. Shetti and Meghana Karanandi,Data Traffic Classification in Software Defined Networks (SDN) using supervised-learning,,171,,2020,10.1016/j.procs.2020.04.299,18770509,"Traffic classification with accuracy is of prime importance in network activities such as security monitoring, traffic engineering, fault detection, accounting of network usage, billing and for providing differentiation in Quality of Service (QoS) parameters of the various network services. Network Traffic Classification is significant in recent days due to rapid growth in the number of internet consumers. The different primitive techniques of network traffic classification have failed to provide reliable accuracy because of 1000 fold scaling in the amount of devices as well as flows. To overcome this drawback, the integration of Software Defined Network (SDN) architecture and machine learning technology is proposed in this paper. Three different supervised learning models, namely Support Vector Machine (SVM), nearest centroid and Naïve Bayes (NB), are applied to classify the data traffic based on the applications in a software-defined network platform. The network traffic traces are captured and flows features are generated, which is sent to the classifier for prediction. The accuracy obtained for SVM is 92.3%, NB is 96.79% and the nearest centroid is 91.02%. The challenges faced are in the live network data traffic capture and classification of the applications in the SDN platform."
Li2023,Joey Li and Munur Sacit Herdem and Jatin Nathwani and John Z. Wen,"Methods and applications for Artificial Intelligence, Big Data, Internet of Things, and Blockchain in smart energy management",Energy and AI,11,,2023,10.1016/j.egyai.2022.100208,26665468,"Information technologies involving artificial Intelligence, big data, Internet of Things devices and blockchain have been developed and implemented in many engineering fields worldwide. Existing review articles focus on developments and characteristics of individual topics and the associated deployment in the energy sector. These technologies, all based on communication, information, and data analysis, are naturally coherent and integrable. This article reviews the literature and patents in four closely related fields and aims to provide a holistic view of how they are related and their integrability in relation to smart energy management strategies. Artificial intelligence models forecast energy use and load profiles as well as schedule resources to ensure reliable performance and effective utilization of energy resources. Training artificial intelligence models requires immense volumes of data. Utilizing big data systems and data mining enables the discovery of new functions and relationships, which determines the performance of artificial intelligence. Data mining also refines the information; thus, artificial intelligence is trained iteratively with more accurate data. Smart energy management can be further enhanced through advanced digital technologies like Internet of Things and blockchain. An Internet of Things platform containing edge, fog and cloud layers helps connect artificial intelligence to other hardware and software devices and systems. Furthermore, an Internet of Things platform efficiently transmits and stores data, improving access and availability to stakeholders for data mining. Emerging technologies such as blockchain and cryptocurrency facilitate energy trading and can be designed in the cloud layer of an Internet of Things platform to supplement data storage. Providing an efficient and seamless integration of artificial intelligence, big data, and advanced digital technologies will be an important factor in the emerging transition of the energy sector to a lower-carbon system."
Elbreki2020,A. M. Elbreki and K. Sopian and A. Fazlizan and A. Ibrahim,An innovative technique of passive cooling PV module using lapping fins and planner reflector,Case Studies in Thermal Engineering,19,,2020,10.1016/j.csite.2020.100607,2214157X,"This study aims to combine planar reflector and backplate extended surface to the PV module to optimize its efficiency. A parametric study based on lapping fin height, number fins, fin spacing, windspeed and solar irradiance on the PV module performance were investigated using CFD, ANSYS, FLUENT. 3-D, Navier–Stokes energy equations were solved using ANSYS, FLUENT to perform numerical computations. Meanwhile, the bare PV module temperature has been calculated using Engineering Equation Solver (EES) software. The results revealed that as the fin pitch increases from 20 to 60 mm this leads to decrease number of fins from 20 to 10 fins which at the same time increased the PV module temperature from 44.13-54.01 °C. The PV module temperature without cooling was reached 64.3 °C; meanwhile with 18 lapping fins and 27.7 mm fin pitch the PV module temperature reduced to 39.73 °C which improved the electrical efficiency to 11.2% when compared to bare PV 9.81% with a temperature difference 24.57 °C at 1000 W/m2. Finally, we conclude that the lapping fins have a superior performance in reducing PV module temperature. Furthermore, it was found that the aluminum thickness of the designed fins above 2 mm showed a negligible effect on the PV power and efficiency."
Papakostas2021,Christos Papakostas and Christos Troussas and Akrivi Krouska and Cleo Sgouropoulou,Exploration of Augmented Reality in Spatial Abilities Training: A Systematic Literature Review for the Last Decade,Informatics in Education,20,1,2021,10.15388/infedu.2021.06,16485831,"This review paper presents a systematic literature review on the use of Augmented Reality (AR) in engineering education, and specifically in student’s spatial ability training, for the last decade. Researchers have explored the benefits of AR, and its application has been of increasing interest in all levels of education. Engineering students tend to have difficulties in acquiring visualization skills, and hence, AR is gaining momentum in enhancing students’ learning achievements. This paper aims to present valuable information to researchers, tutors and software developers of learning technology systems concerning the advantages and limitations of AR in spatial ability training, the incorporation of adaptivity and personalization in AR applications as well as the aspects of spatial ability having been evaluated using AR and the prevalent evaluation methods for AR applications. To this direction, a total of thirty-two (32) studies were reviewed, having been published since 2010. The findings reveal an increase in the number of studies during the last three years. One major conclusion is the improvement of learners’ spatial ability using AR in educational settings, and the noted challenge is the need for more learning content. One research gap that has been identified is the lack of personalization in the developed applications, offering space for future research. Concluding, this area is under-researched, and thus, there is scope for a lot of improvement."
Nisar2022,Kottakkaran Sooppy Nisar and Lanre Akinyemi and Mustafa Inc and Mehmet Şenol and Mohammad Mirzazadeh and Alphonse Houwe and Souleymanou Abbagari and Hadi Rezazadeh,New perturbed conformable Boussinesq-like equation: Soliton and other solutions,Results in Physics,33,,2022,10.1016/j.rinp.2022.105200,22113797,"In recent years, finding exact solutions to nonlinear differential equations has become a fascinating study topic. In the present study, using the modified Kudryashov method and the improved generalized Riccati equation mapping method in the conformable sense, we acquired the exact soliton-type solutions for the new time-fractional perturbed Boussinesq-like equation. This equation has been successfully obtained by the use of asymptotic methods on the defocusing Camassa–Holm nonlinear Schrödinger equation. We obtain the exponential, trigonometric, hyperbolic, and rational type solutions comprising dark solitons, kink solitons, bisymmetry solitons, singular solitons, combined complex solitons, and periodic solutions. These solutions are of great importance in various fields of applied sciences, coastal, and ocean engineering. Furthermore, all acquired solutions have been validated by putting them back into the original equation with the help of the Mathematica package software."
Hijji2021,Mohammad Hijji and Gulzar Alam,A Multivocal Literature Review on Growing Social Engineering Based Cyber-Attacks/Threats during the COVID-19 Pandemic: Challenges and Prospective Solutions,IEEE Access,9,,2021,10.1109/ACCESS.2020.3048839,21693536,"The novel coronavirus (COVID-19) pandemic has caused a considerable and long-lasting social and economic impact on the world. Along with other potential challenges across different domains, it has brought numerous cybersecurity challenges that must be tackled timely to protect victims and critical infrastructure. Social engineering-based cyber-attacks/threats are one of the major methods for creating turmoil, especially by targeting critical infrastructure, such as hospitals and healthcare services. Social engineering-based cyber-attacks are based on the use of psychological and systematic techniques to manipulate the target. The objective of this research study is to explore the state-of-the-art and state-of-the-practice social engineering-based techniques, attack methods, and platforms used for conducting such cybersecurity attacks and threats. We undertake a systematically directed Multivocal Literature Review (MLR) related to the recent upsurge in social engineering-based cyber-attacks/threats since the emergence of the COVID-19 pandemic. A total of 52 primary studies were selected from both formal and grey literature based on the established quality assessment criteria. As an outcome of this research study; we discovered that the major social engineering-based techniques used during the COVID-19 pandemic are phishing, scamming, spamming, smishing, and vishing, in combination with the most used socio-technical method: fake emails, websites, and mobile apps used as weapon platforms for conducting successful cyber-attacks. Three types of malicious software were frequently used for system and resource exploitation are; ransomware, trojans, and bots. We also emphasized the economic impact of cyber-attacks performed on different organizations and critical infrastructure in which hospitals and healthcare were on the top targeted infrastructures during the COVID-19 pandemic. Lastly, we identified the open challenges, general recommendations, and prospective solutions for future work from the researcher and practitioner communities by using the latest technology, such as artificial intelligence, blockchain, and big data analytics."
Biancardo2020,Salvatore Antonio Biancardo and Alessandra Capano and Sara Guerra de Oliveira and Andrej Tibaut,Integration of BIM and procedural modeling tools for road design,Infrastructures,5,4,2020,10.3390/infrastructures5040037,24123811,"Building Information Modeling (BIM) is a design and management methodology strongly used in the Industry of Architecture, Engineering, and Construction (AEC). It allows the creation of a 3D model through parametric modelling in a workflow that updates data, geometry and semantics using the Industry Foundation Classes (IFC) standard. The purpose of this paper is to develop and apply a BIM method for road infrastructures. The creation of the BIM 3D models was carried out using different visual programming software and BIM tools, designing the spatial and parametric representation of the roadway. This way, it has been possible to discover the advantages of using procedural modelling to design road infrastructure through software that are usually used in the mechanical and architectural field. Finally, the interoperability of the software to extract and exchange information between these BIM tools was assessed."
Arif2021,Muhammad Arif and Poom Kumam and Dolat Khan and Wiboonsak Watthayu,Thermal performance of GO-MoS2/ engine oil as Maxwell hybrid nanofluid flow with heat transfer in oscillating vertical cylinder,Case Studies in Thermal Engineering,27,,2021,10.1016/j.csite.2021.101290,2214157X,"Engine oil (EO) is used as a lubricant in the engines of different machineries. The basic need of all phenomena is the rate of heat transfer. To enhance the rate of heat transfer and to save the energy wasted due to high temperature. For this reason in the present study we have taken engine oil as base fluid and molybdenum disulphide and graphene oxide (MoS2 + GO) hybrid nano-composites are suspended in the (EO). Furthermore, the nonlinear nature of viscoelastic non-Newtonian fluids, introduce a unique challenge to physicists and mathematicians. In the past three decades, viscoelastic fluid models are focused to improve its accuracy and reliability. In this article, viscoelastic Maxwell (MoS2 + GO) hybrid nanofluid (MHNF) is considered in oscillating cylindrical tube together with heat transfer. Exact solutions are obtained by using the joint applications of the Laplace and Hankel transforms and the obtained results are portrayed through different figures. All the figures of the given flow model are constructed for unitary nanofluid (MoS2 + EO) as well as hybrid nanofluid (GO + MoS2 + EO). Effects of flow parameters on Maxwell fluid velocity have shown through graph using computational software MATHCAD. From the present study, we have concluded that hybrid nanofluid gives us more satisfactory results than unitary nanofluid. During this analysis we found that the Maxwell hybrid nanofluid (GO + MoS2 + EO) enhance the rate of heat transfer up to 23.17 %. Furthermore, it is worth noting that engine oil has many engineering and industrial applications. Keeping this fact in mind the present study will help to enhance the rate of heat transfer due to which working machines will do better performance and the loss of useful energy will be reduced. Finally, we have present a limiting case by putting Maxwell fluid parameter (λ = 0) our solutions reduced to well-known published results which validate our work."
PavanKalyan2022,Bg Pavan Kalyan and Lalit Kumar,"3D Printing: Applications in Tissue Engineering, Medical Devices, and Drug Delivery",AAPS PharmSciTech,23,4,2022,10.1208/s12249-022-02242-8,15309932,"The gemstone of 3-dimensional (3D) printing shines up from the pyramid of additive manufacturing. Three-dimensional bioprinting technology has been predicted to be a game-changing breakthrough in the pharmaceutical industry since the last decade. It is fast evolving and finds its seats in a variety of domains, including aviation, defense, automobiles, replacement components, architecture, movies, musical instruments, forensic, dentistry, audiology, prosthetics, surgery, food, and fashion industry. In recent years, this miraculous manufacturing technology has become increasingly relevant for pharmaceutical purposes. Computer-aided drug (CAD) model will be developed by computer software and fed into bioprinters. Based on material inputs, the printers will recognize and produce the model scaffold. Techniques including stereolithography, selective laser sintering, selective laser melting, material extrusion, material jetting, inkjet-based, fused deposition modelling, binder deposition, and bioprinting expedite the printing process. Distinct advantages are rapid prototyping, flexible design, print on demand, light and strong parts, fast and cost-effective, and environment friendly. The present review gives a brief description of the conceptional 3-dimensional printing, followed by various techniques involved. A short note was explained about the fabricating materials in the pharmaceutical sector. The beam of light is thrown on the various applications in the pharma and medical arena."
Ma2021,Dan Ma and Baoyi Guan and Luxia Song and Qiyu Liu and Yixuan Fan and Lin Zhao and Tongxin Wang and Zihao Zhang and Zhuye Gao and Siming Li and Hao Xu,A Bibliometric Analysis of Exosomes in Cardiovascular Diseases From 2001 to 2021,Frontiers in Cardiovascular Medicine,8,,2021,10.3389/fcvm.2021.734514,2297055X,"Background: Exosomes in cardiovascular diseases (CVDs) have become an active research field with substantial value and potential. Nevertheless, there are few bibliometric studies in this field. We aimed to visualize the research hotspots and trends of exosomes in CVDs using a bibliometric analysis to help understand the future development of basic and clinical research. Methods: The articles and reviews regarding exosomes in the CVDs were culled from the Web of Science Core Collection, and knowledge maps were generated using CiteSpace and VOSviewer software. Results: A total of 1,039 articles were included. The number of exosome articles in the CVDs increased yearly. These publications came from 60 countries/regions, led by the US and China. The primary research institutions were Shanghai Jiao Tong University and Nanjing Medical University. Circulation Research was the journal and co-cited journal with the most studies. We identified 473 authors among which Lucio Barile had the most significant number of articles and Thery C was co-cited most often. After analysis, the most common keywords are myocardium infarction, microRNA and mesenchymal stem cells. Ischemic heart disease, pathogenesis, regeneration, stem cells, targeted therapy, biomarkers, cardiac protection, and others are current and developing areas of study. Conclusion: We identified the research hotspots and trends of exosomes in CVDs using bibliometric and visual methods. Research on exosomes is flourishing in the cardiovascular medicine. Regenerative medicine, exosome engineering, delivery vehicles, and biomarkers will likely become the focus of future research."
Wu2022,Chao Wu and Yongbo Yuan and Yang Tang and Boquan Tian,"Application of terrestrial laser scanning (Tls) in the architecture, engineering and construction (aec) industry",Sensors,22,1,2022,10.3390/s22010265,14248220,"As a revolutionary technology, terrestrial laser scanning (TLS) is attracting increasing interest in the fields of architecture, engineering and construction (AEC), with outstanding advantages, such as highly automated, non‐contact operation and efficient large‐scale sampling capability. TLS has extended a new approach to capturing extremely comprehensive data of the construction environment, providing detailed information for further analysis. This paper presents a systematic review based on scientometric and qualitative analysis to summarize the progress and the current status of the topic and to point out promising research efforts. To begin with, a brief understanding of TLS is provided. Following the selection of relevant papers through a literature search, a scientometric analysis of papers is carried out. Then, major applications are categorized and presented, including (1) 3D model reconstruction, (2) object recognition, (3) deformation measurement, (4) quality assessment, and (5) progress tracking. For widespread adoption and effective use of TLS, essential problems impacting working effects in application are summarized as follows: workflow, data quality, scan planning, and data processing. Finally, future research directions are suggested, including: (1) cost control of hardware and software, (2) improvement of data processing capability, (3) automatic scan planning, (4) integration of digital technologies, (5) adoption of artificial intelligence."
Cheng2020,Baoquan Cheng and Jingwei Li and Vivian W.Y. Tam and Ming Yang and Dong Chen,A BIM-LCA approach for estimating the greenhouse gas emissions of large-scale public buildings: A case study,Sustainability (Switzerland),12,2,2020,10.3390/su12020685,20711050,"Exiting green building assessment standards sometimes cannot work well for large-scale public buildings due to insufficient attention to the operation and maintenance stage. This paper combines the theory of life cycle assessment (LCA) and building information modeling (BIM) technology, thereby proposing a green building assessment method by calculating the greenhouse gas emissions (GGE) of buildings from cradle to grave. Life cycle GGE (LCGGE) can be divided into three parts, including the materialization stage, the operation and maintenance stage, and the demolition stage. Two pieces of BIM software (Revit and Designbuilder) are applied in this study. A museum in Guangdong, China, with a hot summer and warm winter is selected for a case study. The results show that BIM can provide a rich source of needed engineering information for LCA. In addition, the operation and maintenance stage plays the most important role in the GGE reduction of a building throughout the whole life cycle. This research contributes to the knowledge body concerning green buildings and sustainable construction. It helps to achieve the reduction of GGE over the whole life cycle of a building. This is pertinent to contractors, homebuyers, and governments who are constantly seeking ways to achieve a low-carbon economy."
Safikhani2022,Saeed Safikhani and Stephan Keller and Gerald Schweiger and Johanna Pirker,"Immersive virtual reality for extending the potential of building information modeling in architecture, engineering, and construction sector: systematic review",International Journal of Digital Earth,15,1,2022,10.1080/17538947.2022.2038291,17538955,"The field of architecture, engineering, and construction (AEC) is continually striving to use resources efficiently and manage complex processes. Now more than ever, there is a strong need for digital transformation in AEC. The improvement in the accessibility of consumer-based head-mounted displays (HMD) is attracting different entertainment and research fields to immersive virtual reality (VR) applications. Building Information Modeling (BIM) is known as a promising technology in AEC. The full potential of BIM is not yet employed to empower this field, however, and this could be a result of some barriers still to be surmounted by BIM in both technological and management perspectives. One of these barriers is the communication and collaboration between design, construction, operation, and maintenance phases. VR can fill this gap by providing additional capabilities for BIM which either were not available before or were not possible to employ in practical ways. In this paper, we systematically review recent research around the application of VR in BIM and discuss the results using the PRISMA flowchart. We discuss the most commonly used technologies, software, and evaluation methods and the various applications of VR in the reviewed papers. Finally, we extend the discussion by summarizing the potential future work in this area."
Khorsandroo2021,Sajad Khorsandroo and Adrián Gallego Sánchez and Ali Saman Tosun and J. M. Arco and Roberto Doriguzzi-Corin,Hybrid SDN evolution: A comprehensive survey of the state-of-the-art,Computer Networks,192,,2021,10.1016/j.comnet.2021.107981,13891286,"Software-Defined Networking (SDN) is an evolutionary networking paradigm which has been adopted by large network and cloud providers, among which are Tech Giants. However, embracing a new and futuristic paradigm as an alternative to well-established and mature legacy networking paradigm requires a lot of time along with considerable financial resources and technical expertise. Consequently, many enterprises cannot afford it. A compromise solution then is a hybrid networking environment (a.k.a. Hybrid SDN (hSDN)) in which SDN functionalities are leveraged while existing traditional network infrastructures are acknowledged. Recently, hSDN has been seen as a viable networking solution for a diverse range of businesses and organizations. Accordingly, the body of literature on hSDN research has improved remarkably. On this account, we present this paper as a comprehensive state-of-the-art survey which expands upon hSDN from many different perspectives."
Ansari2020,Md Tarique Jamal Ansari and Fahad Ahmed Al-Zahrani and Dhirendra Pandey and Alka Agrawal,A fuzzy TOPSIS based analysis toward selection of effective security requirements engineering approach for trustworthy healthcare software development,BMC Medical Informatics and Decision Making,20,1,2020,10.1186/s12911-020-01209-8,14726947,"Background: Today's healthcare organizations want to implement secure and quality healthcare software as cyber-security is a significant risk factor for healthcare data. Considering security requirements during trustworthy healthcare software development process is an essential part of the quality software development. There are several Security Requirements Engineering (SRE) methodologies, framework, process, standards available today. Unfortunately, there is still a necessity to improve these security requirements engineering approaches. Determining the most suitable security requirements engineering method for trustworthy healthcare software development is a challenging process. This study is aimed to present security experts' perspective on the relative importance of the criteria for selecting effective SRE method by utilizing the multi-criteria decision making methods. Methods: The study was planned and conducted to identify the most appropriate SRE approach for quality and trustworthy software development based on the security expert's knowledge and experience. The hierarchical model was evaluated by using fuzzy TOPSIS model. Effective SRE selection criteria were compared in pairs. 25 security experts were asked to response the pairwise criteria comparison form. Results: The impact of the recognized selection criteria for effective security requirements engineering approaches has been evaluated quantitatively. For each of the 25 participants, comparison matrixes were formed based on the scores of their responses in the form. The consistency ratios (CR) were found to be smaller than 10% (CR = 9.1% < 10%). According to pairwise comparisons result; with a 0.842 closeness coefficient (Ci), STORE methodology is the most effective security requirements engineering approach for trustworthy healthcare software development. Conclusions: The findings of this research study demonstrate various factors in the decision-making process for the selection of a reliable method for security requirements engineering. This is a significant study that uses multi-criteria decision-making tools, specifically fuzzy TOPSIS, which used to evaluate different SRE methods for secure and trustworthy healthcare application development."
VanVinh2022,Pham Van Vinh and Le Quang Huy,Finite element analysis of functionally graded sandwich plates with porosity via a new hyperbolic shear deformation theory,Defence Technology,18,3,2022,10.1016/j.dt.2021.03.006,22149147,"This study focusses on establishing the finite element model based on a new hyperbolic sheareformation theory to investigate the static bending, free vibration, and buckling of the functionally graded sandwich plates with porosity. The novel sandwich plate consists of one homogenous ceramic core and two different functionally graded face sheets which can be widely applied in many fields of engineering and defence technology. The discrete governing equations of motion are carried out via Hamilton's principle and finite element method. The computation program is coded in MATLAB software and used to study the mechanical behavior of the functionally graded sandwich plate with porosity. The present finite element algorithm can be employed to study the plates with arbitrary shape and boundary conditions. The obtained results are compared with available results in the literature to confirm the reliability of the present algorithm. Also, a comprehensive investigation of the effects of several parameters on the bending, free vibration, and buckling response of functionally graded sandwich plates is presented. The numerical results shows that the distribution of porosity plays significant role on the mechanical behavior of the functionally graded sandwich plates."
Berger2020,Thorsten Berger and Jan Philipp Steghöfer and Tewfik Ziadi and Jacques Robin and Jabier Martinez,The state of adoption and the challenges of systematic variability management in industry,Empirical Software Engineering,25,3,2020,10.1007/s10664-019-09787-6,15737616,"Handling large-scale software variability is still a challenge for many organizations. After decades of research on variability management concepts, many industrial organizations have introduced techniques known from research, but still lament that pure textbook approaches are not applicable or efficient. For instance, software product line engineering—an approach to systematically develop portfolios of products—is difficult to adopt given the high upfront investments; and even when adopted, organizations are challenged by evolving their complex product lines. Consequently, the research community now mainly focuses on re-engineering and evolution techniques for product lines; yet, understanding the current state of adoption and the industrial challenges for organizations is necessary to conceive effective techniques. In this multiple-case study, we analyze the current adoption of variability management techniques in twelve medium- to large-scale industrial cases in domains such as automotive, aerospace or railway systems. We identify the current state of variability management, emphasizing the techniques and concepts they adopted. We elicit the needs and challenges expressed for these cases, triangulated with results from a literature review. We believe our results help to understand the current state of adoption and shed light on gaps to address in industrial practice."
Wohlin2022,Claes Wohlin and Marcos Kalinowski and Katia Romero Felizardo and Emilia Mendes,Successful combination of database search and snowballing for identification of primary studies in systematic literature studies,Information and Software Technology,147,,2022,10.1016/j.infsof.2022.106908,09505849,"Background: A good search strategy is essential for a successful systematic literature study. Historically, database searches have been the norm, which was later complemented with snowball searches. Our conjecture is that we can perform even better searches if combining these two search approaches, referred to as a hybrid search strategy. Objective: Our main objective was to compare and evaluate a hybrid search strategy. Furthermore, we compared four alternative hybrid search strategies to assess whether we could identify more cost-efficient ways of searching for relevant primary studies. Methods: To compare and evaluate the hybrid search strategy, we replicated the search procedure in a systematic literature review (SLR) on industry–academia collaboration in software engineering. The SLR used a more “traditional” approach to searching for relevant articles for an SLR, while our replication was executed using a hybrid search strategy. Results: In our evaluation, the hybrid search strategy was superior in identifying relevant primary studies. It identified 30% more primary studies and even more studies when focusing only on peer-reviewed articles. To embrace individual viewpoints when assessing research articles and minimise the risk of missing primary studies, we introduced two new concepts, wild cards and borderline articles, when performing systematic literature studies. Conclusions: The hybrid search strategy is a strong contender for being used when performing systematic literature studies. Furthermore, alternative hybrid search strategies may be viable if selected wisely in relation to the start set for snowballing. Finally, the two new concepts were judged as essential to cater for different individual judgements and to minimise the risk of excluding primary studies that ought to be included."
Rahman2021,Md Mostafizer Rahman and Yutaka Watanobe and Keita Nakamura,A bidirectional lstm language model for code evaluation and repair,Symmetry,13,2,2021,10.3390/sym13020247,20738994,"Programming is a vital skill in computer science and engineering-related disciplines. However, developing source code is an error-prone task. Logical errors in code are particularly hard to identify for both students and professionals, and a single error is unexpected to end-users. At present, conventional compilers have difficulty identifying many of the errors (especially logical errors) that can occur in code. To mitigate this problem, we propose a language model for evaluating source codes using a bidirectional long short-term memory (BiLSTM) neural network. We trained the BiLSTM model with a large number of source codes with tuning various hyperparameters. We then used the model to evaluate incorrect code and assessed the model’s performance in three prin-cipal areas: source code error detection, suggestions for incorrect code repair, and erroneous code classification. Experimental results showed that the proposed BiLSTM model achieved 50.88% cor-rectness in identifying errors and providing suggestions. Moreover, the model achieved an F-score of approximately 97%, outperforming other state-of-the-art models (recurrent neural networks (RNNs) and long short-term memory (LSTM))."
Campos2020,Neila Campos and Maria Nogal and Cristina Caliz and Angel A. Juan,Simulation-based education involving online and on-campus models in different European universities,International Journal of Educational Technology in Higher Education,17,1,2020,10.1186/s41239-020-0181-y,23659440,"Simulation-based education (SE) refers to the use of simulation software, tools, and serious games to enrich the teaching and learning processes. Advances in both computer hardware and software allow for employing innovative methodologies that make use of SE tools to enhance the learning experience. Moreover, thanks to the globalisation of e-learning practices, these educational experiences can be made available to students from different geographical regions and universities, which promotes the development of international and inter-university cooperation in education. This paper provides a review of recent works in the SE subject, with a focus on the areas of engineering, science, and management. It also discusses some experiences in SE involving different European universities and learning models. Finally, it also points out open challenges as well as noticeable trends."
Gupta2020,Deepak Gupta and Joel J.P.C. Rodrigues and Shirsh Sundaram and Ashish Khanna and Valery Korotaev and Victor Hugo C. de Albuquerque,Usability feature extraction using modified crow search algorithm: a novel approach,Neural Computing and Applications,32,15,2020,10.1007/s00521-018-3688-6,14333058,"For the purpose of usability feature extraction and prediction, an innovative metaheuristic algorithm is introduced. Generally, the term “usability” is defined by the several researchers with respect to the hierarchical-based software usability model and it has become one of the important methods in terms of software quality. In hierarchically based software, its usability factors, attributes, and its characteristics are combined. The paper presented an algorithm, i.e., modified crow search algorithm (MCSA) mainly for extraction of usability features from hierarchical model with the optimal solution under the search for useful features. MCSA is an extension of original crow search algorithm (CSA), which is a naturally inspired algorithm. The mechanism of this algorithm is based on the process of hiding food and prevents theft and hence introduced this CSA in the field of software engineering practices as an inspiration. The algorithm generates a particular number of selected features/attributes and is applied on software development life cycles models, finding out the best among them. The results of the presented algorithm are compared with the standard binary bat algorithm (BBA), original CSA, and modified whale optimization algorithm (MWOA). The outcomes conclude that the proposed MCSA performs well than the standard BBA and original CSA as the proposed algorithms generate fewer number of feature selection equal to 17 than 18 in BBA, 23 in CSA, and 19 in MWOA."
Haq2020,Fazal Haq and Seifedine Kadry and Yu Ming Chu and Mair Khan and M. Ijaz Khan,Modeling and theoretical analysis of gyrotactic microorganisms in radiated nanomaterial Williamson fluid with activation energy,Journal of Materials Research and Technology,9,5,2020,10.1016/j.jmrt.2020.07.025,22387854,"Here flow behavior of stratified Williamson nanofluid over porous surface of stretching cylinder is examined. The concept of gyrotactic miscroorganisms is implemented to control the random movement of suspended nanoparticles. Effects of magnetic field is accounted. Further chemical reaction with Arrhenius activation energy is considered for the modeling of concentration equation. Brownian motion and thermophoresis effects are further considered. The flow model is obtained by employing the boundary layer assumptions. Appropriate transformations are used to reduced the dimensional system into non-dimensional ones. NDSolve code in MATHEMATICA software is used to tackle the obtained non-dimensional flow expressions. Behavior of velocity, mass concentration, temperature and motile microorganisms versus involved variables is examined graphically. The engineering curiosity like skin friction coefficient, heat and mass transfer rates (Nusselt and Sherwood numbers) and density number are computed and analyzed. Important observations are highlighted at the end."
Chakraborty2020,Joymallya Chakraborty and Suvodeep Majumder and Zhe Yu and Tim Menzies,Fairway: A way to build fair ML software,,,,2020,10.1145/3368089.3409697,,"Machine learning software is increasingly being used to make decisions that affect people's lives. But sometimes, the core part of this software (the learned model), behaves in a biased manner that gives undue advantages to a specific group of people (where those groups are determined by sex, race, etc.). This ""algorithmic discrimination""in the AI software systems has become a matter of serious concern in the machine learning and software engineering community. There have been works done to find ""algorithmic bias""or ""ethical bias""in the software system. Once the bias is detected in the AI software system, the mitigation of bias is extremely important. In this work, we a)explain how ground-truth bias in training data affects machine learning model fairness and how to find that bias in AI software,b)propose a method Fairway which combines pre-processing and in-processing approach to remove ethical bias from training data and trained model. Our results show that we can find bias and mitigate bias in a learned model, without much damaging the predictive performance of that model. We propose that (1) testing for bias and (2) bias mitigation should be a routine part of the machine learning software development life cycle. Fairway offers much support for these two purposes."
Jger2021,Sebastian Jäger and Arndt Allhorn and Felix Bießmann,A Benchmark for Data Imputation Methods,Frontiers in Big Data,4,,2021,10.3389/fdata.2021.693674,2624909X,"With the increasing importance and complexity of data pipelines, data quality became one of the key challenges in modern software applications. The importance of data quality has been recognized beyond the field of data engineering and database management systems (DBMSs). Also, for machine learning (ML) applications, high data quality standards are crucial to ensure robust predictive performance and responsible usage of automated decision making. One of the most frequent data quality problems is missing values. Incomplete datasets can break data pipelines and can have a devastating impact on downstream ML applications when not detected. While statisticians and, more recently, ML researchers have introduced a variety of approaches to impute missing values, comprehensive benchmarks comparing classical and modern imputation approaches under fair and realistic conditions are underrepresented. Here, we aim to fill this gap. We conduct a comprehensive suite of experiments on a large number of datasets with heterogeneous data and realistic missingness conditions, comparing both novel deep learning approaches and classical ML imputation methods when either only test or train and test data are affected by missing data. Each imputation method is evaluated regarding the imputation quality and the impact imputation has on a downstream ML task. Our results provide valuable insights into the performance of a variety of imputation methods under realistic conditions. We hope that our results help researchers and engineers to guide their data preprocessing method selection for automated data quality improvement."
Zhang2022,Wengang Zhang and Liang Han and Xin Gu and Lin Wang and Fuyong Chen and Hanlong Liu,Tunneling and deep excavations in spatially variable soil and rock masses: A short review,Underground Space (China),7,3,2022,10.1016/j.undsp.2020.03.003,24679674,"In an urbanization process, infrastructure elements such as tunnels and deep excavations are widely used to service the development of cities. Owing to the lengthy geological processes of geomaterials and the limited availability of site-specific test data, soil and rock properties exhibiting spatial variability are frequently encountered in geological and geotechnical engineering. This paper presents a comprehensive review of the application of spatial variability in tunneling and deep excavation over the past 20 years. It is found that the spatial variability is generally modeled as a random field (RF) in finite element software, based on random field theory (RFT). This model has been widely used in the design, stability evaluation, and probabilistic analysis of tunnels and excavations. Previous works have proven that the performance of tunnels and deep excavations can be better captured by considering the spatial variability, as compared with conventional deterministic analysis methods. Nonetheless, current research still faces many factual scientific problems. Therefore, this paper also identifies some research gaps, as well as recommendations for further investigations."
Poutievski2022,Leon Poutievski and Omid Mashayekhi and Joon Ong and Arjun Singh and Mukarram Tariq and Rui Wang and Jianan Zhang and Virginia Beauregard and Patrick Conner and Steve Gribble and Rishi Kapoor and Stephen Kratzer and Nanfang Li and Hong Liu and Karthik Nagaraj and Jason Ornstein and Samir Sawhney and Ryohei Urata and Lorenzo Vicisano and Kevin Yasumura and Shidong Zhang and Junlan Zhou and Amin Vahdat,Jupiter evolving: Transforming googles datacenter network via optical circuit switches and software-defined networking,,,,2022,10.1145/3544216.3544265,,"We present a decade of evolution and production experience with Jupiter datacenter network fabrics. In this period Jupiter has delivered 5x higher speed and capacity, 30% reduction in capex, 41% reduction in power, incremental deployment and technology refresh all while serving live production traffic. A key enabler for these improvements is evolving Jupiter from a Clos to a direct-connect topology among the machine aggregation blocks. Critical architectural changes for this include: A datacenter interconnection layer employing Micro-Electro-Mechanical Systems (MEMS) based Optical Circuit Switches (OCSes) to enable dynamic topology reconfiguration, centralized Software-Defined Networking (SDN) control for traffic engineering, and automated network operations for incremental capacity delivery and topology engineering. We show that the combination of traffic and topology engineering on direct-connect fabrics achieves similar throughput as Clos fabrics for our production traffic patterns. We also optimize for path lengths: 60% of the traffic takes direct path from source to destination aggregation blocks, while the remaining transits one additional block, achieving an average block-level path length of 1.4 in our fleet today. OCS also achieves 3x faster fabric reconfiguration compared to pre-evolution Clos fabrics that used a patch panel based interconnect."
Ford2022,Denae Ford and Margaret Anne Storey and Thomas Zimmermann and Christian Bird and Sonia Jaffe and Chandra Maddila and Jenna L. Butler and Brian Houck and Nachiappan Nagappan,A Tale of Two Cities: Software Developers Working from Home during the COVID-19 Pandemic,ACM Transactions on Software Engineering and Methodology,31,2,2022,10.1145/3487567,15577392,"The COVID-19 pandemic has shaken the world to its core and has provoked an overnight exodus of developers who normally worked in an office setting to working from home. The magnitude of this shift and the factors that have accompanied this new unplanned work setting go beyond what the software engineering community has previously understood to be remote work. To find out how developers and their productivity were affected, we distributed two surveys (with a combined total of 3,634 responses that answered all required questions) weeks apart to understand the presence and prevalence of the benefits, challenges, and opportunities to improve this special circumstance of remote work. From our thematic qualitative analysis and statistical quantitative analysis, we find that there is a dichotomy of developer experiences influenced by many different factors (that for some are a benefit, while for others a challenge). For example, a benefit for some was being close to family members but for others having family members share their working space and interrupting their focus, was a challenge. Our surveys led to powerful narratives from respondents and revealed the scale at which these experiences exist to provide insights as to how the future of (pandemic) remote work can evolve."
Canedo2020,Edna Dias Canedo and Bruno Cordeiro Mendes,Software requirements classification using machine learning algorithms,Entropy,22,9,2020,10.3390/E22091057,10994300,"The correct classification of requirements has become an essential task within software engineering. This study shows a comparison among the text feature extraction techniques, and machine learning algorithms to the problem of requirements engineer classification to answer the two major questions ""Which works best (Bag ofWords (BoW) vs. Term Frequency-Inverse Document Frequency (TF-IDF) vs. Chi Squared (CHI2) for classifying Software Requirements into Functional Requirements (FR) and Non-Functional Requirements (NF), and the sub-classes of Non-Functional Requirements?"" and ""Which Machine Learning Algorithm provides the best performance for the requirements classification task?"". The data used to perform the research was the PROMISE_exp, a recently made dataset that expands the already known PROMISE repository, a repository that contains labeled software requirements. All the documents from the database were cleaned with a set of normalization steps and the two feature extractions, and feature selection techniques used were BoW, TF-IDF and CHI2 respectively. The algorithms used for classification were Logist Regression (LR), Support Vector Machine (SVM), Multinomial Naive Bayes (MNB) and k-Nearest Neighbors (kNN). The novelty of our work is the data used to perform the experiment, the details of the steps used to reproduce the classification, and the comparison between BoW, TF-IDF and CHI2 for this repository not having been covered by other studies. This work will serve as a reference for the software engineering community and will help other researchers to understand the requirement classification process. We noticed that the use of TF-IDF followed by the use of LR had a better classification result to differentiate requirements, with an F-measure of 0.91 in binary classification (tying with SVM in that case), 0.74 in NF classification and 0.78 in general classification. As future work we intend to compare more algorithms and new forms to improve the precision of our models."
Lenarduzzi2021,Valentina Lenarduzzi and Terese Besker and Davide Taibi and Antonio Martini and Francesca Arcelli Fontana,"A systematic literature review on Technical Debt prioritization: Strategies, processes, factors, and tools",Journal of Systems and Software,171,,2021,10.1016/j.jss.2020.110827,01641212,"Background: Software companies need to manage and refactor Technical Debt issues. Therefore, it is necessary to understand if and when refactoring of Technical Debt should be prioritized with respect to developing features or fixing bugs. Objective: The goal of this study is to investigate the existing body of knowledge in software engineering to understand what Technical Debt prioritization approaches have been proposed in research and industry. Method: We conducted a Systematic Literature Review of 557 unique papers published until 2020, following a consolidated methodology applied in software engineering. We included 44 primary studies. Results: Different approaches have been proposed for Technical Debt prioritization, all having different goals and proposing optimization regarding different criteria. The proposed measures capture only a small part of the plethora of factors used to prioritize Technical Debt qualitatively in practice. We present an impact map of such factors. However, there is a lack of empirical and validated set of tools. Conclusion: We observed that Technical Debt prioritization research is preliminary and there is no consensus on what the important factors are and how to measure them. Consequently, we cannot consider current research conclusive. In this paper, we therefore outline different directions for necessary future investigations."
Bhatti2021,Muhammad Mubashir Bhatti and Sara I. Abdelsalam,Thermodynamic entropy of a magnetized Ree-Eyring particle-fluid motion with irreversibility process: A mathematical paradigm,ZAMM Zeitschrift fur Angewandte Mathematik und Mechanik,101,6,2021,10.1002/zamm.202000186,15214001,"This article deals with the entropy generation and irreversibility process under the effects of partial slip on magnetic dusty liquid induced by peristaltic wave through a porous channel. The Ree-Eyring fluid model has been used for a governing flow. Mathematical modelling is based on Ohm's law, continuity equation, Darcy law and momentum equation. Analytical solutions are presented for fluid and particle phase. The effects of different pertinent parameters are considered for Newtonian and non-Newtonian cases. Numerical integration has been carried out using a computational software to analyse the pumping characteristics. The behaviour of velocity profile, trapping mechanism, entropy generation, Bejan number, temperature distribution, and pressure rise are investigated. From the obtained results, it is noticed that entropy generation and Bejan number enhanced due to the presence of Brinkman number as well as with the strong influence of non-Newtonian effects. The presence of particles in the fluid decelerates the flow. The magnetic and fluid parameter perform in a similar manner on peristaltic pumping. The temperature profile shows increasing behaviour against higher values of Brinkman number and magnetic field. The present analysis presents different interesting results that help examine dusty fluid with other rheological working fluid models. This work is also applicable in smart fluid pumping systems in aerospace and nuclear industry as well as peristaltic electromagnetic micro-pumps in bio-medical engineering."
Spencer2021,David A. Spencer and Bruce Betts and John M. Bellardo and Alex Diaz and Barbara Plante and Justin R. Mansell,The LightSail 2 solar sailing technology demonstration,Advances in Space Research,67,9,2021,10.1016/j.asr.2020.06.029,18791948,"The LightSail 2 mission is the culmination of a decade-long program sponsored by The Planetary Society to advance solar sailing technology. The objective of LightSail 2 is to demonstrate controlled solar sailing in Earth orbit using a CubeSat platform. The LightSail 2 attitude is controlled using a single-axis momentum wheel and magnetic torque rods. During solar sailing operations, two 90 degree slews are performed each orbit to harness momentum from solar photons. Flight data show that LightSail 2 is successfully controlling its orientation relative to the Sun, and the controlled thrust from solar radiation pressure is measurably reducing the rate of orbital decay. The Planetary Society declared LightSail 2 mission success on July 31, 2019. This paper provides an overview of the LightSail 2 mission implementation, including the design of the flight system and flight software, and the pre-launch testing program. A summary of LightSail 2 mission operations is provided, including a description of the ground system. Solar sailing performance is presented, and anomalies encountered during the mission are discussed. The flight team continues to refine solar sailing performance and conduct on-orbit imaging for engineering purposes and to engage public interest. The LightSail program is entirely donor-funded, with over 50,000 contributors around the globe."
Balbin2020,Paul Patrick F. Balbin and Jackson C.R. Barker and Carson K. Leung and Marvin Tran and Riley P. Wall and Alfredo Cuzzocrea,Predictive analytics on open big data for supporting smart transportation services,,176,,2020,10.1016/j.procs.2020.09.202,18770509,"In the current era of big data, huge quantities of valuable data, which may be of different levels of veracity, are being generated at a rapid rate. Embedded into these big data are implicit, previously unknown and potentially useful information and valuable knowledge that can be discovered by data science solutions, which apply techniques like data mining. There has been a trend that more and more collections of these big data have been made openly available in science, government and non-profit organizations so that people could collaboratively study and analysis these open big data. In this article, we focus on open big data for public transit because public transit (e.g., bus) as a means of transportation is a vital part of many people's lives. As time is a precious resource, bus delays could negatively affect commuters' plans. Unfortunately, they are inevitable. Hence, many existing works focused on predicting bus delays. However, predicting on-time or early buses is also important. For instance, commuters who come to a bus stop on time may still miss their buses if the buses leave early. So, in this article, we examine open big data about bus performance (e.g., early, on-time, and late stops). We analyze the data with frequent pattern mining and make predictions with decision-tree based classification. For illustration, we perform predictive analytics on real-life open big data available on Winnipeg Open Data Portal, about bus performance from Winnipeg Transit. It shows the benefits of predictive analytics on open big data for supporting smart transportation services."
Chen2020,Zhenpeng Chen and Yanbin Cao and Yuanqiang Liu and Haoyu Wang and Tao Xie and Xuanzhe Liu,A comprehensive study on challenges in deploying deep learning based software,,,,2020,10.1145/3368089.3409759,,"Deep learning (DL) becomes increasingly pervasive, being used in a wide range of software applications. These software applications, named as DL based software (in short as DL software), integrate DL models trained using a large data corpus with DL programs written based on DL frameworks such as TensorFlow and Keras. A DL program encodes the network structure of a desirable DL model and the process by which the model is trained using the training data. To help developers of DL software meet the new challenges posed by DL, enormous research efforts in software engineering have been devoted. Existing studies focus on the development of DL software and extensively analyze faults in DL programs. However, the deployment of DL software has not been comprehensively studied. To fill this knowledge gap, this paper presents a comprehensive study on understanding challenges in deploying DL software. We mine and analyze 3,023 relevant posts from Stack Overflow, a popular Q&A website for developers, and show the increasing popularity and high difficulty of DL software deployment among developers. We build a taxonomy of specific challenges encountered by developers in the process of DL software deployment through manual inspection of 769 sampled posts and report a series of actionable implications for researchers, developers, and DL framework vendors."
Riccio2020,Vincenzo Riccio and Paolo Tonella,Model-based exploration of the frontier of behaviours for deep learning system testing,,,,2020,10.1145/3368089.3409730,,"With the increasing adoption of Deep Learning (DL) for critical tasks, such as autonomous driving, the evaluation of the quality of systems that rely on DL has become crucial. Once trained, DL systems produce an output for any arbitrary numeric vector provided as input, regardless of whether it is within or outside the validity domain of the system under test. Hence, the quality of such systems is determined by the intersection between their validity domain and the regions where their outputs exhibit a misbehaviour. In this paper, we introduce the notion of frontier of behaviours, i.e., the inputs at which the DL system starts to misbehave. If the frontier of misbehaviours is outside the validity domain of the system, the quality check is passed. Otherwise, the inputs at the intersection represent quality deficiencies of the system. We developed DeepJanus, a search-based tool that generates frontier inputs for DL systems. The experimental results obtained for the lane keeping component of a self-driving car show that the frontier of a well trained system contains almost exclusively unrealistic roads that violate the best practices of civil engineering, while the frontier of a poorly trained one includes many valid inputs that point to serious deficiencies of the system."
Herrera2020,Manuel Herrera and Marco Pérez-Hernández and Ajith Kumar Parlikad and Joaquín Izquierdo,Multi-agent systems and complex networks: Review and applications in systems engineering,Processes,8,3,2020,10.3390/pr8030312,22279717,"Systems engineering is an ubiquitous discipline of Engineering overlapping industrial, chemical, mechanical, manufacturing, control, software, electrical, and civil engineering. It provides tools for dealing with the complexity and dynamics related to the optimisation of physical, natural, and virtual systems management. This paper presents a review of how multi-agent systems and complex networks theory are brought together to address systems engineering and management problems. The review also encompasses current and future research directions both for theoretical fundamentals and applications in the industry. This is made by considering trends such as mesoscale, multiscale, and multilayer networks along with the state-of-art analysis on network dynamics and intelligent networks. Critical and smart infrastructure, manufacturing processes, and supply chain networks are instances of research topics for which this literature review is highly relevant."
Eliyan2021,Lubna Fayez Eliyan and Roberto Di Pietro,DoS and DDoS attacks in Software Defined Networks: A survey of existing solutions and research challenges,Future Generation Computer Systems,122,,2021,10.1016/j.future.2021.03.011,0167739X,"Software Defined Networking (SDN) is a new networking paradigm where forwarding hardware is decoupled from control decisions. It promises to dramatically simplify network management and enable innovation and evolution. In SDN, network intelligence is logically centralized in software-based controllers (the control plane), while network devices (OpenFlow Switches) become simple packet-forwarding devices (the data plane) that can be programmed via an open interface (OpenFlow protocol). Such decoupling of the control plane from the data plane introduces various challenges that include security, reliability, load balancing, and traffic engineering. Dreadful security challenges in SDNs are denial of service (DoS) and distributed denial of service (DDoS) attacks. For instance, in SDNs, DoS/DDoS attacks could flood the control plane, the data plane, or the communication channel. Attacking the control plane could result in failure of the entire network, while attacking the data plane or the communication channel results in packet drop and network unavailability. In this paper we deliver several contributions that shed light on the field of DoS/DDoS attacks in SDNs, providing a complete background about the area, including attacks and analysis of the existing solutions. In particular, our contributions can be summarized as follow: we review and systematize the state-of-the-art solutions that address both DoS and DDoS attacks in SDNs through the lenses of intrinsic and extrinsic approaches. Moreover, the discussed countermeasures are organized accordingly to their focus, be it on detection, mitigation, prevention, or graceful degradation. Further, we survey the different approaches and tools adopted to implement the revised solutions. Finally, we also highlight possible future research directions to address DoS/DDoS attacks in SDNs."
Liu2020,Kui Liu and Shangwen Wang and Anil Koyuncu and Kisub Kim and Tegawende F. Bissyande and Dongsun Kim and Peng Wu and Jacques Klein and Xiaoguang Mao and Yves Le Traon,On the efficiency of test suite based program repair a systematic assessment of 16 automated repair systems for java programs,,,,2020,10.1145/3377811.3380338,02705257,"Test-based automated program repair has been a prolific field of research in software engineering in the last decade. Many approaches have indeed been proposed, which leverage test suites as aweak, but affordable, approximation to program specifications. Although the literature regularly sets new records on the number of benchmark bugs that can be fixed, several studies increasingly raise concerns about the limitations and biases of state-of-the-art approaches. For example, the correctness of generated patches has been questioned in a number of studies, while other researchers pointed out that evaluation schemes may be misleading with respect to the processing of fault localization results. Nevertheless, there is little work addressing the efficiency of patch generation, with regard to the practicality of program repair. In this paper, we fill this gap in the literature, by providing an extensive review on the efficiency of test suite based program repair. Our objective is to assess the number of generated patch candidates, since this information is correlated to (1) the strategy to traverse the search space efficiently in order to select sensical repair attempts, (2) the strategy to minimize the test effort for identifying a plausible patch, (3) as well as the strategy to prioritize the generation of a correct patch. To that end, we"
Islam2020,Md Johirul Islam and Rangeet Pan and Giang Nguyen and Hridesh Rajan,Repairing deep neural networks: Fix patterns and challenges,,,,2020,10.1145/3377811.3380378,02705257,"Significant interest in applying Deep Neural Network (DNN) has fueled the need to support engineering of software that uses DNNs. Repairing software that uses DNNs is one such unmistakable SE need where automated tools could be beneficial; however, we do not fully understand challenges to repairing and patterns that are utilized when manually repairing DNNs. What challenges should automated repair tools address What are the repair patterns whose automation could help developers Which repair patterns should be assigned a higher priority for building automated bug repair tools This work presents a comprehensive study of bug fix patterns to address these questions. We have studied 415 repairs from Stack Overflow and 555 repairs from GitHub for five popular deep learning libraries Caffe, Keras, Tensorflow, Theano, and Torch to understand challenges in repairs and bug repair patterns. Our key findings reveal that DNN bug fix patterns are distinctive compared to traditional bug fix patterns; the most common bug fix patterns are fixing data dimension and neural network connectivity; DNN bug fixes have the potential to introduce adversarial vulnerabilities; DNN bug fixes frequently introduce new bugs; and DNN bug localization, reuse of trained model, and coping with frequent releases are major challenges faced by developers when fixing bugs. We also contribute a benchmark of 667 DNN (bug, repair) instances."
KUANG2021,Lichun KUANG and He LIU and Yili REN and Kai LUO and Mingyu SHI and Jian SU and Xin LI,Application and development trend of artificial intelligence in petroleum exploration and development,Petroleum Exploration and Development,48,1,2021,10.1016/S1876-3804(21)60001-0,18763804,"Aiming at the actual demands of petroleum exploration and development, this paper describes the research progress and application of artificial intelligence (AI) in petroleum exploration and development, and discusses the applications and development directions of AI in the future. Machine learning has been preliminarily applied in lithology identification, logging curve reconstruction, reservoir parameter estimation, and other logging processing and interpretation, exhibiting great potential. Computer vision is effective in picking of seismic first breaks, fault identification, and other seismic processing and interpretation. Deep learning and optimization technology have been applied to reservoir engineering, and realized the real-time optimization of waterflooding development and prediction of oil and gas production. The application of data mining in drilling, completion, and surface facility engineering etc. has resulted in intelligent equipment and integrated software. The potential development directions of artificial intelligence in petroleum exploration and development are intelligent production equipment, automatic processing and interpretation, and professional software platform. The highlights of development will be digital basins, fast intelligent imaging logging tools, intelligent seismic nodal acquisition systems, intelligent rotary-steering drilling, intelligent fracturing technology and equipment, real-time monitoring and control of zonal injection and production."
Brough2021,Daniel Brough and João Ramos and Bertrand Delpech and Hussam Jouhara,Development and validation of a TRNSYS type to simulate heat pipe heat exchangers in transient applications of waste heat recovery,International Journal of Thermofluids,9,,2021,10.1016/j.ijft.2020.100056,26662027,"Heat pipe heat exchangers (HPHEs) are being more frequently used in energy intensive industries as a method of low-grade waste heat recovery. Prior to the installation of a HPHE, the effect of the heat exchanger within the system requires modelling to simulate the overall impact. From this, potential savings and emission reductions can be determined, and the utilisation of the waste heat can be optimised. One such simulation software is TRNSYS. Currently available heat exchanger simulation components in TRNSYS use averaged values such as a constant effectiveness, constant heat transfer coefficient or conductance for the inputs, which are fixed during the entire simulation. These predictions are useful in a steady-state controlled temperature environment such as a heat treatment facility, but not optimal for the majority of energy recovery applications which operate with fluctuating conditions. A transient TRNSYS HPHE component has been developed using the Effectiveness-Number of Transfer Units (ɛ-NTU) method and validated against experimental results. The model predicts outlet temperatures and energy recovery well within an accuracy of 15% and an average of 4.4% error when compared to existing experimental results, which is acceptable for engineering applications."
Chazette2020,Larissa Chazette and Kurt Schneider,Explainability as a non-functional requirement: challenges and recommendations,Requirements Engineering,25,4,2020,10.1007/s00766-020-00333-1,1432010X,"Software systems are becoming increasingly complex. Their ubiquitous presence makes users more dependent on their correctness in many aspects of daily life. As a result, there is a growing need to make software systems and their decisions more comprehensible, with more transparency in software-based decision making. Transparency is therefore becoming increasingly important as a non-functional requirement. However, the abstract quality aspect of transparency needs to be better understood and related to mechanisms that can foster it. The integration of explanations into software has often been discussed as a solution to mitigate system opacity. Yet, an important first step is to understand user requirements in terms of explainable software behavior: Are users really interested in software transparency and are explanations considered an appropriate way to achieve it? We conducted a survey with 107 end users to assess their opinion on the current level of transparency in software systems and what they consider to be the main advantages and disadvantages of embedded explanations. We assess the relationship between explanations and transparency and analyze its potential impact on software quality. As explainability has become an important issue, researchers and professionals have been discussing how to deal with it in practice. While there are differences of opinion on the need for built-in explanations, understanding this concept and its impact on software is a key step for requirements engineering. Based on our research results and on the study of existing literature, we offer recommendations for the elicitation and analysis of explainability and discuss strategies for the practice."
Jia2021,Zhipeng Jia and Emmett Witchel,"Nightcore: Efficient and scalable serverless computing for latency-sensitive, interactive microservices",,,,2021,10.1145/3445814.3446701,,"The microservice architecture is a popular software engineering approach for building flexible, large-scale online services. Serverless functions, or function as a service (FaaS), provide a simple programming model of stateless functions which are a natural substrate for implementing the stateless RPC handlers of microservices, as an alternative to containerized RPC servers. However, current serverless platforms have millisecond-scale runtime overheads, making them unable to meet the strict sub-millisecond latency targets required by existing interactive microservices. We present Nightcore, a serverless function runtime with microsecond-scale overheads that provides container-based isolation between functions. Nightcore's design carefully considers various factors having microsecond-scale overheads, including scheduling of function requests, communication primitives, threading models for I/O, and concurrent function executions. Nightcore currently supports serverless functions written in C/C++, Go, Node.js, and Python. Our evaluation shows that when running latency-sensitive interactive microservices, Nightcore achieves 1.36×-2.93× higher throughput and up to 69% reduction in tail latency."
Zhu2021,Chun Zhu and Manchao He and Murat Karakus and Xiaohu Zhang and Zhigang Tao,Numerical simulations of the failure process of anaclinal slope physical model and control mechanism of negative Poisson’s ratio cable,Bulletin of Engineering Geology and the Environment,80,4,2021,10.1007/s10064-021-02148-y,14359537,"Occurrence of toppling failure has been prominent due to the increasing of infrastructure construction, such as road slopes, dams, and hydroelectric stations. Many scholars have done research on the toppling failure characteristics, but paid less attention to the comparison of numerical simulations and physical models in order to propose reasonable and effective stability control methods. Based on previous tests on physical model and field investigations, a numerical model of an anaclinal slope using the three-dimension distinct element code (3DEC) software has been built to simulate the failure process of the physical model. Based on the prominent mechanical properties of the engineering-scale and model-scale negative Poisson’s ratio (NPR) cables, a numerical simulation model of the NPR cable has been developed. The numerical model has been used to simulate the effects of different types of model-scale cables on controlling the deformation of the anaclinal slope. The numerical results show that standard cables, i.e., Poisson’s ratio (PR) cables, cannot control the large deformations of the slope, and the slope finally fails. On the opposite, NPR cables can absorb the deformation energy and maintain the stable constant resistance force during the tensile process. This thereby allows avoiding tensile breakage of cables under the effect of large deformations of the slope and driving the slope towards a new equilibrium state. Through the comparison between the numerical simulation results and the physical model test results, the accuracy and rationality of the numerical simulations have been proven. The numerical model developed in this study can be used for future research works on the failure mechanism of anaclinal slopes and the control effect of NPR cables. It thereby lays a foundation for applying the NPR cables to control the toppling deformations of similar anaclinal slopes."
Karampatsis2020,Rafael Michael Karampatsis and Charles Sutton,How Often Do Single-Statement Bugs Occur?: The ManySStuBs4J Dataset,,,,2020,10.1145/3379597.3387491,,"Program repair is an important but difficult software engineering problem. One way to achieve acceptable performance is to focus on classes of simple bugs, such as bugs with single statement fixes, or that match a small set of bug templates. However, it is very difficult to estimate the recall of repair techniques for simple bugs, as there are no datasets about how often the associated bugs occur in code. To fill this gap, we provide a dataset of 153,652 single statement bug-fix changes mined from 1,000 popular open-source Java projects, annotated by whether they match any of a set of 16 bug templates, inspired by state-of-the-art program repair techniques. In an initial analysis, we find that about 33% of the simple bug fixes match the templates, indicating that a remarkable number of single-statement bugs can be repaired with a relatively small set of templates. Further, we find that template fitting bugs appear with a frequency of about one bug per 1,600-2,500 lines of code (as measured by the size of the project's latest version). We hope that the dataset will prove a resource for both future work in program repair and studies in empirical software engineering."
Rangaiah2020,Gade Pandu Rangaiah and Zemin Feng and Andrew F. Hoadley,Multi-objective optimization applications in chemical process engineering: Tutorial and review,Processes,8,5,2020,10.3390/PR8050508,22279717,"This tutorial and review of multi-objective optimization (MOO) gives a detailed explanation of the 5 steps to create, solve, and then select the optimum result. Unlike single-objective optimization, the fifth step of selection or ranking of solutions is often overlooked by the authors of papers dealing with MOO applications. It is necessary to undertake a multi-criteria analysis to choose the best solution. A review of the recent publications using MOO for chemical process engineering problems shows a doubling of publications between 2016 and 2019. MOO applications in the energy area have seen a steady increase of over 20% annually over the last 10 years. The three key methods for solving MOO problems are presented in detail, and an emerging area of surrogate-assisted MOO is also described. The objectives used in MOO trade off conflicting requirements of a chemical engineering problem; these include fundamental criteria such as reaction yield or selectivity; economics; energy requirements; environmental performance; and process control. Typical objective functions in these categories are described, selection/ranking techniques are outlined, and available software for MOO are listed. It is concluded that MOO is gaining popularity as an important tool and is having an increasing use and impact in chemical process engineering."
Wolny2020,Sabine Wolny and Alexandra Mazak and Christine Carpella and Verena Geist and Manuel Wimmer,Thirteen years of SysML: a systematic mapping study,Software and Systems Modeling,19,1,2020,10.1007/s10270-019-00735-y,16191374,"The OMG standard Systems Modeling Language (SysML) has been on the market for about thirteen years. This standard is an extended subset of UML providing a graphical modeling language for designing complex systems by considering software as well as hardware parts. Over the period of thirteen years, many publications have covered various aspects of SysML in different research fields. The aim of this paper is to conduct a systematic mapping study about SysML to identify the different categories of papers, (i) to get an overview of existing research topics and groups, (ii) to identify whether there are any publication trends, and (iii) to uncover possible missing links. We followed the guidelines for conducting a systematic mapping study by Petersen et al. (Inf Softw Technol 64:1–18, 2015) to analyze SysML publications from 2005 to 2017. Our analysis revealed the following main findings: (i) there is a growing scientific interest in SysML in the last years particularly in the research field of Software Engineering, (ii) SysML is mostly used in the design or validation phase, rather than in the implementation phase, (iii) the most commonly used diagram types are the SysML-specific requirement diagram, parametric diagram, and block diagram, together with the activity diagram and state machine diagram known from UML, (iv) SysML is a specific UML profile mostly used in systems engineering; however, the language has to be customized to accommodate domain-specific aspects, (v) related to collaborations for SysML research over the world, there are more individual research groups than large international networks. This study provides a solid basis for classifying existing approaches for SysML. Researchers can use our results (i) for identifying open research issues, (ii) for a better understanding of the state of the art, and (iii) as a reference for finding specific approaches about SysML."
Garcia2020,Joshua Garcia and Yang Feng and Junjie Shen and Sumaya Almanee and Yuan Xia and Qi Alfred Chen,A comprehensive study of autonomous vehicle bugs,,,,2020,10.1145/3377811.3380397,02705257,"Self-driving cars, or Autonomous Vehicles (AVs), are increasingly becoming an integral part of our daily life. About 50 corporations are actively working on AVs, including large companies such as Google, Ford, and Intel. Some AVs are already operating on public roads, with at least one unfortunate fatality recently on record. As a result, understanding bugs in AVs is critical for ensuring their security, safety, robustness, and correctness. While previous studies have focused on a variety of domains (e.g., numerical software; machine learning; and error-handling, concurrency, and performance bugs) to investigate bug characteristics, AVs have not been studied in a similar manner. Recently, two software systems for AVs, Baidu Apollo and Autoware, have emerged as frontrunners in the opensource community and have been used by large companies and governments (e.g., Lincoln, Volvo, Ford, Intel, Hitachi, LG, and the US Department of Transportation). From these two leading AV software systems, this paper describes our investigation of 16,851 commits and 499 AV bugs and introduces our classification of those bugs into 13 root causes, 20 bug symptoms, and 18 categories of software components those bugs often affect. We identify 16 major findings from our study and draw broader lessons from them to guide the research community towards future directions in software bug detection, localization, and repair."
Rohi2020,Godall Rohi and O'tega Ejofodomi and Godswill Ofualagba,"Autonomous monitoring, analysis, and countering of air pollution using environmental drones",Heliyon,6,1,2020,10.1016/j.heliyon.2020.e03252,24058440,"The effect of air pollution on the environment, economic and health of the people in the affected countries cannot be overemphasized. This paper investigates large scale air pollution elimination to remove pollutants that are already in existence in the environment. This method involves the use of Environmental Drones (E-drones) to autonomously monitor the air quality at a specific location. The E-drone flies up to a predetermined height (Ealtitude) every hour, measures the air pollutants at that location, implements on-board pollution abatement solutions for pollutants above the recommended threshold, and then flies back down to its location on the ground. The advantages of this system is its ability to measure air pollution concentration of CO2, CO, NH3, SO2, PM, O3 and NO2, detect when they are too high, and implement on-board pollution abatement solutions as needed. This system's novelty lies in the fact that it not only detects when there is excessive pollution, but it also automatically deals with and abates the detected air pollution above earth. When multiple E-drones are used in different locations, a custom software generates an Air Quality Health Index (AQHI) map of the region that can be used for present and long-term environmental analysis."
Song2021,Ying Qing Song and Shan Ali Khan and Muhammad Imran and Hassan Waqas and Sami Ullah Khan and M. Ijaz Khan and Sumaira Qayyum and Yu Ming Chu,Applications of modified Darcy law and nonlinear thermal radiation in bioconvection flow of micropolar nanofluid over an off centered rotating disk,Alexandria Engineering Journal,60,5,2021,10.1016/j.aej.2021.03.053,11100168,"To improve the heat efficiency base fluids (water, engine oil, glycol), the interaction of nanoparticles (nanotubes, droplets, nanowires, metals and non-metals) into such traditional liquids is the most frequent mechanism and attained the researchers attention, especially in current decade. The nanofluid is a suspension of submerged solid particles in base fluids. The nano-materials convinced the applications in the field of nanotechnology, thermal engineering, industrial and bio-engineering. Following to such motivating applications in mind, current research reports the stagnation point flow of radiative micropolar nanofluid over an off centered rotating disk with applications of motile microorganisms. The novel dynamic of thermal radiation and activation energy are also incorporated. The appropriate transformations are utilized to reduce the partial differential equations into dimensionless forms. A numerical shooting scheme is used to obtain the approximate solution with MATLAB software. The effects of prominent parameter on velocity profile, nanofluid temperature, concentration of nanoparticles and microorganism profile are physically incorporated."
Cico2021,Orges Cico and Letizia Jaccheri and Anh Nguyen-Duc and He Zhang,Exploring the intersection between software industry and Software Engineering education - A systematic mapping of Software Engineering Trends,Journal of Systems and Software,172,,2021,10.1016/j.jss.2020.110736,01641212,"Context: Software has become ubiquitous in every corner of modern societies. During the last five decades, software engineering has also changed significantly to advance the development of various types and scales of software products. In this context, Software Engineering Education plays an important role in keeping students updated with software technologies, processes, and practices that are popular in industries. Objective: We investigate from literature the extent Software Engineering Education addresses major Software Engineering Trends in the academic setting. Method: We conducted a systematic mapping study about teaching major Software Engineering Trends in project courses. We classified 126 papers based on their investigated Software Engineering Trends, specifically Software Engineering processes and practices, teaching approaches, and the evolution of Software Engineering Trends over time. Results: We reveal that Agile Software Development is the major trend. The other Trends, i.e., Software Implementation, Usability and Value, Global Software Engineering, and Lean Software Startup, are relatively small in the academic setting, but continuously growing in the last five years. System of Systems is the least investigated among all Trends. Conclusions: The study points out the possible gaps between Software Industry and Education, which implies actionable insights for researchers, educators, and practitioners."
Huang2022,Chaoran Huang and Volker J. Sorger and Mario Miscuglio and Mohammed Al-Qadasi and Avilash Mukherjee and Lutz Lampe and Mitchell Nichols and Alexander N. Tait and Thomas Ferreira de Lima and Bicky A. Marquez and Jiahui Wang and Lukas Chrostowski and Mable P. Fok and Daniel Brunner and Shanhui Fan and Sudip Shekhar and Paul R. Prucnal and Bhavin J. Shastri,Prospects and applications of photonic neural networks,Advances in Physics: X,7,1,2022,10.1080/23746149.2021.1981155,23746149,"Neural networks have enabled applications in artificial intelligence through machine learning, and neuromorphic computing. Software implementations of neural networks on conventional computers that have separate memory and processor (and that operate sequentially) are limited in speed and energy efficiency. Neuromorphic engineering aims to build processors in which hardware mimics neurons and synapses in the brain for distributed and parallel processing. Neuromorphic engineering enabled by photonics (optical physics) can offer sub-nanosecond latencies and high bandwidth with low energies to extend the domain of artificial intelligence and neuromorphic computing applications to machine learning acceleration, nonlinear programming, intelligent signal processing, etc. Photonic neural networks have been demonstrated on integrated platforms and free-space optics depending on the class of applications being targeted. Here, we discuss the prospects and demonstrated applications of these photonic neural networks."
Jacques2021,Sébastien Jacques and Abdeldjalil Ouahabi and Thierry Lequeu,Remote knowledge acquisition and assessment during the covid-19 pandemic,International Journal of Engineering Pedagogy,10,6,2021,10.3991/IJEP.V10I6.16205,21924880,"On 16 March 2020, as a result of the unprecedented global health crisis linked to the emergence of a new form of coronavirus (COVID-19), the 74 universities of France closed their doors, forcing nearly 1.6 million students, as well as their teachers, to find solutions and initiatives that could ensure continuity in teaching. In the reliance on videoconferencing tools, chat, the sharing of documents/ tutorials/videos/podcasts, and the use of social networks, many ideas have emerged, but no consensus has developed nor has a common way of doing things been adopted by a majority of teachers. Some software tools, such as Zoom, have also been questioned over data security issues or excessive intrusion into the student learning process. Nevertheless, in these uncertain times, much had to be done so that students can acquire the requisite knowledge, develop skills, and build on what they have learned. How can we ensure that the learning process is as smooth as possible for everyone involved? How can we evaluate knowledge and skills learned at a distance, and their relevance? Four groups of electronic and electrical engineering students in France were monitored during the containment period in order to provide answers to these questions. Lectures, tutorials, practical work, and projects were carried out using theMicrosoft Teams and Zoomvideo conferencing and chat tools to complement activitiesmade available through the digital work environment. In order to ensure equity among all students, especially in view of the digital divide, open access tools/software/applications have been promoted. In the various surveys completed, the engineering students asserted their complete satisfaction with the learning process, the use of distance tools, and the level ofmastery of these tools by their teachers. The results of the various knowledge tests show that, for the same course, distance learning does not reduce the performance of the engineering students. Indeed, they obtained local grades similar to those expected in face-to-face teaching. The results presented in this article are not intended to highlight the virtues of distance education, but rather to open up a debate and reflect more widely on the sustainability of this transformation of education in universities."
Al-Saqqa2020,Samar Al-Saqqa and Samer Sawalha and Hiba Abdelnabi,Agile software development: Methodologies and trends,International Journal of Interactive Mobile Technologies,14,11,2020,10.3991/ijim.v14i11.13269,18657923,"Software engineering is a discipline that undergone many improvements that aims to keep up with the new advancements in technologies and the modern business requirements through developing effective approaches to reach the final software product, agile software development is one of these successful approaches. Agile software development is a lightweight approach that was proposed to overcome the convolutional development methods' limitations and to reduce the overhead and the cost while providing flexibility to adopt the changes in requirements at any stage, this is done by managing the tasks and their coordination through a certain set of values and principles. In this work, a comprehensive review that outlines the main agile values and principles, and states the key differences that distinguish agile methods over the traditional ones are presented. Then a discussion of the most popular agile methodologies; their life cycles, their roles, and their advantages and disadvantages are outlined. The recent state of art trends that adopts agile development especially in cloud computing, big data, and coordination are also explored. And finally, this work highlights how to choose the best suitable agile methodology that must be selected according to the task at hand, how sensitive the product is and the organization structure."
Russo2021,Daniel Russo and Paul H.P. Hanel and Seraphina Altnickel and Niels van Berkel,Predictors of well-being and productivity among software professionals during the COVID-19 pandemic – a longitudinal study,Empirical Software Engineering,26,4,2021,10.1007/s10664-021-09945-9,15737616,"The COVID-19 pandemic has forced governments worldwide to impose movement restrictions on their citizens. Although critical to reducing the virus’ reproduction rate, these restrictions come with far-reaching social and economic consequences. In this paper, we investigate the impact of these restrictions on an individual level among software engineers who were working from home. Although software professionals are accustomed to working with digital tools, but not all of them remotely, in their day-to-day work, the abrupt and enforced work-from-home context has resulted in an unprecedented scenario for the software engineering community. In a two-wave longitudinal study (N = 192), we covered over 50 psychological, social, situational, and physiological factors that have previously been associated with well-being or productivity. Examples include anxiety, distractions, coping strategies, psychological and physical needs, office set-up, stress, and work motivation. This design allowed us to identify the variables that explained unique variance in well-being and productivity. Results include (1) the quality of social contacts predicted positively, and stress predicted an individual’s well-being negatively when controlling for other variables consistently across both waves; (2) boredom and distractions predicted productivity negatively; (3) productivity was less strongly associated with all predictor variables at time two compared to time one, suggesting that software engineers adapted to the lockdown situation over time; and (4) longitudinal analyses did not provide evidence that any predictor variable causal explained variance in well-being and productivity. Overall, we conclude that working from home was per se not a significant challenge for software engineers. Finally, our study can assess the effectiveness of current work-from-home and general well-being and productivity support guidelines and provides tailored insights for software professionals."
Halabi2020,Osama Halabi,Immersive virtual reality to enforce teaching in engineering education,Multimedia Tools and Applications,79,3-4,2020,10.1007/s11042-019-08214-8,15737721,"Prior studies on the use of digital prototyping and virtual reality (VR) in designing as well as evaluating new products have shown that VR reduces both development time and costs whilst augmenting student motivation and creativity. The current study demonstrates that VR and 3D prototyping in the context of project-based learning (PBL) promote effective communication, increase problem solving skills, and enhance learning outcomes. VR and digital prototyping have been extensively used in industries for the purpose of product design and usability evaluation. In the context of engineering education, many research studies have attempted to explore the effect of VR on teamwork, engagement, retention, and motivation. In this paper, VR is used in conjunction with PBL in self-directed approach to design and implement a product using 3D software whilst also using virtual reality immersive CAVE display to evaluate their design. The hypothesis is that the use of VR with a project-based-learning approach to facilitate the attainment of desirable goals in the engineering design project, improved achievement of course learning outcomes and promoted effective communication. According to the research findings, VR approach significantly affected the distribution of cumulative project grades. Students’ project grades improved, particularly the implementation component. In addition, the course outcomes related to project design were better achieved in VR approach. The communication and problem-solving skills were improved in the VR approach as compared to traditional approach."
Chen2020,Jieshan Chen and Mulong Xie and Zhenchang Xing and Chunyang Chen and Xiwei Xu and Liming Zhu and Guoqiang Li,Object detection for graphical user interface: Old fashioned or deep learning or a combination?,,,,2020,10.1145/3368089.3409691,,"Detecting Graphical User Interface (GUI) elements in GUI images is a domain-specific object detection task. It supports many software engineering tasks, such as GUI animation and testing, GUI search and code generation. Existing studies for GUI element detection directly borrow the mature methods from computer vision (CV) domain, including old fashioned ones that rely on traditional image processing features (e.g., canny edge, contours), and deep learning models that learn to detect from large-scale GUI data. Unfortunately, these CV methods are not originally designed with the awareness of the unique characteristics of GUIs and GUI elements and the high localization accuracy of the GUI element detection task. We conduct the first large-scale empirical study of seven representative GUI element detection methods on over 50k GUI images to understand the capabilities, limitations and effective designs of these methods. This study not only sheds the light on the technical challenges to be addressed but also informs the design of new GUI element detection methods. We accordingly design a new GUI-specific old-fashioned method for non-text GUI element detection which adopts a novel top-down coarse-to-fine strategy, and incorporate it with the mature deep learning model for GUI text detection.Our evaluation on 25,000 GUI images shows that our method significantly advances the start-of-the-art performance in GUI element detection."
vanDinter2022,Raymon van Dinter and Bedir Tekinerdogan and Cagatay Catal,Predictive maintenance using digital twins: A systematic literature review,Information and Software Technology,151,,2022,10.1016/j.infsof.2022.107008,09505849,"Context: Predictive maintenance is a technique for creating a more sustainable, safe, and profitable industry. One of the key challenges for creating predictive maintenance systems is the lack of failure data, as the machine is frequently repaired before failure. Digital Twins provide a real-time representation of the physical machine and generate data, such as asset degradation, which the predictive maintenance algorithm can use. Since 2018, scientific literature on the utilization of Digital Twins for predictive maintenance has accelerated, indicating the need for a thorough review. Objective: This research aims to gather and synthesize the studies that focus on predictive maintenance using Digital Twins to pave the way for further research. Method: A systematic literature review (SLR) using an active learning tool is conducted on published primary studies on predictive maintenance using Digital Twins, in which 42 primary studies have been analyzed. Results: This SLR identifies several aspects of predictive maintenance using Digital Twins, including the objectives, application domains, Digital Twin platforms, Digital Twin representation types, approaches, abstraction levels, design patterns, communication protocols, twinning parameters, and challenges and solution directions. These results contribute to a Software Engineering approach for developing predictive maintenance using Digital Twins in academics and the industry. Conclusion: This study is the first SLR in predictive maintenance using Digital Twins. We answer key questions for designing a successful predictive maintenance model leveraging Digital Twins. We found that to this day, computational burden, data variety, and complexity of models, assets, or components are the key challenges in designing these models."
Chen2021,Xieling Chen and Di Zou and Haoran Xie and Fu Lee Wang,"Past, present, and future of smart learning: a topic-based bibliometric analysis",International Journal of Educational Technology in Higher Education,18,1,2021,10.1186/s41239-020-00239-6,23659440,"Innovative information and communication technologies have reformed higher education from the traditional way to smart learning. Smart learning applies technological and social developments and facilitates effective personalized learning with innovative technologies, especially smart devices and online technologies. Smart learning has attracted increasing research interest from the academia. This study aims to comprehensively review the research field of smart learning by conducting a topic modeling analysis of 555 smart learning publications collected from the Scopus database. In particular, it seeks answers to (1) what the major research topics concerning smart learning were, and (2) how these topics evolved. Results demonstrate several major research issues, for example, Interactive and multimedia learning, STEM (science, technology, engineering, and mathematics) education, Attendance and attention recognition, Blended learning for smart learning, and Affective and biometric computing. Furthermore, several emerging topics were identified, for example, Smart learning analytics, Software engineering for e-learning systems, IoT (Internet of things) and cloud computing, and STEM education. Additionally, potential inter-topic directions were highlighted, for instance, Attendance and attention recognition and IoT and cloud computing, Semantics and ontology and Mobile learning, Feedback and assessment and MOOCs (massive open online courses) and course content management, as well as Blended learning for smart learning and Ecosystem and ambient intelligence."
Qin2020,Jiawen Qin and Changcheng Liu and Que Huang,Simulation on fire emergency evacuation in special subway station based on Pathfinder,Case Studies in Thermal Engineering,21,,2020,10.1016/j.csite.2020.100677,2214157X,"As a kind of large volume underground vehicles, the subway greatly relieves the traffic pressure of the city. However, the main defect is a closed space can easily lead to high casualty once there is a fire. In this paper, Pathfinder software was utilized to simulate the evacuation in special subway station. The evacuation in dissimilar status was analyzed by setting up the fire scenario and changing the flow rate in the station. The results showed that the pressure of evacuation is mainly at stairway entrance and the width of exit has little effect on relieving the evacuation pressure. Besides, the number of the people in platform need to be restricted to 500 when the train is full; when the number of passengers in the train is 1542, that of people on the platform has to be limited to 480; when the number of passengers in the train is 1224, no more than 800 people on the platform need to be controlled."
Harel-Canada2020,Fabrice Harel-Canada and Lingxiao Wang and Muhammad Ali Gulzar and Quanquan Gu and Miryung Kim,Is neuron coverage a meaningful measure for testing deep neural networks?,,,,2020,10.1145/3368089.3409754,,"Recent effort to test deep learning systems has produced an intuitive and compelling test criterion called neuron coverage (NC), which resembles the notion of traditional code coverage. NC measures the proportion of neurons activated in a neural network and it is implicitly assumed that increasing NC improves the quality of a test suite. In an attempt to automatically generate a test suite that increases NC, we design a novel diversity promoting regularizer that can be plugged into existing adversarial attack algorithms. We then assess whether such attempts to increase NC could generate a test suite that (1) detects adversarial attacks successfully, (2) produces natural inputs, and (3) is unbiased to particular class predictions. Contrary to expectation, our extensive evaluation finds that increasing NC actually makes it harder to generate an effective test suite: higher neuron coverage leads to fewer defects detected, less natural inputs, and more biased prediction preferences. Our results invoke skepticism that increasing neuron coverage may not be a meaningful objective for generating tests for deep neural networks and call for a new test generation technique that considers defect detection, naturalness, and output impartiality in tandem."
Ye2021,Shuaihua Ye and Zhuangfu Zhao and Denqun Wang,Deformation analysis and safety assessment of existing metro tunnels affected by excavation of a foundation pit,Underground Space (China),6,4,2021,10.1016/j.undsp.2020.06.002,24679674,"The excavation of a foundation pit considerably affects the adjacent structures and underground pipelines owing to the change in the stress state of the surrounding soil, resulting in deformation. The study of an actual engineering case was conducted to examine the influence of excavation on the deformation of adjacent subway tunnels. The finite element analysis software PLAXIS 3D was used to simulate the entire excavation process. The structural design of the foundation pit was optimized based on the simulation results to ensure the stability of the foundation pit and the safety of the existing subway tunnel structure. Finally, the safety evaluation of the excavation of the foundation pit that caused the deformation of the adjacent subway tunnel was performed. The influence of the excavation and unloading of the foundation pit on the subway tunnel is closely related to the distance between the subway and the foundation pit, the amount of earthwork excavated at one time, and the engineering geological conditions. The results of this paper can provide useful reference for the design optimization and safety assessment of similar projects."
Fang2020,Chunrong Fang and Zixi Liu and Yangyang Shi and Jeff Huang and Qingkai Shi,Functional code clone detection with syntax and semantics fusion learning,,,,2020,10.1145/3395363.3397362,,"Clone detection of source code is among the most fundamental software engineering techniques. Despite intensive research in the past decade, existing techniques are still unsatisfactory in detecting ""functional"" code clones. In particular, existing techniques cannot efficiently extract syntax and semantics information from source code. In this paper, we propose a novel joint code representation that applies fusion embedding techniques to learn hidden syntactic and semantic features of source codes. Besides, we introduce a new granularity for functional code clone detection. Our approach regards the connected methods with caller-callee relationships as a functionality and the method without any caller-callee relationship with other methods represents a single functionality. Then we train a supervised deep learning model to detect functional code clones. We conduct evaluations on a large dataset of C++ programs and the experimental results show that fusion learning can significantly outperform the state-of-the-art techniques in detecting functional code clones."
Zheng2020,Peilin Zheng and Zibin Zheng and Jiajing Wu and Hong Ning Dai,XBlock-ETH: Extracting and exploring blockchain data from ethereum,IEEE Open Journal of the Computer Society,1,1,2020,10.1109/OJCS.2020.2990458,26441268,"Blockchain-based cryptocurrencies have received extensive attention recently. Massive data has been stored on permission-less blockchains. The analysis of massive blockchain data can bring huge business values. However, the absence of well-processed up-to-date blockchain datasets impedes big data analytics of blockchain data. To fill this gap, we collect and process the up-to-date on-chain data from Ethereum, which is one of the most popular permission-less blockchains. We name such well-processed Ethereum data as XBlock-ETH, which consists of transactions, smart contracts, and cryptocurrencies (i.e., tokens). However, it is non-trivial to partition and categorize the collected raw Ethereum data to the well-processed datasets since the whole processing procedure requires sophisticated knowledge on software engineering as well as big data analytics. Moreover, we also present basic statistics and exploration for each of the well-processed datasets. Furthermore, we also outline the possible research opportunities based on XBlock-ETH, with the data and code released online."
Arif2022,Muhammad Arif and Poom Kumam and Wiyada Kumam and Zaydan Mostafa,Heat transfer analysis of radiator using different shaped nanoparticles water-based ternary hybrid nanofluid with applications: A fractional model,Case Studies in Thermal Engineering,31,,2022,10.1016/j.csite.2022.101837,2214157X,"The suspension of nanoparticles in the conventional base fluids getting more attention of the scholars and researchers due to its unique thermal performance in different field of engineering sciences. Nanofluid performed well and showed satisfactory results in the heat transport phenomena which attracted the scientists to suspend different combinations of nanoparticles which named as “hybrid nanofluid”. From the experimental investigations it is found that the rate of heat transfer is higher for hybrid nanofluid as compared to unitary nanofluid. Based on the above motivation the present study is focused to consider water-based ternary hybrid nanofluid with three different shaped nanoparticles i.e, spherical shaped aluminum oxide (Al2O3), cylindrical carbon nanotubes (CNT), and platelet shaped (Graphene) for the advance cooling process of radiator. From the present analysis it is found that this advance water-based ternary hybrid nanofluid showed promising enhancement in the heat transfer rate as compared to hybrid and unitary nanofluid. The present problem is formulated in the form of momentum and energy equations in terms of partial differential equations along with physical initial and boundary conditions. Furthermore, we have considered water-based ternary hybrid nanofluid with different shaped nanoparticles in channel. For the exact solutions the Laplace and Fourier transforms are applied. The influence of all the flow parameters is highlighted using the computational software MATHCAD. Using water-based ternary hybrid nanofluid enhances the rate of heat transfer up-to 33.67% which shows a promising thermal performance in the heat transfer rate. Furthermore, we have used nanoparticles in different ratios and found some interesting results which can be applied in different engineering problems specially, in cooling process."
Ihme2022,Matthias Ihme and Wai Tong Chung and Aashwin Ananda Mishra,"Combustion machine learning: Principles, progress and prospects: Combustion machine learning",Progress in Energy and Combustion Science,91,,2022,10.1016/j.pecs.2022.101010,03601285,"Progress in combustion science and engineering has led to the generation of large amounts of data from large-scale simulations, high-resolution experiments, and sensors. This corpus of data offers enormous opportunities for extracting new knowledge and insights—if harnessed effectively. Machine learning (ML) techniques have demonstrated remarkable success in data analytics, thus offering a new paradigm for data-intense analyses and scientific investigations through combustion machine learning (CombML). While data-driven methods are utilized in various combustion areas, recent advances in algorithmic developments, the accessibility of open-source software libraries, the availability of computational resources, and the abundance of data have together rendered ML techniques ubiquitous in scientific analysis and engineering. This article examines ML techniques for applications in combustion science and engineering. Starting with a review of sources of data, data-driven techniques, and concepts, we examine supervised, unsupervised, and semi-supervised ML methods. Various combustion examples are considered to illustrate and to evaluate these methods. Next, we review past and recent applications of ML approaches to problems in combustion, spanning fundamental combustion investigations, propulsion and energy-conversion systems, and fire and explosion hazards. Challenges unique to CombML are discussed and further opportunities are identified, focusing on interpretability, uncertainty quantification, robustness, consistency, creation and curation of benchmark data, and the augmentation of ML methods with prior combustion-domain knowledge."
Nobanee2021,Haitham Nobanee and Fatima Youssef Al Hamadi and Fatma Ali Abdulaziz and Lina Subhi Abukarsh and Aysha Falah Alqahtani and Shayma Khalifa Alsubaey and Sara Mohamed Alqahtani and Hamama Abdulla Almansoori,A bibliometric analysis of sustainability and risk management,Sustainability (Switzerland),13,6,2021,10.3390/su13063277,20711050,"Sustainability practices in a working environment represent superior quality performances, while risks remain to be a challenge. Our study’s primary purpose is to deploy the bibliometric method to analyze the related literature. Bibliometric parameters analyzed using the VOSviewer software were employed to identify citations relevant to sustainability and risk contexts’ critical themes. From 1990–2020, a reflection of 1233 documents appeared in Scopus on sustainability practices and risk management. We searched the current papers, authors, institutes, and keywords on VOSviewer. The bibliometric search provided us an understanding, which reflected that the collected works on literature of sustainability and risk factors, in general, is suggestively increasing. Mainly, in our report, we highlighted six major streams, related to topics such as the moral responsibilities and sustainability development, blockchain technology and minimization of risks, social sustainability and supply chain, environmental impacts, safety engineering and risk identification, and optimization and sustainability practices. The primary purpose of using streams was to cite the key authors and their contributions to the related literature. This bibliometric analysis was developed to obtain further understanding regarding the importance of sustainability to the individual, firms, and the entire economy. Moreover, the factors associated with risk also sought to be examined to prevent or at least minimize its negative impact. It was identified in this paper that sustainability remains an issue in the global perspective that has been challenging the individual and/or the organization’s point of views. Risk factors were also identified as inevitable; hence, everyone must be socially responsible to minimize the negative impact on the economy."
Ceolini2020,Enea Ceolini and Charlotte Frenkel and Sumit Bam Shrestha and Gemma Taverni and Lyes Khacef and Melika Payvand and Elisa Donati,Hand-Gesture Recognition Based on EMG and Event-Based Camera Sensor Fusion: A Benchmark in Neuromorphic Computing,Frontiers in Neuroscience,14,,2020,10.3389/fnins.2020.00637,1662453X,"Hand gestures are a form of non-verbal communication used by individuals in conjunction with speech to communicate. Nowadays, with the increasing use of technology, hand-gesture recognition is considered to be an important aspect of Human-Machine Interaction (HMI), allowing the machine to capture and interpret the user's intent and to respond accordingly. The ability to discriminate between human gestures can help in several applications, such as assisted living, healthcare, neuro-rehabilitation, and sports. Recently, multi-sensor data fusion mechanisms have been investigated to improve discrimination accuracy. In this paper, we present a sensor fusion framework that integrates complementary systems: the electromyography (EMG) signal from muscles and visual information. This multi-sensor approach, while improving accuracy and robustness, introduces the disadvantage of high computational cost, which grows exponentially with the number of sensors and the number of measurements. Furthermore, this huge amount of data to process can affect the classification latency which can be crucial in real-case scenarios, such as prosthetic control. Neuromorphic technologies can be deployed to overcome these limitations since they allow real-time processing in parallel at low power consumption. In this paper, we present a fully neuromorphic sensor fusion approach for hand-gesture recognition comprised of an event-based vision sensor and three different neuromorphic processors. In particular, we used the event-based camera, called DVS, and two neuromorphic platforms, Loihi and ODIN + MorphIC. The EMG signals were recorded using traditional electrodes and then converted into spikes to be fed into the chips. We collected a dataset of five gestures from sign language where visual and electromyography signals are synchronized. We compared a fully neuromorphic approach to a baseline implemented using traditional machine learning approaches on a portable GPU system. According to the chip's constraints, we designed specific spiking neural networks (SNNs) for sensor fusion that showed classification accuracy comparable to the software baseline. These neuromorphic alternatives have increased inference time, between 20 and 40%, with respect to the GPU system but have a significantly smaller energy-delay product (EDP) which makes them between 30× and 600× more efficient. The proposed work represents a new benchmark that moves neuromorphic computing toward a real-world scenario."
Knight2020,Earl E. Knight and Esteban Rougier and Zhou Lei and Bryan Euser and Viet Chau and Samuel H. Boyce and Ke Gao and Kurama Okubo and Marouchka Froment,HOSS: an implementation of the combined finite-discrete element method,Computational Particle Mechanics,7,5,2020,10.1007/s40571-020-00349-y,21964386,"Nearly thirty years since its inception, the combined finite-discrete element method (FDEM) has made remarkable strides in becoming a mainstream analysis tool within the field of Computational Mechanics. FDEM was developed to effectively “bridge the gap” between two disparate Computational Mechanics approaches known as the finite and discrete element methods. At Los Alamos National Laboratory (LANL) researchers developed the Hybrid Optimization Software Suite (HOSS) as a hybrid multi-physics platform, based on FDEM, for the simulation of solid material behavior complemented with the latest technological enhancements for full fluid–solid interaction. In HOSS, several newly developed FDEM algorithms have been implemented that yield more accurate material deformation formulations, inter-particle interaction solvers, and fracture and fragmentation solutions. In addition, an explicit computational fluid dynamics solver and a novel fluid–solid interaction algorithms have been fully integrated (as opposed to coupled) into the HOSS’ solid mechanical solver, allowing for the study of an even wider range of problems. Advancements such as this are leading HOSS to become a tool of choice for multi-physics problems. HOSS has been successfully applied by a myriad of researchers for analysis in rock mechanics, oil and gas industries, engineering application (structural, mechanical and biomedical engineering), mining, blast loading, high velocity impact, as well as seismic and acoustic analysis. This paper intends to summarize the latest development and application efforts for HOSS."
Kasauli2021,Rashidah Kasauli and Eric Knauss and Jennifer Horkoff and Grischa Liebel and Francisco Gomes de Oliveira Neto,Requirements engineering challenges and practices in large-scale agile system development,Journal of Systems and Software,172,,2021,10.1016/j.jss.2020.110851,01641212,"Context: Agile methods have become mainstream even in large-scale systems engineering companies that need to accommodate different development cycles of hardware and software. For such companies, requirements engineering is an essential activity that involves upfront and detailed analysis which can be at odds with agile development methods. Objective: This paper presents a multiple case study with seven large-scale systems companies, reporting their challenges, together with best practices from industry. We also analyze literature about two popular large-scale agile frameworks, SAFe® and LeSS, to derive potential solutions for the challenges. Methods: Our results are based on 20 qualitative interviews, five focus groups, and eight cross-company workshops which we used to both collect and validate our results. Results: We found 24 challenges which we grouped in six themes, then mapped to solutions from SAFe®, LeSS, and our companies, when available. Conclusion: In this way, we contribute a comprehensive overview of RE challenges in relation to large-scale agile system development, evaluate the degree to which they have been addressed, and outline research gaps. We expect these results to be useful for practitioners who are responsible for designing processes, methods, or tools for large scale agile development as well as guidance for researchers."
Waqas2022,Hassan Waqas and Mowffaq Oreijah and Kamel Guedri and Sami Ullah Khan and Song Yang and Sumeira Yasmin and Muhammad Ijaz Khan and Omar T. Bafakeeh and El Sayed Mohamed Tag-ElDin and Ahmed M. Galal,Gyrotactic Motile Microorganisms Impact on Pseudoplastic Nanofluid Flow over a Moving Riga Surface with Exponential Heat Flux,Crystals,12,9,2022,10.3390/cryst12091308,20734352,"Background: The improvement of the thermal conductivity of nanofluids is practical for different processes such as drug delivery, manufacturing of crystals, polymer processing, food and drink, cancer treatment, oil and gas, paper making and for many more. The bioconvection phenomenon has engrossed the attention of numerous researchers for its many applications in biotechnology, mechanical and electrical engineering. Bioconvection nanofluids are more prominent in the fields of biomedicine, pharmacy, nanodrug delivery, biomedical, automotive cooling and the military. Purpose: The major purpose of the current work was to determine the numerical and statistical analysis of a novel thermal radiation and exponential space-based heat source on the bioconvective flow of a pseudoplastic 3D nanofluid past a bidirectional stretched Riga surface. The behavior of the Arrhenius activation energy (AAE) and thermal radiation are also disclosed. Methodology: Suitable similarity transformations were used to transmute the partial differential equations of the flow-modeled phenomena into the structure of ordinary differential ones. The numerical solutions for the renewed set of ODEs were tackled by the bvp4c shooting algorithm built-in MATLAB software. Furthermore, the statistical analysis was computed by applying response surface methodology (RSM). Research implications: The numerical analysis is valid for the incompressible three-dimensional, magnetized flow of a pseudoplastic bioconvection nanofluid through a bidirectional surface with Riga plate aspects in the occurrence of activation energy. Social implications: The flow across three dimensions has quite important implementations in various fields, for example, polymer production, material production technology, the manufacturing of nano-biopolymer computer graphics, industry, powered engineering, aeroplane configurations, etc. The current analysis is more applicable in nanotechnology. Results: The consequences of flow control parameters over flow profiles were studied and explained under the graphic structures. Numerical outcomes were computed and discussed in detail. From the results, it was noted that the velocity field was increased via a larger mixed convection parameter. The temperature distribution was boosted via the thermal Biot number. The concentration of nanoparticles declined via the greater Lewis number. Furthermore, the motile microorganisms field was reduced via the Peclet number. Originality: Until now, no investigation has been recognized to examine the consequences of the bioconvection flow of three-dimensional pseudoplastic nanofluids past a Riga plate containing motile microorganisms utilizing the shooting method called bvp4c. Conclusions: From the results, it was concluded that nanofluids are more helpful for heat transfer increments. Furthermore, from the experimental design observed, the response declined via the thermophoresis parameter, which was significant from the ANOVA observed model."
He2022,Hongwen He and Fengchun Sun and Zhenpo Wang and Cheng Lin and Chengning Zhang and Rui Xiong and Junjun Deng and Xiaoqing Zhu and Peng Xie and Shuo Zhang and Zhongbao Wei and Wanke Cao and Li Zhai,China's battery electric vehicles lead the world: achievements in technology system architecture and technological breakthroughs,Green Energy and Intelligent Transportation,1,1,2022,10.1016/j.geits.2022.100020,27731537,"Developing new energy vehicles has been a worldwide consensus, and developing new energy vehicles characterized by pure electric drive has been China's national strategy. After more than 20 years of high-quality development of China's electric vehicles (EVs), a technological R & D layout of “Three Verticals and Three Horizontals” has been created, and technological advantages have been accumulated. As a result, China's new energy vehicle market has ranked first in the world since 2015. To systematically solve the key problems of battery electric vehicles (BEVs) such as “driving range anxiety, long battery charging time, and driving safety hazards”, China took the lead in putting forward a “system engineering-based technology system architecture for BEVs” and clarifying its connotation. This paper analyzes the research status and progress of the three core components of this architecture, namely, “BEV platform, charging/swapping station, and real-time operation monitoring platform”, and their key technological points. The three major demonstration projects of the 2008 Beijing Olympic Games, the 2022 Beijing Winter Olympics, and the intelligent and connected autonomous battery electric bus project are discussed to specify the applications of BEVs in China. The key research directions for upgrading BEV technologies remain to be further improving the vehicle-level all-climate environmental adaptability and all-day safety of BEVs, systematically solving the charging problem of BEVs and improving their application convenience, and safeguarding safety with early warning and implementing active/passive safety protection for the whole life cycle of power batteries on the basis of BEVs' operation big data. BEVs have acquired new technological features such as intelligent and networked technology empowerment, extensive integration of control-by-wire systems, a platform of chassis hardware, and modularization of functional software."
Steuernagel2020,Burkhard Steuernagel and Kamil Witek and Simon G. Krattinger and Ricardo H. Ramirez-Gonzalez and Henk Jan Schoonbeek and Guotai Yu and Erin Baggs and Agnieszka I. Witek and Inderjit Yadav and Ksenia V. Krasileva and Jonathan D.G. Jones and Cristobal Uauy and Beat Keller and Christopher J. Ridout and Brande B.H. Wulff,The NLR-annotator tool enables annotation of the intracellular immune receptor repertoire,Plant Physiology,183,2,2020,10.1104/pp.19.01273,15322548,"Disease resistance genes encoding nucleotide-binding and leucine-rich repeat (NLR) intracellular immune receptor proteins detect pathogens by the presence of pathogen effectors. Plant genomes typically contain hundreds of NLR-encoding genes. The availability of the hexaploid wheat (Triticum aestivum) cultivar Chinese Spring reference genome allows a detailed study of its NLR complement. However, low NLR expression and high intrafamily sequence homology hinder their accurate annotation. Here, we developed NLR-Annotator, a software tool for in silico NLR identification independent of transcript support. Although developed for wheat, we demonstrate the universal applicability of NLR-Annotator across diverse plant taxa. We applied our tool to wheat and combined it with a transcript-validated subset of genes from the reference gene annotation to characterize the structure, phylogeny, and expression profile of the NLR gene family. We detected 3, 400 full-length NLR loci, of which 1, 560 were confirmed as expressed genes with intact open reading frames. NLRs with integrated domains mostly group in specific subclades. Members of another subclade predominantly locate in close physical proximity to NLRs carrying integrated domains, suggesting a paired helper function. Most NLRs (88%) display low basal expression (in the lower 10 percentile of transcripts). In young leaves subjected to biotic stress, we found up-regulation of 266 of the NLRs. To illustrate the utility of our tool for the positional cloning of resistance genes, we estimated the number of NLR genes within the intervals of mapped rust resistance genes. Our study will support the identification of functional resistance genes in wheat to accelerate the breeding and engineering of disease-resistant varieties."
Kontogeorgis2021,Georgios M. Kontogeorgis and Ralf Dohrn and Ioannis G. Economou and Jean Charles De Hemptinne and Antoonten Kate and Susanna Kuitunen and Miranda Mooijer and Ljudmila Fele Zilnik and Velisa Vesovic,Industrial requirements for thermodynamic and transport properties: 2020,Industrial and Engineering Chemistry Research,60,13,2021,10.1021/acs.iecr.0c05356,15205045,"This paper reports the results of an investigation of industrial requirements for thermodynamic and transport properties carried out during the years 2019-2020. It is a follow-up of a similar investigation performed and published 10 years ago by the Working Party (WP) of Thermodynamics and Transport Properties of European Federation of Chemical Engineering (EFCE).1 The main goal was to investigate the advances in this area over the past 10 years, to identify the limitations that still exist, and to propose future R&D directions that will address the industrial needs. An updated questionnaire, with two new categories, namely, digitalization and comparison to previous survey/changes over the past 10 years, was sent to a broad number of experts in companies with a diverse activity spectrum, in oil and gas, chemicals, pharmaceuticals/biotechnology, food, chemical/ mechanical engineering, consultancy, and power generation, among others, and in software suppliers and contract research laboratories. Very comprehensive answers were received by 37 companies, mostly from Europe (operating globally), but answers were also provided by companies in the USA and Japan. The response rate was about 60%, compared to 47% in the year 2010. The paper is written in such a way that both the majority and minority points of view are presented, and although the discussion is focused on needs and challenges, the benefits of thermodynamics and success stories are also reported. The results of the survey are thematically structured and cover changes, challenges, and further needs for a number of areas of interest such as data, models, systems, properties, and computational aspects (molecular simulation, algorithms and standards, and digitalization). Education and collaboration are discussed and recommendations on the future research activities are also outlined. In addition, a few initiatives, books, and reviews published in the past decade are briefly discussed. It is a long paper and, to provide the reader with a more complete understanding of the survey, many (anonymous) quotations (indicated with ""."" and italics) from the industrial colleagues who have participated in the survey are provided. To help disseminate the specific information of interest only to particular industrial sectors, the paper has been written in such a way that the individual sections can also be read independently of each other."
Alsariera2020,Yazan Ahmad Alsariera and Victor Elijah Adeyemo and Abdullateef Oluwagbemiga Balogun and Ammar Kareem Alazzawi,AI Meta-Learners and Extra-Trees Algorithm for the Detection of Phishing Websites,IEEE Access,8,,2020,10.1109/ACCESS.2020.3013699,21693536,"Phishing is a type of social web-engineering attack in cyberspace where criminals steal valuable data or information from insensitive or uninformed users of the internet. Existing countermeasures in the form of anti-phishing software and computational methods for detecting phishing activities have proven to be effective. However, new methods are deployed by hackers to thwart these countermeasures. Due to the evolving nature of phishing attacks, the need for novel and efficient countermeasures becomes crucial as the effect of phishing attacks are often fatal and disastrous. Artificial Intelligence (AI) schemes have been the cornerstone of modern countermeasures used for mitigating phishing attacks. AI-based phishing countermeasures or methods possess their shortcomings particularly the high false alarm rate and the inability to interpret how most phishing methods perform their function. This study proposed four (4) meta-learner models (AdaBoost-Extra Tree (ABET), Bagging-Extra tree (BET), Rotation Forest-Extra Tree (RoFBET) and LogitBoost-Extra Tree (LBET)) developed using the extra-tree base classifier. The proposed AI-based meta-learners were fitted on phishing website datasets (currently with the newest features) and their performances were evaluated. The models achieved a detection accuracy not lower than 97% with a drastically low false-positive rate of not more 0.028. In addition, the proposed models outperform existing ML-based models in phishing attack detection. Hence, we recommend the adoption of meta-learners when building phishing attack detection models."
Noroozi2020,Reza Noroozi and Mahdi Bodaghi and Hamid Jafari and Ali Zolfagharian and Mohammad Fotouhi,Shape-adaptive metastructures with variable bandgap regions by 4D printing,Polymers,12,3,2020,10.3390/polym12030519,20734360,"This article shows how four-dimensional (4D) printing technology can engineer adaptive metastructures that exploit resonating self-bending elements to filter vibrational and acoustic noises and change filtering ranges. Fused deposition modeling (FDM) is implemented to fabricate temperature-responsive shape-memory polymer (SMP) elements with self-bending features. Experiments are conducted to reveal how the speed of the 4D printer head can affect functionally graded prestrain regime, shape recovery and self-bending characteristics of the active elements. A 3D constitutive model, along with an in-house finite element (FE) method, is developed to replicate the shape recovery and self-bending of SMP beams 4D-printed at different speeds. Furthermore, a simple approach of prestrain modeling is introduced into the commercial FE software package to simulate material tailoring and self-bending mechanism. The accuracy of the straightforward FE approach is validated against experimental observations and computational results from the in-house FE MATLAB-based code. Two periodic architected temperature-sensitive metastructures with adaptive dynamical characteristics are proposed to use bandgap engineering to forbid specific frequencies from propagating through the material. The developed computational tool is finally implemented to numerically examine how bandgap size and frequency range can be controlled and broadened. It is found out that the size and frequency range of the bandgaps are linked to changes in the geometry of self-bending elements printed at different speeds. This research is likely to advance the state-of-the-art 4D printing and unlock potentials in the design of functional metastructures for a broad range of applications in acoustic and structural engineering, including sound wave filters and waveguides."
Kumar2020,Sunil Kumar and Ranbir Kumar and Jagdev Singh and K. S. Nisar and Devendra Kumar,An efficient numerical scheme for fractional model of HIV-1 infection of CD4+ T-cells with the effect of antiviral drug therapy,Alexandria Engineering Journal,59,4,2020,10.1016/j.aej.2019.12.046,11100168,"In the present paper, a HIV-1 infection of CD4+ T-cells with the impact of drug treatment model of arbitrary order have been examined with the aid of Legendre wavelet operational matrix method. The technique employed to convert differential equations of arbitrary order into the linear or nonlinear algebraic equations, which are easy to handle by mathematical software. The comparison among the discussed methods with RK4, homotopy perturbation scheme, homotopy analysis approach, Laplace adomian-decomposition technique and Adam-Bashforth predictor-corrector scheme divulge that the present technique is authentic and valid for other engineering problems. It is noticed that the suggested scheme are more efficient and effective."
Vujovi2021,Željko Vujović,Classification Model Evaluation Metrics,International Journal of Advanced Computer Science and Applications,12,6,2021,10.14569/IJACSA.2021.0120670,21565570,"The purpose of this paper was to confirm the basic assumption that classification models are suitable for solving the problem of data set classifications. We selected four representative models: BaiesNet, NaiveBaies, MultilayerPerceptron, and J48, and applied them to a four-class classification of a specific set of hepatitis C virus data for Egyptian patients. We conducted the study using the WEKA software classification model, developed at Waikato University, New Zealand. Defeat results were obtained. None of the four classes envisaged has been determined reliably. We have described all 16 metrics, which are used to evaluate classification models, listed their characteristics, mutual differences, and the parameter that evaluates each of these metrics. We have presented comparative, tabular values that give each metric for each classification model in a concise form, detailed class accuracy with a table of best and worst metric values, confusion matrices for all four classification models, and a type I and II error table for all four classification models. In addition to the 16 metric classifications, which we described, we listed seven other metrics, which we did not use because we did not have the opportunity to show their application on the selected data set. Metrics were negatively rated selected, standard reliable, classification models. This led to the conclusion that the data in the selected data set should be pre-processed to be reliably classified by the classification model."
SivaKumar2020,Ram Shankar Siva Kumar and Magnus Nystrom and John Lambert and Andrew Marshall and Mario Goertzel and Andi Comissoneru and Matt Swann and Sharon Xia,Adversarial machine learning-industry perspectives,,,,2020,10.1109/SPW50608.2020.00028,,"Based on interviews with 28 organizations, we found that industry practitioners are not equipped with tactical and strategic tools to protect, detect and respond to attacks on their Machine Learning (ML) systems. We leverage the insights from the interviews and enumerate the gaps in securing machine learning systems when viewed in the context of traditional software security development. We write this paper from the perspective of two personas: developers/ML engineers and security incident responders. The goal of this paper is to layout the research agenda to amend the Security Development Lifecycle for industrial-grade software in the adversarial ML era."
Mangano2020,Francesco Guido Mangano and Oleg Admakin and Matteo Bonacina and Henriette Lerner and Vygandas Rutkunas and Carlo Mangano,Trueness of 12 intraoral scanners in the full-arch implant impression: A comparative in vitro study,BMC Oral Health,20,1,2020,10.1186/s12903-020-01254-9,14726831,"Background: The literature has not yet validated the use of intraoral scanners (IOSs) for full-arch (FA) implant impression. Hence, the aim of this in vitro study was to assess and compare the trueness of 12 different IOSs in FA implant impression. Methods: A stone-cast model of a totally edentulous maxilla with 6 implant analogues and scanbodies (SBs) was scanned with a desktop scanner (Freedom UHD®) to capture a reference model (RM), and with 12 IOSs (ITERO ELEMENTS 5D®; PRIMESCAN® and OMNICAM®; CS 3700® and CS 3600®; TRIOS3®; i-500®; EMERALD S® and EMERALD®; VIRTUO VIVO® and DWIO®; RUNEYES QUICKSCAN®). Ten scans were taken using each IOS, and each was compared to the RM, to evaluate trueness. A mesh/mesh method and a nurbs/nurbs method were used to evaluate the overall trueness of the scans; linear and cross distances between the SBs were used to evaluate the local trueness of the scans. The analysis was performed using reverse engineering software (Studio®, Geomagics; Magics®, Materialise). A statistical evaluation was performed. Results: With the mesh/mesh method, the best results were obtained by CS 3700® (mean error 30.4 μm) followed by ITERO ELEMENTS 5D® (31.4 μm), i-500® (32.2 μm), TRIOS 3® (36.4 μm), CS 3600® (36.5 μm), PRIMESCAN® (38.4 μm), VIRTUO VIVO® (43.8 μm), RUNEYES® (44.4 μm), EMERALD S® (52.9 μm), EMERALD® (76.1 μm), OMNICAM® (79.6 μm) and DWIO® (98.4 μm). With the nurbs/nurbs method, the best results were obtained by ITERO ELEMENTS 5D® (mean error 16.1 μm), followed by PRIMESCAN® (19.3 μm), TRIOS 3® (20.2 μm), i-500® (20.8 μm), CS 3700® (21.9 μm), CS 3600® (24.4 μm), VIRTUO VIVO® (32.0 μm), RUNEYES® (33.9 μm), EMERALD S® (36.8 μm), OMNICAM® (47.0 μm), EMERALD® (51.9 μm) and DWIO® (69.9 μm). Statistically significant differences were found between the IOSs. Linear and cross distances between the SBs (local trueness analysis) confirmed the data that emerged from the overall trueness evaluation. Conclusions: Different levels of trueness were found among the IOSs evaluated in this study. Further studies are needed to confirm these results."
Leymann2020,Frank Leymann and Johanna Barzen,The bitter truth about gate-based quantum algorithms in the NISQ era,Quantum Science and Technology,5,4,2020,10.1088/2058-9565/abae7d,20589565,"Implementing a gate-based quantum algorithm on an noisy intermediate scale quantum (NISQ) device has several challenges that arise from the fact that such devices are noisy and have limited quantum resources. Thus, various factors contributing to the depth and width as well as to the noise of an implementation of a gate-based algorithm must be understood in order to assess whether an implementation will execute successfully on a given NISQ device. In this contribution, we discuss these factors and their impact on algorithm implementations. Especially, we will cover state preparation, oracle expansion, connectivity, circuit rewriting, and readout: these factors are very often ignored when presenting a gate-based algorithm but they are crucial when implementing such an algorithm on near-term quantum computers. Our contribution will help developers in charge of realizing gate-based algorithms on such machines in (i) achieving an executable implementation, and (ii) assessing the success of their implementation on a given machine."
VanDerGiessen2020,Erik Van Der Giessen and Peter A. Schultz and Nicolas Bertin and Vasily V. Bulatov and Wei Cai and Gábor Csányi and Stephen M. Foiles and M. G.D. Geers and Carlos González and Markus Hütter and Woo Kyun Kim and Dennis M. Kochmann and Javier Llorca and Ann E. Mattsson and Jörg Rottler and Alexander Shluger and Ryan B. Sills and Ingo Steinbach and Alejandro Strachan and Ellad B. Tadmor,Roadmap on multiscale materials modeling,Modelling and Simulation in Materials Science and Engineering,28,4,2020,10.1088/1361-651X/ab7150,1361651X,"Modeling and simulation is transforming modern materials science, becoming an important tool for the discovery of new materials and material phenomena, for gaining insight into the processes that govern materials behavior, and, increasingly, for quantitative predictions that can be used as part of a design tool in full partnership with experimental synthesis and characterization. Modeling and simulation is the essential bridge from good science to good engineering, spanning from fundamental understanding of materials behavior to deliberate design of new materials technologies leveraging new properties and processes. This Roadmap presents a broad overview of the extensive impact computational modeling has had in materials science in the past few decades, and offers focused perspectives on where the path forward lies as this rapidly expanding field evolves to meet the challenges of the next few decades. The Roadmap offers perspectives on advances within disciplines as diverse as phase field methods to model mesoscale behavior and molecular dynamics methods to deduce the fundamental atomic-scale dynamical processes governing materials response, to the challenges involved in the interdisciplinary research that tackles complex materials problems where the governing phenomena span different scales of materials behavior requiring multiscale approaches. The shift from understanding fundamental materials behavior to development of quantitative approaches to explain and predict experimental observations requires advances in the methods and practice in simulations for reproducibility and reliability, and interacting with a computational ecosystem that integrates new theory development, innovative applications, and an increasingly integrated software and computational infrastructure that takes advantage of the increasingly powerful computational methods and computing hardware."
Homayounfar2020,S. Zohreh Homayounfar and Trisha L. Andrew,"Wearable Sensors for Monitoring Human Motion: A Review on Mechanisms, Materials, and Challenges",SLAS Technology,25,1,2020,10.1177/2472630319891128,24726311,"The emergence of flexible wearable electronics as a new platform for accurate, unobtrusive, user-friendly, and longitudinal sensing has opened new horizons for personalized assistive tools for monitoring human locomotion and physiological signals. Herein, we survey recent advances in methodologies and materials involved in unobtrusively sensing a medium to large range of applied pressures and motions, such as those encountered in large-scale body and limb movements or posture detection. We discuss three commonly used methodologies in human gait studies: inertial, optical, and angular sensors. Next, we survey the various kinds of electromechanical devices (piezoresistive, piezoelectric, capacitive, triboelectric, and transistive) that are incorporated into these sensor systems; define the key metrics used to quantitate, compare, and optimize the efficiency of these technologies; and highlight state-of-the-art examples. In the end, we provide the readers with guidelines and perspectives to address the current challenges of the field."
Sulzer2021,Valentin Sulzer and Scott G. Marquis and Robert Timms and Martin Robinson and S. Jon Chapman,Python Battery Mathematical Modelling (PyBaMM),Journal of Open Research Software,9,,2021,10.5334/JORS.309,20499647,"As the UK battery modelling community grows, there is a clear need for software that uses modern software engineering techniques to facilitate cross-institutional collaboration and democratise research progress. The Python package PyBaMM aims to provide a flexible platform for implementation and comparison of new models and numerical methods. This is achieved by implementing models as expression trees and processing them in a modular fashion through a pipeline. Comprehensive testing provides robustness to changes and hence eases the implementation of model extensions. PyBaMM is open source and available on GitHub. For more information visit www.pybamm.org."
Nex2022,F. Nex and C. Armenakis and M. Cramer and D. A. Cucci and M. Gerke and E. Honkavaara and A. Kukko and C. Persello and J. Skaloud,UAV in the advent of the twenties: Where we stand and what is next,ISPRS Journal of Photogrammetry and Remote Sensing,184,,2022,10.1016/j.isprsjprs.2021.12.006,09242716,"The use of Unmanned Aerial Vehicles (UAVs) has surged in the last two decades, making them popular instruments for a wide range of applications, and leading to a remarkable number of scientific contributions in geoscience, remote sensing and engineering. However, the development of best practices for high quality of UAV mapping are often overlooked representing a drawback for their wider adoption. UAV solutions then require an inter-disciplinary research, integrating different expertise and combining several hardware and software components on the same platform. Despite the high number of peer-reviewed papers on UAVs, little attention has been given to the interaction between research topics from different domains (such as robotics and computer vision) that impact the use of UAV in remote sensing. The aim of this paper is to (i) review best practices for the use of UAVs for remote sensing and mapping applications and (ii) report on current trends - including adjacent domains - for UAV use and discuss their future impact in photogrammetry and remote sensing. Hardware developments, navigation and acquisition strategies, and emerging solutions for data processing in innovative applications are considered in this analysis. As the number and the heterogeneity of debated topics are large, the paper is organized according to very specific questions considered most relevant by the authors."
Sanchis2020,Raquel Sanchis and Óscar García-Perales and Francisco Fraile and Raul Poler,Low-code as enabler of digital transformation in manufacturing industry,Applied Sciences (Switzerland),10,1,2020,10.3390/app10010012,20763417,"Currently, enterprises have to make quick and resilient responses to changing market requirements. In light of this, low-code development platforms provide the technology mechanisms to facilitate and automate the development of software applications to support current enterprise needs and promote digital transformation. Based on a theory-building research methodology through the literature and other information sources review, the main contribution of this paper is the current characterisation of the emerging low-code domain following the foundations of the computer-aided software engineering field. A context analysis, focused on the current status of research related to the low-code development platforms, is performed. Moreover, benchmarking among the existing low-code development platforms addressed to manufacturing industry is analysed to identify the current lacking features. As an illustrative example of the emerging low-code paradigm and respond to the identified uncovered features, the virtual factory open operating system (vf-OS) platform is described as an open multi-sided low-code framework able to manage the overall network of a collaborative manufacturing and logistics environment that enables humans, applications, and Internet of Things (IoT) devices to seamlessly communicate and interoperate in the interconnected environment, promoting resilient digital transformation."
Herrera-Franco2020,Gricelda Herrera-Franco and Néstor Montalván-Burbano and Paúl Carrión-Mero and Boris Apolo-Masache and María Jaya-Montalvo,Research trends in geotourism: A bibliometric analysis using the scopus database,Geosciences (Switzerland),10,10,2020,10.3390/geosciences10100379,20763263,"Geodiversity has elements of exceptional scientific value that are considered to represent geoheritage, or geological heritage. One way to conserve and promote the knowledge of these elements is through the initiatives of United Nations Educational, Scientific, and Cultural Organization (UNESCO) Global Geoparks, which, over a decade ago, began to notably highlight a new sustainable tourism alternative called geotourism, or geological tourism, that promotes the protection of the unique geological resources of territory and, at the same time, provides social, economic, and environmental benefits. This study aims to investigate the scientific information related to geotourism in the Scopus database through a bibliometric analysis, using the VOSviewer software, for the evaluation of the structure, conceptual evolution, and trends of geotourism following related publications. The research comprises four study phases: (i) search criteria of the research field; (ii) search and selection of documents; (iii) software and data extraction; and (iv) analysis of results and trends. The results present geotourism as a scientific discipline that is in a phase of exponential research growth and exhibits its scientific productivity from 1984 to 2019, where three main periods are differentiated: introduction, theoretical development, and diversification of information. The most active research area is geomorphological heritage, which is very far from the emerging line of research of engineering geology in geotourism. However, growing exploration during the last six years has generated the development of various geoscientific branches promoted by geotourism that, currently, present their research area trends such as geosites, geoheritages, and geoparks."
Mencarelli2020,Luca Mencarelli and Qi Chen and Alexandre Pagot and Ignacio E. Grossmann,A review on superstructure optimization approaches in process system engineering,Computers and Chemical Engineering,136,,2020,10.1016/j.compchemeng.2020.106808,00981354,"In this paper, we survey the main superstructure-based approaches in process system engineering, with a particular emphasis on the existing literature for automated superstructure generation. We examine both classical and more recent representations in terms of generality, ease of use, and tractability. We also discuss the implications that different representations may have on strategies for algebraic modeling and optimization. We then review the state-of-the-art in software implementations to support synthesis. Finally, we examine the use of evolutionary—recently referred to as superstructure-free—approaches, in which algorithmic procedures dynamically generate and evaluate candidate process structures."
Zhang2020,Lijun Zhang and Muhammad Mubashir Bhatti and Marin Marin and Khaled S. Mekheimer,Entropy analysis on the blood flow through anisotropically tapered arteries filled with magnetic zinc-oxide (ZnO) nanoparticles,Entropy,22,10,2020,10.3390/E22101070,10994300,"The present analysis deals with the entropy analysis of the blood flow through an anisotropically tapered arteries under the suspension of magnetic Zinc-oxide (ZnO) nanoparticles (NPs). The Jeffrey fluid model is contemplated as blood that is electrically conducting and incompressible. The lubrication approach is used for the mathematical modeling. The second law of thermodynamics is used to examine the entropy generation. The exact solutions are obtained against velocity and temperature profile with the use of computational software. The results for Entropy, Velocity, Bejan number, temperature profile, and impedance profile are discussed by plotting the graphs. ZnO-NPs have promising applications in biomedical engineering due to its low toxicity, economically reliable, and excellent biocompatibility. ZnO-NPs also emerged in medicine i.e., antibacterial and anticancer activity, and also beneficial in antidiabetic treatment. The monitoring of the blood temperature in the case of the tapered artery has supreme importance in controlling the temperature of blood in the living environment. The presence of a magnetic field is advantageous to manage and control the blood motion at different temperatures. The present outcomes are enriched to give valuable information for the research scientists in the field biomedical science, who are looking to examine the blood flow with stenosis conditions and also beneficial in treating multiple diseases."
Kherwa2020,Pooja Kherwa and Poonam Bansal,Topic Modeling: A Comprehensive Review,EAI Endorsed Transactions on Scalable Information Systems,7,24,2020,10.4108/eai.13-7-2018.159623,20329407,"Topic modelling is the new revolution in text mining. It is a statistical technique for revealing the underlying semantic structure in large collection of documents. After analysing approximately 300 research articles on topic modeling, a comprehensive survey on topic modelling has been presented in this paper. It includes classification hierarchy, Topic modelling methods, Posterior Inference techniques, different evolution models of latent Dirichlet allocation (LDA) and its applications in different areas of technology including Scientific Literature, Bioinformatics, Software Engineering and analysing social network is presented. Quantitative evaluation of topic modeling techniques is also presented in detail for better understanding the concept of topic modeling. At the end paper is concluded with detailed discussion on challenges of topic modelling, which will definitely give researchers an insight for good research."
Genheden2020,Samuel Genheden and Amol Thakkar and Veronika Chadimová and Jean Louis Reymond and Ola Engkvist and Esben Bjerrum,"AiZynthFinder: a fast, robust and flexible open-source software for retrosynthetic planning",Journal of Cheminformatics,12,1,2020,10.1186/s13321-020-00472-1,17582946,"We present the open-source AiZynthFinder software that can be readily used in retrosynthetic planning. The algorithm is based on a Monte Carlo tree search that recursively breaks down a molecule to purchasable precursors. The tree search is guided by an artificial neural network policy that suggests possible precursors by utilizing a library of known reaction templates. The software is fast and can typically find a solution in less than 10 s and perform a complete search in less than 1 min. Moreover, the development of the code was guided by a range of software engineering principles such as automatic testing, system design and continuous integration leading to robust software with high maintainability. Finally, the software is well documented to make it suitable for beginners. The software is available at http://www.github.com/MolecularAI/aizynthfinder."
Faddegon2020,Bruce Faddegon and José Ramos-Méndez and Jan Schuemann and Aimee McNamara and Jungwook Shin and Joseph Perl and Harald Paganetti,"The TOPAS tool for particle simulation, a Monte Carlo simulation tool for physics, biology and clinical research",Physica Medica,72,,2020,10.1016/j.ejmp.2020.03.019,1724191X,"Purpose: This paper covers recent developments and applications of the TOPAS TOol for PArticle Simulation and presents the approaches used to disseminate TOPAS. Materials and methods: Fundamental understanding of radiotherapy and imaging is greatly facilitated through accurate and detailed simulation of the passage of ionizing radiation through apparatus and into a patient using Monte Carlo (MC). TOPAS brings Geant4, a reliable, experimentally validated MC tool mainly developed for high energy physics, within easy reach of medical physicists, radiobiologists and clinicians. Requiring no programming knowledge, TOPAS provides all of the flexibility of Geant4. Results: After 5 years of development followed by its initial release, TOPAS was subsequently expanded from its focus on proton therapy physics to incorporate radiobiology modeling. Next, in 2018, the developers expanded their user support and code maintenance as well as the scope of TOPAS towards supporting X-ray and electron therapy and medical imaging. Improvements have been achieved in user enhancement through software engineering and a graphical user interface, calculational efficiency, validation through experimental benchmarks and QA measurements, and either newly available or recently published applications. A large and rapidly increasing user base demonstrates success in our approach to dissemination of this uniquely accessible and flexible MC research tool. Conclusions: The TOPAS developers continue to make strides in addressing the needs of the medical community in applications of ionizing radiation to medicine, creating the only fully integrated platform for four-dimensional simulation of all forms of radiotherapy and imaging with ionizing radiation, with a design that promotes inter-institutional collaboration."
Ali2022,Raza Ali and Joon Huang Chuah and Mohamad Sofian Abu Talip and Norrima Mokhtar and Muhammad Ali Shoaib,Structural crack detection using deep convolutional neural networks,Automation in Construction,133,,2022,10.1016/j.autcon.2021.103989,09265805,"Convolutional Neural Networks (CNN) have immense potential to solve a broad range of computer vision problems. It has achieved encouraging results in numerous applications of engineering, medical, and other research fields due to the advancement in hardware, data collection procedures, and efficient algorithms. These innovations have changed the way how specific problems are solved as compared to conventional methods. This article presents a review of CNN implementation on civil structure crack detection. The review highlights the significant research that has been performed to detect structure cracks through classification and segmentation of crack images with CNN in the perspective of image pre-processing techniques, processing hardware, software tools, datasets, network architectures, learning procedures, loss functions, and network performance. The key contribution of this review article is the study and analysis of the most recent developments on crack detection using CNN. Additionally, this work also presents a discussion on crack detection through a manual process, image processing techniques, and machine learning methods along with their limitations. Finally, this article aims for assisting the readers to understand the motivation and methodology of the various CNN-based crack detection methods and to invoke them for exploring the solutions of challenges outlined in future research."
Krassowski2020,Michal Krassowski and Vivek Das and Sangram K. Sahu and Biswapriya B. Misra,State of the Field in Multi-Omics Research: From Computational Needs to Data Mining and Sharing,Frontiers in Genetics,11,,2020,10.3389/fgene.2020.610798,16648021,"Multi-omics, variously called integrated omics, pan-omics, and trans-omics, aims to combine two or more omics data sets to aid in data analysis, visualization and interpretation to determine the mechanism of a biological process. Multi-omics efforts have taken center stage in biomedical research leading to the development of new insights into biological events and processes. However, the mushrooming of a myriad of tools, datasets, and approaches tends to inundate the literature and overwhelm researchers new to the field. The aims of this review are to provide an overview of the current state of the field, inform on available reliable resources, discuss the application of statistics and machine/deep learning in multi-omics analyses, discuss findable, accessible, interoperable, reusable (FAIR) research, and point to best practices in benchmarking. Thus, we provide guidance to interested users of the domain by addressing challenges of the underlying biology, giving an overview of the available toolset, addressing common pitfalls, and acknowledging current methods’ limitations. We conclude with practical advice and recommendations on software engineering and reproducibility practices to share a comprehensive awareness with new researchers in multi-omics for end-to-end workflow."
KanYeung2021,Andy Wai Kan Yeung and Anela Tosevska and Elisabeth Klager and Fabian Eibensteiner and Daniel Laxar and Jivko Stoyanov and Marija Glisic and Sebastian Zeiner and Stefan Tino Kulnik and Rik Crutzen and Oliver Kimberger and Maria Kletecka-Pulker and Atanas G. Atanasov and Harald Willschke,Virtual and augmented reality applications in medicine: Analysis of the scientific literature,Journal of Medical Internet Research,23,2,2021,10.2196/25499,14388871,"Background: Virtual reality (VR) and augmented reality (AR) have recently become popular research themes. However, there are no published bibliometric reports that have analyzed the corresponding scientific literature in relation to the application of these technologies in medicine. Objective: We used a bibliometric approach to identify and analyze the scientific literature on VR and AR research in medicine, revealing the popular research topics, key authors, scientific institutions, countries, and journals. We further aimed to capture and describe the themes and medical conditions most commonly investigated by VR and AR research. Methods: The Web of Science electronic database was searched to identify relevant papers on VR research in medicine. Basic publication and citation data were acquired using the “Analyze” and “Create Citation Report” functions of the database. Complete bibliographic data were exported to VOSviewer and Bibliometrix, dedicated bibliometric software packages, for further analyses. Visualization maps were generated to illustrate the recurring keywords and words mentioned in the titles and abstracts. Results: The analysis was based on data from 8399 papers. Major research themes were diagnostic and surgical procedures, as well as rehabilitation. Commonly studied medical conditions were pain, stroke, anxiety, depression, fear, cancer, and neurodegenerative disorders. Overall, contributions to the literature were globally distributed with heaviest contributions from the United States and United Kingdom. Studies from more clinically related research areas such as surgery, psychology, neurosciences, and rehabilitation had higher average numbers of citations than studies from computer sciences and engineering. Conclusions: The conducted bibliometric analysis unequivocally reveals the versatile emerging applications of VR and AR in medicine. With the further maturation of the technology and improved accessibility in countries where VR and AR research is strong, we expect it to have a marked impact on clinical practice and in the life of patients."
Manickam2022,Pandiaraj Manickam and Siva Ananth Mariappan and Sindhu Monica Murugesan and Shekhar Hansda and Ajeet Kaushik and Ravikumar Shinde and S. P. Thipperudraswamy,Artificial Intelligence (AI) and Internet of Medical Things (IoMT) Assisted Biomedical Systems for Intelligent Healthcare,Biosensors,12,8,2022,10.3390/bios12080562,20796374,"Artificial intelligence (AI) is a modern approach based on computer science that develops programs and algorithms to make devices intelligent and efficient for performing tasks that usually require skilled human intelligence. AI involves various subsets, including machine learning (ML), deep learning (DL), conventional neural networks, fuzzy logic, and speech recognition, with unique capabilities and functionalities that can improve the performances of modern medical sciences. Such intelligent systems simplify human intervention in clinical diagnosis, medical imaging, and decision-making ability. In the same era, the Internet of Medical Things (IoMT) emerges as a next-generation bio-analytical tool that combines network-linked biomedical devices with a software application for advancing human health. In this review, we discuss the importance of AI in improving the capabilities of IoMT and point-of-care (POC) devices used in advanced healthcare sectors such as cardiac measurement, cancer diagnosis, and diabetes management. The role of AI in supporting advanced robotic surgeries developed for advanced biomedical applications is also discussed in this article. The position and importance of AI in improving the functionality, detection accuracy, decision-making ability of IoMT devices, and evaluation of associated risks assessment is discussed carefully and critically in this review. This review also encompasses the technological and engineering challenges and prospects for AI-based cloud-integrated personalized IoMT devices for designing efficient POC biomedical systems suitable for next-generation intelligent healthcare."
Sherkatghanad2020,Zeinab Sherkatghanad and Mohammadsadegh Akhondzadeh and Soorena Salari and Mariam Zomorodi-Moghadam and Moloud Abdar and U. Rajendra Acharya and Reza Khosrowabadi and Vahid Salari,Automated Detection of Autism Spectrum Disorder Using a Convolutional Neural Network,Frontiers in Neuroscience,13,,2020,10.3389/fnins.2019.01325,1662453X,"Background: Convolutional neural networks (CNN) have enabled significant progress in speech recognition, image classification, automotive software engineering, and neuroscience. This impressive progress is largely due to a combination of algorithmic breakthroughs, computation resource improvements, and access to a large amount of data. Method: In this paper, we focus on the automated detection of autism spectrum disorder (ASD) using CNN with a brain imaging dataset. We detected ASD patients using most common resting-state functional magnetic resonance imaging (fMRI) data from a multi-site dataset named the Autism Brain Imaging Exchange (ABIDE). The proposed approach was able to classify ASD and control subjects based on the patterns of functional connectivity. Results: Our experimental outcomes indicate that the proposed model is able to detect ASD correctly with an accuracy of 70.22% using the ABIDE I dataset and the CC400 functional parcellation atlas of the brain. Also, the CNN model developed used fewer parameters than the state-of-art techniques and is hence computationally less intensive. Our developed model is ready to be tested with more data and can be used to prescreen ASD patients."
Lutellier2020,Thibaud Lutellier and Hung Viet Pham and Lawrence Pang and Yitong Li and Moshi Wei and Lin Tan,CoCoNuT: Combining context-aware neural translation models using ensemble for program repair,,,,2020,10.1145/3395363.3397369,,"Automated generate-and-validate (GV) program repair techniques (APR) typically rely on hard-coded rules, thus only fixing bugs following specific fix patterns. These rules require a significant amount of manual effort to discover and it is hard to adapt these rules to different programming languages. To address these challenges, we propose a new G&V technique - CoCoNuT, which uses ensemble learning on the combination of convolutional neural networks (CNNs) and a new context-aware neural machine translation (NMT) architecture to automatically fix bugs in multiple programming languages. To better represent the context of a bug, we introduce a new context-aware NMT architecture that represents the buggy source code and its surrounding context separately. CoCoNuT uses CNNs instead of recurrent neural networks (RNNs), since CNN layers can be stacked to extract hierarchical features and better model source code at different granularity levels (e.g., statements and functions). In addition, CoCoNuT takes advantage of the randomness in hyperparameter tuning to build multiple models that fix different bugs and combines these models using ensemble learning to fix more bugs. Our evaluation on six popular benchmarks for four programming languages (Java, C, Python, and JavaScript) shows that CoCoNuT correctly fixes (i.e., the first generated patch is semantically equivalent to the developer's patch) 509 bugs, including 309 bugs that are fixed by none of the 27 techniques with which we compare."
Al-Ketan2021,Oraib Al-Ketan and Rashid K. Abu Al-Rub,MSLattice: A free software for generating uniform and graded lattices based on triply periodic minimal surfaces,Material Design and Processing Communications,3,6,2021,10.1002/mdp2.205,25776576,"Nature-inspired materials based on triply periodic minimal surfaces (TPMS) are very attractive in many engineering disciplines because of their topology-driven properties. However, their adoption across different research and engineering fields is limited by the complexity of their design process. In this work, we present MSLattice, a software that allows users to design uniform, and functionally grade lattices and surfaces based on TPMS using two approaches, namely, the sheet networks and solid networks. The software allows users to control the type of TPMS topology, relative density, cell size, relative density grading, cell size grading, and hybridization between lattices. These features make MSLattice a complete design platform for users in different engineering disciplines, especially in applications that employ additive manufacturing (3D printing) and computational modeling. We demonstrate the capability of the software using several examples."
Altman2021,Ehud Altman and Kenneth R. Brown and Giuseppe Carleo and Lincoln D. Carr and Eugene Demler and Cheng Chin and Brian Demarco and Sophia E. Economou and Mark A. Eriksson and Kai Mei C. Fu and Markus Greiner and Kaden R.A. Hazzard and Randall G. Hulet and Alicia J. Kollár and Benjamin L. Lev and Mikhail D. Lukin and Ruichao Ma and Xiao Mi and Shashank Misra and Christopher Monroe and Kater Murch and Zaira Nazario and Kang Kuen Ni and Andrew C. Potter and Pedram Roushan and Mark Saffman and Monika Schleier-Smith and Irfan Siddiqi and Raymond Simmonds and Meenakshi Singh and I. B. Spielman and Kristan Temme and David S. Weiss and Jelena Vučković and Vladan Vuletić and Jun Ye and Martin Zwierlein,Quantum Simulators: Architectures and Opportunities,PRX Quantum,2,1,2021,10.1103/PRXQuantum.2.017003,26913399,"Quantum simulators are a promising technology on the spectrum of quantum devices from specialized quantum experiments to universal quantum computers. These quantum devices utilize entanglement and many-particle behavior to explore and solve hard scientific, engineering, and computational problems. Rapid development over the last two decades has produced more than 300 quantum simulators in operation worldwide using a wide variety of experimental platforms. Recent advances in several physical architectures promise a golden age of quantum simulators ranging from highly optimized special purpose simulators to flexible programmable devices. These developments have enabled a convergence of ideas drawn from fundamental physics, computer science, and device engineering. They have strong potential to address problems of societal importance, ranging from understanding vital chemical processes, to enabling the design of new materials with enhanced performance, to solving complex computational problems. It is the position of the community, as represented by participants of the National Science Foundation workshop on ""Programmable Quantum Simulators,""that investment in a national quantum simulator program is a high priority in order to accelerate the progress in this field and to result in the first practical applications of quantum machines. Such a program should address two areas of emphasis: (1) support for creating quantum simulator prototypes usable by the broader scientific community, complementary to the present universal quantum computer effort in industry; and (2) support for fundamental research carried out by a blend of multi-investigator, multidisciplinary collaborations with resources for quantum simulator software, hardware, and education.This document is a summary from a U.S. National Science Foundation supported workshop held on 16-17 September 2019 in Alexandria, VA. Attendees were charged to identify the scientific and community needs, opportunities, and significant challenges for quantum simulators over the next 2-5 years."
KosakovskyPond2020,Sergei L. Kosakovsky Pond and Art F.Y. Poon and Ryan Velazquez and Steven Weaver and N. Lance Hepler and Ben Murrell and Stephen D. Shank and Brittany Rife Magalis and Dave Bouvier and Anton Nekrutenko and Sadie Wisotsky and Stephanie J. Spielman and Simon D.W. Frost and Spencer V. Muse,HyPhy 2.5 - A Customizable Platform for Evolutionary Hypothesis Testing Using Phylogenies,Molecular Biology and Evolution,37,1,2020,10.1093/molbev/msz197,15371719,"HYpothesis testing using PHYlogenies (HyPhy) is a scriptable, open-source package for fitting a broad range of evolutionary models to multiple sequence alignments, and for conducting subsequent parameter estimation and hypothesis testing, primarily in the maximum likelihood statistical framework. It has become a popular choice for characterizing various aspects of the evolutionary process: natural selection, evolutionary rates, recombination, and coevolution. The 2.5 release (available from www.hyphy.org) includes a completely re-engineered computational core and analysis library that introduces new classes of evolutionary models and statistical tests, delivers substantial performance and stability enhancements, improves usability, streamlines end-to-end analysis workflows, makes it easier to develop custom analyses, and is mostly backward compatible with previous HyPhy releases."
Xie2020,Hualin Xie and Yanwei Zhang and Zhilong Wu and Tiangui Lv,"A bibliometric analysis on land degradation: Current status, development, and future directions",Land,9,1,2020,10.3390/LAND9010028,2073445X,"Land degradation is a global issue receiving much attention currently. In order to objectively reveal the research situation of land degradation, bibliometrix and biblioshiny software packages have been used to conduct data mining and quantitative analysis on research papers in the fields of land degradation during 1990-2019 (data update time was 8 April 2019) in theWeb of Science core collection database. The results show that: (1) during the past 20 years, the number of papers on land degradation has increased. According to the number of articles, it is divided into four stages: a low-production exploration period, a developmental sprout period, expansion of the promotion period, and a high-yield active period. (2) Land-degradation research covers 93 countries or regions. The top five countries in terms of research volume are China, the United States, the United Kingdom, Germany, and Australia. China, the United States, and the United Kingdom are the most important countries for international cooperation in the field of land degradation. However, cooperation between countries is not very close overall. (3) Land degradation, degradation, desertification, remote sensing, soil erosion, and soil degradation are high-frequency keywords in the field of land degradation in recent years. (4) The research hotspots in the field of land degradation mainly focus on research directions such as restoration and reconstruction of land degradation, and sustainable management of land resources. (5) The themes of various periods in the field of land degradation are diversified, and the evolutionary relationship is complex. There are 15 evolutionary paths with regard to dynamic monitoring of land degradation, environmental governance of land degradation, and responses of land degradation to land-use change. Finally, the paper concludes that the research directions on land degradation in future include the process, mechanism, and effect of land degradation, the application of new technologies, new monitoring methods for land degradation, theory enhancement, methods and models of ecological restoration, reconstruction of degraded land, multidisciplinary integrated system research, constructing a policy guarantee system for the reconstruction of degraded land, and strengthening research on land resource engineering."
Ivanov2021,Dmitry Ivanov and Christopher S. Tang and Alexandre Dolgui and Daria Battini and Ajay Das,Researchers' perspectives on Industry 4.0: multi-disciplinary analysis and opportunities for operations management,International Journal of Production Research,59,7,2021,10.1080/00207543.2020.1798035,1366588X,"While Industry 4.0 has been trending in practice and research, operations management studies in this area remain nascent. Our intent is to understand the current state of research in Industry 4.0 in different disciplines and deduce insights and opportunities for future research in operations management. In this paper, we provide a focused analysis to examine the state-of-the-art research in Industry 4.0. To learn about researchers’ perspectives about Industry 4.0, we conducted a large-scale, cross-disciplinary and global survey on Industry 4.0 topics among researchers in industrial engineering, operations management, operations research, control and data science at the 9th IFAC MIM 2019 Conference in Berlin in August 2019. By using our survey findings and literature analysis, we build structural and conceptual frameworks to understand the current state of knowledge and to propose future research opportunities for operations management scholars. Glossary of Abbreviations AGV: Automated guided vehicle; AI: Artificial intelligence; APS: Advanced planning system: a wide variety of software tools and techniques, with many applications in manufacturing and logistics (including the service sector); BDA: Big data analytics; CAS: Complex adaptive system: a system composed of many interacting parts that evolve and adapt over time; CIM: Computer integrated manufacturing; CPFR: Collaborative planning, forecasting and replenishment; CPS: Cyber-physical system: a seamless integration of computation and physical components; DAMCLS: Decision analysis, modelling, control and learning systems; ERP: Enterprise resource planning; FMS: Flexible manufacturing system; I4.0: Industry 4.0; IFAC: International Federation of Automatic Control: a federation is concerned with the impact of control technology on society; IME: Industrial and mechanical engineering; IoT: Internet-of-Things; IT: Information technology; M2M: Machine-to-machine; MAS: Multi-agent system: a loosely coupled network of software agents that interact to solve problems that are beyond the individual capacities or knowledge of each problem solver; OR: Operations research; RFID: Radio frequency identification: a technology that uses electromagnetic fields to automatically identify and track tags attached to objects; RMS: Reconfigurable manufacturing system: a manufacturing system that can change and evolve rapidly in order to adjust its productivity capacity and functionality; OM: Operations management; T&T: Track and trace system; VCA: VOS viewer co-occurrence analysis: a software tool for visualising bibliometric networks; VMI: Vendor-managed inventory."
Drgoa2020,Ján Drgoňa and Javier Arroyo and Iago Cupeiro Figueroa and David Blum and Krzysztof Arendt and Donghun Kim and Enric Perarnau Ollé and Juraj Oravec and Michael Wetter and Draguna L. Vrabie and Lieve Helsen,All you need to know about model predictive control for buildings,Annual Reviews in Control,50,,2020,10.1016/j.arcontrol.2020.09.001,13675788,"It has been proven that advanced building control, like model predictive control (MPC), can notably reduce the energy use and mitigate greenhouse gas emissions. However, despite intensive research efforts, the practical applications are still in the early stages. There is a growing need for multidisciplinary education on advanced control methods in the built environment to be accessible for a broad range of researchers and practitioners with different engineering backgrounds. This paper provides a unified framework for model predictive building control technology with focus on the real-world applications. From a theoretical point of view, this paper presents an overview of MPC formulations for building control, modeling paradigms and model types, together with algorithms necessary for real-life implementation. The paper categorizes the most notable MPC problem classes, links them with corresponding solution techniques, and provides an overview of methods for mitigation of the uncertainties for increased performance and robustness of MPC. From a practical point of view, this paper delivers an elaborate classification of the most important modeling, co-simulation, optimal control design, and optimization techniques, tools, and solvers suitable to tackle the MPC problems in the context of building climate control. On top of this, the paper presents the essential components of a practical implementation of MPC such as different control architectures and nuances of communication infrastructures within supervisory control and data acquisition (SCADA) systems. The paper draws practical guidelines with a generic workflow for implementation of MPC in real buildings aimed for contemporary adopters of this technology. Finally, the importance of standardized performance assessment and methodology for comparison of different building control algorithms is discussed."
Sheykhmousa2020,Mohammadreza Sheykhmousa and Masoud Mahdianpari and Hamid Ghanbari and Fariba Mohammadimanesh and Pedram Ghamisi and Saeid Homayouni,Support Vector Machine Versus Random Forest for Remote Sensing Image Classification: A Meta-Analysis and Systematic Review,IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,13,,2020,10.1109/JSTARS.2020.3026724,21511535,"Several machine-learning algorithms have been proposed for remote sensing image classification during the past two decades. Among these machine learning algorithms, Random Forest (RF) and Support Vector Machines (SVM) have drawn attention to image classification in several remote sensing applications. This article reviews RF and SVM concepts relevant to remote sensing image classification and applies a meta-analysis of 251 peer-reviewed journal papers. A database with more than 40 quantitative and qualitative fields was constructed from these reviewed papers. The meta-analysis mainly focuses on 1) the analysis regarding the general characteristics of the studies, such as geographical distribution, frequency of the papers considering time, journals, application domains, and remote sensing software packages used in the case studies, and 2) a comparative analysis regarding the performances of RF and SVM classification against various parameters, such as data type, RS applications, spatial resolution, and the number of extracted features in the feature engineering step. The challenges, recommendations, and potential directions for future research are also discussed in detail. Moreover, a summary of the results is provided to aid researchers to customize their efforts in order to achieve the most accurate results based on their thematic applications."
Lu2021,Lu Lu and Xuhui Meng and Zhiping Mao and George Em Karniadakis,DeepXDE: A deep learning library for solving differential equations,SIAM Review,63,1,2021,10.1137/19M1274067,00361445,"Deep learning has achieved remarkable success in diverse applications; however, its use in solving partial differential equations (PDEs) has emerged only recently. Here, we present an overview of physics-informed neural networks (PINNs), which embed a PDE into the loss of the neural network using automatic differentiation. The PINN algorithm is simple, and it can be applied to different types of PDEs, including integro-differential equations, fractional PDEs, and stochastic PDEs. Moreover, from an implementation point of view, PINNs solve inverse problems as easily as forward problems. We propose a new residualbased adaptive refinement (RAR) method to improve the training efficiency of PINNs. For pedagogical reasons, we compare the PINN algorithm to a standard finite element method. We also present a Python library for PINNs, DeepXDE, which is designed to serve both as an educational tool to be used in the classroom as well as a research tool for solving problems in computational science and engineering. Specifically, DeepXDE can solve forward problems given initial and boundary conditions, as well as inverse problems given some extra measurements. DeepXDE supports complex-geometry domains based on the technique of constructive solid geometry and enables the user code to be compact, resembling closely the mathematical formulation. We introduce the usage of DeepXDE and its customizability, and we also demonstrate the capability of PINNs and the userfriendliness of DeepXDE for five different examples. More broadly, DeepXDE contributes to the more rapid development of the emerging scientific machine learning field."
Pizzi2020,Giovanni Pizzi and Valerio Vitale and Ryotaro Arita and Stefan Blügel and Frank Freimuth and Guillaume Géranton and Marco Gibertini and Dominik Gresch and Charles Johnson and Takashi Koretsune and Julen Iba&ntilde;ez-Azpiroz and Hyungjun Lee and Jae Mo Lihm and Daniel Marchand and Antimo Marrazzo and Yuriy Mokrousov and Jamal I. Mustafa and Yoshiro Nohara and Yusuke Nomura and Lorenzo Paulatto and Samuel Poncé and Thomas Ponweiser and Junfeng Qiao and Florian Thöle and Stepan S. Tsirkin and Małgorzata Wierzbowska and Nicola Marzari and David Vanderbilt and Ivo Souza and Arash A. Mostofi and Jonathan R. Yates,Wannier90 as a community code: New features and applications,Journal of Physics Condensed Matter,32,16,2020,10.1088/1361-648X/ab51ff,1361648X,"Wannier90 is an open-source computer program for calculating maximally-localised Wannier functions (MLWFs) from a set of Bloch states. It is interfaced to many widely used electronic-structure codes thanks to its independence from the basis sets representing these Bloch states. In the past few years the development of Wannier90 has transitioned to a community-driven model; this has resulted in a number of new developments that have been recently released in Wannier90 v3.0. In this article we describe these new functionalities, that include the implementation of new features for wannierisation and disentanglement (symmetry-adapted Wannier functions, selectively-localised Wannier functions, selected columns of the density matrix) and the ability to calculate new properties (shift currents and Berry-curvature dipole, and a new interface to many-body perturbation theory); performance improvements, including parallelisation of the core code; enhancements in functionality (support for spinor-valued Wannier functions, more accurate methods to interpolate quantities in the Brillouin zone); improved usability (improved plotting routines, integration with high-throughput automation frameworks), as well as the implementation of modern software engineering practices (unit testing, continuous integration, and automatic source-code documentation). These new features, capabilities, and code development model aim to further sustain and expand the community uptake and range of applicability, that nowadays spans complex and accurate dielectric, electronic, magnetic, optical, topological and transport properties of materials."
Kiranyaz2021,Serkan Kiranyaz and Onur Avci and Osama Abdeljaber and Turker Ince and Moncef Gabbouj and Daniel J. Inman,1D convolutional neural networks and applications: A survey,Mechanical Systems and Signal Processing,151,,2021,10.1016/j.ymssp.2020.107398,10961216,"During the last decade, Convolutional Neural Networks (CNNs) have become the de facto standard for various Computer Vision and Machine Learning operations. CNNs are feed-forward Artificial Neural Networks (ANNs) with alternating convolutional and subsampling layers. Deep 2D CNNs with many hidden layers and millions of parameters have the ability to learn complex objects and patterns providing that they can be trained on a massive size visual database with ground-truth labels. With a proper training, this unique ability makes them the primary tool for various engineering applications for 2D signals such as images and video frames. Yet, this may not be a viable option in numerous applications over 1D signals especially when the training data is scarce or application specific. To address this issue, 1D CNNs have recently been proposed and immediately achieved the state-of-the-art performance levels in several applications such as personalized biomedical data classification and early diagnosis, structural health monitoring, anomaly detection and identification in power electronics and electrical motor fault detection. Another major advantage is that a real-time and low-cost hardware implementation is feasible due to the simple and compact configuration of 1D CNNs that perform only 1D convolutions (scalar multiplications and additions). This paper presents a comprehensive review of the general architecture and principals of 1D CNNs along with their major engineering applications, especially focused on the recent progress in this field. Their state-of-the-art performance is highlighted concluding with their unique properties. The benchmark datasets and the principal 1D CNN software used in those applications are also publicly shared in a dedicated website. While there has not been a paper on the review of 1D CNNs and its applications in the literature, this paper fulfills this gap."
MacRae2020,Clare F. MacRae and Ioana Sovago and Simon J. Cottrell and Peter T.A. Galek and Patrick McCabe and Elna Pidcock and Michael Platings and Greg P. Shields and Joanna S. Stevens and Matthew Towler and Peter A. Wood,"Mercury 4.0: From visualization to analysis, design and prediction",Journal of Applied Crystallography,53,,2020,10.1107/S1600576719014092,16005767,"The program Mercury, developed at the Cambridge Crystallographic Data Centre, was originally designed primarily as a crystal structure visualization tool. Over the years the fields and scientific communities of chemical crystallography and crystal engineering have developed to require more advanced structural analysis software. Mercury has evolved alongside these scientific communities and is now a powerful analysis, design and prediction platform which goes a lot further than simple structure visualization."
Harris2020,Charles R. Harris and K. Jarrod Millman and Stéfan J. van der Walt and Ralf Gommers and Pauli Virtanen and David Cournapeau and Eric Wieser and Julian Taylor and Sebastian Berg and Nathaniel J. Smith and Robert Kern and Matti Picus and Stephan Hoyer and Marten H. van Kerkwijk and Matthew Brett and Allan Haldane and Jaime Fernández del Río and Mark Wiebe and Pearu Peterson and Pierre Gérard-Marchant and Kevin Sheppard and Tyler Reddy and Warren Weckesser and Hameer Abbasi and Christoph Gohlke and Travis E. Oliphant,Array programming with NumPy,Nature,585,7825,2020,10.1038/s41586-020-2649-2,14764687,"Array programming provides a powerful, compact and expressive syntax for accessing, manipulating and operating on data in vectors, matrices and higher-dimensional arrays. NumPy is the primary array programming library for the Python language. It has an essential role in research analysis pipelines in fields as diverse as physics, chemistry, astronomy, geoscience, biology, psychology, materials science, engineering, finance and economics. For example, in astronomy, NumPy was an important part of the software stack used in the discovery of gravitational waves1 and in the first imaging of a black hole2. Here we review how a few fundamental array concepts lead to a simple and powerful programming paradigm for organizing, exploring and analysing scientific data. NumPy is the foundation upon which the scientific Python ecosystem is constructed. It is so pervasive that several projects, targeting audiences with specialized needs, have developed their own NumPy-like interfaces and array objects. Owing to its central position in the ecosystem, NumPy increasingly acts as an interoperability layer between such array computation libraries and, together with its application programming interface (API), provides a flexible framework to support the next decade of scientific and industrial analysis."
DeVincentiis2022,Mirko De Vincentiis and Fabio Cassano and Alessandro Pagano and Antonio Piccinno,QAI4ASE: Quantum artificial intelligence for automotive software engineering,,,,2022,10.1145/3549036.3562059,,"Nowadays, the size and complexity of the automotive development life-cycle increase the possibility of cyber-attacks. In this context, team developers play a primary role in managing cyber security, risk assessment, and all phases of software application development (concept phases, product development, cyber security validation, production, operations, and maintenance). Currently, only generic standards exist and they are difficult to put into operation due to the lack of the required skills and knowledge. Therefore, this paper presents a vision model based on Quantum Artificial Intelligence that supports developers' decisions to integrate concrete design methods in the automotive development life-cycle. Organizations need to develop their process for developing vehicle components that comply with the new automotive standards. We suggest the usage of existing data sources (e.g., existing taxonomies) on Quantum Artificial Intelligence algorithms to suggest the best way, or the correct steps, to follow time by time to achieve user solutions."
Kalinga2021,Ellen A. Kalinga,Learning software development through modeling using object oriented approach with unified modeling language: A case of an online interview system,Journal of Learning for Development,8,1,2021,10.56059/jl4d.v8i1.401,23111550,"This paper demonstrates the learning of software engineering through modeling using Object-Oriented Analysis and Design approach with Unified Modeling Language. An online interview management system case project to the whole class was used to develop the software requirement specification. Through modelling, the processes to be considered in software development were also elaborated, where it starts with the identification of major or basic processes of the domain of application, followed by the identification of activities to be performed under each basic process and, finally, transforming the activities highlighted in the functional requirements presentation. Modeling was practised by students through group case projects, and students were active, engaging and focusing on the learning process in such a way that more than 85.9% of students had the courage to attempt design questions during university examinations."
Lin2023,Jin Lin,Optimization design of electronic commerce system based on the basic principles of software engineering,Advances in Economics and Management Research,6,1,2023,10.56028/aemr.6.1.403.2023,,"Based on the basic principles of software engineering, this paper optimizes the design of e-commerce system from the aspects of demand analysis, system design, coding, implementation, testing and maintenance. Specifically, this paper adopts the object-oriented software development method, using the UML modeling tools to model the system, adopted the MVC architecture mode to realize the stratification of the system, using the front-end technology such as HTML, CSS, JavaScript to realize the user interface design of the system, combined with the back-end technology such as Java, JSP, the Servlet system. Through the system test and the use of code quality control tools, the system is accepted, and the maintenance of the system is planned and designed. Finally, this paper realizes an easy-to-use, stable, safe and efficient e-commerce system."
Galster2022,Matthias Galster and Antonija Mitrovic and Sanna Malinen and Jay Holland,What Soft Skills Does the Software Industry ∗Really∗ Want? An Exploratory Study of Software Positions in New Zealand,,,,2022,10.1145/3544902.3546247,19493789,"Background: Soft skills of software professionals (e.g., communication, interpersonal skills) significantly contribute to project and product success. Aims: We aim to understand (a) what are relevant soft skills in software engineering, (b) how soft skills relate to types of software engineering positions, and (c) how soft skills relate to characteristics of hiring organizations. We focus on organizations in New Zealand, a country with a relatively small but growing software sector characterized by a skills shortage and embedded in a bi-cultural context. Method: We used a qualitative research method and manually analyzed 530 job adverts from New Zealand's largest job portal for technology-related positions. We identified soft skills following an inductive approach, i.e., without a pre-defined set of soft skills. Results: We found explicit references to soft skills in 82% of adverts. We identified 17 soft skills and proposed a contextualized software engineering description. Communication-related soft skills are most in demand, regardless of the type of position. Soft skills related to broader human or societal values (e.g., empathy or cultural awareness) or distributed development are not frequently requested. Soft skills do not depend on company size or core business. Conclusions: Employers explicitly ask for soft skills. Our findings support previous studies that highlight the importance of communication. Characteristics specific to New Zealand do not impact the demand for soft skills. Our findings benefit researchers in human aspects of software engineering and to those responsible for staff, curricula and professional development."
Ramadhayanty2021,Chicy Ramadhayanty and Rafika Dewi Nasution,DEVELOPING ENGLISH READING MATERIALS FOR THE ELEVENTH GRADE STUDENTS OF SOFTWARE ENGINEERING PROGRAM AT SMKN 9 MEDAN,REGISTER: Journal of English Language Teaching of FBS-Unimed,10,3,2021,10.24114/reg.v10i3.29968,2301-5233,"The objective of this study was to develop English reading materials based on the students of Software Engineering needs for the eleventh grade students at SMKN 9 Medan. This study was conducted by using Research and Development design through six stages; gathering data and information, need analysis, designing materials, validating, revising (final product). The subject of the study was grade eleventh of SMKN 9 Medan. The data were gathered by conducting interview to English teacher and distributing questionnaire to XI RPL 3 consists of 24 students to get the students’ need. To collect the data of the research, conducting interview to English teacher and two types of questionnaires were used as the instruments. The developed reading materials were validated by two experts which are the average scores for the analytical exposition material (4.65 and 4.7) and for the explanation text (4.65 and 4.6) show that the reading materials are valid and suitable to use for the eleventh grade students of software engineering program. Based on the results of the expert judgment, the developed materials were considered appropriate, which was very good.Key words: Development, Reading Materials, Software Engineering."
Kamthan2023,Pankaj Kamthan and Nazlie Shahmir,Small Educational Steps Towards Improving the Status of Women in Software Engineering,,2023-July,,2023,10.18293/SEKE2023-002,23259086,"The women in software engineering continue to face a culture of discord that manifests itself in the form of underrepresentation, unpleasantness, and/or inequitableness. This somewhat dire situation was only exacerbated during the COVID-19 pandemic when the women in software engineering education and profession had to deal with multiple ‘crisis’. The status quo is clearly unacceptable, not least because of pervasiveness of software in society. In that regard, relying on a multipronged approach grounded in a body of knowledge, ethicality, and history, this paper proposes certain basic steps in software engineering courses and projects that could be put into practice for improving “gender literacy” among students. These educational steps are illustrated by anecdotes and examples."
Almazroi2021,Abdulwahab Ali Almazroi,A Systematic Mapping Study of Software Usability Studies,International Journal of Advanced Computer Science and Applications,12,9,2021,10.14569/IJACSA.2021.0120927,21565570,"Among software quality attributes “software usability” is considered as one of the vital factors in software engineering literature. Software usability is the ability for users to generally understand, use, and learn a software with ease. Due to the importance of usability in software quality, a considerable amount of literature is published in the past decade. Few review and survey studies are also published to critically review the existing literature in the domain. However, there is limited research covering systematic mapping study of software usability. Mapping studies help in analyzing the general trends and research productivity in a research area. To fill this gap, this work critically examines the overall research productivity, demographics, trends, and challenges of software usability. The objective is to classify the current contributions and trends in the area of software usability. We retrieved 9,874 research articles from six research databases and 62 works are selected as primary studies using an evidence-based approach. The result of this mapping study shows that software usability is an active research area, with a promising number of works published in the last decade (2011 - 2020). We identified that the current literature spans over multiple article classes of which investigative papers, model proposals and evaluation papers are the most frequently published article types. We found experiments and theoretical validations to be the most common validation techniques. In terms of application domains; web, software development and mobile applications are the most frequent domains where usability studies are conducted. We identified that future usability studies should focus more on field studies as well as on the usability testing of scientific software packages. It will be of importance to consider ethical issues in usability testing as well."
Farina2022,Mirko Farina and Arina Fedorovskaya and Egor Polivtsev and Giancarlo Succi,Software Engineering and Filmmaking: A Literature Review,Frontiers in Computer Science,4,,2022,10.3389/fcomp.2022.884533,26249898,"Software development is a complex process that requires skills in mathematics and physics. Moreover, it usually includes collaboration with other people. To get a precise understanding of the way such a process is organized, we need to understand its essence. Technical knowledge is crucially important for any developer; however, another important characteristic of any software engineer is creativity. In this article, we look at one particular artistic practice [filmmaking] that involves both these latter characteristics to determine whether insights from such a practice can be applied in the IT industry and vice versa."
Drave2022,Imke Drave and Judith Michael and Erik Müller and Bernhard Rumpe and Simon Varga,Model-Driven Engineering of Process-Aware Information Systems,SN Computer Science,3,6,2022,10.1007/s42979-022-01334-3,26618907,"Enterprise information systems created with model-driven software engineering methods need to handle not only data but also business processes in an automated way. This paper shows how to engineer process-aware information systems following the model-driven and generative software engineering paradigms. Existing approaches realize either the generation of automated or manual activities but do not employ model-driven engineering of all system aspects through systematic language composition. A generative approach that additionally uses process modeling languages allows developers to evolve generated data-centric information systems into process-aware information systems. To be usable within our generation process, we have developed a textual BPMN version and a corresponding language tooling to check the soundness of the models. We have included these process models into the generation process of an information system together with other domain-specific modeling languages, e.g., for data structures, and generate an extendable, process-aware information system that is open for continuous regeneration and hand-written additions. This approach allows us to lift a generated data-centric information system to a process-aware information system. Agile development enabled through the opportunity to validate assumptions automatically and adapt changes efficiently, enhances the engineering process as well as the generated systems themselves."
Erlenhov2020,Linda Erlenhov and Francisco Gomes De Oliveira Neto and Philipp Leitner,An empirical study of bots in software development: Characteristics and challenges from a practitioner's perspective,,,,2020,10.1145/3368089.3409680,,"Software engineering bots - automated tools that handle tedious tasks - are increasingly used by industrial and open source projects to improve developer productivity. Current research in this area is held back by a lack of consensus of what software engineering bots (DevBots) actually are, what characteristics distinguish them from other tools, and what benefits and challenges are associated with DevBot usage. In this paper we report on a mixed-method empirical study of DevBot usage in industrial practice. We report on findings from interviewing 21 and surveying a total of 111 developers. We identify three different personas among DevBot users (focusing on autonomy, chat interfaces, and ""smartness""), each with different definitions of what a DevBot is, why developers use them, and what they struggle with.We conclude that future DevBot research should situate their work within our framework, to clearly identify what type of bot the work targets, and what advantages practitioners can expect. Further, we find that there currently is a lack of general purpose ""smart""bots that go beyond simple automation tools or chat interfaces. This is problematic, as we have seen that such bots, if available, can have a transformative effect on the projects that use them."
Guinea-Cabrera2024,Miguel A. Guinea-Cabrera and Juan A. Holgado-Terriza,Digital Twins in Software Engineering—A Systematic Literature Review and Vision,Applied Sciences (Switzerland),14,3,2024,10.3390/app14030977,20763417,"Featured Application: Secure Integration of the IoT and Digital Twins. Digital twins are a powerful consequence of digital transformation. In fact, they have been applied to many industries to enhance operations, predict needs, improve decision making, or optimize performance, even though the definition of digital twins is still evolving. However, their impact on the software industry is still limited. Thus, this work aims to analyze the current adoption of digital twins in the software industry as a potential path to integrate them into application lifecycle management. To achieve this objective, first, the significant characteristics of current digital twins are analyzed in their application to manufacturing to understand how the knowledge and the lessons learned can be transferred to the software industry. Second, a systematic literature review was conducted on Scopus, the Web of Science, and the ScienceDirect database. The literature review revealed 93 documents after data screening and cleaning 251 initial documents. Our main findings are that digital twins are already influencing and will significantly affect the software industry, revolutionizing various aspects of the software development lifecycle. This study tackles what identifies a digital twin in the software industry, the specific domains and areas where they can be applied in the software lifecycle, and the proposed approaches explored to build digital twins for developing, deploying, and maintaining software systems. Finally, this study proposes some guidelines for building digital twins in the context of application lifecycle management. Determining an appropriate roadmap shortly is essential to achieve a widespread applicability to building suitable digital twins and preparing organizations for the software industry."
tefani2020,Polona Štefanič and Vlado Stankovski,Multi-criteria decision-making approach for container-based cloud applications: The SWITCH and ENTICE workbenches,Tehnicki Vjesnik,27,3,2020,10.17559/TV-20190105200327,18486339,"Many emerging smart applications rely on the Internet of Things (IoT) to provide solutions to time-critical problems. When building such applications, a software engineer must address multiple Non-Functional Requirements (NFRs), including requirements for fast response time, low communication latency, high throughput, high energy efficiency, low operational cost and similar. Existing modern container-based software engineering approaches promise to improve the software lifecycle; however, they fail short of tools and mechanisms for NFRs management and optimisation. Our work addresses this problem with a new decision-making approach based on a Pareto Multi-Criteria optimisation. By using different instance configurations on various geo-locations, we demonstrate the suitability of our method, which narrows the search space to only optimal instances for the deployment of the containerised microservice.This solution is included in two advanced software engineering environments, the SWITCH workbench, which includes an Interactive Development Environment (IDE) and the ENTICE Virtual Machine and container images portal. The developed approach is particularly useful when building, deploying and orchestrating IoT applications across multiple computing tiers, from Edge-Cloudlet to Fog-Cloud data centres."
Kraleva2020,Radoslava Kraleva and Mehrudin Sabani and Velin Kralev and Dafina Kostadinova,An approach to designing and developing an LMS framework appropriate for young pupils,International Journal of Electrical and Computer Engineering,10,2,2020,10.11591/ijece.v10i2.pp1577-1591,20888708,"The new people generation Z can be viewed in terms of software engineering as specific users who have high requirements regarding the functions and interface of the software applications, connectivity to social networks and instant communication via the Internet. In recent years, the number of electronic learning systems increased, but students are still not motivated to learn. This requires developing new conceptual models of training and learning software, tailored to the skills and preferences of the end-users. The young students: from kindergartens to preschools and primary schools are special users who have not been studied exhaustively. To present the problem related to the development of learning and training software thoroughly, the most commonly used standards and current trends, as well as the advantages and disadvantages of LMS platforms have been reviewed. The commonly used software design and development technologies have been discussed. We proposed a strategy for developing a web-based e-learning management system according to the possibilities of young pupils as a specific user. We described a software architecture, based on SCORM's specification, and we developed an LMS prototype. The basic methodology used in the design and creation of the system we propose is user-centered design."
Shanbhag2022,Shriram Shanbhag and Sridhar Chimalakonda,Exploring the under-explored terrain of non-open source data for software engineering through the lens of federated learning,,,,2022,10.1145/3540250.3560883,,"The availability of open source projects on platforms like GitHub has led to the wide use of the artifacts from these projects in software engineering research. These publicly available artifacts have been used to train artificial intelligence models used in various empirical studies and the development of tools. However, these advancements have missed out on the artifacts from non-open source projects due to the unavailability of the data. A major cause for the unavailability of the data from non-open source repositories is the issue concerning data privacy. In this paper, we propose using federated learning to address the issue of data privacy to enable the use of data from non-open source to train AI models used in software engineering research. We believe that this can potentially enable industries to collaborate with software engineering researchers without concerns about privacy. We present the preliminary evaluation of the use of federated learning to train a classifier to label bug-fix commits from an existing study to demonstrate its feasibility. The federated approach achieved an F1 score of 0.83 compared to a score of 0.84 using the centralized approach. We also present our vision of the potential implications of the use of federated learning in software engineering research."
ulha2022,Davut Çulha,Gamification of Open Inquiry-based Learning of Blockchain Technologies,U.Porto Journal of Engineering,8,1,2022,10.24840/2183-6493_008.001_0003,21836493,"Inquiry-based Learning is an efficient learning method. This method is applied in many science courses. It is a research-oriented method which awakens personal curiosity of learners. Learners are motivated intrinsically, and they learn by research. Wide research areas can be investigated using Inquiry-based Learning. It can be applied to all learning areas. In the literature, Inquiry-based Learning is applied to science education. In this paper, Inquiry-based Learning is applied to a software engineering course. The application is novel because the area is a non-science area. Cryptocurrencies, which are applications of blockchain technologies, are selected as the research area. Research is done by students freely. Results of research are combined and shared among students. Students are assessed. Inquiry-based Learning is applied in a least structured way. In other words, open Inquiry-based Learning is applied. To empower it, gamification is used. A methodology for open Inquiry-based Learning is presented. The effect of gamification is measured."
Saleh2021,Ibrahim Ahmed Saleh and Asmaa Hadi Albayati and Kifaa Hadi Thanoon,Measure the Software Quality based on Grasshopper Optimization Algorithm,International Journal of Computing and Digital Systems,10,1,2021,10.12785/IJCDS/100186,2210142X,"Software quality is very essential function from development during the early life of software engineering. Software Quality helps to detect errors and potential errors during initial stage of design and software development process. In this paper, Grasshopper Optimization Algorithm (GOA) is used to improve software quality. Where multiple quality measures were used to calculate the quality standards that were used in testing the approved software. As software testing focuses on software defect. In addition this paper presents GOA to extract the best features of extraction to testing and evaluation of a set of software applications. The paper depended on NASA standards data. The result and experiment show that improved performance quality for all classification methods applied in the research grasshopper optimization algorithm based on feature selection and bagging for Software defect prediction."
Duda2024,Sebastian Duda and Peter Hofmann and Nils Urbach and Fabiane Völter and Amelie Zwickel,The Impact of Resource Allocation on the Machine Learning Lifecycle: Bridging the Gap between Software Engineering and Management,Business and Information Systems Engineering,66,2,2024,10.1007/s12599-023-00842-7,18670202,"An organization’s ability to develop Machine Learning (ML) applications depends on its available resource base. Without awareness and understanding of all relevant resources as well as their impact on the ML lifecycle, we risk inefficient allocations as well as missing monopolization tendencies. To counteract these risks, the study develops a framework that interweaves the relevant resources with the procedural and technical dependencies within the ML lifecycle. To rigorously develop and evaluate this framework the paper follows the Design Science Research paradigm and builds on a literature review and an interview study. In doing so, it bridges the gap between the software engineering and management perspective to advance the ML management discourse. The results extend the literature by introducing not yet discussed but relevant resources, describing six direct and indirect effects of resources on the ML lifecycle, and revealing the resources’ contextual properties. Furthermore, the framework is useful in practice to support organizational decision-making and contextualize monopolization tendencies."
Rajamohan2020,Vineeth Rajamohan and Mounica Santhapur and Sergiu Dascalu,A modern game-based technique for learning software engineering course,,,,2020,10.34190/GBL.20.179,20490992,"Traditional teaching methods can be perceived as ineffective and boring by many students. Although teachers continuously incorporate novel teaching methods, it is believed that today's institutions face major problems around student motivation and engagement. Game-based learning is one possible solution to address this problem. Due to the abilities to teach and emphasize not only knowledge but also important skills such as problem-solving, teamwork, and communication the use of game-based learning is a promising approach. The main goals of this work were to encourage the use of game-based learning techniques over traditional learning methods, develop a quiz game to learn software engineering topics, conduct a study to compare the effectiveness of game-based learning with traditional learning, and compare gender effect on game-based learning with traditional learning to understand if game-based learning technique is effective across both genders. The quiz game developed displays the questions for the user to answer and provides an explanation for the questions that are answered incorrectly and awards trophies for every correct answer. Qualitative and quantitative analysis is done to study the effectiveness of game-based learning. Our study was a 14 participants x 2 learning technique experiment. Quantitative analyses included marks and trophies earned in traditional paper-based quiz and quiz game as well as gender effect on game-based learning and traditional learning. The questionnaire was designed using a five-point Likert scale. The factors included in the qualitative analysis were user interface design, functions and feedback, and learning. Based on analyses we noticed a better overall performance of the participants using game-based learning compared to traditional learning. In addition, both male and female participants performed better in quiz game over paper quiz. The qualitative outcomes show that participants were satisfied with game-based learning and that they believe that the quiz game offers more scope of learning software engineering concepts compared to paper-based quiz."
Jiang2023,Cuiyuan Jiang,Research on talent cultivation of software engineering under the background of integration of industry and education,SHS Web of Conferences,166,,2023,10.1051/shsconf/202316601031,,"With the rapid development of information technology, The Internet, Cloud Computing, Big Data Etc. Various emerging scientific and technological means and thinking modes are quietly and profoundly changing the thinking, production and learning methods of human society. At present, the education industry generally regards the integration of production and education as the path of vocational education development and personnel training, This paper discusses the current situation of the integration of industry and education, Analyze the problems in the process of the integration of industry and education, The talent cultivation scheme of software engineering under the background of integration of industry and education is proposed."
Maryani2022,Maryani and Harjanto Prabowo and Ford Lumban Gaol and Ahmad Nizar Hidayanto,Comparison of the System Development Life Cycle and Prototype Model for Software Engineering,International Journal of Emerging Technology and Advanced Engineering,12,4,2022,10.46338/ijetae0422_19,22502459,"The concept of system lifecycle models was born, emphasizing the importance of adhering to them. Some methodical technique to developing a new or improved system Several models were suggested, including the waterfall model, the V-shaped model, the iterative model, the spiral model, and the Bing bang model are all examples of models., etc. The comparative analysis is the emphasis of this paper. These Software Development Life Cycle Models were examined. Organizations' operations are expanding, and the need to automate certain processes has grown. As a result, it was thought that standard and structural technique, or It is suggested that a methodology be established in the industry so that the transition from a manual to an automated system may be made as smoothly as possible."
Ahmad2021,Yasir Ahmad and Wan Mohd Nasir Wan-Kadir and Sadia Husain and Noraini Ibrahim,An Intuitionistic Fuzzy Based Approach to Resolve Detected Ambiguities in the User Requirements Document,IEEE Access,9,,2021,10.1109/ACCESS.2021.3104294,21693536,"Ambiguous user requirements are usually considered problematic in software engineering. Therefore, many studies have been conducted on its avoidance and detection. However, the detected ambiguities were resolved manually using interviews, brainstorming, and group discussion sessions among the elicitors and stakeholders for whom the software was developed. If not addressed efficiently, it gives rise to the explicit issues of additional time and cost involved and the stakeholders' availability to clarify them during multiple sessions. However, if appropriately addressed, it can reveal some implicit issues, such as tacit knowledge, hesitation, and terminological discrepancies. Identifying these implicit issues is not easy, as it requires expert elicitation skills that usually come with experience. In addition to the increasing demand for an automated approach to address these implicit issues, the recent COVID 19 pandemics has also amplified the demand to address the explicit issue of stakeholder availability. This paper proposes an implementable semi-automated approach to help elicitors address these demands. The proposed approach uses intuitionistic fuzzy logic to address hesitation and statistical functions to identify discordance and tacit knowledge. It also uses the heuristic knowledge gained in each iteration to improve itself. We implemented it in an online tool and conducted controlled experiments to evaluate our approach, and the results were compared. We achieved precision, recall, and F1 score of 0.769, 1, and 0.869, respectively, during our experiments. The results show that the proposed approach may minimize the explicit issues and help novice elicitors address the implicit issues discussed earlier."
Takan2023,Savas Takan and Gokmen Katipoglu,Relational Logging Design Pattern,"Computers, Materials and Continua",75,1,2023,10.32604/cmc.2023.035282,15462226,"Observability and traceability of developed software are crucial to its success in software engineering. Observability is the ability to comprehend a system’s internal state from the outside. Monitoring is used to determine what causes system problems and why. Logs are among the most critical technology to guarantee observability and traceability. Logs are frequently used to investigate software events. In current log technologies, software events are processed independently of each other. Consequently, current logging technologies do not reveal relationships. However, system events do not occur independently of one another. With this perspective, our research has produced a new log design pattern that displays the relationships between events. In the design we have developed, the hash mechanism of blockchain technology enables the display of the logs’ relationships. The created design pattern was compared to blockchain technology, demonstrating its performance through scenarios. It has been determined that the recommended log design pattern outperforms blockchain technology in terms of time and space for software engineering observability and traceability. In this context, it is anticipated that the log design pattern we provide will strengthen the methods used to monitor software projects and ensure the traceability of relationships."
Nguyen-Duc2024,Anh Nguyen-Duc and Dron Khanna and Giang Huong Le and Des Greer and Xiaofeng Wang and Luciana Martinez Zaina and Gerardo Matturro and Jorge Melegati and Eduardo Guerra and Petri Kettunen and Sami Hyrynsalmi and Henry Edison and Afonso Sales and Rafael Chanin and Didzis Rutitis and Kai Kristian Kemell and Abdullah Aldaeej and Tommi Mikkonen and Juan Garbajosa and Pekka Abrahamsson,Work-from-home impacts on software project: A global study on software development practices and stakeholder perceptions,Software - Practice and Experience,54,5,2024,10.1002/spe.3306,1097024X,"Context: The COVID-19 pandemic has had a disruptive impact on how people work and collaborate across all global economic sectors, including software business. While remote working is not new for software engineers, forced WFH situations come with both limitations and opportunities. As the ‘new normal’ for working might be based on the current state of Work-from-home (WFH), it is useful to understand what has happened and learn from that. Objective: This study aims to gain insights into how their WFH arrangement impacts project management and software engineering. We are also interested in exploring these impacts in different contexts, such as startups and established companies. Method: We conducted a global-scale, cross-sectional survey during the spring and summer 2021. Our results are based on quantitative and qualitative analysis of 297 valid responses. Results: We characterize the profile of WFH in both spatial and temporal aspects, together with a set of common collaborative tools and coordination and control mechanisms. We revealed some areas of project management that are relatively more challenging during WFH situations, such as coordination, communication and project planning. We also revealed a mixed picture of the perceived impact of WFH on different software engineering activities. Conclusion: WFH is a situational phenomenon which can have both negative and positive impact on software teams. For practitioners, we suggest a unified approach to consider the context of WFH, collaborative tools, associated coordination and control approaches and a process that resolve those aspects that are sensitive to physical interaction."
Lawlor2021,Brendan Lawlor and Roy D. Sleator,The roles of code in biology,Science Progress,104,2,2021,10.1177/00368504211010570,20477163,"The way in which computer code is perceived and used in biological research has been a source of some controversy and confusion, and has resulted in sub-optimal outcomes related to reproducibility, scalability and productivity. We suggest that the confusion is due in part to a misunderstanding of the function of code when applied to the life sciences. Code has many roles, and in this paper we present a three-dimensional taxonomy to classify those roles and map them specifically to the life sciences. We identify a “sweet spot” in the taxonomy—a convergence where bioinformaticians should concentrate their efforts in order to derive the most value from the time they spend using code. We suggest the use of the “inverse Conway maneuver” to shape a research team so as to allow dedicated software engineers to interface with researchers working in this “sweet spot.” We conclude that in order to address current issues in the use of software in life science research such as reproducibility and scalability, the field must reevaluate its relationship with software engineering, and adapt its research structures to overcome current issues in bioinformatics such as reproducibility, scalability and productivity."
Thant2023,Khin Shin Thant and Myat Mon Khaing and Hlaing Htake Khaung Tin,Mapping the Course Objectives and Program Learning Outcomes of Software Engineering Course by Bloom’s Revised Taxonomy,Middle East Research Journal of Engineering and Technology,3,03,2023,10.36348/merjet.2023.v03i03.001,27897737,"The outcomes of software engineering can have a significant impact on the success of a software project and the satisfaction of its stakeholders. The results of the study provide a clear understanding of how success can be achieved in a program. Bloom’s Revised Taxonomy is a framework used to classify learning outcomes in terms of different levels of complexity and cognitive ability. It can be used to assess the active learning outcomes of a software engineering course. Bloom's Revised Taxonomy can be applied to assess the active learning outcomes of a software engineering course. Assessing the active learning outcomes of a software engineering course using Bloom's Revised Taxonomy, educators can ensure that students are gaining a deep understanding of the material and are able to apply what they have learned in new and innovative ways. The main goal of the survey was to enable computer science students to use real-world software to help them make better decisions. Other objectives of this research include: (1) students having good communication with teachers gives them the opportunity to feel motivated and involved in the software engineering learning process and (2) Improve teaching and learning methods for Software Engineering. This research design is for undergraduate computer science students."
Cico2020,Orges Cico,Software Startups in growth phase SE practices adopted to SEE,,,,2020,10.1145/3377812.3381406,02705257,"Context: Software has become ubiquitous in every corner of modern societies. During the last five decades, software engineering has also changed significantly to advance the development of various types and scales of software products. In this context, Software Engineering Education plays an essential role in keeping students updated with software technologies, processes, and practices that are popular in industries. Aim: In this PhD work, I want to answer the following research questions: To what extent software engineering trends are present in software engineering educationƒ In which way software startup in growth phase characteristics can be transferred into software engineering education contextƒ What is the impact of software startup engineering in the curriculum and to software engineering studentsƒ Method: I utilize literature review and mix-methods approaches (quantitative and qualitative data and methods triangulation) in gathering empirical evidence. More precisely, I split my research method into two phases. The first phase of the research will acquire knowledge and insight based on the existing literature review. The second research phase will split the focus in two directions. Firstly, I shall gather empirical evidence on how software startup practices are present in software engineering education. Secondly, I will conduct parallel investigations into SE practices in growth phase software startups. Expected Results: I argue that software startup engineering practices are an ultimate tool for software engineering education approaches. I expect students to acquire software engineering skills in a more realistic context while using software startup in growth phase practices."
Campoverde-Molina2021,Milton Campoverde-Molina and Sergio Luján-Mora and Llorenç Valverde,Systematic literature review on software architecture of educational websites,IET Software,15,4,2021,10.1049/sfw2.12024,17518814,"The modern world greatly depends on information systems and the software that governs them. The software architecture defines and designs the holistic structure that the software will have, its components, the interaction between them and all the development is done around it. The purpose of this systematic literature review (SLR) is to analyse the software architectures used in educational websites, methodologies, technological components and empirical results. The search of the SLR yielded 23 studies from the most significant academic sources in education and software engineering. The results of the SLR show that the analysed educational websites were proposing a software architecture, developing a system, proposing a model, assessing of a platform, proposing a Folksonomy-Based Ontology Maintenance, reviewing smart home design, and proposing a Web-based platform to aid parallel, distributed and high-performance computing education. Of the 23 selected studies, 13 carried out an evaluation of their research with either students, teachers, professionals or a combination of these. In conclusion, the selected studies present narrated experiments of projects or individuals that seek to improve collaborative learning in the educational area. Finally, an important finding is that the proposed software architectures do not contemplate laws or quality standards for universal access."
Baldassarre2021,Maria Teresa Baldassarre and Danilo Caivano and Simone Romano and Francesco Cagnetta and Victor Fernandez-Cervantes and Eleni Stroulia,PhyDSLK: a model-driven framework for generating exergames,Multimedia Tools and Applications,80,18,2021,10.1007/s11042-021-10980-3,15737721,"In recent years, we have been witnessing a rapid increase of research on exergames—i.e., computer games that require users to move during gameplay as a form of physical activity and rehabilitation. Properly balancing the need to develop an effective exercise activity with the requirements for a smooth interaction with the software system and an engaging game experience is a challenge. Model-driven software engineering enables the fast prototyping of multiple system variants, which can be very useful for exergame development. In this paper, we propose a framework, PhyDSLK, which eases the development process of personalized and engaging Kinect-based exergames for rehabilitation purposes, providing high-level tools that abstract the technical details of using the Kinect sensor and allows developers to focus on the game design and user experience. The system relies on model-driven software engineering technologies and is made of two main components: (i) an authoring environment relying on a domain-specific language to define the exergame model encapsulating the gameplay that the exergame designer has envisioned and (ii) a code generator that transforms the exergame model into executable code. To validate our approach, we performed a preliminary empirical evaluation addressing development effort and usability of the PhyDSLK framework. The results are promising and provide evidence that people with no experience in game development are able to create exergames with different complexity levels in one hour, after a less-than-two-hour training on PhyDSLK. Also, they consider PhyDSLK usable regardless of the exergame complexity."
Gasca-Hurtado2021,Gloria Piedad Gasca-Hurtado and María Clara Gómez-Álvarez and Liliana Machuca-Villegas and Mirna Muñoz,Design of a gamification strategy to intervene in social and human factors associated with software process improvement change resistance,IET Software,15,6,2021,10.1049/sfw2.12045,17518814,"Best practices of software development and software process improvement (SPI) are people-centred. Both have a high level of influence of social and human factors inherent to the individuals involved in these processes. At the same time, software process improvement is inherent to the change of the conditions and the behaviour of individuals. In this work, the authors show the results of applying a deductive approach for a method based on gamification and social and human factors to influence the productivity of the software development team. The results show that it is possible to abstract all the design aspects of an existing strategy called Interplanetary Mission and identify the social and human factors involved in change resistance in SPI initiatives. The strategy execution results show that gamification strategies could help to mitigate change resistance in SPI initiatives for software development teams and their productivity, understanding this term in software engineering context as the ratio between output and input within the software development production process."
Al-Ahmad2023,Ahmad S. Al-Ahmad and Hasan Kahtan and Yehia I. Alzoubi,Overview on Case Study Penetration Testing Models Evaluation,Emerging Science Journal,7,3,2023,10.28991/ESJ-2023-07-03-025,26109182,"Model evaluation is a cornerstone of scientific research as it represents the findings' accuracy and model performance. A case study is commonly used in evaluating software engineering models. Due to criticism in terms of generalization from a single case study and testers, deciding on the number of case studies used for evaluation and the number of testers has been one of the researchers’ challenges. Multiple case studies with multiple testers can be difficult in some domains, such as penetration testing, due to the complexity and time needed to prepare test cases. This study aims to review the literature and examine the evaluation methods used pertaining to the number of case studies and testers involved. This study is beneficial for researchers, students, and penetration testers as it provides case study design steps that are useful to determine the appropriate number of test cases and testers required. The paper's findings and novelty highlight that a single case study with a single tester is enough to evaluate a model. It also strikes a balance between what is enough for the evaluation and the need to reduce criticisms of a single case study by using two case studies with a single tester."
Yasir2020,Muntadher Naeem Yasir and Muayad Sadik Croock,Software engineering based self-checking process for cyber security system in VANET,International Journal of Electrical and Computer Engineering,10,6,2020,10.11591/ijece.v10i6.pp5844-5852,20888708,"Newly, the cyber security of vehicle ad hoc network (VANET) includes two practicable: Vehicle to vehicle (V2V) and Vehicle to Infrastructure (V2I) that have been considered due to importance. It has become possible to keep pace with the development in the world. The people safety is a priority in the development of technology in general and particular in of VANET for police vehicles. In this paper, we propose a software engineering based self-checking process to ensure the high redundancy of the generated keys. These keys are used in underlying cyber security system for VANET. The proposed self-checking process emploies a set of NIST tests including frequency, block and runs as a threshold for accepting the generated keys. The introduced cyber security system includes three levels: Firstly, the registration phase that asks vehicles to register in the system, in which the network excludes the unregistered ones. In this phase, the proposed software engineeringbased self-checking process is adopted. Secondly, the authentication phase that checks of the vehicles after the registration phase. Thirdly, the proposed system that is able to detect the DOS attack. The obtained results show the efficient performance of the proposed system in managing the security of the VANET network. The self-checking process increased the randomness of the generated keys, in which the security factor is increased."
Herkert2020,Joseph Herkert and Jason Borenstein and Keith Miller,The Boeing 737 MAX: Lessons for Engineering Ethics,Science and Engineering Ethics,26,6,2020,10.1007/s11948-020-00252-y,14715546,"The crash of two 737 MAX passenger aircraft in late 2018 and early 2019, and subsequent grounding of the entire fleet of 737 MAX jets, turned a global spotlight on Boeing’s practices and culture. Explanations for the crashes include: design flaws within the MAX’s new flight control software system designed to prevent stalls; internal pressure to keep pace with Boeing’s chief competitor, Airbus; Boeing’s lack of transparency about the new software; and the lack of adequate monitoring of Boeing by the FAA, especially during the certification of the MAX and following the first crash. While these and other factors have been the subject of numerous government reports and investigative journalism articles, little to date has been written on the ethical significance of the accidents, in particular the ethical responsibilities of the engineers at Boeing and the FAA involved in designing and certifying the MAX. Lessons learned from this case include the need to strengthen the voice of engineers within large organizations. There is also the need for greater involvement of professional engineering societies in ethics-related activities and for broader focus on moral courage in engineering ethics education."
Ramin2020,Frederike Ramin and Christoph Matthies and Ralf Teusner,More than Code: Contributions in Scrum Software Engineering Teams,,,,2020,10.1145/3387940.3392241,,"Motivated and competent team members are a vital part of Agile Software development and make or break any project's success. Motivation is fostered by continuous progress and recognition of efforts. These concepts are founding pillars of the Scrum methodology, which focuses on self-organizing teams. The types of contributions Scrum development team members make to a project's progress are not only technical. However, a comprehensive model comprising the varied contributions in modern software engineering teams is not yet established. We propose a model that incorporates contributions of all Scrum roles, explicitly including those which are not directly related to project artifacts. It improves the visibility of performed tasks, acts as a starting point for team retrospection, and serves as a foundation for discussion in the research community."
Lindsjrn2021,Yngve Lindsjørn and Steffen Almås and Viktoria Stray,Exploring motivation and teamwork in a large software engineering capstone course during the coronavirus pandemic,Nordic Journal of STEM Education,5,1,2021,10.5324/njsteme.v5i1.3938,,"In the spring of 2020, the Department of Informatics covered a 20 ECTS capstone course in Software Engineering, mainly focusing on developing a complex application. The course used active learning methods, and 240 students were working in 42 cross-functional, agile teams. The pandemic caused by the coronavirus had a significant impact on the teaching given by the University of Oslo, as all physical education and collaboration among the teams had to be digital from March 12. At the end of the semester, we conducted a survey that focused on 1) aspects of teamwork (e.g., communication and coordination in the teams) and the relation to team performance (e.g., the application product) and 2) the students’ motivation and ability to cooperate through digital platforms. A total of 151 respondents in 41 agile student teams answered the survey. This study aimed to investigate how the teamwork and motivation of the students were affected by having to work virtually. The results are compared to results from the same course in 2019 and a similar survey on 71 professional teams published in 2016. Our results show that the teamwork was evaluated similarly to both the evaluation of survey conducted in 2019 and on the professional teams in 2016.  The motivation among the students remained high, even though they had to collaborate virtually."
Chikasha2022,Samuel Chikasha and Wim Van Petegem and Katie Goeman and Martin Valcke and Servious Mbiza,Acceptance of Pedagogical Agent (PA) enhanced eLearning communities by software engineering students in Southern Africa,,,,2022,10.5821/conference-9788412322262.1384,,"Covid19 outbreak has seen eLearning becoming a viable alternative to the traditional face-to-face teaching globally. Software Engineering education has not been an exception to these changes. The use of multimedia enhanced eLearning communities is also on the increase in the teaching of software engineering. However, there is limited research on the acceptance of such technologies by African learners. Some of the multimedia being used to enhance these learning communities includes animated pedagogical agents (PAs) combining text. animation, audio, and video. Considering learner differences and aiming to achieve personalized learning, there is a need for institutions to understand how such technologies are being accepted by learners and the factors that influence the acceptance. This study focuses on the acceptance of pedagogical agent enhanced eLearning communities by Southern African learners in the teaching of Software Engineering. The aim of the study is to identify the factors that influence the acceptance of such communities. This will help eLearning designers to try and address the needs of learners in different contexts to achieve personalized learning. This study involved 137 software engineering students from South Africa and Zimbabwe who were being introduced to eLearning community enhanced with PAs. The unified theory of acceptance and use of technology2 (UTAUT2) was used in this study. The study revealed that only performance expectancy, and hedonic motivation constructs had an effect on behavioral intention to use these eLearning communities enhanced with PAs."
Che2021,Ferdinand Ndifor Che and Kenneth David Strang and Narasimha Rao Vajjhala,Using Experiential Learning to Improve Student Attitude and Learning Quality in Software Engineering Education,International Journal of Innovative Teaching and Learning in Higher Education,2,1,2021,10.4018/ijitlhe.20210101.oa2,2644-1624,"Experiential learning (EL) has great potential to transform students’ learning experience. Few studies, however, have focused on the use of EL in computer science education. The purpose of this study was to examine students' experiences with EL in computer science. Data were collected to examine the influence of EL on students' attitudes and quality of learning. The antecedent variables included student involvement, learning expectancy, instructor impact, course structure, and prior experience. PLS-SEM with PLSc was used to test generated hypotheses. The findings indicated that student involvement positively correlated with attitudes and learning expectancy. Instructor impact is positively associated with student involvement, quality of learning, and attitudes. Prior experience positively correlated with learning expectancy. Finally, course structure positively moderated the relationship between student involvement and learning expectancy. It is concluded that EL is a promising pedagogy to improve student attitudes and quality of learning in software engineering education."
Nour2023,Tarek Nour and Noura Albaladi,Software Requirement Engineering: Traceability Techniques and Tools,International Journal of Computers and Informatics,2,4,2023,10.59992/ijci.2023.v2n4p1,,Requirement Traceability is one of the activities in managing requirements. It is important for software projects and is affecting the quality of software products. Requirement Traceability is a method to analyze the effect of changes among various software development lifecycle parts. Agile methodologies have been presented as an alternative to traditional software engineering methodologies. The transformation between traditional and agile methodologies is a hard task so the need for traceability grows. This paper introduces traceability research at the requirement engineering on the traceability literature published during the last years. It also investigates and discusses the requirements for traceability issues. It finally presents several requirement traceability techniques and tools to support traceability.
Croock2020,Muayad Sadik Croock and Saja Dhyaa Khuder and Zahraa Abbas Hassan,Self-checking method for fault tolerance solution in wireless sensor network,International Journal of Electrical and Computer Engineering,10,4,2020,10.11591/ijece.v10i4.pp4416-4425,20888708,"Recently, the wireless sensor network (WSN) has been considered in different application, particularly in emergency systems. Therefore, it is important to keep these networks in high reliability using software engineering techniques in the field of fault tolerance. This paper proposed a fault node detection method in WSN using the self-checking technique according to the rules of software engineering. Then, the detected faulted node is covered employing the reading of nearest neighbor nodes (sensors). In addition, the proposed method sends a message for maintenance to solve the fault. The proposed method can reduce the time between the detection and recovery of a fault to prevent the confusion of adopting wrong readings, in which the detection is making with mistake. Moreover, it guarantees the reliability of the WSN, in terms of operation and data transmission. The proposed method has been tested over different scenarios and the obtained results show the superior efficiency in terms of recovery, reliability, and continuous data transmission."
Dittrich2020,Yvonne Dittrich and Christian Bo Michelsen and Paolo Tell and Pernille Lous and Allan Ebdrup,Exploring the evolution of software practices,,,,2020,10.1145/3368089.3409766,,"When software products and services are developed and maintained over longer time, software engineering practices tend to drift away from both structured and agile methods. Nonetheless, in many cases the evolving practices are far from ad hoc or chaotic. How are the teams involved able to coordinate their joint development?This article reports on an ethnographic study of a small team at a successful provider of software as a service. What struck us was the very explicit way in which the team adopted and adapted their practices to fit the needs of the evolving development. The discussion relates the findings to the concepts of social practices and methods in software engineering, and explores the differences between degraded behavior and the coordinated evolution of development practices. The analysis helps to better understand how software engineering practices evolve, and thus provides a starting point for rethinking software engineering methods and their relation to software engineering practice."
Gunawan2024,Arie Gunawan and Munir Munir and Yudi Wibisono and Chairul Furqon,INTEGRATION OF BLOCKCHAIN TECHNOLOGY IN DIGITAL LIBRARIES: A SOFTWARE ENGINEERING DESIGN,JITK (Jurnal Ilmu Pengetahuan dan Teknologi Komputer),9,2,2024,10.33480/jitk.v9i2.5010,2685-8223,"This research aims to design software engineering that integrates blockchain technology in digital libraries to improve system security and reliability. This integration is expected to overcome challenges related to data security, service reliability, and efficiency in digital library management. The research methodology involves collecting data through literature, expert interviews, and observations, on the implementation of blockchain technology in digital libraries, then analyzing data to support data design such as etherum, smart contracts, address, node.js, solidity, metamask, and sublime text, then using the Agile Extreme Programming (XP) method for software development. The research results include the design of a decentralized blockchain architecture, the use of smart contracts, and the application of cryptographic techniques to enhance security. Immutability testing in the context of blockchain involves verifying data consistency, validating the process of adding data, testing the ability to delete data, testing against attacks, and activities on immutable data. These tests were conducted using the Truffle framework. The results show that the system is able to maintain data integrity well."
Rindell2021,Kalle Rindell and Jukka Ruohonen and Johannes Holvitie and Sami Hyrynsalmi and Ville Leppänen,Security in agile software development: A practitioner survey,Information and Software Technology,131,,2021,10.1016/j.infsof.2020.106488,09505849,"Context: Software security engineering provides the means to define, implement and verify security in software products. Software security engineering is performed by following a software security development life cycle model or a security capability maturity model. However, agile software development methods and processes, dominant in the software industry, are viewed to be in conflict with these security practices and the security requirements. Objective: Empirically verify the use and impact of software security engineering activities in the context of agile software development, as practiced by software developer professionals. Method: A survey (N=61) was performed among software practitioners in Finland regarding their use of 40 common security engineering practices and their perceived security impact, in conjunction with the use of 16 agile software development items and activities. Results: The use of agile items and activities had a measurable effect on the selection of security engineering practices. Perceived impact of the security practices was lower than the rate of use would imply: This was taken to indicate a selection bias, caused by e.g. developers’ awareness of only certain security engineering practices, or by difficulties in applying the security engineering practices into an iterative software development workflow. Security practices deemed to have most impact were proactive and took place in the early phases of software development. Conclusion: Systematic use of agile practices conformed, and was observed to take place in conjunction with the use of security practices. Security activities were most common in the requirement and implementation phases. In general, the activities taking place early in the life cycle were also considered most impactful. A discrepancy between the level of use and the perceived security impact of many security activities was observed. This prompts research and methodological development for better integration of security engineering activities into software development processes, methods, and tools."
Niu2022,Changan Niu and Chuanyi Li and Bin Luo and Vincent Ng,Deep Learning Meets Software Engineering: A Survey on Pre-Trained Models of Source Code,,,,2022,10.24963/ijcai.2022/775,10450823,"Recent years have seen the successful application of deep learning to software engineering (SE). In particular, the development and use of pre-trained models of source code has enabled state-of-the-art results to be achieved on a wide variety of SE tasks. This paper provides an overview of this rapidly advancing field of research and reflects on future research directions."
Fronza2021,Ilenia Fronza and Xiaofeng Wang,Social loafing prevention in agile software development teams using team expectations agreements,IET Software,15,3,2021,10.1049/sfw2.12019,17518814,"Social loafing is a common issue encountered by many software development teams. In practice, agile software development teams often use team expectations agreements (TEAs), which contain rules to prevent social loafing behaviour. However, few studies in the software engineering literature examine how these rules are specified in TEAs and whether they serve the purpose. The authors intend to provide a better understanding on how to specify rules in TEAs to prevent social loafing behaviour in agile software development teams. A mixed-method approach using both qualitative and quantitative data was employed. The results of our study show that agile software development teams specify the expectations of the whole team on meeting attendance and contribution, respect of tasks, roles and teammates, and collaboration with each other. These rules have the potential to prevent social loafing behaviours. However, specifying the rules in TEAs alone is not sufficient to make them effective. It needs to be complemented with team commitment to TEA rules. The work of the authors contributes to a better understanding of how to use TEAs to prevent social loafing in agile software development teams, and provides a conceptual basis for future research to investigate social loafing prevention using TEAs."
Alsaber2020,Leanah Alsaber and Ebtesam Al Elsheikh and Sarah Aljumah and Nor Shahida Mohd Jamail,“Scrumbear” framework for solving traditional scrum model problems,Bulletin of Electrical Engineering and Informatics,10,1,2020,10.11591/eei.v10i1.2487,23029285,"Software engineering is a discipline that is little understood by people. It defines how software is developed and maintained to meet the clients’ requirements. Software engineers follow certain systems and standards in their work to meet the clients’ desires. It is on this background that engineers follow specific models in coming up with the final product. One of the models highly used is scrum, which is one of the agile methodologies. However, despite being highly used, it has inherent flaws that need to be corrected. Those flaws are product owner continues changing; do not accept changes in working scrum, sprint’s release time limitation, finally wasting team time within each sprint. This paper presents a new framework, which is an updated version of the current Scrum, to overcome the scum models mentioned issues. In this study, a new framework is presented in a way that is understandable and needed by software developer’s team upon the collected qualitative and quantitative data. The implementation was by making some changes to the current scrum model leading to the “Scrumbear”, certain flaws can be corrected. One of the presented changes involve adding the control master rule to ensure controlling the requirements changing."
Bedwell2023,Kylie Bedwell and Giacomo Garaccione and Riccardo Coppola and Luca Ardito and Maurizio Morisio,BIPMIN: A Gamified Framework for Process Modeling Education,Information (Switzerland),14,1,2023,10.3390/info14010003,20782489,"Business Process Modeling is a skill that is becoming sought after for computer engineers, with Business Process Modeling Notation (BPMN) being one example of the tools used in modeling activities. Students of the Master of Computer Engineering course at Politecnico di Torino learn about BPMN in dedicated courses but often underperform on BPMN-related exercises due to difficulties understanding how to model processes. In recent years, there has been a surge of studies that employ gamification (using game elements in non-recreative contexts to obtain benefits) as a tool in Computer Engineering education to increase students’ engagement with the learning process. This study aims to use the principles of gamification to design a supplementary learning tool for the teaching of information systems technology. In particular, to improve student understanding and use of BPMN diagrams. This study also analyzes the usability and motivation of the participants in using different game elements in increasing student motivation and performance. As part of the study, a prototype web application was developed, which implemented three different designs, each incorporating different game elements relating to either progress, competition, or rewards. An evaluation was then conducted on the prototype to evaluate the performance of the practitioners in performing BPMN modeling tasks with the gamified tool, the usability of the proposed mechanics and the enjoyment of the individual game mechanics that were implemented. With the usage of the gamified tool, the users of the experimental sample were able to complete BPMN modeling tasks with performances compatible with estimates made through expert judgement (i.e., gamification had no negative effect on performance), and were motivated to check the correctness of their models many times during the task execution. The system was evaluated as highly usable (85.8 System Usability Score); the most enjoyed game elements were rewards, levels, progress bars and aesthetics."
Al-Ahmadi2022,Haneen Hassan Al-Ahmadi,The Significance of Software Engineering to Forecast the Public Health Issues: A Case of Saudi Arabia,Frontiers in Public Health,10,,2022,10.3389/fpubh.2022.900075,22962565,"In the recent years, public health has become a core issue addressed by researchers. However, because of our limited knowledge, studies mainly focus on the causes of public health issues. On the contrary, this study provides forecasts of public health issues using software engineering techniques and determinants of public health. Our empirical findings show significant impacts of carbon emission and health expenditure on public health. The results confirm that support vector machine (SVM) outperforms the forecasting of public health when compared to multiple linear regression (MLR) and artificial neural network (ANN) technique. The findings are valuable to policymakers in forecasting public health issues and taking preemptive actions to address the relevant health concerns."
Abdulmajeed2021,Ashraf Abdulmunim Abdulmajeed and Marwa Adeeb Al-Jawaherry and Tawfeeq Mokdad Tawfeeq,Predict the required cost to develop Software Engineering projects by Using Machine Learning,,1897,1,2021,10.1088/1742-6596/1897/1/012029,17426596,"Software project cost prediction is a very important task during building and developing software projects. This process helps software project engineers to accurately manage and plan their resources in terms of cost estimation. However, Need for accurate cost development prediction model for a software project is not a simple procedure. Predicting the cost required while developing software engineering projects is the most difficult challenge that attracts the attention of researchers and practitioners. This paper adopts a new model in estimating the cost of building or developing software engineering projects using a machine learning approach. The results proves that machine learning methods can be used to predict program cost with high accuracy rate compared with traditional software estimation techniques. The proposed model in this research was trained on the NASA (National Aeronautics and Space Administration) data set, which contains the characteristics of 60 projects in addition to the real cost of the projects. An analysis of the results of the implementation for the proposed methods showed that the cost Predicting process using K-Nearest Neighbours algorithm (KNN), Cascade Neural Networks (CNN) and Elman Neural Networks (ENN) It has the ability to predict the costs required to build or develop software engineering projects, K-Nearest Neighbours algorithm has shown high accuracy for Predict the required cost to develop Software Engineering projects Compared to Cascade Neural Networks and Elman Neural Networks ENN."
Reinhart2021,Alex Reinhart and Christopher R. Genovese,Expanding the Scope of Statistical Computing: Training Statisticians to Be Software Engineers,Journal of Statistics and Data Science Education,29,S1,2021,10.1080/10691898.2020.1845109,26939169,"Traditionally, statistical computing courses have taught the syntax of a particular programming language or specific statistical computation methods. Since Nolan and Temple Lang’s seminal paper, we have seen a greater emphasis on data wrangling, reproducible research, and visualization. This shift better prepares students for careers working with complex datasets and producing analyses for multiple audiences. But, we argue, statisticians are now often called upon to develop statistical software, not just analyses, such as R packages implementing new analysis methods or machine learning systems integrated into commercial products. This demands different skills. We describe a graduate course that we developed to meet this need by focusing on four themes: programming practices, software design, important algorithms and data structures, and essential tools and methods. Through code review and revision, and a semester-long software project, students practice all the skills of software engineering. The course allows students to expand their understanding of computing as applied to statistical problems while building expertise in the kind of software development that is increasingly the province of the working statistician. We see this as a model for the future evolution of the computing curriculum in statistics and data science."
Naveed2024,Hira Naveed and Chetan Arora and Hourieh Khalajzadeh and John Grundy and Omar Haggag,Model driven engineering for machine learning components: A systematic literature review,Information and Software Technology,169,,2024,10.1016/j.infsof.2024.107423,09505849,"Context: Machine Learning (ML) has become widely adopted as a component in many modern software applications. Due to the large volumes of data available, organizations want to increasingly leverage their data to extract meaningful insights and enhance business profitability. ML components enable predictive capabilities, anomaly detection, recommendation, accurate image and text processing, and informed decision-making. However, developing systems with ML components is not trivial; it requires time, effort, knowledge, and expertise in ML, data processing, and software engineering. There have been several studies on the use of model-driven engineering (MDE) techniques to address these challenges when developing traditional software and cyber–physical systems. Recently, there has been a growing interest in applying MDE for systems with ML components. Objective: The goal of this study is to further explore the promising intersection of MDE with ML (MDE4ML) through a systematic literature review (SLR). Through this SLR, we wanted to analyze existing studies, including their motivations, MDE solutions, evaluation techniques, key benefits and limitations. Method: Our SLR is conducted following the well-established guidelines by Kitchenham. We started by devising a protocol and systematically searching seven databases, which resulted in 3934 papers. After iterative filtering, we selected 46 highly relevant primary studies for data extraction, synthesis, and reporting. Results: We analyzed selected studies with respect to several areas of interest and identified the following: (1) the key motivations behind using MDE4ML; (2) a variety of MDE solutions applied, such as modeling languages, model transformations, tool support, targeted ML aspects, contributions and more; (3) the evaluation techniques and metrics used; and (4) the limitations and directions for future work. We also discuss the gaps in existing literature and provide recommendations for future research. Conclusion: This SLR highlights current trends, gaps and future research directions in the field of MDE4ML, benefiting both researchers and practitioners."
GKishorekumar2023,G.Kishorekumar and Mrs.P.Uma M.E,A SURVEY ON PERSPECTIVES ON THE GAP BETWEEN THE SOFTWARE INDUSTRY AND THE SOFTWARE ENGINEERING EDUCATION,international journal of engineering technology and management sciences,7,2,2023,10.46647/ijetms.2023.v07i02.098,,"In 1989, the gap between the software industry and software engineering education was first mentioned. Its existence has been regularly reported on since then, and solutions to close it have been proposed. However, after thirty years, all efforts to close the gap have failed. In this study, we argue that the gap between industry and academia exists for a variety of interconnected reasons. To take a broader look at the problem from the standpoint of all related entities, we provide a detailed overview of the profession and identify the entities, extract the causes that stem from these entities, and discuss what each entity should do, report and analyse the results of a questionnaire that was administered to students and recent graduates, highlight the highlights of interviews with students, recent graduates, and academics, We also contribute to finding solutions by considering all entities involved, which allows us to reach out to all of them and find out what they can do to acknowledge and close the gap. Our research concludes that the gap requires constant attention and hard work from all parties involved, and that everyone should be on the lookout for new technologies, learn to embrace changes, and adapt to them in order to keep the gap to a minimum."
Ahmad2023,Khlood Ahmad and Mohamed Abdelrazek and Chetan Arora and Muneera Bano and John Grundy,Requirements practices and gaps when engineering human-centered Artificial Intelligence systems,Applied Soft Computing,143,,2023,10.1016/j.asoc.2023.110421,15684946,"Context: Engineering Artificial Intelligence (AI) software is a relatively new area with many challenges, unknowns, and limited proven best practices. Big companies such as Google, Microsoft, and Apple have provided a suite of recent guidelines to assist engineering teams in building human-centered AI systems. Objective: The practices currently adopted by practitioners for developing such systems, especially during Requirements Engineering (RE), are little studied and reported to date. Method: This paper presents the results of a survey conducted to understand current industry practices in RE for AI (RE4AI) and to determine which key human-centered AI guidelines should be followed. Our survey is based on mapping existing industrial guidelines, best practices, and efforts in the literature. Results: We surveyed 29 professionals and found most participants agreed that all the human-centered aspects we mapped should be addressed in RE. Further, we found that most participants were using UML or Microsoft Office to present requirements. Conclusion: We identify that most of the tools currently used are not equipped to manage AI-based software, and the use of UML and Office may pose issues with the quality of requirements captured for AI. Also, all human-centered practices mapped from the guidelines should be included in RE."
Coupette2023,Corinna Coupette and Dirk Hartung and Janis Beckedorf and Maximilian Böther and Daniel Martin Katz,Law Smells: Defining and Detecting Problematic Patterns in Legal Drafting,Artificial Intelligence and Law,31,2,2023,10.1007/s10506-022-09315-w,15728382,"Building on the computer science concept of code smells, we initiate the study of law smells, i.e., patterns in legal texts that pose threats to the comprehensibility and maintainability of the law. With five intuitive law smells as running examples—namely, duplicated phrase, long element, large reference tree, ambiguous syntax, and natural language obsession—, we develop a comprehensive law smell taxonomy. This taxonomy classifies law smells by when they can be detected, which aspects of law they relate to, and how they can be discovered. We introduce text-based and graph-based methods to identify instances of law smells, confirming their utility in practice using the United States Code as a test case. Our work demonstrates how ideas from software engineering can be leveraged to assess and improve the quality of legal code, thus drawing attention to an understudied area in the intersection of law and computer science and highlighting the potential of computational legal drafting."
Shynkarenko2021,Viktor Shynkarenko and Oleksandr Zhevaho,Application of Constructive Modeling and Process Mining Approaches to the Study of Source Code Development in Software Engineering Courses,Journal of Communications Software and Systems,17,4,2021,10.24138/JCOMSS-2021-0046,18466079,"We present an approach of constructing a source code history for a modern code review. Practically, it is supposed to be used in programming training, especially within initial stages. The developed constructor uses constructive-synthesizing modeling tools to classify a source code history by fine-grained changes and to construct an event log file aimed to provide information on students’ coding process. Current research applies Process Mining techniques to the software engineering domain to identify software engineering skills. By better understanding of the way students design programs, we will help novices learn programming. This research provides an innovative method of using code and development process review in teaching programming skills and is aimed to encourage using code review and monitoring coding practice in educational purposes. The standard method of evaluation takes into consideration only a final result, which doesn’t meet modern requirements of teaching programming."
Jawawi2022,Dayang Norhayati Abang Jawawi and Nor Azizah Saadon and Shahliza Abdul Halim and Radziah Mohamad and Mohd Adham Isa and Rosbi Mamat,Collaborative Assignment and Project for Teaching Embedded and Real-Time Software Engineering Course,,2433,,2022,10.1063/5.0072912,15517616,"Teaching embedded and real-time software development course in curriculum is important because such software and hardware is used in many systems towards industrial revolution 4.0 (4IR). There is a wide range of undergraduate and postgraduate programs offering similar courses in their computing and engineering curriculum. School of Computing under the Faculty of Engineering offers a course named Real-Time Software Engineering for undergraduate final year students in Software Engineering program as an elective course. This course implements alternative assessment projects as part of the course assessment criteria. The goal of this project is to ensure students appreciate complexity of software development project and equip them with multi-discipline skills in developing embedded real-time systems. It is also important to boost students' confident level in developing software with physical hardware. The projects adopt a collaborative assignment and project approach by incorporating physical mobile robots and Internet of Things (IoT) system into the problem-based and project-based learning environment. In the teaching and learning activities, learning occurs as students collaborate in completing their projects and engage with real-time concepts, theories, and practices in solving the physical systems' problem."
Kavalchuk2020,Aliaksei Kavalchuk and Alec Goldenberg and Ishtiaque Hussain,An empirical study of teaching qualities of popular computer science and software engineering instructors using RateMyProfessor.com data,,,,2020,10.1145/3377814.3381700,02705257,"The employment opportunity for Computer Science (CS), Information Technology and Software Engineering and Development (SE) related occupations is projected to grow much faster than the average of all other occupations. Therefore, increase in student enrollment, retention and graduation rate is becoming very important, so is the need for effective teaching in these subjects. Many universities commonly use formal, institutional Student Evaluation of Teaching (SET) systems to measure the teaching effectiveness. After each semester, through SET, students provide feedback and comments for their courses and instructors. However, evaluations are private and only a handful people have access to these. Therefore, these evaluations cannot be utilized to create a common understanding of the students' expectations, perspective, desired characteristics of the courses and instructors. On the other hand, third party online platforms like RateMyProfessor.com (RMP) are public, solicit anonymous student feedback and host tremendous amount of data about the instructors and their courses. These platforms are also popular among students. We mined and analyzed the RMP data for some research questions, e.g.: What are the common characteristics of the popular CS instructors? How different are they for the SE instructors? Are there any examples of special characteristics, tools and techniques popular CS instructors use? We captured and analyzed more than 9,000 students' comments for over 300 CS instructors for the top 20 universities in the U.S. and Canada. The paper contributes by presenting the findings for the research questions and making the data and the scripts available for public use for future research."
Huss2023,Moe Huss and Daniel R. Herber and John M. Borky,Comparing Measured Agile Software Development Metrics Using an Agile Model-Based Software Engineering Approach versus Scrum Only,Software,2,3,2023,10.3390/software2030015,,"This study compares the reliability of estimation, productivity, and defect rate metrics for sprints driven by a specific instance of the agile approach (i.e., scrum) and an agile model-Bbased software engineering (MBSE) approach called the integrated Scrum Model-Based System Architecture Process (sMBSAP) when developing a software system. The quasi-experimental study conducted ten sprints using each approach. The approaches were then evaluated based on their effectiveness in helping the product development team estimate the backlog items that they could build during a time-boxed sprint and deliver more product backlog items (PBI) with fewer defects. The commitment reliability (CR) was calculated to compare the reliability of estimation with a measured average scrum-driven value of 0.81 versus a statistically different average sMBSAP-driven value of 0.94. Similarly, the average sprint velocity (SV) for the scrum-driven sprints was 26.8 versus 31.8 for the MBSAP-driven sprints. The average defect density (DD) for the scrum-driven sprints was 0.91, while that of the sMBSAP-driven sprints was 0.63. The average defect leakage (DL) for the scrum-driven sprints was 0.20, while that of the sMBSAP-driven sprints was 0.15. The t-test analysis concluded that the sMBSAP-driven sprints were associated with a statistically significant larger mean CR, SV, DD, and DL than that of the scrum-driven sprints. The overall results demonstrate formal quantitative benefits of an agile MBSE approach compared to an agile alone, thereby strengthening the case for considering agile MBSE methods within the software development community. Future work might include comparing agile and agile MBSE methods using alternative research designs and further software development objectives, techniques, and metrics."
Brun2022,Yuriy Brun,The promise and perils of using machine learning when engineering software (keynote paper),,,,2022,10.1145/3549034.3570200,,"Machine learning has radically changed what computing can accomplish, including the limits of what software engineering can do. I will discuss recent software engineering advances machine learning has enabled, from automatically repairing software bugs to data-driven software systems that automatically learn to make decisions. Unfortunately, with the promises of these new technologies come serious perils. For example, automatically generated program patches can break as much functionality as they repair. And self-learning, data-driven software can make decisions that result in unintended consequences, including unsafe, racist, or sexist behavior. But to build solutions to these shortcomings we may need to look no further than machine learning itself. I will introduce multiple ways machine learning can help verify software properties, leading to higher-quality systems."
Hasselbring2020,Wilhelm Hasselbring and Alexander Krause and Christian Zirkelbach,"ExplorViz: Research on software visualization, comprehension and collaboration",Software Impacts,6,,2020,10.1016/j.simpa.2020.100034,26659638,"ExplorViz supports research on software visualization, software comprehension tasks and software collaboration. To achieve this, ExplorViz provides multi-level visualization from the software landscape layer toward the level of individual software applications. Via immersive 3D visualizations in virtual reality, ExplorViz also supports collaboration in software development teams. The research tool development commenced in 2012, and grew toward a powerful, extensible open-source software that has been employed in a variety of software engineering research projects. In this paper, we review its history, development and research impact."
Izadi2022,Maliheh Izadi and Matin Nili Ahmadabadi,On the Evaluation of NLP-based Models for Software Engineering,,,,2022,10.1145/3528588.3528665,,"NLP-based models have been increasingly incorporated to address SE problems. These models are either employed in the SE domain with little to no change, or they are greatly tailored to source code and its unique characteristics. Many of these approaches are considered to be outperforming or complementing existing solutions. However, an important question arises here: Are these models evaluated fairly and consistently in the SE community?. To answer this question, we reviewed how NLP-based models for SE problems are being evaluated by researchers. The findings indicate that currently there is no consistent and widely-accepted protocol for the evaluation of these models. While different aspects of the same task are being assessed in different studies, metrics are defined based on custom choices, rather than a system, and finally, answers are collected and interpreted case by case. Consequently, there is a dire need to provide a methodological way of evaluating NLP-based models to have a consistent assessment and preserve the possibility of fair and efficient comparison."
Gonales2021,L. J. Gonçales and K. Farias and L. Kupssinskü and M. Segalotto,The effects of applying filters on EEG signals for classifying developers' code comprehension,Journal of Applied Research and Technology,19,6,2021,10.22201/icat.24486736e.2021.19.6.1299,24486736,"EEG signals are a relevant indicator for measuring aspects related to human factors in software engineering. EEG is used in software engineering to train machine learning techniques for various applications, including classifying task difficulty and developers' experience. The EEG signal contains noise such as abnormal readings, electrical interference, and eye movements, which are usually not of interest to the analysis, and therefore contribute to the lack of precision of the machine learning techniques. However, research on software engineering has not evidenced how effective the filtering of EEG signals is, even with evident benefits of filtering the EEG signals in signal processing and clinical image studies. In this paper, we analyzed the effects of using filtered EEG signals for classifying developers' code comprehension. This filter consists of high and low pass filtering designed with an FIR filter using a Hamming window. This filtering process also removes abnormal signals and executes the Independent Component Analysis (ICA) using the fast ICA method for removing EOG components. We applied the filtered EEG signals to train a random forest (RF) machine learning technique to classify the developers' code comprehension. This study also trained another random forest classifier with unfiltered EEG data. We evaluated both models using 10-fold cross-validation. This work measures the classifiers' effectiveness using the f-measure metric. This work used the t-test, Wilcoxon, and U Mann Whitney to analyze the difference in the effectiveness measures (f-measure) between the classifier trained with filtered EEG and the classifier trained with unfiltered EEG. The tests pointed out a significant difference after applying EEG filters to classify developers' code comprehension with the random forest classifier. The conclusion is that the EEG filters significantly improve the effectiveness of classifying code comprehension using the random forest technique."
Ozkaya2023,Ipek Ozkaya,"Application of Large Language Models to Software Engineering Tasks: Opportunities, Risks, and Implications",IEEE Software,40,3,2023,10.1109/MS.2023.3248401,19374194,
Striuk2023,Andrii M. Striuk,Enhancing software engineering education in higher education institutions through cloud-based learning tools: methodological and practical perspectives,Educational Dimension,8,,2023,10.31812/ed.600,2708-4604,"Objective: This study aims to develop a comprehensive methodology for integrating cloud-based learning tools into software engineering education for students in higher education institutions (HEIs).Research tasks: The research encompasses exploring the psychological and pedagogical aspects of software engineering education in HEIs, investigating the influence of cloud technologies on higher education, designing a robust model for implementing a methodological system that incorporates cloud-based learning tools, and conducting an experimental implementation of the proposed model.Research focus: The research focuses on the teaching and learning process of software engineering disciplines among students in HEIs, specifically emphasizing the utilization of cloud-based learning tools.Research methods: The research methodology involves analyzing relevant scholarly publications, dissertations, conference proceedings, and seminars, along with studying the experiences of educators through surveys and questionnaires. Additionally, the research incorporates practical experimentation with cloud-based learning tools.Research findings: The study highlights the crucial need to prioritize fundamental knowledge acquisition in software engineering education, which can be facilitated by the mobility features inherent in cloud-based learning tools. Furthermore, it reveals a significant correlation between emerging trends in information technology and the development of methodological teaching systems. The analysis underscores the transformative impact of cloud technologies on learning tools and other key components of the methodological teaching system. The research successfully implements an experimental model that integrates cloud-based learning tools into the methodological system.Conclusions and recommendations: The findings demonstrate that cloud technologies profoundly influence the technological aspects of the methodological system while simultaneously shaping the objectives and curriculum of IT professionals. The developed methodological system, which incorporates cloud technologies, plays a pivotal role in fostering the foundational education of IT professionals."
Gabor2020,Thomas Gabor and Andreas Sedlmeier and Thomy Phan and Fabian Ritz and Marie Kiermeier and Lenz Belzner and Bernhard Kempter and Cornel Klein and Horst Sauer and Reiner Schmid and Jan Wieghardt and Marc Zeller and Claudia Linnhoff-Popien,The scenario coevolution paradigm: adaptive quality assurance for adaptive systems,International Journal on Software Tools for Technology Transfer,22,4,2020,10.1007/s10009-020-00560-5,14332787,"Systems are becoming increasingly more adaptive, using techniques like machine learning to enhance their behavior on their own rather than only through human developers programming them. We analyze the impact the advent of these new techniques has on the discipline of rigorous software engineering, especially on the issue of quality assurance. To this end, we provide a general description of the processes related to machine learning and embed them into a formal framework for the analysis of adaptivity, recognizing that to test an adaptive system a new approach to adaptive testing is necessary. We introduce scenario coevolution as a design pattern describing how system and test can work as antagonists in the process of software evolution. While the general pattern applies to large-scale processes (including human developers further augmenting the system), we show all techniques on a smaller-scale example of an agent navigating a simple smart factory. We point out new aspects in software engineering for adaptive systems that may be tackled naturally using scenario coevolution. This work is a substantially extended take on Gabor et al. (International symposium on leveraging applications of formal methods, Springer, pp 137–154, 2018)."
Dubey2020,Anshu Dubey and Jared O’Neal and Klaus Weide and Saurabh Chawdhary,Distillation of Best Practices from Refactoring FLASH for Exascale,SN Computer Science,1,4,2020,10.1007/s42979-020-0077-x,26618907,"FLASH is a multiphysics software package that was created in 1998 by combining three preexisting packages and has undergone three major revisions. Software design and engineering practices were integrated early in the development and maintenance processes of FLASH, and these processes have evolved strongly at each of the revisions. As high-performance computing enters the age of exascale, challenges along the orthogonal axes of node-level hardware and solver heterogeneity force developers of complex multiphysics software to consider a software architecture overhaul. Because of the nature and scope of necessary changes, an effort to refactor and grow the architecture of the FLASH code has been launched as a separate software project. For this project to succeed, its development team must evaluate, improve, and modernize software processes and policies to meet the unique challenges posed by the exascale era. We describe here our experiences, lessons we have learned, and the methods that we have developed as part of this ongoing project. Within the context of the challenges posed by exascale, we review the FLASH design approach as well as some of the main software engineering processes and tools that have been implemented or updated throughout the lifetime of FLASH. Modernization applied to these processes and tools is also detailed. Reviewing and reevaluating the FLASH experience of establishing and updating software design and engineering practices have been helpful in understanding the needs of the project as it transitions to exascale and in planning the transition. We find that our historical design methodology is still important and relevant. We also believe that using a mixture of plan-based and agile methods is still the best for our project and is in accord with the guidance found in the literature. We present a section on inferences and lessons learned related to software design and engineering practices."
Li2023,Dian Li and Weidong Wang and Yang Zhao,Intelligent Visual Representation for Java Code Data in the Field of Software Engineering Based on Remote Sensing Techniques,Electronics (Switzerland),12,24,2023,10.3390/electronics12245009,20799292,"In the field of software engineering, large and complex code bases may lead to some burden of understanding their structure and meaning for developers. To reduce the burden on developers, we consider a code base visualization method to visually express the meaning of code bases. Inspired by remote sensing imagery, we employ graphical representations to illustrate the semantic connections within Java code bases, aiming to help developers understand its meaning and logic. This approach is segmented into three distinct levels of analysis. First, at the project-level, we visualize Java projects by portraying each file as an element within a code forest, offering a broad overview of the project’s structure. This macro-view perspective aids in swiftly grasping the project’s layout and hierarchy. Second, at the file-level, we concentrate on individual files, using visualization techniques to highlight their unique attributes and complexities. This perspective enables a deeper understanding of each file’s structure and its role within the larger project. Finally, at the component-level, our focus shifts to the detailed analysis of Java methods and classes. We examine these components for complexity and other specific characteristics, providing insights that are crucial for the optimization of code and the enhancement of software quality. By integrating remote sensing technology, our method offers software engineers deeper insights into code quality, significantly enhancing the software development lifecycle and its outcomes."
Dominic2020,James Dominic and Charles Ritter and Paige Rodeghero,Onboarding bot for newcomers to software engineering,,,,2020,10.1145/3379177.3388901,,"Software development teams dedicate considerable resources to training newcomers. Newcomers are new developers to a software project. The software onboarding process is more complicated than onboarding into other organizations. It is much more challenging and time-consuming. The role of a mentor in onboarding newcomers in software engineering is well understood. However, the disruptions to the work of an experienced developer can reduce the quality of their work and job satisfaction. We propose a conversational bot that can help onboard newcomers to a software project instead of an experienced programmer. The bot will act as a mentor for the newcomer, thus putting less stress on experienced programmers. The bot will also be able to scan outside sources, such as stack overflow, for solutions to issues a newcomer may face. The newcomer will be able to interact with the bot using natural language. We will use this bot to assess improvements to code quality in future studies."
Petrovska2024,Olga Petrovska and Lee Clift and Faron Moller and Rebecca Pearsall,Incorporating Generative AI into Software Development Education,,,,2024,10.1145/3633053.3633057,,"This paper explores how Generative AI can be incorporated into software development education. We present examples of formative and summative assessments, which explore various aspects of ChatGPT, including its coding capabilities, its ability to construct arguments as well as ethical issues of using ChatGPT and similar tools in education and the workplace. Our work is inspired by the insights from surveys that show that the learners on our Degree Apprenticeship Programme have a great interest in learning about and exploiting emerging AI technology. Similarly, our industrial partners have a clear interest for their employees to be formally prepared to use GenAI in their software engineering roles. In this vein, it is proposed that embedding the use of GenAI tools in a careful and creative way-by developing assessments which encourage learners to critically evaluate AI output-can be beneficial in helping learners understand the subject material being taught without the risk of the AI tools ""doing the homework""."
TOKDEMR2021,Gul TOKDEMİR,Using Text Mining For Research Trends in Empirical Software Engineering,Politeknik Dergisi,24,3,2021,10.2339/politeknik.831391,,"This paper intends to examine the research trends in Empirical Software Engineering domain within the last two decades using text mining. It studies published articles in the relevant literature with an emphasis on abstracts of 10658 articles published in the literature on Experimental Software Engineering domain. Using a probabilistic topic modelling technique (Latent Dirichlet Allocation), it brings forward the main topics of research within this domain. By further analysis, the paper evaluates the changes of focus in published works in the last two decades and depicts the recent trends in research content wise. Through a timely comparison, it portrays the alteration of interest within empirical software engineering research and proposes a future research agenda to develop an advanced field, beneficial both for academics and practitioners.Bu çalışma Deneysel Yazılım Mühendisliği alanında son yirmi yılda ki araştırma eğilimlerini metin madenciliği tekniklerini kullanarak incelemeyi amaçlamaktadır. Makale özetleri göz önünde bulundurularak, Deneysel Yazılım Mühendisliği ile ilgili literatürde yayınlanmış 10658 makale incelenmiştir. İstatistiksel bir modelleme tekniği olan (Latent Dirichlet Allocation) kullanılarak, bu alandaki temel araştırma konuları bulunarak karşılaştırılmalı olarak incelenmiştir. Bu makalede son yirmi yıl içinde yayınlanmış çalışmalardaki odak değişiklikleri değerlendirilmekte ve araştırma içeriğindeki son eğilimler ortaya çıkarılmaktadır. Karşılaştırmalı değerlendirme yoluyla, deneysel yazılım mühendisliği alanındaki araştırma eğilim değişikliği vurgulanarak, hem akademisyenler hem de uygulayıcılar için faydalı olabilecek ve bu alanın ilerlemesini sağlayacak araştırma gündemi önerilmektedir."
Felizardo2021,Katia Romero Felizardo and Amanda Möhring Ramos and Claudia de Claudia and Érica Ferreira de Souza and Nandamudi L. Vijaykumar and Elisa Yumi Nakagawa,Global and Latin American female participation in evidence-based software engineering: a systematic mapping study,Journal of the Brazilian Computer Society,27,1,2021,10.1186/s13173-021-00109-7,16784804,"Context: While the digital economy requires a new generation of technology for scientists and practitioners, the software engineering (SE) field faces a gender crisis. SE research is a global enterprise that requires the participation of both genders for the advancement of science and evidence-based practice. However, women across the world tend to be significantly underrepresented in such research, receiving less funding and less participation, frequently, than men as authors in research publications. Data about this phenomenon is still sparse and incomplete; particularly in evidence-based software engineering (EBSE), there are no studies that analyze the participation of women in this research area. Objective: The objective of this work is to present the results of a systematic mapping study (SM) conducted to collect and evaluate evidence on female researchers who have contributed to the area of EBSE. Method: Our SM was performed by manually searching studies in the major conferences and journals of EBSE. We identified 981 studies and 183 were authored/co-authored by women and, therefore, included. Results: Contributions from women in secondary studies have globally increased over the years, but it is still concentrated in European countries. Additionally, collaboration among research groups is still fragile, based on a few women as a bridge. Latin American researchers contribute a great deal to the field, despite they do not collaborate as much within their region. Conclusions: The findings from this study are expected to be aggregated to the existing knowledge with respect to women’s contribution to the EBSE area. We expect that our results bring up a reflection on the gender issue and motivate actions and policies to attract female researchers to this area."
Mellado2020,Rafael Mellado and Antonio Faúndez-Ugalde and María Blanco,THE AUDIT PROCESS IN COMPANIES THAT IMPLEMENT SOFTWARE ENGINEERING PROJECTS,"International Journal of Economics, Business and Accounting Research (IJEBAR)",4,02,2020,10.29040/ijebar.v4i02.1066,2622-4771,"The use of technological resources and software has become standardized in today's society, which is why there is a need to be able to update according to the requirements that the market and industry demand from companies that develop products through a software engineering process. The role of the auditor is extremely important since he is the one who must make sure that everything is controlled and that the required needs are being fulfilled, as well as he is concerned about the security of the entities and their internal background. In this context, it is necessary to constantly improve the auditor's procedures and the legislation that regulates them, since the multiple frauds that companies suffer in terms of information obtained easily and quickly, without any major control, are well known, and it is here where care must be taken in order to reduce the levels of violations of access to unauthorized information assets. The objective of this paper is to present everything that surrounds the process of auditing requirements of software engineering projects, both generically and specifically in projects in particular the financial area, generally covering everything that is present in a software engineering project considering the need, what is obtained from them and why they arise in organizations."
Wang2022,Hong Wang and Limin Yuan,Software engineering defect detection and classification system based on artificial intelligence,Nonlinear Engineering,11,1,2022,10.1515/nleng-2022-0042,21928029,"With the increasing reliance on automatic software-based applications, it is important to automate the classification of software defects and ensure software reliability. An automatic software defect classification system based on an expert system is proposed in this article. In this method, DACS first determines the category of software defects through the selection of typical features, then reduces the spatial knowledge base searched by the inference engine and selects the characteristics of a certain type of defect. Make a selection, determine the name of the defect, and finally select different causes and prevention methods for the defect as needed. The DACS structure was built, and the experiment showed that the AI system took 15 s to complete, whereas the traditional mechanism took 48 s; the accuracy of the AI was 99%, whereas the accuracy of the traditional mechanism was only 68%. According to the aforementioned experimental results, the recognition accuracy of the proposed research scheme is higher than that of the traditional mechanism. Hence, the time required to solve the problem of software engineering defect detection and classification is less than that of the traditional mechanism."
Herbsleb2023,James Herbsleb,Global software engineering in the age of GitHub and zoom,Journal of Software: Evolution and Process,35,6,2023,10.1002/smr.2347,20477481,"Much has changed since the inaugural ICGSE conference in 2006. Tools have improved, awareness of cultural differences is widespread, and developments such as the foregrounding of open source have all enhanced our ability to work across geographic divides. But the pervasive and profound impact of software in the world—especially for societal scale systems such as social media—forces new and deeply challenging responsibilities on both developers and academics. We must find better ways of incorporating ethics into our development practices and pay far more attention to harmful unintended consequences as deployed systems interact with and often disrupt crucial social systems."
MubashirAli2020,Mubashir Ali,Requirement Gathering Techniques Widely Used in Global Software Engineering: A Comprehensive Study,Lahore Garrison University Research Journal of Computer Science and Information Technology,4,1,2020,10.54692/lgurjcsit.2020.0401139,2519-7991,"The research paper is solely based upon conducting the study with respect to analysis of the recent work done in the field of Global Software Engineering (GSE). It is massively spreading within various organizations. GSD is significant with respect to cost reduction, efficiency in context of delivery of software and extending economic resources. The working model of GSE is a complex and there are certain threats that are directly targeting the audiences. The factors affecting are the differences in context of Culture and geographic distances that can directly affect the performance of a software process. One of the important aspects that is kept in consideration in Global Requirement engineering is particularly communication. The issues with respect to communication can be divided further into theteam and targeted clients. The study is highlighting the concept is to generate the best practices in Global software engineering (GSE) along with Lexicon model. The research has focused on more than one technique along with demonstrating their advantages to present a solution and a technique that can be adopted to generate requirements in GSE."
Maylawati2022,Dian Sa’adillah Maylawati and Muhammad Ali Ramdhani,Logical Framework of Information Technology: Systematization of Software Development Research,Telfor Journal,14,1,2022,10.5937/telfor2201026S,23349905,"This article aims to present a comprehensive Logical Framework for Information Technology (IT) Research, specifically for developing customized IT applications or software. The methodology of writing this article uses a content analysis with the main source of literature review, Focus Discussion Group, and also based on the experience and knowledge of the authors as lecturers of Software Engineering and Software Project Management. This article shows that although current IT development approaches or methodologies (especially software development methodology) continue to develop, good IT design is carried out through six main stages, namely planning, analysis, design, construction, implementation, and maintenance. The success of IT implementation depends on the good process of all stages of IT design. The involvement of all actors/ stakeholders in IT design is essential to be accommodated at all stages of IT design. Quality also becomes the main goal and controls every process of IT development"
Robyanto2022,Eka Robyanto and Aji Wibawa and Khurin Nabila,Tantangan Generasi Muda untuk Terjun Kedalam Bidang Software Engineering pada Society 5.0,Jurnal Inovasi Teknologi dan Edukasi Teknik,2,1,2022,10.17977/um068v2i12022p27-30,,"Society 5.0 brings significant changes not only on the technical side which provides new challenges for merging the virtual world with the physical world, but also brings changes to the structure and business processes of a company. Because of this, companies are also competing to get experts in the field of Software Engineering or software engineers so they can follow and adapt to the changes brought about by society 5.0. Developments in the world of Information technology (IT) have made the world move towards the era of the Industrial revolution 4.0, this has made IT one of the key elements in everyday life in that era. This makes a career as a software engineer look promising for the younger generation who are confused about determining their career path. In choosing a career to become a software engineer, there are challenges that must be faced by the younger generation in society 5.0. This article will conduct research on the challenges that must be faced and try to find solutions to solve these challenges.
Society 5.0 membawa perubahan signifikan yang bukan hanya pada sisi teknis yang memberikan tantangan baru untuk melakukan penggabungan dunia maya dengan dunia fisik, akan tetapi juga membawa perubahan terhadap struktur dan juga proses bisnis dari sebuah perusahaan. Dikarenakan hal tersebut, perusahaan juga berlomba-lomba untuk mendapatkan tenaga ahli dalam bidang Software Engineering atau para software engineer agar dapat mengikuti dan beradaptasi dengan perubahan-perubahan yang dibawa oleh society 5.0. Perkembangan dalam dunia teknologi Informasi (IT) telah membuat dunia beranjak menuju era revolusi Industri 4.0, hal ini membuat IT menjadi salah satu elemen kunci dalam kehidupan sehari-hari pada era tersebut. Hal ini membuat karir menjadi seorang software engineer terlihat menjanjikan untuk generasi muda yang sedang kebingungan dalam menentukan perjalanan karirnya. Dalam memilih karir menjadi seorang software engineer terdapat tantangan-tantangan yang harus dihadapi oleh para generasi muda di society 5.0, dalam artikel ini akan dilakukan penelitian mengenai tantangan-tantangan yang harus dihadapi serta mencoba untuk mencari solusi untuk menyelesaikan tantangan tersebut."
Andriyani2022,Yanti Andriyani and Ibnu Daqiqil Id and Evfi Mahdiyah and Al Aminuddin,Use Case Realization in Software Reverse Engineering,Ingenierie des Systemes d'Information,27,2,2022,10.18280/isi.270218,21167125,"The Use Case Diagram (UCD) is a visual form of system design that helps software developers comprehend the system behavior. Maintaining and updating the system can be a difficult task when it has no visualization of a system behavior or software requirement specification document. Reverse engineering is an approach used to extract software requirement specifications from the existing systems. Research in reverse engineering has shown various techniques in which the processes are not fully understood. This study analyzes the University Community Services Information System (UCSIS) as the existing system in three processes: identifying the system domain process, elaborating system features by implementing the event table, and constructing the use case realization. The results showed that a UCD could be generated through the reverse engineering process on the existing system. Furthermore, a new feature for system improvement can also be detected using this method. It is expected that the reverse engineering approach in this study can be used as guidance for the software development team in extracting the use case diagram from the existing systems."
Daun2021,Marian Daun and Jennifer Brings and Patricia Aluko Obe and Viktoria Stenkova,Reliability of self-rated experience and confidence as predictors for students’ performance in software engineering: Results from multiple controlled experiments on model comprehension with graduate and undergraduate students,Empirical Software Engineering,26,4,2021,10.1007/s10664-021-09972-6,15737616,"Students’ experience is used in empirical software engineering research as well as in software engineering education to group students in either homogeneous or heterogeneous groups. To do so, students are commonly asked to self-rate their experience, as self-rated experience has been shown to be a good predictor for performance in programming tasks. Another experience-related measurement is participants’ confidence (i.e., how confident is the person that their given answer is correct). Hence, self-rated experience and confidence are used as selector or control variables throughout empirical software engineering research and software engineering education. In this paper, we analyze data from several student experiments conducted in the past years to investigate whether self-rated experience and confidence are also good predictors for students’ performance in model comprehension tasks. Our results show that while students can somewhat assess the correctness of a particular answer to one concrete question regarding a conceptual model (i.e., their confidence), their overall self-rated experience does not correlate with their actual performance. Hence, the use of the commonly used measurement of self-rated experience as a selector or control variable must be considered unreliable for model comprehension tasks."
Deschamps2023,Joran Deschamps and Damian Dalle Nogare and Florian Jug,Better research software tools to elevate the rate of scientific discovery or why we need to invest in research software engineering,Frontiers in Bioinformatics,3,,2023,10.3389/fbinf.2023.1255159,26737647,
Sillaber2021,Christian Sillaber and Bernhard Waltl and Horst Treiblmaier and Ulrich Gallersdörfer and Michael Felderer,Laying the foundation for smart contract development: an integrated engineering process model,Information Systems and e-Business Management,19,3,2021,10.1007/s10257-020-00465-5,16179854,"Smart contracts are seen as the major building blocks for future autonomous blockchain- and Distributed Ledger Technology (DLT)-based applications. Engineering such contracts for trustless, append-only, and decentralized digital ledgers allows mutually distrustful parties to transform legal requirements into immutable and formalized rules. Previous experience shows this to be a challenging task due to demanding socio-technical ecosystems and the specificities of decentralized ledger technology. In this paper, we therefore develop an integrated process model for engineering DLT-based smart contracts that accounts for the specificities of DLT. This model was iteratively refined with the support of industry experts. The model explicitly accounts for the immutability of the trustless, append-only, and decentralized DLT ecosystem, and thereby overcomes certain limitations of traditional software engineering process models. More specifically, it consists of five successive and closely intertwined phases: conceptualization, implementation, approval, execution, and finalization. For each phase, the respective activities, roles, and artifacts are identified and discussed in detail. Applying such a model when engineering smart contracts will help software engineers and developers to better understand and streamline the engineering process of DLTs in general and blockchain in particular. Furthermore, this model serves as a generic framework which will support application development in all fields in which DLT can be applied."
Sankaranarayanan2020,Sreecharan Sankaranarayanan and Siddharth Reddy Kandimalla and Mengxin Cao and Ignacio Maronna and Haokang An and Chris Bogart and R. Charles Murray and Michael Hilton and Majd Sakr and Carolyn Penstein Rosé,Designing for learning during collaborative projects online: tools and takeaways,Information and Learning Science,121,7-8,2020,10.1108/ILS-04-2020-0095,23985348,"Purpose: In response to the evolving COVID-19 pandemic, many universities have transitioned to online instruction. With learning promising to be online, at least in part, for the near future, instructors may be thinking of providing online collaborative learning opportunities to their students who are increasingly isolated from their peers because of social distancing guidelines. This paper aims to provide design recommendations for online collaborative project-based learning exercises based on this research in a software engineering course at the university level. Design/methodology/approach: Through joint work between learning scientists, course instructors and software engineering practitioners, instructional design best practices of alignment between the context of the learners, the learning objectives, the task and the assessment are actualized in the design of collaborative programming projects for supporting learning. The design, first segments a short real-time collaborative exercise into tasks, each with a problem-solving phase where students participate in collaborative programming, and a reflection phase for reflecting on what they learned in the task. Within these phases, a role-assignment paradigm scaffolds collaboration by assigning groups of four students to four complementary roles that rotate after each task. Findings: By aligning each task with granular learning objectives, significant pre- to post-test learning from the exercise as well as each task is observed. Originality/value: The roles used in the paradigm discourage divide-and-conquer tendencies often associated with collaborative projects. By requiring students to discuss conflicting ideas to arrive at a consensus implementation, their ideas are made explicit, thus providing opportunities for clarifying misconceptions through discussion and learning from the collaboration."
Yuste2024,Javier Yuste and Eduardo G. Pardo and Abraham Duarte,General Variable Neighborhood Search for the optimization of software quality,Computers and Operations Research,165,,2024,10.1016/j.cor.2024.106584,03050548,"In the area of Search-Based Software Engineering, software engineering issues are formulated and tackled as optimization problems. Among the problems within this area, the Software Module Clustering Problem (SMCP) consists of finding an organization of a software project that minimizes coupling and maximizes cohesion. Since modular code is easier to understand, the objective of this problem is to increase the quality of software projects, thus increasing their maintainability and reducing the associated costs. In this work we study a recently proposed objective function named Function of Complexity Balance (FCB). Since this problem has been demonstrated to be NP-hard, we propose a new heuristic algorithm based on the General Variable Neighborhood Search (GVNS) schema to tackle the problem. For the GVNS, we propose six different neighborhood structures and categorize them into three different groups. Then, we analyze their contribution to the results obtained by the algorithm. In order to improve the efficiency of the proposed approach, we leverage domain-specific information to perform incremental evaluations of the objective function and to explore only areas of interest in the search space. The proposed algorithm has been tested over a set of real world software repositories, achieving better results than the previous state-of-the-art method, a Hybrid Genetic Algorithm, in terms of both quality and computing times. Furthermore, the relevance of the improvement produced by our proposal has been corroborated by non-parametric statistical tests."
Kazman2020,Rick Kazman and Liliana Pasquale,Software Engineering in Society,IEEE Software,37,1,2020,10.1109/MS.2019.2949322,19374194,
Prez-Castillo2022,Ricardo Pérez-Castillo and Mario Piattini,Design of classical-quantum systems with UML,Computing,104,11,2022,10.1007/s00607-022-01091-4,14365057,"Developers of the many promising quantum computing applications that currently exist are urging companies in many different sectors seriously consider integrating this new technology into their business. For these applications to function, not only are quantum computers required, but quantum software also. Accordingly, quantum software engineering has become an important research field, in that it attempts to apply or adapt existing methods and techniques (or propose new ones) for the analysis, design, coding, and testing of quantum software, as well as playing a key role in ensuring quality in large-scale productions. The design of quantum software nevertheless poses two main challenges: the modelling of software quantum elements must be done in high-level modelling languages; and the need to further develop so-called “hybrid information systems”, which combine quantum and classical software. To address these challenges, we first propose a quantum UML profile for analysing and designing hybrid information systems; we then demonstrate its applicability through various structural and behavioural diagrams such as use case, class, sequence, activity, and deployment. In comparison to certain other quantum domain-specific languages, this UML profile ensures compliance with a well-known international standard that is supported by many tools and is followed by an extensive community."
Barisch-Fritz2023,Bettina Barisch-Fritz and Claudio R. Nigg and Marc Barisch and Alexander Woll,App development in a sports science setting: A systematic review and lessons learned from an exemplary setting to generate recommendations for the app development process,Frontiers in Sports and Active Living,4,,2023,10.3389/fspor.2022.1012239,26249367,"The digital health sector is rapidly growing. With only 4% of publishers out of academic settings, it is under-represented in app development. The objective of this study is to assess the current state of app development with a systematic review and a survey within an exemplary academic setting along the following research questions: (Q1) Are software engineering principles sufficiently known in the sports science app development context? (Q2) Is the role of sports scientists in the context of app development sufficiently understood? The systematic review was conducted by two independent reviewers within databases Pubmed, Scopus, Web of Science, and IEEE Xplore. The PICO schema was used to identify the search term. We subtracted information about five main topics: development process, functional requirements and features, security, technology, and dissemination. The survey was developed by a multidisciplinary team and focused on five main topics. Out of 701 matches, 21 were included in the review. The development process was only described in seven studies. Functional requirements and features were considered in 11 studies, security in 3, technology in 13, and dissemination in 12 with varying details. Twelve respondents [mean age 33(7) years, 58% women] replied to the survey. The survey revealed limited knowledge in realization of security measures, underlying technology and source code management, and dissemination. Respondents were able to provide input on development processes as well as functional requirements and features. The involvement of domain experts is given in seven review studies and described in two more. In 50% of survey respondents, the role in app development is defined as a research assistant. We conclude that there is a varying degree of software engineering knowledge in the sports science app development context (Q1). Furthermore, we found that the role of sports scientists within app development is not sufficiently defined (Q2). We present recommendations for improving the success probability and sustainability of app development and give orientation on the potential roles of sports scientists as domain experts. Future research should focus on the generalizability of these findings and the reporting of the app development process."
Ozkaya2021,Ipek Ozkaya,The Future of Software Engineering Work,IEEE Software,38,5,2021,10.1109/MS.2021.3089729,19374194,
Vitianingsih2021,Anik Vega Vitianingsih and Nanna Suryana and Zahriah Othman,Spatial analysis model for traffic accident-prone roads classification: A proposed framework,IAES International Journal of Artificial Intelligence,10,2,2021,10.11591/ijai.v10.i2.pp365-373,22528938,The classification method in the spatial analysis modeling based on the multi-criteria parameter is currently widely used to manage geographic information systems (GIS) software engineering. The accuracy of the proposed model will play an essential role in the successful software development of GIS. This is related to the nature of GIS used for mapping through spatial analysis. This paper aims to propose a framework of spatial analysis using a hybrid estimation model-based on a combination of multi-criteria decision-making (MCDM) and artificial neural networks (ANNs) (MCDM-ANNs) classification. The proposed framework is based on the comparison of existing frameworks through the concept of a literature review. The model in the proposed framework will be used for future work on the traffic accident-prone road classification through testing with a private or public spatial dataset. Model validation testing on the proposed framework uses metaheuristic optimization techniques. Policymakers can use the results of the model on the proposed framework for initial planning developing GIS software engineering through spatial analysis models.
Srivastava2023,Prateek Srivastava and Nidhi Srivastava and Rashi Agarwal and Pawan Singh,An Intelligent Framework for Estimating Software Development Projects using Machine Learning,International Journal on Recent and Innovation Trends in Computing and Communication,11,5,2023,10.17762/ijritcc.v11i5.6602,23218169,"The IT industry has faced many challenges related to software effort and cost estimation. A cost assessment is conducted after software effort estimation, which benefits customers as well as developers. The purpose of this paper is to discuss various methods for the estimation of software effort and cost in the context of software engineering, such as algorithmic methods, expert judgment methods, analogy-based estimation methods, and machine learning methods, as well as their different aspects. In spite of this, estimation of the effort involved in software development are subject to uncertainty. Several methods have been developed in the literature for improving estimation accuracy, many of which involve the use of machine learning techniques. A machine learning framework is proposed in this paper to address this challenging problem. In addition to being completely independent of algorithmic models and estimation problems, this framework also features a modular architecture. It has high interpretability, learning capability, and robustness to imprecise and uncertain inputs."
Araos2023,Andrés Araos and Crina Damşa and Dragan Gašević,Browsing to learn: How computer and software engineering students use online platforms in learning activities,Journal of Computer Assisted Learning,39,2,2023,10.1111/jcal.12774,13652729,"Background: The surge of online platforms has generated interest in how specialized platforms support formal and informal learning in various disciplinary domains. Knowledge is still limited regarding how undergraduate students navigate and use platforms to learn. Objectives: This study explores computer and software engineering students' learning practices, wherein online platforms are used as resources for both curricular learning activities and students' interest-driven learning. Methods: The learning practices of 73 students were examined using a mixed-methods design and a conceptualization of practices accounting for the context and purpose of their enactment. The dataset includes students' self-reports on domain-specific learning activities, three-month-long web-browsing history of multiple platforms and stimulated-recall interviews. The data were analysed through latent class and thematic analyses. Results and Conclusion: Five distinct patterns were found in the use of online platforms. These patterns show that different types of platforms were used purposely and in combination during curricular and interest-driven activities aimed at learning software development. Moreover, students' purposes were driven by both the need to progress in their learning activities and the development of their interest in software development. The findings highlight the complexity of students' learning with online platforms, which develop quite extensively beyond curricular boundaries. Implications: The findings stress the need to recognize that undergraduate students' learning practices involve multiple online platforms and activities beyond the formal curriculum and that these play a key role in developing students' interests in learning software development. Moreover, our findings indicate the importance of taking into account the way students' learning practices unfold within platforms and how these relate to domain-specific practices."
Parnas2021,David Lorge Parnas,Software Engineering: A Profession in Waiting,Computer,54,5,2021,10.1109/MC.2021.3057685,15580814,"Ongoing efforts to make software development an engineering discipline will fail until we have legislation requiring that creators of certain types of software be licensed, establishing a licensing authority, and detailing the capabilities that a licensed developer must possess."
Gary2020,Kevin A. Gary and Ruben Acuna and Alexandra Mehlhase and Robert Heinrichs and Sohum Sohoni,SCALING TO MEET THE ONLINE DEMAND IN SOFTWARE ENGINEERING,International Journal on Innovations in Online Education,4,1,2020,10.1615/intjinnovonlineedu.2020032599,2377-9519,"… We ask students to submit a survey with information about their knowledge, time zone … S., and Lindquist, T., Itʼs Not What You Think: Lessons Learned Developing an Online Software Engineering … on Computer and Software Engineering Education and Training (CSEE&T), 2017 …"
Medeiros2021,Júlio Medeiros and Ricardo Couceiro and Gonçalo Duarte and João Durães and João Castelhano and Catarina Duarte and Miguel Castelo-Branco and Henrique Madeira and Paulo de Carvalho and César Teixeira,Can EEG be adopted as a neuroscience reference for assessing software programmers’ cognitive load?,Sensors,21,7,2021,10.3390/s21072338,14248220,"An emergent research area in software engineering and software reliability is the use of wearable biosensors to monitor the cognitive state of software developers during software development tasks. The goal is to gather physiologic manifestations that can be linked to error-prone scenarios related to programmers’ cognitive states. In this paper we investigate whether electroen-cephalography (EEG) can be applied to accurately identify programmers’ cognitive load associated with the comprehension of code with different complexity levels. Therefore, a controlled experiment involving 26 programmers was carried. We found that features related to Theta, Alpha, and Beta brain waves have the highest discriminative power, allowing the identification of code lines and demanding higher mental effort. The EEG results reveal evidence of mental effort saturation as code complexity increases. Conversely, the classic software complexity metrics do not accurately represent the mental effort involved in code comprehension. Finally, EEG is proposed as a reference, in par-ticular, the combination of EEG with eye tracking information allows for an accurate identification of code lines that correspond to peaks of cognitive load, providing a reference to help in the future evaluation of the space and time accuracy of programmers’ cognitive state monitored using wearable devices compatible with software development activities."
V2023,Maheshwari V. and Prasanna M.,Vulnerabilities and attacks on the blockchain software engineering landscape,Applied and Computational Engineering,6,1,2023,10.54254/2755-2721/6/20230851,2755-2721,"Blockchain is also known as Distributed Ledger Technology (DLT) and real transparencies of the history of digital assets by decentralization and encryption. It guarantees that the user's data never be erased, making it impossible to alter or falsify. Some people know that the ""Blockchain revolution"" can be compared with the internet and the web in their early days. As a result, all software development around blockchain is growing incredibly. Most software engineers are interested in blockchain technologies as they rush to develop unregulated software. Although some research has been performed on blockchain security and privacy concerns, a thorough analysis state of blockchain security is lacking. This article explores current problems and new principles for blockchain-oriented software engineering (BOSE) and discusses the need for new software engineering practices in the blockchain industry. Also, examine the solutions to improve blockchain protection, which might have been used to develop different blockchain applications, and suggest a few potential directions for moving research into this area."
Mumtaz2022,Mamoona Mumtaz and Naveed Ahmad and M. Usman Ashraf and Ahmed Mohammed Alghamdi and Adel A. Bahaddad and Khalid Ali Almarhabi,"Iteration Causes, Impact, and Timing in Software Development Lifecycle: An SLR",IEEE Access,10,,2022,10.1109/ACCESS.2022.3182703,21693536,"Context: Iteration - performing an activity once it has already been done - is unavoidable and omnipresent during software development. Management of iteration is a challenging task due to the lack of detailed analysis and use of different terms for the iteration at different places in software engineering. Objective: In order to manage iteration in a better way during software development processes, we investigate different iterative situations, the causes, the stages at which it can occur during the development, and its impacts. Method: We use the systematic literature review (SLR) method to search using six bibliographic databases. The SLR includes 153 primary studies published from 2007 to February 2017. Results: We identify twenty-two leading causes, five stages, and positive and negative impacts of iteration. Then, we develop a lucid taxonomy of iteration perspectives based on the causes and timing at which it occurs during the software development lifecycle. Conclusions: The frequently reported causes of iteration are defects, code smell, and conflicts, whereas the least referenced causes are poor management and different methods followed by teams. The most cited phase at which iteration occurs during the software development is maintenance. The most cited positive consequence of iteration is quality improvement, whereas the negative impacts of iteration are increasing time, effort, and cost. Our study provides a framework to understand the nature of iteration, what sources can lead to which iterative perspective, a particular iterative situation can have what kind of impacts on project milestones, and also provide research directions."
Iyenghar2022,Padma Iyenghar and Friedrich Otte and Elke Pulvermueller,AI-guided Model-Driven Embedded Software Engineering,,,,2022,10.5220/0011006200003119,21844348,"In this paper, an use case of Artificial Intelligence (AI) empowered Model Driven Engineering (MDE) in the field of Embedded Software Engineering (ESE) is introduced. In this context, we propose to qualify MDE tools for ESE with an AI assistant or a chatbot. The requirements for the first version of such an assistant and the design concepts involved are discussed. A prototype of such an assistant developed using an open source conversational AI framework used in tandem with an MDE tool for ESE is presented. Empowering MDE tools with such AI assistants, would aid novices in MDE or even non-programmer to learn and adopt model-driven ESE with a not-so-steep learning curve."
Rosca2021,Dionisie Rosca and Luísa Domingues,A systematic comparison of roundtrip software engineering approaches applied to UML class diagram,,181,,2021,10.1016/j.procs.2021.01.240,18770509,"Model-based software engineering contemplates several software developments approaches in which models play an important role. One such approach is round-trip engineering. The objective of this paper is to benchmark the comparative analysis of the round-trip engineering capability of three modelling tools: Papyrus, Modelio and Visual Paradigm. The conclusions drawn throughout the paper will answer the question: How effective are current code generation tools for documenting application evolution? Throughout the discussion, we have pointed out several improvements that these UML modelling tools have recently brought to the discussion. We have also proposed some improvements that we consider desirable to improve Roundtrip Engineering approaches pointing out possible directions and solutions."
Singh2020,Jagdeep Singh and Sachin Bagga and Ranjodh Kaur,Software-based Prediction of Liver Disease with Feature Selection and Classification Techniques,,167,,2020,10.1016/j.procs.2020.03.226,18770509,"Today's health care is very important aspect for every human, so there is a need to provide medical services that are easily available to everyone. In this paper, the main focus is to predict the liver disease based on a software engineering approach using classification and feature selection technique. The implementation of proposed work is done on Indian Liver Patient Dataset (ILPD) from the University of California, Irvine database. The different attributes like age, direct bilirubin, gender, total bilirubin, Alkphos, sgpt, albumin, globulin ratio and sgot etc, of the liver patient dataset, are used to predict the liver diseases risk level. The various classification algorithms such as Logistic Regression, SMO, Random Forest algorithm, Naive Bayes, J48 and k-nearest neighbor (IBk) are implemented on the Liver Patient dataset to find the accuracy. The comparison different classifier results are done of feature selection and without using feature selection technique. The development of intelligent liver disease prediction software (ILDPS) is done by using feature selection and classification prediction techniques based on software engineering model."
Madhi2023,Klaudia Madhi and Lara Marie Reimer and Stephan Jonas,Attribution-based Personas in Virtual Software Engineering Education,,,,2023,10.1109/ICSE-SEET58685.2023.00028,02705257,"The COVID-19 pandemic and the consequent introduction of virtual collaboration have introduced educators to unexpected situations and challenges. One of these challenges is social distance, which minimizes knowledge of another person's character, and leaves room for misconceptions. Perceptions of a person's personality are also referred to as dispositional attributions and, when misplaced, impact the educator-student dynamics. This paper studies dispositional attributions exhibited by software engineering educators in higher education and aims to raise awareness of potential misconceptions affecting the educator-student relationship caused by the virtual setting. We performed an exploratory case study in a practical university course with twelve distributed software engineering teams, each led by one or two educators. The course was conducted entirely virtually during the COVID-19 pandemic. The research process included discovering, categorizing, and modeling attribution-based personas, followed by qualitative and quantitative research methods of semi-structured interviews and survey questionnaires. These personas represent the subjects of potential misconceptions and encapsulate typical behaviors and attributions. Our research created seven personas: the Unprofessional, Ego is the Enemy, The Detached, the Loner, the Underperformer, Hiding but not Seeking, and Distraction Monster. These personas differ primarily in terms of character traits and motivation attributed to them. The results provide evidence that the virtual setting of the course can lead to several dispositional attributions. Educators in virtual software engineering settings should be aware of these attributions and their potential impact on the educator-student relationship."
Chen2022,Xingru Chen and Deepika Badampudi and Muhammad Usman,Reuse in Contemporary Software Engineering Practices – An Exploratory Case Study in A Medium-sized Company,E-Informatica Software Engineering Journal,16,1,2022,10.37190/e-Inf220110,20844840,"Background: Software practice is evolving with changing technologies and practices such as InnerSource, DevOps, and microservices. It is important to investigate the impact of contemporary software engineering (SE) practices on software reuse. Aim: This study aims to characterize software reuse in contemporary SE practices and investigate its implications in terms of costs, benefits, challenges, and potential improvements in a medium-sized company. Method: We performed an exploratory case study by conducting interviews, group discussions, and reviewing company documentation to investigate software reuse in the context of contemporary SE practices in the case company. Results: The results indicate that the development for reuse in contemporary SE practices incurs additional coordination, among other costs. Development with reuse led to relatively fewer additional costs and resulted in several benefits such as better product quality and less development and delivery time. Ownership of reusable assets is challenging in contemporary SE practice. InnerSource practices may help mitigate the top perceived challenges: discoverability and ownership of the reusable assets, knowledge sharing and reuse measurement. Conclusion: Reuse in contemporary SE practices is not without additional costs and challenges. However, the practitioners perceive costs as investments that benefit the company in the long run."
Kannengiesser2022,Udo Kannengiesser and John S. Gero,"What distinguishes a model of systems engineering from other models of designing? An ontological, data-driven analysis",Research in Engineering Design,33,2,2022,10.1007/s00163-021-00382-9,14356066,"This paper investigates how the core technical processes of the INCOSE model of systems engineering differ from other models of designing used in the domains of mechanical engineering, software engineering and service design. The study is based on fine-grained datasets produced using mappings of the different models onto the function-behaviour-structure (FBS) ontology. By representing every model uniformly, the same statistical analyses can be carried out independently of the domain of the model. Results of correspondence analysis, cumulative occurrence analysis and Markov model analysis show that the INCOSE model differs from the other models in its increased emphasis on requirements and on behaviours derived from structure, in the uniqueness of its verification and validation phases, and in some patterns related to the temporal development and frequency distributions of FBS design issues."
Yue2023,Ping Yue and Zhiguo Wang,Improved Multi-objective Particle Swarm Optimization in Software Engineering Supervision,Decision Making: Applications in Management and Engineering,7,2,2023,10.31181/dmame7220241074,26200104,"In the 21st century, the software industry has achieved great development, and the development complexity and volume of software projects are also continuously increasing. Therefore, the design of software engineering supervision network plans is becoming increasingly important. The Pareto optimal solution set construction method, global extremum selection method, and fitness value determination method of the multi-objective particle swarm optimization algorithm have been improved in response to the poor optimization performance and poor convergence and distribution of optimal solutions in existing network planning algorithms. However, traditional methods only optimize one or two objectives of network planning, resulting in inconsistency with actual engineering. Thus, this study establishes a multiobjective model based on resources, duration, cost, and quality for comprehensive optimization. Based on the results, the Pareto optimal solution curves obtained by the proposed algorithm on three classic test functions were consistent with the actual theoretical Pareto frontier curves. The proposed method was applied to engineering project examples. 10 solutions that met the schedule requirements were obtained. Most engineering projects had a quality of over 80%, which verified the practicality of the algorithm. The algorithm has achieved good results in optimizing engineering quality. Therefore, this model can consider various indicators such as resources and costs to obtain software engineering quality improvement plans. It has certain application potential."
Nabot2024,Ahmad Nabot,Software Component Selection: An Optimized Selection Criterion for Component-based Software Engineering (CBSE),International Arab Journal of Information Technology,21,2,2024,10.34028/iajit/21/2/4,23094524,"Component-Based Software Engineering (CBSE) is gaining popularity in software development due to time and cost limitations. As software applications have become integral to people’s lives, developing high-quality and user-friendly applications within a reasonable timeframe and budget has become increasingly challenging. Software development firms frequently use Commercial-Off-The-Shelf (COTS) components to address this challenge and reduce development costs and time. However, selecting appropriate components that meet customer requirements and integrate seamlessly with the target system is a complex task requiring considering the entire software system’s quality. This study investigates the critical factors that software industry practitioners and experts must consider when selecting software components. First, the author asked practitioners to identify the most important quality criteria for an online bookstore from a list using subjective judgment and evaluation grades. Then, the study employed the Evidential Reasoning (ER) approach to tackle the multi-level evaluations and information uncertainty associated with software component selection issues. The ER approach’s primary features, such as weight normalization, probability assessment, uncertainty management, and utility intervals, offer several benefits for COTS selection problems, including cost and time reduction, improved software reliability, effectiveness, and efficiency. This study assessed the quality criteria for an online bookstore using the ER approach and provided analysis results based on the approach's computational steps. Finally, the study ranked the four components according to their weights, evaluation grades, and belief degrees for selection."
EKER2020,Abdulkadir ŞEKER and Banu DİRİ and Halil ARSLAN and Fatih AMASYALI,Summarising big data: public GitHub dataset for software engineering challenges,Cumhuriyet Science Journal,41,3,2020,10.17776/csj.728932,2587-2680,"In open-source software development environments; textual, numerical, and relationship-based data generated are of interest to researchers. Various data sets are available for this data, which is frequently used in areas such as software engineering and natural language processing. However, since these data sets contain all the data in the environment, the problem arises in the terabytes of data processing. For this reason, almost all of the studies using GitHub data use filtered data according to certain criteria. In this context, using a different data set in each study makes a comparison of the accuracy of the studies quite difficult. In order to solve this problem, a common dataset was created and shared with the researchers, which would allow to work on many software engineering problems."
Ortu2020,Marco Ortu and Giuseppe Destefanis and Daniel Graziotin and Michele Marchesi and Roberto Tonelli,How do you Propose Your Code Changes? Empirical Analysis of Affect Metrics of Pull Requests on GitHub,IEEE Access,8,,2020,10.1109/ACCESS.2020.3002663,21693536,"Software engineering methodologies rely on version control systems such as git to store source code artifacts and manage changes to the codebase. Pull requests include chunks of source code, history of changes, log messages around a proposed change of the mainstream codebase, and much discussion on whether to integrate such changes or not. A better understanding of what contributes to a pull request fate and latency will allow us to build predictive models of what is going to happen and when. Several factors can influence the acceptance of pull requests, many of which are related to the individual aspects of software developers. In this study, we aim to understand how the affect (e.g., sentiment, discrete emotions, and valence-arousal-dominance dimensions) expressed in the discussion of pull request issues influence the acceptance of pull requests. We conducted a mining study of large git software repositories and analyzed more than 150,000 issues with more than 1,000,000 comments in them. We built a model to understand whether the affect and the politeness have an impact on the chance of issues and pull requests to be merged - i.e., the code which fixes the issue is integrated in the codebase. We built two logistic classifiers, one without affect metrics and one with them. By comparing the two classifiers, we show that the affect metrics improve the prediction performance. Our results show that valence (expressed in comments received and posted by a reporter) and joy expressed in the comments written by a reporter are linked to a higher likelihood of issues to be merged. On the contrary, sadness, anger, and arousal expressed in the comments written by a reporter, and anger, arousal, and dominance expressed in the comments received by a reporter, are linked to a lower likelihood of a pull request to be merged."
Gold2022,Nicolas E. Gold and Ross Purves and Evangelos Himonides,"Playing, Constructionism, and Music in Early-Stage Software Engineering Education","Multidisciplinary Journal for Education, Social and Technological Sciences",9,1,2022,10.4995/muse.2022.16453,,"Understanding that design involves trade-offs, thinking at multiple levels of abstraction, and considering the cohesion and coupling between sub-components of a larger whole is an important part of software (and other) engineering. It can be challenging to convey such abstract design concepts to novice engineers, especially for materials that are themselves abstract (e.g. software). Such challenges are compounded when teaching at the secondary school stage where students have limited experience of large-scale design problems that motivate the need for abstraction at all. In this paper, we describe a method for introducing these concepts to secondary school students using LEGO® and Raspberry Pi computers, asking them to build musical instruments as an entertaining way of motivating engagement with learning about design through play. The method has been successfully piloted in a series of three classroom sessions and key observations and experiences of using the method are presented."
Xu2022,Aiqiao Xu,Software Engineering Code Workshop Based on B-RRT ∗ FND Algorithm for Deep Program Understanding Perspective,Journal of Sensors,2022,,2022,10.1155/2022/1564178,16877268,"Developers will perform a lot of search behaviors when facing daily work tasks, searching for reusable code fragments, solutions to specific problems, algorithm designs, software documentation, and software tools from public repositories (including open source communities and forum blogs) or private repositories (internal software repositories, source code platforms, communities, etc.) to make full use of existing software development resources and experiences. This paper first takes a deep programmatic understanding view of the software development process. In this paper, we first define the software engineering code search task from the perspective of deep program understanding. Secondly, this paper summarizes two research paradigms of deep software engineering code search and composes the related research results. At the same time, this paper summarizes and organizes the common evaluation methods for software engineering code search tasks. Finally, the results of this paper are combined with an outlook on future research."
Huss2023,Moe Huss and Daniel R. Herber and John M. Borky,An Agile Model-Based Software Engineering Approach Illustrated through the Development of a Health Technology System,Software,2,2,2023,10.3390/software2020011,,"Model-Based Software Engineering (MBSE) is an architecture-based software development approach. Agile, on the other hand, is a light system development approach that originated in software development. To bring together the benefits of both approaches, this article proposes an integrated Agile MBSE approach that adopts a specific instance of the Agile approach (i.e., Scrum) in combination with a specific instance of an MBSE approach (i.e., Model-Based System Architecture Process—“MBSAP”) to create an Agile MBSE approach called the integrated Scrum Model-Based System Architecture Process (sMBSAP). The proposed approach was validated through a pilot study that developed a health technology system over one year, successfully producing the desired software product. This work focuses on determining whether the proposed sMBSAP approach can deliver the desired Product Increments with the support of an MBSE process. The interaction of the Product Development Team with the MBSE tool, the generation of the system model, and the delivery of the Product Increments were observed. The preliminary results showed that the proposed approach contributed to achieving the desired system development outcomes and, at the same time, generated complete system architecture artifacts that would not have been developed if Agile had been used alone. Therefore, the main contribution of this research lies in introducing a practical and operational method for merging Agile and MBSE. In parallel, the results suggest that sMBSAP is a middle ground that is more aligned with federal and state regulations, as it addresses the technical debt concerns. Future work will analyze the results of a quasi-experiment on this approach focused on measuring system development performance through common metrics."
Schlzel2021,Christopher Schölzel and Valeria Blesius and Gernot Ernst and Andreas Dominik,Characteristics of mathematical modeling languages that facilitate model reuse in systems biology: a software engineering perspective,npj Systems Biology and Applications,7,1,2021,10.1038/s41540-021-00182-w,20567189,"Reuse of mathematical models becomes increasingly important in systems biology as research moves toward large, multi-scale models composed of heterogeneous subcomponents. Currently, many models are not easily reusable due to inflexible or confusing code, inappropriate languages, or insufficient documentation. Best practice suggestions rarely cover such low-level design aspects. This gap could be filled by software engineering, which addresses those same issues for software reuse. We show that languages can facilitate reusability by being modular, human-readable, hybrid (i.e., supporting multiple formalisms), open, declarative, and by supporting the graphical representation of models. Modelers should not only use such a language, but be aware of the features that make it desirable and know how to apply them effectively. For this reason, we compare existing suitable languages in detail and demonstrate their benefits for a modular model of the human cardiac conduction system written in Modelica."
Nguyen2024,Hoa T. Nguyen and Muhammad Usman and Rajkumar Buyya,QFaaS: A Serverless Function-as-a-Service framework for Quantum computing,Future Generation Computer Systems,154,,2024,10.1016/j.future.2024.01.018,0167739X,"Quantum computing is rapidly reaching a point in which its application design and engineering aspects must be seriously considered. However, quantum software engineering is still in its infancy, with numerous challenges, especially in dealing with the diversity of quantum programming languages and noisy intermediate-scale quantum (NISQ) systems. To alleviate these challenges, we propose QFaaS, a holistic Quantum Function-as-a-Service framework, which leverages the advantages of the serverless model, DevOps lifecycle, and the state-of-the-art software techniques to advance practical quantum computing for next-generation application development in the NISQ era. Our framework provides essential elements of a serverless quantum system to streamline service-oriented quantum application development in cloud environments, such as combining hybrid quantum–classical computation, automating the backend selection, cold start mitigation, and adapting DevOps techniques. QFaaS offers a full-stack and unified quantum serverless platform by integrating multiple well-known quantum software development kits (Qiskit, Q#, Cirq, and Braket), quantum simulators, and cloud providers (IBM Quantum and Amazon Braket). This paper proposes the concept of quantum function-as-a-service, system design, operation workflows, implementation of QFaaS, and lessons learned on the benefits and limitations of quantum serverless computing. We also present practical use cases with various quantum applications on today's quantum computers and simulators to demonstrate our framework capability to facilitate the ongoing quantum software transition."
Alsharari2023,Abdullah Sabih Alsharari and Wan Mohd Nazmee Wan Zainon and Sukumar Letchmunan and Badiea Abdulkarem Mohammed and Mohammed Sabih Alsharari,A Review of Agile Methods for Requirement Change Management in Web Engineering,,,,2023,10.1109/ICSCA57840.2023.10087734,,"Web-based systems are essential in many different applications, which makes web-based development different from traditional software development. The agile methodology has gained lots of popularity in the last 15 years and become very traditional and acceptable widely among software engineers; nevertheless, the use of agile methods has extended to cover other areas of software engineering field including Requirement Engineering (RE) that fit the need of web engineering. The pace of the current web's software development is fast and dynamic such that the changes of the requirements during software development and after turning to production phase are possible and recurrent. Therefore, agile software engineering conceptuality has evolved as an adequate approach to overcome changes in the web's software's requirements; due to frequent changes in requirements, web engineers call for help of agile software engineering methods, which strive to truly manage changes in requirements rather than preventing these changes. This paper provides a review on the available agile methodologies that used to assess requirement change management in web engineering."
DelPilarBeristain-Colorado2021,María Del Pilar Beristain-Colorado and Jorge Fernando Ambros-Antemate and Marciano Vargas-Treviño and Jaime Gutiérrez-Gutiérrez and Adriana Moreno-Rodriguez and Pedro Antonio Hernández-Cruz and Itandehui Belem Gallegos-Velasco and Rafael Torres-Rosas,Standardizing the development of serious games for physical rehabilitation: Conceptual framework proposal,JMIR Serious Games,9,2,2021,10.2196/25854,22919279,"Background: Serious games have been used as supportive therapy for traditional rehabilitation. However, most are designed without a systematic process to guide their development from the phases of requirement identification, planning, design, construction, and evaluation, which reflect the lack of adaptation of rehabilitation requirements and thus the patient's needs. Objective: The aim of this study was to propose a conceptual framework with standardized elements for the development of information systems by using a flexible and an adaptable process centered on the patient's needs and focused on the creation of serious games for physical rehabilitation. Methods: The conceptual framework is based on 3 fundamental concepts: (1) user-centered design, which is an iterative design process focused on users and their needs at each phase of the process, (2) generic structural activities of software engineering, which guides the independent development process regardless of the complexity or size of the problem, and (3) gamification elements, which allow the transformation of obstacles into positive and fun reinforcements, thereby encouraging patients in their rehabilitation process. Results: We propose a conceptual framework to guide the development of serious games through a systematic process by using an iterative and incremental process applying the phases of context identification, user requirements, planning, design, construction of the interaction devices and video game, and evaluation. Conclusions: This proposed framework will provide developers of serious games a systematic process with standardized elements for the development of flexible and adaptable software with a high level of patient commitment, which will effectively contribute to their rehabilitation process."
Sun2022,Jiao Sun and Q. Vera Liao and Michael Muller and Mayank Agarwal and Stephanie Houde and Kartik Talamadupula and Justin D. Weisz,Investigating Explainability of Generative AI for Code through Scenario-based Design,,,,2022,10.1145/3490099.3511119,,"What does it mean for a generative AI model to be explainable? The emergent discipline of explainable AI (XAI) has made great strides in helping people understand discriminative models. Less attention has been paid to generative models that produce artifacts, rather than decisions, as output. Meanwhile, generative AI (GenAI) technologies are maturing and being applied to application domains such as software engineering. Using scenario-based design and question-driven XAI design approaches, we explore users' explainability needs for GenAI in three software engineering use cases: natural language to code, code translation, and code auto-completion. We conducted 9 workshops with 43 software engineers in which real examples from state-of-the-art generative AI models were used to elicit users' explainability needs. Drawing from prior work, we also propose 4 types of XAI features for GenAI for code and gathered additional design ideas from participants. Our work explores explainability needs for GenAI for code and demonstrates how human-centered approaches can drive the technical development of XAI in novel domains."
Mikalsen2023,Marius Mikalsen and Torgeir Dingsøyr,Feedback as a process in a large semi-capstone software engineering course,,,,2023,10.1145/3593434.3593961,,"Feedback involves dialogic processes whereby learners make sense of information from various sources and use it to enhance their work or learning strategies. This is an essential catalyst for learning. However, it takes work to achieve, particularly in large-scale courses. In this research-in-progress paper, we critically reflect on our approach to achieving feedback as a process in a large-scale semi-capstone software engineering course. We describe the steps taken over three years to improve feedback and critically reflect on how feedback is done using critical evaluation perspectives of peers, own reflections, students' perspectives, and theory. Based on these reflections, we discuss four action items planned for this year, including strengthening the community of practice amongst teaching assistants, using relative assessments, improving rubrics, and using technology for feedback. We also discuss how we plan to evaluate these action items."
Jiang2023,Jing Jiang and Xianyang Wang and Yangcheng Xia and Huoyuan Wang and Pingping Tong and Juanjuan Hong,Remolding Software Engineering Course Based on Engineering Education Concepts,Journal of Contemporary Educational Research,7,11,2023,10.26689/jcer.v7i11.5504,2208-8466,"The core of the engineering education concepts is student-centered teaching, result-oriented teaching, and continuous improvement. It is necessary to carry out course remolding of software engineering course guided by engineering education concepts. This paper first analyzes the pain points of teaching the course of software engineering, and then introduces how to remold the course based on engineering education concepts, including reformulating the teaching objectives, teaching modes, and assessment methods. Through teaching practice, we find that the improved course has achieved a better teaching result, which reflects the advantages of the engineering education concepts."
Mohanani2022,Rahul Mohanani and Paul Ralph and Burak Turhan and Vladimir Mandic,How Templated Requirements Specifications Inhibit Creativity in Software Engineering,IEEE Transactions on Software Engineering,48,10,2022,10.1109/TSE.2021.3112503,19393520,"Desiderata is a general term for stakeholder needs, desires or preferences. Recent experiments demonstrate that presenting desiderata as templated requirements specifications leads to less creative solutions. However, these experiments do not establish how the presentation of desiderata affects design creativity. This study, therefore, aims to explore the cognitive mechanisms by which presenting desiderata as templated requirements specifications reduces creativity during software design. Forty-two software designers, organized into 21 pairs, participated in a dialog-based protocol study. Their interactions were transcribed and the transcripts were analyzed in two ways: (1) using inductive process coding and (2) using an a-priori coding scheme focusing on fixation and critical thinking. Process coding shows that participants exhibited seven categories of behavior: making design moves, uncritically accepting, rejecting, grouping, questioning, assuming and considering quality criteria. Closed coding shows that participants tend to accept given requirements and priority levels while rejecting newer, more innovative design ideas. Overall, the results suggest that designers fixate on desiderata presented as templated requirements specifications, hindering critical thinking. More precisely, requirements fixation mediates the negative relationship between specification formality and creativity."
Bjarnason2023,Elizabeth Bjarnason and Patrik Åberg and Nauman bin Ali,Software selection in large-scale software engineering: A model and criteria based on interactive rapid reviews,Empirical Software Engineering,28,2,2023,10.1007/s10664-023-10288-w,15737616,"Context: Software selection in large-scale software development continues to be ad hoc and ill-structured. Previous proposals for software component selection tend to be technology-specific and/or do not consider business or ecosystem concerns. Objective: Our main aim is to develop an industrially relevant technology-agnostic method that can support practitioners in making informed decisions when selecting software components for use in tools or in products based on a holistic perspective of the overall environment. Method: We used method engineering to iteratively develop a software selection method for Ericsson AB based on a combination of published research and practitioner insights. We used interactive rapid reviews to systematically identify and analyse scientific literature and to support close cooperation and co-design with practitioners from Ericsson. The model has been validated through a focus group and by practical use at the case company. Results: The model consists of a high-level selection process and a wide range of criteria for assessing and for evaluating software to include in business products and tools. Conclusions: We have developed an industrially relevant model for component selection through active engagement from a company. Co-designing the model based on previous knowledge demonstrates a viable approach to industry-academia collaboration and provides a practical solution that can support practitioners in making informed decisions based on a holistic analysis of business, organisation and technical factors."
Cadavid2022,Héctor Cadavid and Vasilios Andrikopoulos and Paris Avgeriou and P. Chris Broekema,System and software architecting harmonization practices in ultra-large-scale systems of systems: A confirmatory case study,Information and Software Technology,150,,2022,10.1016/j.infsof.2022.106984,09505849,"Context: The challenges posed by the architecting of System of Systems (SoS) has motivated a significant number of research efforts in the area. However, literature is lacking when it comes to the interplay between the disciplines involved in the architecting process, a key factor in addressing these challenges. Objective: This paper aims to contribute to this line of research by confirming and extending previously characterized architecting harmonization practices from Systems and Software Engineering, adopted in an ultra-large-scale SoS. Methods: We conducted a confirmatory case study on the Square-Kilometre Array (SKA) project to evaluate and extend the findings of our exploratory case on the LOFAR/LOFAR2.0 radio-telescope projects. In doing so, a pre-study was conducted to map the findings of the previous study with respect to the SKA context. A survey was then designed, through which the views of 46 SKA engineers were collected and analyzed. Results: The study confirmed in various degrees the four practices identified in the exploratory case, and provided further insights about them: (1) the friction between disciplines caused by long-term system requirements, and how they can be ameliorated through intermediate, short-term requirements; (2) the way design choices with a cross-cutting impact on multiple agile teams have an indirect impact on the system architecture; (3) how these design choices are often caused by the criteria that guided early system decomposition; (4) the seemingly recurrent issue with the lack of details about the dynamic elements of the interfaces; and (5) the use of machine-readable interface specifications for aligning hardware/software development processes. Conclusions: The findings of this study and its predecessor support the importance of a cross-disciplinary view in the Software Engineering research agenda in SoS as a whole, not to mention their value as a convergence point for research on SoS architecting from the Systems and Software Engineering standpoints."
Zhou2020,Xin Zhou,How to treat the use of grey literature in software engineering,,,,2020,10.1145/3379177.3390305,,"Context: Following on other scientific disciplines, such as health sciences, the use of grey literature (GL) is becoming widespread in Software Engineering (SE) research. Whilst the number of papers incorporating GL on SE is increasing, there is little empirically known about different aspects of the use of GL in SE research. In particular, there is a lack of excellent evaluation standard for the quality of GL. Aim: Our research is aimed at systematically reviewing the use of GL in SE, empirically exploring SE researchers' views on GL and providing a guide for using GL in SE and for quality assessment of the GL to be included. Method: We used a mixed-methods approach for this research. We carried out a Systematic Literature Review (SLR) of the use of GL in SE. Then we surveyed the authors of the papers included in the SLR (as GL users) and the invited experts in the SE community on the use of GL in SE research. Results: We systematically selected and reviewed 102 SE secondary studies that incorporate GL in SE research, from which we identified two groups based on their reporting: 1) 76 reviews only claim their use of GL; 2) 26 reviews report the results by including GL. We also obtained 20 replies from the GL users and 24 replies from the invited SE experts. Conclusion: There is no common understanding of the meaning of GL in SE. Researchers define the scopes and the definitions of GL in a variety of ways. We found five main reasons of using GL in SE research. The findings have enabled us to propose a conceptual model for how GL works in SE research lifecycle. In the next workThere is a need for research to develop guidelines for using GL in SE and for assessing quality of GL."
Fulcini2023,Tommaso Fulcini and Riccardo Coppola and Luca Ardito and Marco Torchiano,"A Review on Tools, Mechanics, Benefits, and Challenges of Gamified Software Testing",ACM Computing Surveys,55,14 s,2023,10.1145/3582273,15577341,"Gamification is an established practice in Software Engineering to increase effectiveness and engagement in many practices. This manuscript provides a characterization of the application of gamification to the Software Testing area. Such practice in fact reportedly suffers from low engagement by both personnel in industrial contexts and learners in educational contexts. Our goal is to identify the application areas and utilized gamified techniques and mechanics, the provided benefits and drawbacks, as well as the open challenges in the field. To this purpose, we conducted a Multivocal Literature Review to identify white and grey literature sources addressing gamified software testing.We analyzed 73 contributions and summarized the most common gamified mechanics, concepts, tools, and domains where they are mostly applied. We conclude that gamification in software testing is mostly applied to the test creation phase with simple white-box unit or mutation testing tools and is mostly used to foster good behaviors by promoting the testers' accomplishment. Key research areas and main challenges in the field are: careful design of tailored gamified mechanics for specific testing techniques; the need for technological improvements to enable crowdsourcing, cooperation, and concurrency; the necessity for empirical and large-scale evaluation of the benefits delivered by gamification mechanics."
Rodriguez2020,Sarah L. Rodriguez and Erin E. Doran and Rachel E. Friedensen and Elizabeth Martínez-Podolsky and Paul S. Hengesteg,"Inclusion & marginalization: How perceptions of design thinking pedagogy influence computer, electrical, and software engineering identity","International Journal of Education in Mathematics, Science and Technology",8,4,2020,10.46328/IJEMST.V8I4.952,2147611X,"Engineering identity plays a vital role in the persistence of engineering students, yet limited research exists on how particular pedagogical approaches influence engineering identity at the college level. This qualitative case study explored how undergraduate student perceptions of design thinking pedagogy influence computer, electrical, and software engineering identity. The study found that design thinking pedagogy reinforces the recognition of an engineering identity, particularly for those from historically marginalized groups (i.e., women, people of color). Intentional implementation, including organization and framing of design thinking pedagogy, was an essential foundation for fostering student interest in the course and connecting to their role as engineers. This study suggests that design thinking is a fruitful area to explore to create more inclusive engineering environments. This study's findings will assist educational stakeholders in understanding the design thinking pedagogy and engineering identity experiences of CES undergraduate engineering majors. Findings may encourage institutions to view the engineering curriculum in terms of identity development and understand how intersectional identities influence the ways students, particularly those from marginalized backgrounds, experience the environment."
Wiesmann2023,Dirk Wiesmann,Avoidance of the term agile in software engineering: Necessary and possible,Journal of Software: Evolution and Process,35,12,2023,10.1002/smr.2566,20477481,"The term agile is frequently used in IT especially in the area of software engineering. The broad pervasiveness of the term agile is astonishing, because it is not well defined in the area of software engineering. The fuzziness of the term agile comes with the danger of ill-founded decisions and ideological discussions. Both slow down the gain in knowledge in the domain of software engineering. The term agile should therefore not be used either in science nor in the industry. This article provides a critical discussion of the term agile. By using a three-layered abstraction concept for software development processes, the use of the term agile can be avoided."
Almetwaly2024,Amani Ali Ibrahim Almetwaly and Ibrahim Eskandar Ibrahim Fadhel,Integrate between information systems engineering and software engineering theories for successful quality engineering measurement of software: Valid instrument pre-results,Computer Software and Media Applications,6,1,2024,10.24294/csma.v6i1.3382,,"An extensive number of instruments and systems assessment tools are weak and not good enough in the appraisal of systems’ quality engineer success measurement. Thus, the comprehension of systems’ success is very serious. One of the purposes of this research topic is to develop a successful, novel, validated instrument for measuring system quality success based on the integration between theories of information systems (Seddon and DeLone & McLean) and software engineering theory (ISO 25010). To ensure the quality of the instrument before use, eight academic experts have validated. The reason for expanding the number of experts to eight is to accurately build and evaluate the instrument because this instrument is the first one erring. After expert validation done successfully, researchers started the process of instrument pre-test. Pre-test verification and validation results done by test the instrument of 74 users. The results of the statistical testing were perfect. The Composite reliability proposed value is 0.7, The average variance extracted value is 0.5. Cronbach’s alpha is higher than 0.7. The value of Spearman’s reliable rhea is >0.6. Results approved that this instrument is strong and perfect to be used as a valid tool for system success measurement."
Myllyaho2022,Lalli Myllyaho and Mikko Raatikainen and Tomi Männistö and Jukka K. Nurminen and Tommi Mikkonen,On misbehaviour and fault tolerance in machine learning systems,Journal of Systems and Software,183,,2022,10.1016/j.jss.2021.111096,01641212,"Machine learning (ML) provides us with numerous opportunities, allowing ML systems to adapt to new situations and contexts. At the same time, this adaptability raises uncertainties concerning the run-time product quality or dependability, such as reliability and security, of these systems. Systems can be tested and monitored, but this does not provide protection against faults and failures in adapted ML systems themselves. We studied software designs that aim at introducing fault tolerance in ML systems so that possible problems in ML components of the systems can be avoided. The research was conducted as a case study, and its data was collected through five semi-structured interviews with experienced software architects. We present a conceptualisation of the misbehaviour of ML systems, the perceived role of fault tolerance, and the designs used. Common patterns to incorporating ML components in design in a fault tolerant fashion have started to emerge. ML models are, for example, guarded by monitoring the inputs and their distribution, and enforcing business rules on acceptable outputs. Multiple, specialised ML models are used to adapt to the variations and changes in the surrounding world, and simpler fall-over techniques like default outputs are put in place to have systems up and running in the face of problems. However, the general role of these patterns is not widely acknowledged. This is mainly due to the relative immaturity of using ML as part of a complete software system: the field still lacks established frameworks and practices beyond training to implement, operate, and maintain the software that utilises ML. ML software engineering needs further analysis and development on all fronts."
Samir2023,Mina Samir and Nada Sherief and Walid Abdelmoez,Improving Bug Assignment and Developer Allocation in Software Engineering through Interpretable Machine Learning Models,Computers,12,7,2023,10.3390/computers12070128,2073431X,"Software engineering is a comprehensive process that requires developers and team members to collaborate across multiple tasks. In software testing, bug triaging is a tedious and time-consuming process. Assigning bugs to the appropriate developers can save time and maintain their motivation. However, without knowledge about a bug’s class, triaging is difficult. Motivated by this challenge, this paper focuses on the problem of assigning a suitable developer to a new bug by analyzing the history of developers’ profiles and analyzing the history of bugs for all developers using machine learning-based recommender systems. Explainable AI (XAI) is AI that humans can understand. It contrasts with “black box” AI, which even its designers cannot explain. By providing appropriate explanations for results, users can better comprehend the underlying insight behind the outcomes, boosting the recommender system’s effectiveness, transparency, and confidence. The trained model is utilized in the recommendation stage to calculate relevance scores for developers based on expertise and past bug handling performance, ultimately presenting the developers with the highest scores as recommendations for new bugs. This approach aims to strike a balance between computational efficiency and accurate predictions, enabling efficient bug assignment while considering developer expertise and historical performance. In this paper, we propose two explainable models for recommendation. The first is an explainable recommender model for personalized developers generated from bug history to know what the preferred type of bug is for each developer. The second model is an explainable recommender model based on bugs to identify the most suitable developer for each bug from bug history."
Dhaini2021,Mahdi Dhaini and Mohammad Jaber and Amin Fakhereldine and Sleiman Hamdan and Ramzi A. Haraty,Green Computing Approaches - A Survey,Informatica (Slovenia),45,1,2021,10.31449/inf.v45i1.2998,18543871,"The upsurge in global warming and release of greenhouse gases are major issues that intensified over the past years due to the increasing usage of technological resources in our daily routines. That is why a call for going green in the technological field is highly recommended. This paper reviews various approaches of green computing in five main areas - software engineering, cloud computing, mobile computing, data centers, and the educational sector."
Glasauer2023,Christina Glasauer,The PREVENT-Model: Human and Organizational Factors Fostering Engineering of Safe and Secure Robotic Systems,Journal of Systems and Software,195,,2023,10.1016/j.jss.2022.111548,01641212,"The rise of robotic systems demands increasingly elaborated safety and security measures in order to prevent damage to property as well as human and economic harm. While technical expertise of engineers is a prerequisite and international standards constitute an overall frame for safety and security by design, human and organizational factors also shape the development of responsible technology. The current study derives an overarching model covering both human and organizational factors facilitating the development of safe and secure robotic systems. In a qualitative interview study, we conducted three focus groups with experts from robotics, software engineering, and related domains. The interviews were analyzed by three independent coders using a qualitative content analytical approach. The resulting integrative PREVENT-Model comprises 17 individual factors and 13 organizational factors that enhance safety and security by design. We embed each factor into existing research and derive implications for practitioners and possible courses of action. The PREVENT-Model serves as a roadmap to develop tailored measures. Based on this framework, practitioners can improve personnel development programs, organizational structures, working environments, and other aspects crucial for the development of safe and secure robotic systems."
Mikkelsplass2023,Stine Aurora Mikkelsplass and John Eidar Simensen and Ricardo Colomo-Palacios,Software and Systems Engineers in ICS Security: Graduate-Level Curricula and Industry Needs,International Journal of Human Capital and Information Technology Professionals,14,1,2023,10.4018/IJHCITP.333857,19473486,"The introduction of Industry 4.0 and IIoT has enabled the interconnection of information technology (IT) and operational technology (OT) and exposed industrial control systems to cyber threats. Industrial cybersecurity requires knowledge, skill, and collaboration between IT and OT. A comparison of graduate curricula of software engineering and systems engineering identifies competencies related to industrial control systems cybersecurity. Industry experts are interviewed to identify needs for cybersecurity skills and competencies. Results from the mapping are discussed in the context of software and systems engineering challenges in ICS cybersecurity and leveraged against industry experiences and needs expressed through interviews with three OT and IT industry professionals. The curricula mapping reveals variations in both how they are organised and expressed to the extent that subjective interpretation is required for evaluation and comparison. The interviews with the industry experts indicate a gap between graduate competence from the curricula and industry needs."
Prazina2023,Irfan Prazina and Seila Becirovic and Emir Cogo and Vensada Okanovic,Methods for Automatic Web Page Layout Testing and Analysis: A Review,IEEE Access,11,,2023,10.1109/ACCESS.2023.3242549,21693536,"Methods for automatic analysis of user interfaces are essential for a wide range of applications in computer science and software engineering. These methods are used in software security, document archiving, human-computer interaction, software engineering, and data science. Even though these methods are essential, no single research systematically lists most of the methods and their characteristics. This paper aims to give an overview of different solutions and their applications in the separate processes of automatic analysis of user interfaces. The main focus is on the techniques that analyze web page layouts and web page structure. Web pages' style, type of content, and even structure constantly (often drastically) change, as do methods that analyze them. The fact that most methods use very different datasets and web pages of various complexities are some of the reasons that the direct comparison of methods is difficult, if not impossible. Another fact is that the vast applications of methods practically solve similar problems. With these facts in mind, in the paper, we surveyed relevant scientific articles, categorized them, and provided an overview of how these methods have developed over time."
Dekkati2022,Sreekanth Dekkati,Automotive Software Engineering: Real-World Necessity and Significance,Engineering International,10,1,2022,10.18034/ei.v10i1.674,,"The automobile industry is undergoing a fundamental shift as it transitions from a mechanical to a software-intensive business, in which most innovation and competition depend on software engineering expertise. This shift is occurring due to the industry's shift from a mechanical to an electronic focus. Over the past few decades, the significance of software engineering in the automobile industry has grown substantially. As a result, it has garnered a great deal of interest from academics and industry professionals. Even though a considerable amount of information concerning automotive software engineering has been published in various scholarly journals, there needs to be a comprehensive study of this information. This systematic mapping project aims to classify and analyze the literature linked to automotive software engineering to offer a structured body of knowledge, identify well-established themes, and uncover research gaps. This study considers 679 publications from various academic fields and subfields published between 1990 and 2015. The primary studies were dissected and categorized based on five distinct dimensions of interest. In addition, potential holes in the research, as well as suggestions for directions for further investigation, are offered. The literature mainly focused on three different areas: system and software architecture and design, qualification testing, and reuse. These were the issues that were discussed the most frequently. There were fewer comparative and validation studies, and the research body needs to contain practitioner-oriented suggestions. Overall, the research activity on automotive software engineering has a high industrial relevance, but its scientific quality is relatively lower."
Robillard2024,Martin P. Robillard and Deeksha M. Arya and Neil A. Ernst and Jin L.C. Guo and Maxime Lamothe and Mathieu Nassif and Nicole Novielli and Alexander Serebrenik and Igor Steinmacher and Klaas Jan Stol,Communicating Study Design Trade-offs in Software Engineering,ACM Transactions on Software Engineering and Methodology,33,5,2024,10.1145/3649598,15577392,"Reflecting on the limitations of a study is a crucial part of the research process. In software engineering studies, this reflection is typically conveyed through discussions of study limitations or threats to validity. In current practice, such discussions seldom provide sufficient insight to understand the rationale for decisions taken before and during the study, and their implications. We revisit the practice of discussing study limitations and threats to validity and identify its weaknesses. We propose to refocus this practice of self-reflection to a discussion centered on the notion of trade-offs. We argue that documenting trade-offs allows researchers to clarify how the benefits of their study design decisions outweigh the costs of possible alternatives. We present guidelines for reporting trade-offs in a way that promotes a fair and dispassionate assessment of researchers' work."
Shulga2023,Tatiana Erikovna Shulga and Dmitrii Eduardovich Khramov,Life cycle ontology of software engineering,"Vestnik of Astrakhan State Technical University. Series: Management, computer science and informatics",2023,2,2023,10.24143/2072-9502-2023-2-66-74,2072-9502,"The article highlights the problem of presenting knowledge on the models of software life cycle, the importance of which can be explained by the rapid progress of software engineering methods, by the absence of a formally easily extensible knowledge model in this subject area, and by the fact that cycle time selection models and the proposed development methodology have a significant impact on the success of software projects. System analysis 
of the main types of software development methodologies, life cycle models and their phases has been carried out. The results of studying the representation of software life cycle models in the form of ontologies are presented. The ontology “Software development life cycle” (SDLC) has been developed. It is designed to represent knowledge about various models of the software life cycle, phases (stages) of the life cycle inherent in different models, and the possibility of describing the recurrence of phases. The ontology allows describing models both within predictive development methodologies (waterfall, incremental) and within agile development methodologies (Scrum, Kanban). Classes, properties and axioms of the ontology are described, on the basis of which it is possible to produce a formal logical inference. The SDLC ontology is developed on top of the Semantic Web formats (in OWL language), published in the public domain and presents a developing, easily extensible project. This can probably be used in the field of software development for practical or research purposes. There is also introduced the idea of a software shell that uses the presented ontology, which will allow, according to the given parameters, to choose the most appropriate methodology for the project, which will simplify the development process, avoid errors and reduce development time."
Guimares2021,G. Guimarães and M. Perkusich and D. Albuquerque and E. N. Guimarães and H. Almeida and D. Santos and A. Perkusich,A comparative study of psychometric instruments in software engineering,,2021-July,,2021,10.18293/SEKE2021-108,23259086,"Over the years, researchers have explored the influence of human factors in software engineering, showing that the team members' personalities might affect teamwork. However, it is challenging to measure software engineers' personalities due to the number of available psychometric instruments and the possibility of using different scales and classifications. Our study compares the personality traits measured by three psychometric instruments used in Software Engineering: Big Five Inventory (BFI), 16 Personality Factors (16PF), and Context Cards (CC). For this purpose, we executed an empirical study in which we collected data from 29 software developers for each of the evaluated instruments. As a result, we identified a moderate correlation between BFI and 16PF, confirming the current state-of-the-art. For the remaining combinations, there was a weak correlation. As implications for this research, there is a need to empirically evaluate BFI and CC (context-specific survey) in terms of construct validity since they have moderate to low correlation."
Ren2023,Wei Ren and Stephen Barrett,"Test-driven development, engagement in activity, and maintainability: An empirical study",IET Software,17,4,2023,10.1049/sfw2.12135,17518814,"The software engineering community aims to achieve and maintain high-efficient software engineering practical activities. One of the techniques used for this purpose is Test-Driven Development (TDD), which is a cyclic and test-centered development method positively related to maintainability, which may vary depending on the context and application. Previous research has indicated that TDD's benefits on maintainability may be due to the improved focus on coding and testing activities. However, limited research has investigated the role of engagement in software quality. Therefore, this study investigates the relationship between TDD methods, engagement level in development activities, and maintainability. The primary research questions addressed in this study are does following TDD methods improve the engagement level in development activities? And does a higher engagement level leads to better maintainability? The study employs a statistical analysis technique, regression analysis to explore the research questions. The results show that TDD behaviours improve the engagement level, and engaging in development activities has a significant positive impact on maintainability. Additionally, the study finds that the positive impact of focussing on testing, such as writing more test cases, is more pronounced compared to coding activities. Our study adds to current software engineering literature that not only personal expertise but the engagement level in the development process are associated with software quality and calls for the emphasis on developer's engagement."
Zhi2023,Qiang Zhi and Wanxu Pu and Jianguo Ren and Zhengshu Zhou,A Defect Detection Method for the Primary Stage of Software Development,"Computers, Materials and Continua",74,3,2023,10.32604/cmc.2023.035846,15462226,"In the early stage of software development, a software requirements specification (SRS) is essential, and whether the requirements are clear and explicit is the key. However, due to various reasons, there may be a large number of misunderstandings. To generate high-quality software requirements specifications, numerous researchers have developed a variety of ways to improve the quality of SRS. In this paper, we propose a questions extraction method based on SRS elements decomposition, which evaluates the quality of SRS in the form of numerical indicators. The proposed method not only evaluates the quality of SRSs but also helps in the detection of defects, especially the description problem and omission defects in SRSs. To verify the effectiveness of the proposed method, we conducted a controlled experiment to compare the ability of checklist-based review (CBR) and the proposed method in the SRS review. The CBR is a classicmethod of reviewing SRS defects. After a lot of practice and improvement for a long time, CBR has excellent review ability in improving the quality of software requirements specifications. The experimental results with 40 graduate studentsmajoring in software engineering confirmed the effectiveness and advantages of the proposed method. However, the shortcomings and deficiencies of the proposed method are also observed through the experiment. Furthermore, the proposed method has been tried out by engineers with practical work experience in software development industry and received good feedback."
Carlier2023,Stéphanie Carlier and Vince Naessens and Femke De Backere and Filip De Turck,A Software Engineering Framework for Reusable Design of Personalized Serious Games for Health: Development Study,JMIR Serious Games,11,,2023,10.2196/40054,22919279,"Background: The use of serious games in health care is on the rise, as these games motivate treatment adherence, reduce treatment costs, and educate patients and families. However, current serious games fail to offer personalized interventions, ignoring the need to abandon the one-size-fits-all approach. Moreover, these games, with a primary objective other than pure entertainment, are costly and complex to develop and require the constant involvement of a multidisciplinary team. No standardized approach exists on how serious games can be personalized, as existing literature focuses on specific use cases and scenarios. The serious game development domain fails to consider any transfer of domain knowledge, which means this labor-intensive process must be repeated for each serious game. Objective: We proposed a software engineering framework that aims to streamline the multidisciplinary design process of personalized serious games in health care and facilitates the reuse of domain knowledge and personalization algorithms. By focusing on the transfer of knowledge to new serious games by reusing components and personalization algorithms, the comparison and evaluation of different personalization strategies can be simplified and expedited. In doing so, the first steps are taken in advancing the state of the art of knowledge regarding personalized serious games in health care. Methods: The proposed framework aimed to answer 3 questions that need to be asked when designing personalized serious games: Why is the game personalized? What parameters can be used for personalization? and How is the personalization achieved? The 3 involved stakeholders, namely, the domain expert, the (game) developer, and the software engineer, were each assigned a question and then assigned responsibilities regarding the design of the personalized serious game. The (game) developer was responsible for all the game-related components; the domain expert was in charge of the modeling of the domain knowledge using simple or complex concepts (eg, ontologies); and the software engineer managed the personalization algorithms or models integrated into the system. The framework acted as an intermediate step between game conceptualization and implementation; it was illustrated by developing and evaluating a proof of concept. Results: The proof of concept, a serious game for shoulder rehabilitation, was evaluated using simulations of heart rate and game scores to assess how personalization was achieved and whether the framework responded as expected. The simulations indicated the value of both real-time and offline personalization. The proof of concept illustrated how the interaction between different components worked and how the framework was used to simplify the design process. Conclusions: The proposed framework for personalized serious games in health care identifies the responsibilities of the involved stakeholders in the design process, using 3 key questions for personalization. The framework focuses on the transferability of knowledge and reusability of personalization algorithms to simplify the design process of personalized serious games."
Jolak2023,Rodi Jolak and Andreas Wortmann and Grischa Liebel and Eric Umuhoza and Michel R.V. Chaudron,Design thinking and creativity of colocated versus globally distributed software developers,Journal of Software: Evolution and Process,35,5,2023,10.1002/smr.2377,20477481,"Designing software is an activity in which software developers think and make design decisions that shape the structure and behavior of software products. Designing software is one of the least understood software engineering activities. In a collaborative design setting, various types of distances can lead to challenges and effects that potentially affect how software is designed. To contribute to a better understanding of collaborative software design, we investigate how communication gaps caused by social and geographic distances affect its design thinking and the creativity of its discussions. To this end, we conducted a multiple-case study exploring the design thinking and creativity of colocated and distributed software developers in a collaborative design setting. Compared with colocated developers, distributed developers spend less time on exploring the problem space, which could be related to different sociotechnical challenges, such as lack of awareness and common understanding. Distributed development does not seem to affect the creativity of their activities. Developers engaging in collaborative design need to be aware that problem space exploration is reduced in a distributed setting. Unless distributed teams take compensatory measures, this could adversely affect the development. Regarding the effect distance has on creativity, our results are inconclusive and further studies are needed."
Roca2024,Isis Roca and Óscar Pastor and Carlos Cetina and Lorena Arcega,Co-evolving scenarios and simulated players to locate bugs that arise from the interaction of software models of video games,Information and Software Technology,169,,2024,10.1016/j.infsof.2024.107412,09505849,"Context: Game Software Engineering (GSE) is a field that focuses on developing and maintaining the software part of video games. A key component of video game development is the utilization of game engines, with many engines using software models to capture various aspects of the game. Objective: A challenge that GSE faces is the localization of bugs, mainly when working with large and intricated software models. Additionally, the interaction between software models (i.e. bosses, enemies, or environmental elements) during gameplay is often a significant source of bugs. In response to this challenge, we propose a co-evolution approach for bug localization in the software models of video games, called CoEBA. Methods: The CoEBA approach leverages Search-Based Software Engineering (SBSE) techniques to locate bugs in software models while considering their interactions. We conducted an evaluation in which we applied our approach to a commercial video game, Kromaia. We compared our approach with a state-of-the-art baseline approach that relied on the bug localization approach used by Kromaia's developers and a random search used as a sanity check. Results: Our co-evolution approach outperforms the baseline approach in precision, recall, and F-measure. In addition, to provide evidence of the significance of our results, we conducted a statistical analysis. that shows significant differences in precision and recall values. Conclusion: The proposed approach, CoEBA, which considers the interaction between software models, can identify and locate bugs that other bug localization approaches may have overlooked."
Hunt2022,Lucy Hunt and Maria Angela Ferrario,"A Review of How Whistleblowing is Studied in Software Engineering, and the Implications for Research and Practice",,,,2022,10.1109/ICSE-SEIS55304.2022.9793939,02705257,"Harmful software has resulted in loss of life, societal and environmental damage alongside economic losses from fines and sales embargoes. When someone perceives their team or organisation is creating or operating harmful software (e.g., defective, vulnerable, malicious or illegal), one way to attempt to change the situation is to 'blow the whistle' and disclose the situation internally or externally. Studying harmful situations and the effectiveness of interventions, up to and including whistleblowing, can help identify technical and human successes and failings in software engineering (SE). The aim of this paper is to explore the extent to which whistleblowing is studied in SE with the objective of identifying themes, research approaches, gaps and concerns, and the implications for future SE research and practice. We find that whistleblowing is an under-explored area of SE research, and where research exists, it often takes the view that reporting harm is a matter of individual moral responsibility; we argue this poorly reflects SE collaborative practice where professional responsibilities are distributed across the software development lifecycle. We conclude by 1) recommending approaches that can help a more timely identification and mitigation of harm in SE; 2) suggesting mechanisms for improving the effectiveness and the personal safety of harm-reporting in SE, and 3) reflecting on the role that professional bodies can have in supporting harm reporting, up to and including whistleblowing."
Ali2022,Shaukat Ali and Tao Yue and Rui Abreu,When software engineering meets quantum computing,Communications of the ACM,65,4,2022,10.1145/3512340,15577317,
Snchez2021,Pedro Sánchez and Diego Alonso,On the definition of quantum programming modules,Applied Sciences (Switzerland),11,13,2021,10.3390/app11135843,20763417,"There are no doubts that quantum programming and, in general, quantum computing, is one of the most promising areas within computer science and one of the areas where most expectations are being placed in recent years. Although the days when reliable and affordable quantum computers will be available is still a long way off, the explosion of programming languages for quantum programming has grown exponentially in recent years. The software engineering community has been quick to react to the need to adopt and adapt well-known tools and methods for software development, and for the design of new ones tailored to this new programming paradigm. However, many key aspects for its success depend on the establishment of an appropriate conceptual framework for the conception and design of quantum programs. This article discusses the concept of module, key in the software engineering discipline, and establishes initial criteria for determining the cohesion and coupling levels of a module in the field of quantum programming as a first step towards a sound quantum software engineering. As detailed in the article, the conceptual differences between classical and quantum computing are so pronounced that the translation of classical concepts to the new programming approach is not straightforward."
Ali2023,Josh Mahmood Ali,Introduction for human-centric software engineering,Advances in Engineering Innovation,5,1,2023,10.54254/2977-3903/5/2023032,2977-3903,"In the evolving landscape of software development, Human-Centric Software Engineering (HCSE) is emerging as a pivotal paradigm, prioritizing human needs and experiences at the core of software engineering processes. This research delves into the fundamental principles of HCSE, its implications on software quality, and the enhanced user satisfaction it promises. Through comprehensive surveys, case study analyses, and user feedback sessions, this study reaffirms the increasing significance of HCSE in modern software development. Despite its evident benefits, the research also sheds light on the organizational barriers hindering its broad adoption. As software systems continue to intertwine with our daily lives, the research underscores the imperative shift from mere functionality to creating holistic, human-centered software experiences."
Serban2020,Alex Serban and Koen Van Der Blom and Holger Hoos and Joost Visser,Adoption and effects of software engineering best practices in machine learning,,,,2020,10.1145/3382494.3410681,19493789,"Background. The increasing reliance on applications with machine learning (ML) components calls for mature engineering techniques that ensure these are built in a robust and future-proof manner. Aim. We aim to empirically determine the state of the art in how teams develop, deploy and maintain software with ML components. Method. We mined both academic and grey literature and identified 29 engineering best practices for ML applications. We conducted a survey among 313 practitioners to determine the degree of adoption for these practices and to validate their perceived effects. Using the survey responses, we quantified practice adoption, differentiated along demographic characteristics, such as geography or team size. We also tested correlations and investigated linear and non-linear relationships between practices and their perceived effect using various statistical models. Results. Our findings indicate, for example, that larger teams tend to adopt more practices, and that traditional software engineering practices tend to have lower adoption than ML specific practices. Also, the statistical models can accurately predict perceived effects such as agility, software quality and traceability, from the degree of adoption for specific sets of practices. Combining practice adoption rates with practice importance, as revealed by statistical models, we identify practices that are important but have low adoption, as well as practices that are widely adopted but are less important for the effects we studied. Conclusion. Overall, our survey and the analysis of responses received provide a quantitative basis for assessment and step-wise improvement of practice adoption by ML teams."
Jahn2022,Sabrina Jahn and Nicolas Kaul and Jürgen Mottok,Using or Misusing? Introducing Misuse Cases in a Software Engineering Course for Undergraduate Engineering Students,,1,,2022,10.1145/3502718.3524823,1942647X,"Today's cyberphysical systems are increasingly prone to misuse. To secure existing and future software systems, introducing concepts of IT-Security and Secure Software Engineering (SecSE) in Software Engineering (SE) courses is essential for academic education of future software engineers. This is not only important for computer science students, but also for engineering students studying topics of computing and SE. However, only little research exists on integrating these topics into traditional SE courses, especially for engineering students in non-computer science majors. To narrow this gap, this paper contributes with the design and evaluation of an exercise on modeling misuse cases alongside use cases, based on the inductive teaching method problem-based learning (PBL). The exercise is part of an educational design research investigating which learning content and teaching methods are suitable for integrating IT-Security and SecSE topics into traditional SE education of engineering students to convey factual knowledge as well as raise awareness and interest for both topics during software development. We present the integration of the exercise design into a traditional SE course for engineering students and its evaluation to examine its suitability. We evaluated the exercise design regarding the suitability of the design components, the learning content of misuse cases and the intended learning goals as well as its impact on students' motivation, and their interest in IT-security. The paper then presents indications on the feasibility and success of the exercise design for teaching misuse cases to engineering students and sparking their interest in IT-Security."
Akbar2021,Muhammad Azeem Akbar and Wishal Naveed and Sajjad Mahmood and Saima Rafi and Ahmed Alsanad and Abeer Abdul-Aziz Alsanad and Abdu Gumaei and Abdulrahman Alothaim,Prioritization of global software requirements' engineering barriers: An analytical hierarchy process,IET Software,15,4,2021,10.1049/sfw2.12022,17518814,"Software industry is adopting global software development (GSD) due to its potential to produce quality products at a lower cost. However, the GSD firms face many challenges that make development activities more complicated, especially related to the requirements engineering (RE) process. The objectives of this article are to investigate and prioritize the barriers faced by the GSD organizations during the RE process. First, we identified 17 barriers related to the RE process in the GSD projects. Next, the identified barriers were further validated with real-world GSD practitioners using a questionnaire survey. Finally, we applied the analytical hierarchy process to prioritize the investigated barriers with respect to their significance for the RE process in the GSD domain. The results show that coordination is the most significant barrier category for the RE process in GSD projects. Lack of standard and procedure for RE in GSD, lack of synchronized communication infrastructure, and lack of mutual understanding between the overseas RE teams are also high-ranked barriers for the RE process in GSD. The authors believe that the findings of this study will assist practitioners and researchers in developing effective strategies and plans for the successful implementation of the RE process in the GSD context."
Schmiedmayer2022,Paul Schmiedmayer and Robert Chatley and Jan Philip Bernius and Stephan Krusche and Konstantin Chaika and Kirill Krinkin and Bernd Bruegge,Global software engineering in a global classroom,,,,2022,10.1145/3510456.3514163,,"Due to globalization, many software projects have become large-scale and distributed tasks that require software engineers to learn and apply techniques for distributed requirements analysis, modeling, development, and deployment. Globally-distributed projects require special skills in communication across different locations and time zones in all stages of the project. There has been advancement in teaching these concepts at universities, but adapting global software engineering in a curriculum is still in infancy.The main reasons are the effort and coordination required by teachers to set up the project, manage distributed development and enable distributed delivery. It becomes even more difficult when teaching distributed software engineering involving Internet of Things (IoT) applications. The situation has changed with recent advances in continuous deployment and cloud platform services that make globally-distributed projects more feasible, teachable, and learnable, even for short-term projects. However, no experience report in education research describes a truly distributed global setup in continuous software engineering for IoT applications.This paper describes a ten-day project involving three universities in different countries with 21 students located across the world to substantiate this claim. It provides teachers with recommendations for conducting a global software engineering course in a global setting. Recommendations include access for all students to (remote) hardware, stable network infrastructure in all locations, the use of a central development platform for continuous integration and deployment, and the application of distributed pair deployment."
Butler2023,Simon Butler and Jonas Gamalielsson and Björn Lundell and Christoffer Brax and Anders Mattsson and Tomas Gustavsson and Jonas Feist and Bengt Kvarnström and Erik Lönroth,On business adoption and use of reproducible builds for open and closed source software,Software Quality Journal,31,3,2023,10.1007/s11219-022-09607-z,15731367,"Reproducible builds (R-Bs) are software engineering practices that reliably create bit-for-bit identical binary executable files from specified source code. R-Bs are applied in some open source software (OSS) projects and distributions to allow verification that the distributed binary has been built from the released source code. The use of R-Bs has been advocated in software maintenance and R-Bs are applied in the development of some OSS security applications. Nonetheless, industry application of R-Bs appears limited, and we seek to understand whether awareness is low or if significant technical and business reasons prevent wider adoption. Through interviews with software practitioners and business managers, this study explores the utility of applying R-Bs in businesses in the primary and secondary software sectors and the business and technical reasons supporting their adoption. We find businesses use R-Bs in the safety-critical and security domains, and R-Bs are valuable for traceability and support collaborative software development. We also found that R-Bs are valued as engineering processes and are seen as a badge of software quality, but without a tangible value proposition. There are good engineering reasons to use R-Bs in industrial software development, and the principle of establishing correspondence between source code and binary offers opportunities for the development of further applications."
Casamayor2022,Rodrigo Casamayor and Lorena Arcega and Francisca Pérez and Carlos Cetina,Bug Localization in Game Software Engineering: Evolving Simulations to Locate Bugs in Software Models of Video Games,,,,2022,10.1145/3550355.3552440,,"Video games have characteristics that differentiate their development and maintenance from classic software development and maintenance. These differences have led to the coining of the term Game Software Engineering to name the emerging subfield that intersects Software Engineering and video games. One of these differences is that video game developers perceive more difficulties than other non-game developers when it comes to locating bugs. Our work proposes a novel way to locate bugs in video games by means of evolving simulations. As the baseline, we have chosen BLiMEA, which targets classic software engineering and uses bug reports and the defect localization principle to locate bugs. We also include Random Search as a sanity check in the evaluation. We evaluate the approaches in a commercial video game (Kromaia). The results for F-measure range from 46.80%. to 70.28% for five types of bugs. Our approach improved the results of the baseline by 20.29% in F-measure. To the best of our knowledge, this is the first approach that is designed specifically for bug localization in video games. A focus group with professional video game developers has confirmed the acceptance of our approach. Our approach opens a new research direction for bug localization for both game software engineering and possibly classic software engineering."
Isomttnen2021,Ville Isomöttönen and Emmi Ritvos,Digging into group establishment: Intervention design and evaluation,Journal of Systems and Software,178,,2021,10.1016/j.jss.2021.110974,01641212,"Previous research has documented challenges in students’ group work. An identifiable segment of the previous research that relates to improving students’ group work conditions is the study of group formation and self- and peer-assessment. Though studies that primarily focus on how to address the conditions of students’ group work and the existing problems can be found, there are not many related to higher education settings. On this ground, the present article advances a qualitative evaluation of the intervention that promotes student groups’ self-awareness and thereby self-regulation toward fair group work during a software engineering project. An inductive thematic analysis was applied to the students’ written reflections on the intervention. To further understand the results, the concept of “group establishment,” referring to destructiveness that complicates individuals’ truthful living at the group level, was employed to reflect on the resulting themes. Hoggett (1998) provided this articulation by synthesizing previous results in psychoanalytic theory. Students’ experiences with the intervention revealed several value gains, including personally identified benefits as well as open group mood, consolidation of grouping, conceptual learning about group work, and regulation for task allocation. Noted challenges included dishonesty and a personal role conflict, and some students reported minor effects on group performance. Students valued safety in the intervention situation and argued that the intervention was needed from outside the group. A summative review of the students’ experiences suggests that the intervention was useful for all groups. The results are discussed from a pedagogic and the aforementioned psychoanalytic perspective, and remarks are made for software engineering education."
Almeida2023,Fernando Almeida and Pedro Carneiro,Perceived Importance of Metrics for Agile Scrum Environments,Information (Switzerland),14,6,2023,10.3390/info14060327,20782489,"Metrics are key elements that can give us valuable information about the effectiveness of agile software development processes, particularly considering the Scrum environment. This study aims to learn about the metrics adopted to assess agile development processes and explore the impact of how the role performed by each member in Scrum contributed to increasing/reducing the perception of the importance of these metrics. The impact of years of experience in Scrum on this perception was also explored. To this end, a quantitative study was conducted with 191 Scrum professionals in companies based in Portugal. The results show that the Scrum role is not a determining factor, while individuals with more years of experience have a higher perception of the importance of metrics related to team performance. The same conclusion is observed for the business value metric of the product backlog and the percentage of test automation in the testing phase. The findings allow for extending the knowledge about Scrum project management processes and their teams, in addition to offering important insights into the implementation of metrics for software engineering companies that adopt Scrum."
Leme2023,Samir Lemeš,The Role of Software Engineering in Industry 4.0,,,,2023,10.5644/pi2023.209.05,,"In Industry 4.0, software engineering plays a critical role in enabling and driving the digital transformation of industries. Key roles of software engineering in Industry 4.0 include the development of IoT and connectivity, automation and robotics, data analytics and artificial intelligence (AI), cybersecurity, human-machine interfaces, software integration and system interoperability. Software engineering provides the foundation for Industry 4.0 by developing the software systems and infrastructure required for automation, connectivity, data analysis, and cybersecurity. It enables the digital transformation of industries, driving efficiency, innovation, and new business models"
Dajda2021,Jacek Dajda and Michał Idzik and Jakub Sroka and Mikołaj Sikora and Wiktor Pawłowski and Maciej Smołka and Przemysław Jabłecki and Filip Ślazyk and Maciej Malawski and Emilia Majerz and Aleksandra Pasternak and Witold Dzwinel,CURRENT TRENDS IN SOFTWARE ENGINEERING BACHELOR THESES,Computing and Informatics,40,4,2021,10.31577/CAI_2021_4_930,25858807,This article presents short analysis and observations on current trends and directions in conducting engineering theses in the field of computer science. This report is based on collected bachelor theses in AGH Computer Science Department for academic year 2020/2021 as well as the conducted competition for the best engineering theses held during XXII KKIO 2021 Software Engineering Conference. The awarded works are briefly presented as an illustration to the drawn conclusions.
Tuape2021,Micheal Tuape and Victoria T. Hasheela-Mufeti and Anna Kayanda and Jari Porras and Jussi Kasurinen,Software Engineering in Small Software Companies: Consolidating and Integrating Empirical Literature into a Process Tool Adoption Framework,IEEE Access,9,,2021,10.1109/ACCESS.2021.3113328,21693536,"Small software companies face numerous challenges of complexity, unstructured software development processes and scarce resources. This notwithstanding, the companies have dominated the software market by 80 percent. The practice and products of these companies are still persistently marred by quality issues arising from the processes, with evidence indicating that process tools do not fit the unique contexts in which they operate. Significant strides have been made to transform software development practice; however, the challenges are still apparent. Hence the need to establish how knowledge areas are applied in process practice, understand the context of software development and its implication in practice, how process tools are utilised in practice and evaluate the quality of research in software literature. The researchers undertook a systematic mapping study to determine the state of practice in the empirical literature on software engineering of SSCs by examining and classifying 1096 publications. Other than the finding that research quality was low and affecting generalisation and transferability, the results also revealed exciting findings, which we finally consolidated and integrated to develop two contributions (i) a software development process adoption theoretical framework that provides essential insights into understanding software development and (ii) a 3-point guideline for research quality. By solving the adoption of process tools in software development, this paper presents one of the most significant contributions to transforming practice in software development and research in small software companies."
Zhang2022,Tianyi Zhang,Design and Implementation of Software Engineering Network Teaching System Based on CS Mode,Security and Communication Networks,2022,,2022,10.1155/2022/7436988,19390122,"With the rapid development of computer network technology, all aspects of human life have begun to show a trend of networking and intelligence, and the education industry is no exception. This study wants to improve the quality of classroom teaching and student performance. Through demand analysis, this paper understands the teaching characteristics of software engineering and the shortcomings of the original teaching methods and develops a comprehensive network teaching system (TS). This article introduces the framework design of the system around screenshots, electronic whiteboards, and image compression schemes; starting from three aspects of teaching courseware, exercise library, and teaching practice, it illustrates the advantages of intelligent and personalized teaching. From the experimental data, it can be seen that the excellent performance rate (score>80) of students who study software engineering courses through the network TS after one semester is 75.56%, which is compared with the 46% excellent rate in the class using traditional methods. It can be seen that an intelligent and personalized network TS can play an excellent role in stimulating students' enthusiasm for learning and improving students' academic performance in practice."
Kitchenham2022,Barbara Kitchenham and Lech Madeyski and Giuseppe Scanniello and Carmine Gravino,The Importance of the Correlation in Crossover Experiments,IEEE Transactions on Software Engineering,48,8,2022,10.1109/TSE.2021.3070480,19393520,"Context: In empirical software engineering, crossover designs are popular for experiments comparing software engineering techniques that must be undertaken by human participants. However, their value depends on the correlation (r) between the outcome measures on the same participants. Software engineering theory emphasizes the importance of individual skill differences, so we would expect the values of r to be relatively high. However, few researchers have reported the values of r. Goal: To investigate the values of r found in software engineering experiments. Method: We undertook simulation studies to investigate the theoretical and empirical properties of r. Then we investigated the values of r observed in 35 software engineering crossover experiments. Results: The level of r obtained by analysing our 35 crossover experiments was small. Estimates based on means, medians, and random effect analysis disagreed but were all between 0.2 and 0.3. As expected, our analyses found large variability among the individual r estimates for small sample sizes, but no indication that r estimates were larger for the experiments with larger sample sizes that exhibited smaller variability. Conclusions: Low observed r values cast doubts on the validity of crossover designs for software engineering experiments. However, if the cause of low r values relates to training limitations or toy tasks, this affects all Software Engineering (SE) experiments involving human participants. For all human-intensive SE experiments, we recommend more intensive training and then tracking the improvement of participants as they practice using specific techniques, before formally testing the effectiveness of the techniques."
Huang2024,Yifan Huang and Yaqiong Wang and Pengcheng Wang,E-learning in Software Engineering Education for Computer Software Development in the Big Data Context,Computer-Aided Design and Applications,21,S22,2024,10.14733/cadaps.2024.S22.1-17,16864360,"In the context of wide application of information technology, fully mastering the technical methods and technologies of software development and understanding the essential characteristics of software development can fully improve the technical capacity of application software, and it also plays a supporting role in solving technical problems related to software development. In the era of big data, economic development and technological innovation are prerequisites for the development of the era; most information technology applications require software as a carrier. We must develop appropriate software according to our needs in the application process. Application software technology can improve the efficiency of software development. The developed software can effectively improve users' understanding and application ability. This paper introduces the importance of the software development method and analyzes its application practice in software development. The case study shows that the method in this paper has obtained 87% utility approval in the practical application process."
Manamendra2021,M. A. S. C. Manamendra and H. M. R. P. Herath,The Value Creation Determinants of the Global Software Engineering Industry,Sri Lanka Journal of Marketing,7,3,2021,10.4038/sljmuok.v7i3.74,1800-4989,"Every requirement in the software application does not add value to the end users equally where the stakeholders are benefited through them. Majority of the current software engineering practices are formed and done in a value neutral way. With the recent demand for the global software engineering, many organizations have decided to either initiate their own technology centers in the low-cost countries or to partner with outsourced companies to get the benefits of it. However, the practitioners tried applying the traditional software development processes without a proper customization of them to best suit for the distributed teams. Due to the nature of this industry, those traditional processes have further eroded the value with unnecessary checkpoints and gates. This research therefore was carried out to identify the value creation dimensions and their influence creating value with special reference the global software engineering industry. For that, a systematic literature review was carried out and findings were categorized to five areas based on the research questions. They are benefit realization in the delivery process, eliciting and reconciling the requirements for stakeholder value proposition, business case analysis against the business processes, assessment of the value, risk, and opportunity through the frameworks and finally value based monitoring and control through the software development methodology. Mapping the development processes against these areas can change the value addition on the global software engineering industry."
Agarwal2023,Krish Agarwal,Case Study: Comparing Different Software Engineering Models in Enhancing the Productivity of a Transportation Company,INTERANTIONAL JOURNAL OF SCIENTIFIC RESEARCH IN ENGINEERING AND MANAGEMENT,07,07,2023,10.55041/ijsrem20188,,"This case study aims to investigate how various software engineering life cycle models can impact the productivity of a transportation company. It also examines how the transition from manual to automated processes can optimize the company's output in terms of cost, labour, and time. By exploring different software engineering models, this study seeks to identify the most effective approach for enhancing the transportation company's productivity. The study recognizes that while some software engineering models can lead to increased profitability, others may have limitations or flaws that need to be addressed. Ultimately, the goal of the study is to provide insights into the advantages and disadvantages of different software engineering models and their impact on the productivity of a transportation company. Key Words: Case study, Software Engineering life cycle model, Productivity, Optimizes, profit-maximizing tactics, Waterfall model, Incremental Model, Spiral Model, customer- end and driver-end application"
Garca-Pealvo2021,Francisco José García-Peñalvo and Alicia García-Holgado and Andrea Vázquez-Ingelmo and José Carlos Sánchez-Prieto,"Planning, communication and active methodologies: Online assessment of the software engineering subject during the COVID-19 crisis",RIED-Revista Iberoamericana de Educacion a Distancia,24,2,2021,10.5944/ried.24.2.27689,13903306,"The coronavirus pandemic has had a high impact worldwide. The health crisis has not only had an impact on people’s own health and on health systems, but has also affected other areas. In the educational context, the lockdown measures implemented by different governments have challenged the learning ecosystem. In the case of higher education in Spain, with a strong attendance factor in most public universities, face-to-face classes were interrupted after a month in the second term, ending the academic year in the online mode. This change has meant a great effort on the teachers’ side to transform the face-to-face teaching into the online approach, which in many cases has meant quite a comprehensive new design of the subject, changing the evaluation process and the methodologies used. This work presents a success case of online assessment developed in the Software Engineering I subject of the Degree in Computer Engineering at the University of Salamanca. The objective is to show how the previous use of active methodologies and the integration of educational technologies in classroom-based teaching facilitates the transformation of assessment to an online or blended approach while maintaining a high degree of student involvement and satisfaction. After presenting a comparison between the face-to-face approach and the adaptation to the online approach, an analysis is made of the learning results and student satisfaction concerning previous academic years. The results show that the change of approach has not reduced the satisfaction results obtained in previous courses. In terms of learning outcomes, there is an overall increase in the grades obtained by students in all assessment items."
Aprilianti2024,Dhila Aprilianti,AR Application Design for SV IPB Software Engineering Technology Study Program using Design Thinking Method,"Journal of Applied Science, Technology & Humanities",1,1,2024,10.62535/phjrce82,,"The application of Augmented Reality (AR) is steadily increasing in the development of modern applications, which combines virtual elements with the real world to provide an immersive user experience. AR applications are crucial to the IPB Software Engineering Technology Study Program. Design Thinking methods ensure a thorough understanding of user needs and drive innovative solutions. The design thinking method is used to design AR applications with a focus on user experiences and interactive multimedia. Design processes include learning user experiences, gathering ideas, making prototypes, and then performing application testing and evaluation. The aim of this research is to create AR applications that meet user requirements and have interesting and meaningful interactive media features. The results of this production and research will also contribute to the development of the IPB Software Engineering Technology study program."
Candela-Uribe2022,Christian A. Candela-Uribe and Luis E. Sepúlveda-Rodríguez and Julio C. Chavarro-Porras and John A. Sanabria-Ordoñez and José Luis Garrido and Carlos Rodríguez-Domínguez and Gabriel Guerrero-Contreras,SMS-Builder: An adaptive software tool for building systematic mapping studies,SoftwareX,17,,2022,10.1016/j.softx.2021.100935,23527110,"A Systematic Mapping Study is an instrument frequently used to carry out a search process, identification, and classification of studies in different fields. Researchers in front of this type of process have a challenge while managing the data about these studies. This paper presents a software tool that has been created to help those who need to build a systematic mapping study. In addition, this work follows the evidence-based software engineering approach and extends it through a software tool by including different ways of adapting this process."
Murakami2022,Yukasa Murakami and Masateru Tsunoda,Replicated Study of Effectiveness Evaluation of Cutting-Edge Software Engineering,IEICE Transactions on Information and Systems,E105D,1,2022,10.1587/transinf.2021MPL0002,17451361,"Although many software engineering studies have been conducted, it is not clear whether they meet the needs of software development practitioners. Some studies evaluated the effectiveness of software engineering research by practitioners, to clarify the research satisfies the needs of the practitioners. We performed replicated study of them, recruiting practitioners who mainly belong to SMEs (small and medium-sized enterprises) to the survey. We asked 16 practitioners to evaluate cuttingedge software engineering studies presented in ICSE 2016. In the survey, we set the viewpoint of the evaluation as the effectiveness for the respondent's own work. As a result, the ratio of positive answers (i.e., the answers were greater than 2 on a 5-point scale) was 33.3%, and the ratio was lower than past studies. The result was not affected by the number of employees in the respondent's company, but would be affected by the viewpoint of the evaluation."
Pink2024,Sarah Pink and Emma Quilty and John Grundy and Rashina Hoda,"Trust, artificial intelligence and software practitioners: an interdisciplinary agenda",AI and Society,,,2024,10.1007/s00146-024-01882-7,14355655,"Trust and trustworthiness are central concepts in contemporary discussions about the ethics of and qualities associated with artificial intelligence (AI) and the relationships between people, organisations and AI. In this article we develop an interdisciplinary approach, using socio-technical software engineering and design anthropological approaches, to investigate how trust and trustworthiness concepts are articulated and performed by AI software practitioners. We examine how trust and trustworthiness are defined in relation to AI across these disciplines, and investigate how AI, trust and trustworthiness are conceptualised and experienced through an ethnographic study of the work practices of nine practitioners in the software industry. We present key implications of our findings for the generation of trust and trustworthiness and for the training and education of future software practitioners."
Aghdam2022,Mahdi Yousefzadeh Aghdam and Seyed Reza Kamel Tabbakh and Seyed Javad Seyed Mahdavi Chabok and Maryam Kairabadi,An Agent Based Model for Developing Air Traffic Management Software,Journal of Information Systems and Telecommunication,10,37,2022,10.52547/jist.15635.10.37.28,23452773,"The Air Traffic Management system is a complex issue that faces factors such as Aircraft Crash Prevention, air traffic controllers pressure, unpredictable weather conditions, flight emergency situations, airplane hijacking, and the need for autonomy on the fly. agent-based software engineering is a new aspect in software engineering that can provide autonomy. agent-based systems have some properties such: cooperation of agents with each other in order to meet their goals, autonomy in function, learning and Reliability that can be used for air traffic management systems. In this paper, we first study the agent-based software engineering and its methodologies, and then design a agent-based software model for air traffic management. The proposed model has five modules.this model is designed for aircraft,air traffic control and navigations aids factors based on the Belief-Desire-Intention (BDI) architecture. The agent-based system was designed using the agent-tool under the multi-agent system engineering (MaSE) methodology, which was eventually developed by the agent-ATC toolkit. In this model, we consider agents for special occasions such as emergency flights‟ and hijacking airplanes in airport air traffic management areas which is why the accuracy of the work increased. It also made the flight‟s sequence arrangement in take-off and landing faster, which indicates a relative improvement in the parameters of the air traffic management"
Al-Fedaghi2020,Sabah Al-Fedaghi,Conceptual Software Engineering Applied to Movie Scripts and Stories,Journal of Computer Science,16,12,2020,10.3844/jcssp.2020.1718.1730,15526607,"According to researchers, a large proportion of research software (software on which researchers rely) is fragile and the source of numerous problems that plague computational science. This study introduces another application of software engineering tools, conceptual modeling, which can be applied to other fields of research. One way to strengthen the relationship between software engineering and other fields is to develop a good way to perform conceptual modeling that is capable of addressing the peculiarities of these fields of study. This study concentrates on humanities and social sciences, which are usually considered “softer” and further away from abstractions and (abstract) machines. Specifically, we focus on conceptual modeling as a software engineering tool (e.g., UML) in the area of stories and movie scripts. Researchers in the humanities and social sciences might not use the same degree of formalization that engineers do, but they still find conceptual modeling useful. Current modeling techniques (e.g., UML) fail in this task because they are geared toward the creation of software systems. Similar Conceptual Modeling Language (e.g., ConML) has been proposed with the humanities and social sciences in mind and, as claimed, can be used to model “anything.” This study is a venture in this direction, where a software modeling technique, Thinging Machine (TM), is applied to movie scripts and stories. The paper presents a novel approach to developing diagrammatic static/dynamic models of movie scripts and stories. The TM model diagram serves as a neutral and independent representation for narrative discourse and can be used as a communication instrument among participants. It is based on the notion of thing/machine (or thimac). Things and events are defined as what can be created, released, transferred, received, accepted and processed. Machines are what create, process, release, transfer and receive things. The examples presented include examples from Propp’s model of fairytales; the railway children and an actual movie script seem to point to the viability of the approach."
Luo2024,Ke Luo and Wei Deng,Software engineering database programming control system based on embedded system,Applied Mathematics and Nonlinear Sciences,9,1,2024,10.2478/amns.2023.1.00473,24448656,"To improve the programming efficiency of a software database, this paper analyzes the possibility of running an embedded system in a software engineering database by constructing an embedded software data programming control system with the help of an independent time step algorithm and main function. By analyzing and calculating the format of DXF files and commonly used group code values, the format of file storage and the method of expressing data in the graphical element processing module are summarized to ensure the accuracy of software processing and the stability of the software processing process. According to the laws summarized in the analysis, the necessity of introducing an embedded system in the software data programming control process was proposed and simulated, and tested. In the test process, we focus on the process time consumption, space resource occupation rate, running accuracy, and step length data. The test results show that the maximum programming process time of the embedded software data programming control system is only 4.5s, the minimum software space resource occupation rate is 19.7%, the highest operation accuracy is 98.9%, and the calculation time per step is about 0.002s, which is significantly better than the programming system based on remote wireless synchronization system and the computer software programming system based on C language technology. The data calculation results of the independent time step algorithm and the main function prove the feasibility of introducing embedded systems in the software programming process and improving the reusability of software programming code for embedded systems."
Hou2022,Quanjun Hou,Design of a Visual Training System for Software Engineering Education Based on Knowledge Graphs,International Journal of Emerging Technologies in Learning,17,24,2022,10.3991/ijet.v17i24.35659,18630383,"With the increase of the content and difficulty of software engineering education courses, software engineering education visual training system came into being, but the technology is not mature at present, and the representation learning algorithm part of the visual training system needs to be optimized. In order to solve this problem, the research proposes to optimize the take model by using the trans representation algorithm, and embed the optimized knowledge map into the new software engineering education visual training system. The performance of TEKEE model is verified by comparing TEKEE model with Trans E model, Trans D model and TEKED model. The experimental results show that the MR value of the optimized TEKEE model is 62, Hits@10 The value is 0.92, which is better than the other three representation learning models. In terms of the bearing capacity test of the visual training system, the response time of the business operation of the research and design training system is 1.22 seconds, and the CPU occupancy of the application server is 12.5%, all of which are normal. The experimental results show that the performance of the optimized TEKEE model has indeed been greatly improved, and the visual training system composed of the optimized knowledge map has a very good carrying capacity, which can provide a new idea for the software engineering education and training system."
Vives2023,Luis Vives and Karin Melendez and Abraham Dávila,A Systematic Mapping Study of ISO/IEC 29110 and Software Engineering Education,Proceedings of the Institute for System Programming of the RAS,35,1,2023,10.15514/ispras-2023-35(1)-12,20798156,"This article presents a study of the publications made on the ISO/IEC 29110 standard in the university context, especially from the perspective of software engineering education. ISO 29110 is a life cycle profiles for very small entities on systems and software engineering standard, published in many parts. ISO 29110, since its publication in 2011 and its continuous evolution to these days, is the subject of study in different contexts, with education being a relevant axis. Considering, that software engineering education has implications in the software industry in emerging countries, it is necessary to identify and consolidate the work done in this context. In this study, the main research question was what researches have been done at ISO 29110 in the training of software engineers? To answer this question, a systematic mapping study (SMS) was performed. In the SMS, 241 articles were obtained with search string and 17 of them became as primary study after a process selection. Based on these studies, it was possible to determine that the software engineering Basic profile of ISO 29110 and its processes (Project Management and Software Implementation) have been the most studied. Besides, it was identified that project-oriented learning and gamification techniques have been the most used ISO 29110 learning strategies in the training of future software industry professionals."
Yamada2020,Yuki Yamada and Kiichi Furukawa and Atsuo Hazeyama,Fully online implementation of introductory software engineering education including web application development practice,,,,2020,10.1109/TALE48869.2020.9368334,,"Owing to the COVID-19 pandemic, a number of educational institutes have been forced to execute online distributed education. In traditional courses conducting not only lectures but also practice, the instructor and teaching assistants walk around the class, check how the students are doing, find students who are having trouble, and guide them. However, in distributed education, it is difficult to do the same things as in a collocated environment; therefore, some measures are required. We have been conducting introductory software engineering education that applies web application development in addition to lectures. In the fully online implementation of this course, the following issues need to be considered: (1) the creation of a mechanism to mitigate problems regarding the building of a software engineering environment, and (2) the creation of a question and answer process during application development on a remote meeting system. This paper describes some solutions to the two issues."
Pishchukhina2024,Olga Pishchukhina and Daria Gordieieva and Austen Rainer,Delivering computing module for the large part-time software development class from pre- to post-pandemic: An online learning experience,Journal of Systems and Software,210,,2024,10.1016/j.jss.2024.111959,01641212,"Introduction: Covid-19 pandemic brought dramatic changes to higher education settings, particularly for curriculum delivery, moving quickly to online learning. This paper discusses teaching experience covering practices of technology-enhanced learning for the MSc Computing Foundations module (20 CATS) for a large class studying part-time Software Engineering course at the School of Electronics, Electrical Engineering and Computer Science (EEECS), Queen's University Belfast (QUB) during 2019–2022 academic years. We compare on-campus learning with the abrupt shift to online learning during the pandemic and with sustainable online learning a year later. The objective of this study is to answer how part-time software development students perceived their technology-enhanced learning experience from pre- to post-pandemic and to evaluate the impact of the shift to online learning for the part-time class Methods: This study is based on data collected during 2019/2020, 2020/2021, 2021/2022 academic years. Methodology types employed in this study include online observation with statistics collected from the virtual learning environment (VLE) Canvas, quantitative analysis, individual student surveys on teaching techniques and module content Results: This study provided an effective online teaching method for Computer Foundation module, reviewing the impact of different curriculum items and online educational activities starting with content delivery – both synchronous and asynchronous – and moving on to VLE Canvas discussion forums, ungraded formative quizzes, in-term formative assessment in the form of mock exam and, finally, to online summative assessment delivered on VLE Canvas. We investigate positive and negative aspects of technology-enhanced learning from pre- to post-pandemic according to part-time adult students studying the MSc software development program and focus on how this effective learning environment contributes to education practice with a view what developments are worth retaining post-pandemic and what did not work well. Analysis of the data from individual student surveys on teaching techniques and module content for the Computing Foundations module allowed us to conclude that students perceived very positive their technology-enhanced learning experience after shifting to online learning. We also found out that сhanging the module delivery format (from face-to-face to online) did not affect the results of the students’ performance. Conclusion and implications: Adaptation of the MSc Computing Foundation module to a new model of distance learning has proved to be successful, so we can conclude that this delivery format is appropriate for this target audience. This study explored the effectiveness of the pedagogical approach while also gaining valuable insights into the software development student experience of learning in the VLE. The findings from this study may contribute to developing effective teaching practices in software engineering education and adult learning, and improve the preparation of future software professionals in the IT industry."
Lenarduzzi2021,Valentina Lenarduzzi and Oscar Dieste and Davide Fucci and Sira Vegas,Towards a methodology for participant selection in software engineering experiments a vision of the future,,,,2021,10.1145/3475716.3484273,19493789,"Background. Software Engineering (SE) researchers extensively perform experiments with human subjects. Well-defined samples are required to ensure external validity. Samples are selected purposely or by convenience, limiting the generalizability of results. Objective.We aim to depict the current status of participants selection in empirical SE, identifying the main threats and how they are mitigated. We draft a robust approach to participants selection. Method. We reviewed existing participants selection guidelines in SE, and performed a preliminary literature review to find out how participants selection is conducted in SE in practice. Results. We outline a new selection methodology, by 1) defining the characteristics of the desired population, 2) locating possible sources of sampling available for researchers, and 3) identifying and reducing the ""distance"" between the selected sample and its corresponding population. Conclusion. We propose a roadmap to develop and empirically validate the selection methodology."
Stray2020,Viktoria Stray and Nils Brede Moe,Understanding coordination in global software engineering: A mixed-methods study on the use of meetings and Slack,,170,,2020,10.1016/j.jss.2020.110717,01641212,"Given the relevance of coordination in the field of global software engineering, this work was carried out to further understand coordination mechanisms. Specifically, we investigated meetings and the collaboration tool Slack. We conducted a longitudinal case study using a mixed-methods approach with surveys, observations, interviews, and chat logs. Our quantitative results show that employees in global projects spend 7 h 45 min per week on average in scheduled meetings and 8 h 54 min in unscheduled meetings. Furthermore, distributed teams were significantly larger than co-located teams, and people working in distributed teams spent somewhat more time in meetings per day. We found that low availability of key people, absence of organizational support for unscheduled meetings and unbalanced activity from team members in meetings and on Slack were barriers for effective coordination across sites. The positive aspects of using collaboration tools in distributed teams were increased team awareness and informal communication and reduced the need for e-mail. Our study emphasizes the importance of reflecting on how global software engineering teams use meetings and collaboration tools to coordinate. We provide practical advice for conducting better meetings and give suggestions for more efficient use of collaboration tools in global projects."
Ngandu2023,Matipa Ricky Ngandu and David Risinamhodzi and Godwin Pedzisai Dzvapatsva and Courage Matobobo,Capturing student interest in software engineering through gamification: a systematic literature review,Discover Education,2,1,2023,10.1007/s44217-023-00069-4,27315525,"ICT tools in education are widely used to support the aim of achieving learning outcomes by improving critical areas such as student engagement, participation, and motivation. In this study, we examine literature to explore how game elements are used in capturing students’ interest, which the study suggests is fundamental to the teaching and learning of Software Engineering in higher education. Given the potential of alternative ICT tools such as flipped classrooms to increase interest in learning activities, there is a gap in similar literature on capturing interest in gamified environments, which has the potential to improve the achievement of learning outcomes. We applied flow theory to provide a guiding frame for our study. Following a systematic literature review for our data, we analysed 15 papers from the initial 342 articles, which were extracted from IEEE Xplore and Science Direct databases. The main finding in the reviewed papers underscores the positive impact of gamified learning environments on capturing student interest when teaching and learning Software Engineering. While the reviewed papers were not conclusive in identifying the best game elements for capturing students’ interest, we found, that game elements such as points and leaderboards were the most common mechanisms used to advance students' interest when studying Software Engineering courses. The findings also suggest that different game elements are used in gamified environments to increase participation and engagement. The paper adds voice to the practical implications of gamification for teaching and learning. Although our study requires empirical evidence to validate our claims, we believe it sets the stage for further discussion. In the future, comparative studies of game elements in similar environments will be beneficial for identifying the ones that are more engaging and assessing their long-term impacts."
Hinchey2023,Mike Hinchey and Amit Jain and Manju Kaushik and Sanjay Misra,Guest Editorial: Intelligence for systems and software engineering,Innovations in Systems and Software Engineering,19,1,2023,10.1007/s11334-023-00526-1,16145054,
Moreno2020,Oswaldo Alberto Madrid Moreno and Marco Alberto Núñez Ramírez,"University Support, Teaching Competencies And Entrepreneurial Competencies In Software Engineering And Graphic Design Students",International Journal of Emerging Technologies in Learning,16,2,2020,10.3991/ijet.v16i02.17327,18630383,"The objective of this research was to evaluate how university support and teaching entrepreneurial competencies affect students' entrepreneurial competencies. As an empirical test, a sample of 201 software engineering and graphic design students from a university in Northwest Mexico was taken, where it was possible to corroborate three hypotheses. The results suggest that entrepreneurial competencies are influenced by external aspects, where the university can be understood as an open system, but also, as a part of a wider system within the triple helix. This shows that the joint work between the contextual and individual part can benefit the generation of innovation and development for society, especially in a country where there is a lack of R&D, although the practice of entrepreneurship is high. As a result, the university can play a relevant role when developing entrepreneurial competencies to train students to become entrepreneurs through innovation and technology."
Hannousse2021,Abdelhakim Hannousse,Searching relevant papers for software engineering secondary studies: Semantic Scholar coverage and identification role,IET Software,15,1,2021,10.1049/sfw2.12011,17518806,"Searching relevant papers is a fundamental task for the elaboration of secondary studies. This task is known to be tedious and time-consuming when it is made manually, especially with the presence of several academic repositories. Recently, Semantic Scholar has emerged as a new artificial intelligence-based search engine enabling a set of valuable features. The present study investigates the role of Semantic Scholar in retrieving relevant papers for performing secondary studies in software engineering. For this sake, an examination is performed to check the ability of Semantic Scholar to locate included papers in recent and well-established secondary studies. Afterwards, a hybrid and automatic search strategy is introduced making use of Semantic Scholar as a sole search engine and it incorporates: automatic search, snowballing, and use of Computer Science Ontology (CSO) and Software Engineering Body of Knowledge (SWEBOK) for refining queries. The proposed strategy is validated by replicating the search of high-quality secondary studies in the software engineering field. To guarantee objectivity, a systematic search is conducted of recent secondary studies published in the field since 2015. For the coverage test, Semantic Scholar is examined to locate primary papers of selected secondary studies and identify missing venues. The proposed search strategy is used to check the ability to retrieve primary papers of each secondary study. The systematic search yielded 20 high-quality secondary studies with 1337 distinct primary papers. The coverage test revealed that Semantic Scholar covers 98.88% of the papers. The proposed search strategy enabled the full replication of 13 studies and more than 90% for the 7 remaining studies."
Gong2022,Weiwei Gong,Database Programming Technology Based on Computer Software Engineering,,2173,1,2022,10.1088/1742-6596/2173/1/012073,17426596,"Based on practical application, this paper further discusses the database programming technology in computer software engineering. Computer technology has developed to a certain extent and is still active in various fields. However, because the demand for computers in various industries and the requirements for technical performance are different, software engineers are required to develop software systems suitable for enterprises according to their own production characteristics. Because the efficiency and quality of computer software can not reach at present, the programming technology level of database may have a certain impact on the software system.In order for database programming technology to play a full role in various fields, it is necessary to increase investment in database programming technology.This paper analyzes the database technology of computer software engineering in detail. In the process of establishing the actual database programming system, we make full use of the file creation and file access of the database to improve the database programming technology in the current computer software engineering, and then improve the stability of computer software. This paper analyzes computer software engineering, summarizes database programming program, fully realizes the application value of program technology in actual production, and combines database programming technology with the design of computer software engineering project, so as to promote the continuous innovation and development of computer software technology in China."
Tenbergen2024,Bastian Tenbergen and Stephan Krusche,The Future of Software Engineering Education and Training in the Age of AI,IEEE Software,41,2,2024,10.1109/MS.2023.3345960,19374194,"This special issue highlights the critical role of industry-academia collaboration in ensuring the practical relevance of the curriculum. The overall aim is to prepare software engineering students to be adaptable, ethical, and forward-thinking professionals in an artificial intelligence-influenced technological landscape."
Nguyen2020,Tien N. Nguyen and Shaohua Wang,Representation learning for software engineering and programming languages,,,,2020,10.1145/3416506.3423581,,"Recently, deep learning (DL) and machine learning (ML) methods have been massively and successfully applied in various software engineering (SE) and programming languages (PL) tasks. The results are promising and exciting, and lead to further opportunities of exploring the amenability of DL and ML to different SE and PL tasks. Notably, the choice of the representations on which DL and ML methods are applied critically impacts the performance of the DL and ML methods. The rapidly developing field of representation learning (RL) in artificial intelligence is concerned with questions surrounding how we can best learn meaningful and useful representations of data. A broad view of the RL in SE and PL can include the topics, e.g., deep learning, feature learning, compositional modeling, structured prediction, and reinforcement learning. This workshop will advance the pace of research in the unique intersection of representation learning and SE and PL, which will, in the long term, lead to more effective solutions to common software engineering tasks such as coding, maintenance, testing, and porting. In addition to attracting the community of researchers who usually attend FSE, we have made intensive efforts to attract researchers from the RL (broadly AI) community to the workshop, specially from local, very strong groups in local universities, and research labs in the nation."
Monteiro2022,Rodrigo Henrique Barbosa Monteiro and Maurício Ronny De Almeida Souza and Sandro Ronaldo Bezerra Oliveira and Elziane Monteiro Soares,The Adoption of a Framework to Support the Evaluation of Gamification Strategies in Software Engineering Education,,2,,2022,10.5220/0011040900003182,21845026,"Context: gamification has been largely used to increase the engagement and motivation of students and professionals in their organizations, with a variety of models/frameworks for developing gamified approaches. Problem: the empirical data published so far are not sufficient to elucidate the phenomena resulting from the use of gamification, as there is no standardization in the specification of evaluation strategies, methods of analysis and reporting of results. Objective: therefore, the objective of this study is to present and discuss the use of a framework for the evaluation of gamification in the context of software engineering education and training. Method: for this, we executed a case study, in which the framework was used to support the design of an evaluation study for a gamification case in a software process improvement research group in a public university. Results: We report the main findings from observations and reports from the applicator of the case study, and 11 recommendations for the design of evaluation studies supported by the framework. Our main findings are: providing examples of usage of the framework improves its understanding, the framework helped the applicator in understanding that qualitative and quantitative data could be use in compliment to each other, and it helped streamlining the design of the evaluation study, considering the consistency between data to be collected, evaluation questions, and the goals of the evaluation study."
Ramachandran2023,Muthu Ramachandran,S3EF-HBCAs: Secure and Sustainable Software Engineering Framework for Healthcare Blockchain Applications,Blockchain in Healthcare Today,6,2,2023,10.30953/bhty.v6.286,25738240,"Blockchain applications in healthcare have grown rapidly. They include record-keeping, clinical trials, medical supply chains, patient monitoring, etc., where blockchain characteristics are needed to improve safety, privacy, and security. Blockchain technology is one of the most significant disruptive technologies today. However, Porru et al.1 reported that it lacks processes, tools, and techniques. Therefore, this paper provides a systematic framework for a secure and sustainable software engineering framework for healthcare blockchain applications (S3EF-HBCA). S3EF-HBCA is a significant contribution that includes requirements engineering for healthcare, business process modeling for healthcare, domain modeling for healthcare, a reference architecture for healthcare, and validation by a case study on electronic healthcare record management system (EHR), and simulation with business process modeling notation (BPMN) tools. The simulation shows it has taken 10.45 min to process 100 instances of real-time data and service requests. The overall result shows encouragement regarding process, tools, standards, and testing."
Castellanos-Ardila2021,Julieth Patricia Castellanos-Ardila and Barbara Gallina and Guido Governatori,Compliance-aware engineering process plans: the case of space software engineering processes,Artificial Intelligence and Law,29,4,2021,10.1007/s10506-021-09285-5,15728382,"Safety-critical systems manufacturers have the duty of care, i.e., they should take correct steps while performing acts that could foreseeably harm others. Commonly, industry standards prescribe reasonable steps in their process requirements, which regulatory bodies trust. Manufacturers perform careful documentation of compliance with each requirement to show that they act under acceptable criteria. To facilitate this task, a safety-centered planning-time framework, called ACCEPT, has been proposed. Based on compliance-by-design, ACCEPT capabilities (i.e., processes and standards modeling, and automatic compliance checking) permit to design Compliance-aware Engineering Process Plans (CaEPP), which are able to show the planning-time allocation of standard demands, i.e., if the elements set down by the standard requirements are present at given points in the engineering process plan. In this paper, we perform a case study to understand if the ACCEPT produced models could support the planning of space software engineering processes. Space software is safety and mission-critical, and it is often the result of industrial cooperation. Such cooperation is coordinated through compliance with relevant standards. In the European context, ECSS-E-ST-40C is the de-facto standard for space software production. The planning of processes in compliance with project-specific ECSS-E-ST-40C applicable requirements is mandatory during contractual agreements. Our analysis is based on qualitative criteria targeting the effort dictated by task demands required to create a CaEPP for software development with ACCEPT. Initial observations show that the effort required to model compliance and processes artifacts is significant. However, such an effort pays off in the long term since models are, to some extend, reusable and flexible. The coverage level of the models is also analyzed based on design decisions. In our opinion, such a level is adequate since it responds to the information needs required by the ECSS-E-ST-40C framework."
Almeida2022,Fernando Almeida and Eduardo Espinheira,Adoption of Large-Scale Scrum Practices through the Use of Management 3.0,Informatics,9,1,2022,10.3390/informatics9010020,22279709,"Software engineering companies have progressively incorporated agile project management methodologies. Initially, this migration occurred mostly in the context of startups, but in recent years it has also sparked interest from other companies with larger and more geographically dispersed teams. One of the frameworks used for large-scale agile implementation is the LeSS framework. This study seeks to explore how Management 3.0 principles can be applied in the context of the ten practices proposed in the LeSS framework. To this end, a qualitative research methodology based on four case studies is used to identify and explore the role of Management 3.0 in software management and development processes that adopt this agile paradigm. The findings show that the principles of Management 3.0 are relevant to the implementation of the LeSS framework practices, especially in fostering team values and personal values; however, distinct principles between the two paradigms are also identified, namely the greater rigidity of processes advocated in the LeSS framework and a greater focus on process automation."
Liubarets2022,Vladyslava Liubarets and Alina Lyubyma,Heuristic Teaching Methods Of Professional Junior Bachelors In Software Engineering,International Science Journal of Education & Linguistics,1,2,2022,10.46299/j.isjel.20220102.1,,"The article theoretically substantiates the methods of heuristic training of professional bachelors in software engineering. The historical aspect of the origin of heuristics is realized. Scientific and methodological literature on this issue is analyzed. The main definitions of the heuristic learning theory are revealed. The meaning of the concepts: “heuristics”, “heuristic learning” is specified. The main methods and techniques of professional junior bachelors’ readiness formation of in software engineering for heuristic activity in the process of professional training are indicated. The application of heuristic methods of professional junior bachelors in software engineering is argued. The solution of many problems of different CAS and systems of automation of mathematical calculations is defined. The specificity of professional disciplines in the IT field is directly related to abstraction, algorithmic modeling and database structures. The study of educational and methodological literature on the topic of graduation work and the development of existing experience of teachers on this topic and its use in their activities, gave the opportunity to determine the main directions of further work and use the results of this study. preparation of future bachelors of software engineering. The expected pedagogical effect from the use of еру heuristic activities is stated: increasing the motivation of professional junior bachelors in software engineering to heuristic activities; increasing their interest in self-development and self-improvement."
Paasivaara2024,Maria Paasivaara and Xiaofeng Wang,Unveiling the Spectrum of Hybrid Work in Software Engineering: Research Directions,,489 LNBIP,,2024,10.1007/978-3-031-48550-3_14,18651356,"Despite the heated debate on whether hybrid work would be the new normal in the post-pandemic world, it is an exciting, if not new, research phenomenon for Software Engineering (SE) researchers. Hybrid work has a wide range of dimensions and aspects that need exploration and understanding for modern software companies to truly benefit from it. In this paper, we propose a framework that incorporates multiple perspectives on hybrid work in software engineering. We applied the framework to group a set of research topics collected at the GoHyb (Global and Hybrid Work in Software Engineering) workshop collocated with the XP2023 conference, and extrapolated some new topics based on the framework, to demonstrate various research questions that can be asked on hybrid work in software engineering. We conclude the paper with a remark on the need of more contextual and nuanced understanding of hybrid work in software engineering."
Safaruddin2023,Safaruddin and Husain Syam and Darmawang,Development of Microlearning Content for Software Engineering Courses by Using ADDIE Model,Asian Journal of Education and Social Studies,49,3,2023,10.9734/ajess/2023/v49i31175,,"This research aims to: (1) Develop an e-learning design model based on micro-learning content, (2) Analyze the validity of the developed micro-learning content-based e-learning design model, (3) Analyze the practicality of the microlearning content-based e-learning design model for Engineering courses Software, (4) Analyzing the effectiveness of e-learning design models based on microlearning content for Software Engineering courses. This type of research is Research and development (R&D) with the Analysis, Design, Development, Implementation, Evaluation (ADDIE) development model. Data collection techniques used questionnaires and tests. The data analysis technique used qualitative descriptive statistical analysis. The resulting product was validated by model experts, material experts and media experts, while the trials carried out were small group trials and expanded trials. The results of the research are: (1) producing an e-Learning Design Development Model based on Microlearning Content and model supporting products with research and development stages consisting of 5 stages, namely Analysis, Design, Development, Implementation. and Evaluation which are divided into 3 major stages, namely a) Microlearning Modeling; b) Microlearning Development; c) Microlearning Implementation, (2) The results of assessing the validity of model and model product by experts respectively produced an average score of 87.1% and 84.5%, both of which were categorized as very feasible or valid, in order that the model product is suitable for testing in field, (3) The results of the practicality test of the model from student and lecturer respondents respectively obtained an average score of 87.5% and 87.2%, both of which were categorized as very good or very practical, (4) The model was declared effective based on the results of the score calculation N-Gain is based on student pre-test and post-test scores. The N-Gain score obtained was more than 76% or categorized as effective. This showed an increase in students' abilities before and after using the model for the Software Engineering course."
Budgen2020,David Budgen and Pearl Brereton and Nikki Williams and Sarah Drummond,What support do systematic reviews provide for evidence-informed teaching about software engineering practice?,E-Informatica Software Engineering Journal,14,1,2020,10.37190/e-Inf200101,20844840,"Background: The adoption of the evidence-based research paradigm by software engineering researchers has created a growing knowledge base provided by the outcomes from systematic reviews. Aim: We set out to identify and catalogue a sample of the knowledge provided by systematic reviews, to determine what support they can provide for an evidence-informed approach to teaching about software engineering practice. Method: We undertook a tertiary study (a mapping study of systematic reviews) covering the period to the end of 2015. We identified and catalogued those reviews that had findings or made recommendations that were considered relevant to teaching about industry practice. Results: We examined a sample of 276 systematic reviews, selecting 49 for which we could clearly identify practice-oriented findings and recommendations that were supported by the data analysis provided in the review. We have classified these against established software engineering education knowledge categories and discuss the extent and forms of knowledge provided for each category. Conclusion: While systematic reviews can provide knowledge that can inform teaching about practice, relatively few systematic reviews present the outcomes in a form suitable for this purpose. Using a suitable format for presenting a summary of outcomes could improve this. Additionally, the increasing number of published systematic reviews suggests that there is a need for greater coordination regarding the cataloguing of their findings and recommendations."
Pizard2020,Sebastián Pizard and Diego Vallespir,Developing a Taxonomy for Software Engineering Education Through an Empirical Approach,,23,2,2020,10.19153/cleiej.23.2.5,07175000,"Background: With just over 50 years since birth, software engineering gathers more and more topics. This diversity, which shows how broad and prolific the area is, also greatly fragments knowledge. Efforts to develop classifications and taxonomies can collaborate in ordering this knowledge. Objective: This work aims to contribute to organizing software engineering education knowledge, a sub-area in which formalization is still necessary. Method: We propose a process for the construction of controlled vocabularies. We instantiated this process twice; first, using automatic clustering techniques to analyze over 1,000 articles; and then, we focused on concepts related to teaching techniques and methods. Results: We present a taxonomy with 60 terms with covers concepts to be taught, methods to use, and where to do it. The ‘teaching approaches and methods’ category covers 26 terms with their definitions and most relevant references. Implications: The taxonomy can be used by teachers and researchers to understand the breadth of the field, to place their research initiatives in a broader context and to conduct more rigorous searches in the literature. We believe it is necessary to continue working on the taxonomy’s expansion and also to carry out validation activities, if possible, including experts’ validation."
Aleem2023,Saiqa Aleem and Faheem Ahmed,Practicing Equity Diversity Inclusion (EDI) in Software Development Teams: A Systematic Literature Survey,IEEE Access,11,,2023,10.1109/ACCESS.2023.3312681,21693536,"Human factors in successful software projects have always been a critical element in software engineering, however, it has always been overshadowed by focusing more on technology and underlying processes. This work is inspired by the recent increasing interest from the software engineering research community in human factors and software development by leveraging and understanding some examples of human factors such as Equity, Diversity, and Inclusion (EDI) which were not given due research consideration earlier. We performed a systematic literature review (SLR) to review the state-of-the-art literature on practicing EDI in software development teams despite of country or culture. We found that evidence of comprehensive research about practicing EDI in software development teams is limited, the up-to-date majority focus is on the topic of diversity, whereas research on topics of practicing equity and inclusion in software development teams is sporadic. It is expected that investigating the impact of human factors in the context of EDI's triangle will generate new knowledge. This will allow software practitioners to understand the benefits of practicing EDI in managing software development teams as well as provide opportunities to incorporate them into the core development process activities. In the end, future research directions for EDI practices in software development teams are also identified."
Lu2022,Litao Lu,Key Technologies of Software Engineering Based on T-ACO Algorithm,Mobile Information Systems,2022,,2022,10.1155/2022/4429373,1574-017X,"The main core of software engineering key technologies is the development of software services, ensuring the scientificity, security, and stability of the application software engineering system. At present, China’s economic development urgently needs the support of software engineering technology. Based on the T-ACO algorithm, the scientificity of software engineering and the accuracy of data have been significantly improved compared with traditional software engineering technology. It plays an important role in promoting the follow-up software engineering technology. In order to effectively analyze the key technology of engineering software, an improved ant colony algorithm based on T distribution is proposed in this paper. Because the basic ant colony algorithm is easy to fall into the local optimum and the optimization accuracy is low, in the optimization process, at the beginning of the pheromone update, the introduction of the T distribution is helpful for the basic ant colony algorithm to make up for its shortcomings. Adding pheromone variables to the basic ant colony algorithm improves the diversity of the ant colony, thereby eliminating the limitations of local optimal solutions. At the same time, the T-ACO algorithm also improves the search accuracy and convergence speed of automatic data generation in software engineering. In this paper, the performance of the T-ACO algorithm is simulated by experiments. Experimental analysis shows that when the population size is small, the T-ACO algorithm may sometimes not converge to the optimal solution, but when the population size is large (≥50), the T-ACO algorithm may converge to the optimal solution. It can realize the coverage of the total path by the output test case set. While the other two algorithms can achieve full path coverage, they are not stable, resulting in an average coverage between 90% and 100%. The T-ACO algorithm not only has good accuracy in creating test case sets, but also has good algorithm performance, and it is suitable as a multipath test case creation algorithm."
Bomstrm2023,Henri Bomström and Markus Kelanti and Elina Annanperä and Kari Liukkunen and Terhi Kilamo and Outi Sievi-Korte and Kari Systä,Information needs and presentation in agile software development,Information and Software Technology,162,,2023,10.1016/j.infsof.2023.107265,09505849,"Context: Agile software companies applying the DevOps approach require collaboration and information sharing between practitioners in various roles to produce value. Adopting new development practices affects how practitioners collaborate, requiring companies to form a closer connection between business strategy and software development. However, the types of information management, sales, and development needed to plan, evaluate features, and reconcile their expectations with each other need to be clarified. Objective: To support practitioners in collaborating and realizing changes to their practices, we investigated what information is needed and how it should be represented to support different stakeholders in their tasks. Compared to earlier research, we adopted a holistic approach – by including practitioners throughout the development process – to better understand the information needs from a broader viewpoint. Method: We conducted six workshops and 12 semi-structured interviews at three Finnish small and medium-sized enterprises from different software domains. Thematic analysis was used to identify information-related issues and information and visualization needs for daily tasks. Three themes were constructed as the result of our analysis. Results: Visual information representation catalyzes stakeholder discussion, and supporting information exchange between stakeholder groups is vital for efficient collaboration in software product development. Additionally, user-centric data collection practices are needed to understand how software products are used and to support practitioners’ daily information needs. We also found that a passive way of representing information, such as a dashboard that would disturb practitioners only when attention is needed, was preferred for daily information needs. Conclusion: The software engineering community should consider reviewing the information needs of practitioners from a more holistic view to better understand how tooling support can benefit information exchange between stakeholder groups when making product development decisions and how those tools should be built to accommodate different stakeholder views."
Wiesmayr2022,Bianca Wiesmayr,Towards facilitating software engineering for production systems in Industry 4.0 with behavior models,,,,2022,10.1109/ICSE-Companion55297.2022.9793804,02705257,"With the growing adoption of Industry 4.0 concepts in production systems, new challenges arise in engineering control software. Highly distributed control with tight real-time constraints and safety regulations results in increasingly complex software. Current research focuses on increasing the abstraction with new architectures and modularization of software. The presented PhD research addresses modeling of the interactions between control software components, and of the emergent behavior of these compositions. Such behavior models can support the initial implementation, and facilitate (semi-)automated testing and monitoring of control software. Finally, visualizing behavior in a model can enhance understandability of existing control software, when software developers need not access abstracted hierarchy levels to deduct their functionality. This work aims at optimizing the benefit of behavior models in developing control software: Modeling the expected behavior directly for new software will allow using them throughout the software life-cycle. For legacy software, the initial development effort of behavior models will be minimized by automatically capturing behavior models from the implementation. The approach is evaluated in case studies and user studies to integrate experiences from the industrial domain into this software engineering research."
Gjorgjevikj2023,Ana Gjorgjevikj and Kostadin Mishev and Ljupcho Antovski and Dimitar Trajanov,Requirements Engineering in Machine Learning Projects,IEEE Access,11,,2023,10.1109/ACCESS.2023.3294840,21693536,"Over the last decade, machine learning methods have revolutionized a large number of domains and provided solutions to many problems that people could hardly solve in the past. The availability of large amounts of data, powerful processing architectures, and easy-to-use software frameworks have made machine learning a popular, readily available, and affordable option in many different domains and contexts. However, the development and maintenance of production-level machine learning systems have proven to be quite challenging, as these activities require an engineering approach and solid best practices. Software engineering offers a mature development process and best practices for conventional software systems, but some of them are not directly applicable to the new programming paradigm imposed by machine learning. The same applies to the requirements engineering best practices. Therefore, this article provides an overview of the requirements engineering challenges in the development of machine learning systems that have been reported in the research literature, along with their proposed solutions. Furthermore, it presents our approach to overcoming those challenges in the form of a case study. Through this mixed-method study, the article tries to identify the necessary adjustments to (1) the best practices for conventional requirements engineering and (2) the conventional understanding of certain types of requirements to better fit the specifics of machine learning. Moreover, the article tries to emphasize the relevance of properly conducted requirements engineering activities in addressing the complexity of machine learning systems, as well as to motivate further discussion on the requirements engineering best practices in developing such systems."
Shepherd2022,David C. Shepherd and Felipe Fronchetti and Yu Liu and Daqing Hou and Jan Dewaters and Mary Margaret Small,Project-Sized Scaffolding for Software Engineering Courses,,,,2022,10.1145/3524487.3527362,,"Students can often graduate with a degree in computer science without working with legacy code bases, yet when they join the workforce they will almost certainly work on an existing project with thousands, if not millions of lines of existing code. In order to give students a realistic experience without overwhelming them, we added scaffolding to an existing open source project and used it in our third year software engineering course. We asked students to complete a series of 5 tasks, from bug fixing to feature addition, with this scaffolded project. Our scaffolding consisted of enhanced documentation, demonstration videos, compilation videos, enhanced task descriptions, and hints for task completion. After running this course project we collected feedback via a survey (n=87) and a small focus group (n=7). We found that students appreciated the realistic experience, but that they recommend further scaffolding, especially within source code, to better balance between diﬃculty and learning."
Saarimki2020,Nyyti Saarimäki and Valentina Lenarduzzi and Sira Vegas and Natalia Juristo and Davide Taibi,Cohort studies in software engineering: A vision of the future,,,,2020,10.1145/3382494.3422160,19493789,Background. Most Mining Software Repositories (MSR) studies cannot obtain causal relations because they are not controlled experiments. The use of cohort studies as defined in epidemiology could help to overcome this shortcoming. Objective. Propose the adoption of cohort studies in MSR research in particular and empirical Software Engineering (SE) in general. Method. We run a preliminary literature review to show the current state of the practice of cohort studies in SE. We explore how cohort studies overcome the issues that prevent the identification of causality in this type of non-experimental designs. Results. The basic mechanism used by cohort studies to try to obtain causality consists of controlling potentially confounding variables. This is articulated by means of different techniques. Conclusion. Cohort studies seem to be a promising approach to be used in MSR in particular and SE in general.
Jie2022,Zhang Jie,Analysis and Construction of Software Engineering OBE Talent Training System Structure Based on Big Data,Security and Communication Networks,2022,,2022,10.1155/2022/3208318,19390122,"Software engineering is one of the most active fields of entrepreneurship and innovation in the world, and it is also the core field of the information technology industry. Software talents as the foundation and support are an important weight to determine the future direction of my country's software engineering. How to make colleges and universities cultivate compound software talents with innovative ability and engineering ability and how to guide students to closely combine innovative thinking with social practice are a major challenge faced by the current software process education in colleges and universities in my country. At present, the overall quality of software engineers is poor, which cannot meet the needs of enterprises and training objectives. This paper puts forward the application of the OBE (outcome-based education) model in the training of software talents, which can effectively solve the current problems of talent quality and social demand. The analysis shows that there is a high correlation between collaborative education and satisfaction, and the collaborative education model can effectively improve satisfaction. The investment of scientific research funds can effectively improve the overall quality of scientific research team members. The OBE talent training mode can effectively improve the overall test effect, whether in the experimental set or the test set, the test result of the OBE talent training structure is still the highest, the accuracy rate can reach 94.23%, the recall rate can reach 94.51%, and the F1 value can reach 95.13%. It is fully explained that the identification accuracy is the highest when the OBE talent-training structure is adopted."
Mancl2021,Dennis Mancl and Steven D. Fraser,"The Future of Software Engineering: Where Will Machine Learning, Agile, and Virtualization Take Us Next?",,426,,2021,10.1007/978-3-030-88583-0_23,18651356,"Software has become the lifeblood of the 21st century, enabling a broad range of commercial, medical, educational, agricultural, and government applications. These applications are designed and deployed through a variety of software best practices. With the onset of the COVID-19 pandemic, developers have embraced virtualization (remote working) and a variety of strategies to manage the complexity of global development on multiple platforms. However, evolving hazards such as network security, algorithm bias, and the combination of careless developers and deliberate attacks continue to be a challenge. An XP2021 panel organized and chaired by Steven Fraser debated the future of software engineering and related topics such education, ethics, and tools. The panel featured Anita Carleton (CMU’s SEI), Priya Marsonia (Cognizant), Bertrand Meyer (SIT, Eiffel Software), Landon Noll (Independent Consultant), and Kati Vilkki (Reaktor)."
Novielli2021,Nicole Novielli and Fabio Calefato and Filippo Lanubile and Alexander Serebrenik,Assessment of off-the-shelf SE-specific sentiment analysis tools: An extended replication study,Empirical Software Engineering,26,4,2021,10.1007/s10664-021-09960-w,15737616,"Sentiment analysis methods have become popular for investigating human communication, including discussions related to software projects. Since general-purpose sentiment analysis tools do not fit well with the information exchanged by software developers, new tools, specific for software engineering (SE), have been developed. We investigate to what extent off-the-shelf SE-specific tools for sentiment analysis mitigate the threats to conclusion validity of empirical studies in software engineering, highlighted by previous research. First, we replicate two studies addressing the role of sentiment in security discussions on GitHub and in question-writing on Stack Overflow. Then, we extend the previous studies by assessing to what extent the tools agree with each other and with the manual annotation on a gold standard of 600 documents. We find that different SE-specific sentiment analysis tools might lead to contradictory results at a fine-grain level, when used off-the-shelf. Conversely, platform-specific tuning or retraining might be needed to take into account differences in platform conventions, jargon, or document lengths."
Estrada-Molina2022,Odiel Estrada-Molina,A Systematic Mapping of Variables Studied in Research Related to Education in Informatics And Computing,Journal of Engineering Education Transformations,36,2,2022,10.16920/jeet/2022/v36i2/22159,23941707,"Previous theoretical studies (reviews and systematic mappings) have only focused on certain variables of the education of Informatics and Computing such as game-based learning, project-based learning, and problem-based learning. Therefore, the objective of this article was to carry out a systematic mapping (2010-2019) to determine which variables are studied in research related to the education of informatics and computing. We performed a systematic mapping to IEEE Xplore (2010-2019). The protocol corresponds to the PRISMA guidelines for systematic reviews and its contextualization to the performance of systematic mappings. When applying the protocol, 160 articles were finally selected of which 154 are indexed in Scopus (96.25%) and 132 indexed in Scopus and WoS (82.5%). The results highlight that the most studied variables are teaching programming, teaching software engineering, teamwork, collaborative learning, educational technology, assessment, project-based learning, problem-based learning, and game-based learning. There is evidence of a cause-effect relationship (multiple correlations) between the dependent variables: teaching of software engineering and teaching of programming with the independent variables: didactic models based on m-learning, e-learning, and b-learning, project-based learning, problem-based learning, artificial intelligence, and educational technology. It concludes by identifying the principal's studies (higher scientific productivity) and the most studied variables in the didactics of Informatics and Computing."
Bader2021,Johannes Bader and Sonia Seohyun Kim and Frank Sifei Luan and Satish Chandra and Erik Meijer,AI in Software Engineering at Facebook,IEEE Software,38,4,2021,10.1109/MS.2021.3061664,19374194,"How can artificial intelligence help software engineers better do their jobs and advance the state of the practice? We describe three productivity tools that learn patterns from software artifacts: code search using natural language, code recommendation, and automatic bug fixing."
Lambiase2024,Stefano Lambiase and Gemma Catolino and Fabiano Pecorelli and Damian A. Tamburri and Fabio Palomba and Willem Jan van den Heuvel and Filomena Ferrucci,An Empirical Investigation Into the Influence of Software Communities’ Cultural and Geographical Dispersion on Productivity,Journal of Systems and Software,208,,2024,10.1016/j.jss.2023.111878,01641212,"Estimating and understanding software development productivity represent crucial tasks for researchers and practitioners. Although different works focused on evaluating the impact of human factors on productivity, a few explored the influence of cultural/geographical diversity in software development communities. More particularly, all previous treatise addresses cultural aspects as abstract concepts without providing a quantitative representation. Improved knowledge of these matters might help project managers to assemble more productive teams and tool vendors to design software analytics toolkits that may better estimate productivity. This paper has the goal of enlarging the existing body of knowledge on the factors affecting productivity by focusing on cultural and geographical dispersion of a development community—namely, how diverse a community is in terms of cultural attitudes and geographical collocation of the members who belong to it. To reach this goal, we performed a mixed-method empirical study. First, we built a statistical model relating dispersion metrics with the productivity of 25 open-source communities on GITHUB. Then, we performed a confirmatory survey with 140 practitioners. The key results of our study indicate that cultural and geographical dispersion considerably impact productivity, thus encouraging managers and practitioners to consider such aspects during all the phases of the software development lifecycle. We conclude our paper by elaborating on the main insights from our analyses and instilling implications that may drive further research."
Kokol2022,Peter Kokol,Software Quality: How Much Does It Matter?,Electronics (Switzerland),11,16,2022,10.3390/electronics11162485,20799292,"Interconnected computers and software systems have become an indispensable part of people’s lives in the period of digital transformation. Consequently, software quality research is becoming more and more critical. There have been multiple attempts to synthesise knowledge gained in software quality research; however, they were focused mainly on single aspects of software quality and did not structure the knowledge holistically. To fill this gap, we harvested software quality publications indexed in the Scopus bibliographic database. We analysed them using synthetic content analysis which is a triangulation of bibliometrics and content analysis. The search resulted in 15,468 publications. The performance bibliometric analysis showed that the production of research publications relating to software quality is currently following an exponential growth trend and that the software quality research community is growing. The most productive country was the United States, followed by China. The synthetic content analysis revealed that the published knowledge could be structured into six themes, the most important being the themes regarding software quality improvement by enhancing software engineering, advanced software testing and improved defect and fault prediction with machine learning and data mining."
Niva2023,Anu Niva and Jouni Markkula and Elina Annanpera,Junior Software Engineers' International Communication and Collaboration Competences,IEEE Access,11,,2023,10.1109/ACCESS.2023.3340409,21693536,"Present-day Software Engineering working environment is highly international. Teams are commonly formed of people from different nationalities and cultural backgrounds. The team's productivity and efficiency depend significantly on its members' international communication and collaboration competences. Therefore, understanding the required competences is essential. The software organizations hiring new software engineers need to be able to define and describe the competences, and the junior software engineers applying for their first jobs should know what competences they need to possess and demonstrate. In this study, to increase understanding of necessary communication and collaboration competences in the international Software Engineering working environment, competences were, first, analyzed from the job advertisements applicable to junior software engineers and, second, identified by a literature review. The results were compared to identify what competences junior software engineers should learn and demonstrate, to be competent in the international software engineer job markets. The job advertisement findings show that the international operational environment expects extensive competence in collaboration, high competence in English, and considerable competence in local language and communication. Intercultural competence and other languages are hardly expected. The literature review emphasizes inter-related communication, collaboration, intercultural, and language competences at various levels. Eventually, junior software engineers should demonstrate a tolerant and adaptable attitude, cooperativeness, independence, openness, courage to influence, oral and written social interaction skills, fluent professional English and local language communication skills as well as field-specific and general collaboration methods. The findings benefit juniors and Software Engineering education through which also employers obtain more competent jobseekers."
Hans2023,R. T. Hans and S. Marebane and J. Coosner,The importance of software engineering code of ethics in a university of technology teaching environment,South African Journal of Higher Education,37,4,2023,10.20853/37-4-5282,,"Positive consideration of software engineering codes of ethics by computing educators promotes inclusion in the teaching of software development courses. For computing educators, this is significant because they contribute immensely to the development of software engineering graduates, not only in terms of teaching technical skills but also in ethical development. This study aims to investigate the perceived importance of codes of ethics by lecturers who teach software development courses at a University of Technology in South Africa. The data was collected using an online survey from 103 educators from two computing departments in a South African UoT; 44 responses were received. Data was analyzed using descriptive statistics to evaluate the responses; the Pearson Chi-square test was applied to assess the level of association between variables of interest for more conclusive results in addressing the objective of the study. The results of this study indicated that the majority of participants were males; female participants amounted only to 18.2 per cent. Results also reported that most participants agreed with all the statements tested to determine the perceived importance of Software Engineering Codes of Ethics to educators. In addition, an association was presented between the importance of a software engineering code of ethics to an educator and three other variables (the need to teach students about ethical behaviour, an obligation for software engineers to consider the ethical implications of their systems and sex of the respondents) respectively. This study recommended that institutions of higher learning consider finding permanent ways of inculcating a culture of ethical conduct into its staff members, encouraging educators to take up professional memberships with professional bodies. These measures will ensure that software development educators are trained to maintain high standards within their profession, embracing the use and adherence to a code of ethics in the teaching of software development courses."
Azameti2022,Adams Addison Kobla Azameti and Godfred Koi-Akrofi and Nelson Agbodo and Julius K. Amegadzie,A Model-Driven Optical Clinic Management Systems: Systematic Software Engineering Approach,EAI Endorsed Transactions on Pervasive Health and Technology,8,30,2022,10.4108/eai.16-3-2022.173610,24117145,"INTRODUCTION: eHealth systems in a modern hospital and clinic require stringent measures to coordinate the operations of doctors, nurses, pharmacies for improved health care delivery. OBJECTIVES: The primary objective is to perform a comparative analysis to devise a novel approach to address the needs of a hospital information management system. This has triggered an urgent response to develop Optical Clinic Management System (OCMS) to address the limitation of the existing system. This intervention would promote the good health and well-being of humankind to meet the Sustainable Development Goals 3 (SDGs 3). METHODS: The study proposed a Design Science Research Methodology (DSRM) approach in Software Engineering as a catalyst to design OCMS to capture patients’ up-to-date records for medical diagnosis. The system is to assist Clinicians to prescribe medications based on a patient’s medical history by clicking a computer button. RESULTS: The limitations discovered during systems analysis and design of the existing systems were addressed during system evaluation and testing. It was observed that the proposed optical clinic management systems received a 98% acceptance for the implementation. CONCLUSION: This study explores the problem facing clinic and hospital administration and established major factors affecting the existing systems. It was discovered that the paper-based management systems used to keep patients’ medical records were found to be unreliable and therefore unsafe to be used as the basis to prescribe medication for patients, hence the need for this comprehensive system to address the problem for effective health care delivery. The situation in the existing system incidentally led to misplaced and unstructured handling of patient clinical records that may inadvertently make the clinicians administer medications with no reference to the patient’s previous diagnosis due to the lost file. Hence, the aftermath of the Covid-19 pandemic and its global destruction of human lives should motivate African leaders to invest adequate resources in the development of information technology applications for robust health information systems to improve health care delivery in Africa."
Semerikov2020,Serhiy Semerikov and Andrii Striuk and Larysa Striuk and Mykola Striuk and Hanna Shalatska,Sustainability in Software Engineering Education: A case of general professional competencies,,166,,2020,10.1051/e3sconf/202016610036,22671242,"The article considers the application of the sustainable development concept to software engineering specialists training. A system of general professional competencies is designed to build sustainable professional competence of software engineering specialist: 1) ability for abstract thinking, analysis and synthesis; 2) ability to apply knowledge in practical situations; 3) ability to communicate in native language; 4) ability to communicate in a foreign language; 5) ability to learn and acquire up-to-date knowledge; 6) ability to search, process and analyze information from various sources; 7) ability to work in a team; 8) ability to act on the basis of ethical considerations; 9) commitment to preserving the environment; 10) ability to act in a socially responsible and conscious manner; 11) ability to realize the rights and obligations as a member of society, to recognize the civil society values and the need for its sustainable development, the rule of law, human rights and freedoms; 12) ability to preserve and enhance the moral, cultural, scientific values and society achievements based on an understanding of the history and patterns of the subject area development, its place in the general system of knowledge about nature and society and in the development of society, equipment's and technology, to use various types and forms of physical activity for active recreation and healthy lifestyle; 13) ability to apply fundamental and interdisciplinary knowledge to successfully solve software engineering problems; 14) ability to evaluate and take into account economic, social, technological and environmental factors affecting the sphere of professional activity; 15) ability for lifelong learning."
Roldan-Molina2020,Gabriela R. Roldan-Molina and Jose R. Mendez and Iryna Yevseyeva and Vitor Basto-Fernandes,Ontology fixing by using software engineering technology,Applied Sciences (Switzerland),10,18,2020,10.3390/APP10186328,20763417,"This paper presents OntologyFixer, a web-based tool that supports a methodology to build, assess, and improve the quality of ontology web language (OWL) ontologies. Using our software, knowledge engineers are able to fix low-quality OWL ontologies (such as those created from natural language documents using ontology learning processes). The fixing process is guided by a set of metrics and fixing mechanisms provided by the tool, and executed primarily through automated changes (inspired by quick fix actions used in the software engineering domain). To evaluate the quality, the tool supports numerical and graphical quality assessments, focusing on ontology content and structure attributes. This tool follows principles, and provides features, typical of scientific software, including user parameter requests, logging, multithreading execution, and experiment repeatability, among others. OntologyFixer architecture takes advantage of model view controller (MVC), strategy, template, and factory design patterns; and decouples graphical user interfaces (GUI) from ontology quality metrics, ontology fixing, and REST (REpresentational State Transfer) API (Application Programming Interface) components (used for pitfall identification, and ontology evaluation). We also separate part of the OntologyFixer functionality into a new package called OntoMetrics, which focuses on the identification of symptoms and the evaluation of the quality of ontologies. Finally, OntologyFixer provides mechanisms to easily develop and integrate new quick fix methods."
Kirova2024,Vassilka D. Kirova and Cyril S. Ku and Joseph R. Laracy and Thomas J. Marlowe,Software Engineering Education Must Adapt and Evolve for an LLM Environment,,1,,2024,10.1145/3626252.3630927,,"In the era of artificial intelligence (AI), generative AI, and Large Language Models (LLMs) in particular, have become increasingly significant in various sectors. LLMs such as GPT expand their applications, from content creation to advanced code completion. They offer unmatched opportunities but pose unique challenges to the software engineering domain. This paper discusses the necessity and urgency for software engineering education to adapt and evolve to prepare software engineers for the emerging LLM environment. While existing literature and social media have investigated AI's integration into various educational spheres, there is a conspicuous gap in examining the specifics of LLMs' implications for software engineering education. We explore the goals of software engineering education, and changes to software engineering, software engineering education, course pedagogy, and ethics. We argue that a holistic approach is needed, combining technical skills, ethical awareness, and adaptable learning strategies. This paper seeks to contribute to the ongoing conversation about the future of software engineering education, emphasizing the importance of adapting and evolving to remain in sync with rapid advancements in AI and LLMs. It is hoped that this exploration will provide valuable insights for educators, curriculum developers, and policymakers in software engineering."
Bose2020,R. P.Jagadeesh Chandra Bose and Kapil Singi and Vikrant Kaulgud and Sanjay Podder and Adam P. Burden,Software Engineering in a Governed World: Opportunities and Challenges,,,,2020,10.1145/3387940.3392270,,"Modern software applications are becoming ubiquitous and pervasive affecting various aspects of our lives and livelihoods. At the same time, the risks to which these systems expose the organizations and end users are also growing dramatically. Governments and regulatory bodies are moving towards developing laws, regulations, and guidelines for several software applications (e.g., those that use data, are based on AI/ML etc.) across different domains. These mandates impose several challenges in the way how software is built and delivered, primary amongst them is to ensure that software and its delivery processes are compliant. There is a need for governance frameworks that enable the recording, monitoring, and analysis of various activities throughout the application development life cycle making the development processes transparent, traceable, verifiable, auditable, and adhering to regulations and best practices, thereby enabling trustworthiness of software. In this paper, we discuss about the challenges and opportunities of software engineering in the governance era."
Gillani2022,Maryam Gillani and Hafiz Adnan Niaz and Ata Ullah,Integration of Software Architecture in Requirements Elicitation for Rapid Software Development,IEEE Access,10,,2022,10.1109/ACCESS.2022.3177659,21693536,"Software Architecture describes system components and their connections. Requirement elicitation catering the perspective of software architecture is quite challenging and relatively less explored research area for the rapid software development. It has gain growing interest due to reusability of existing modules with less cost and quick developmental time. Software architecture in the context of requirement engineering is an abstraction of software system performing a particular task with the help of group of executable architectural components. In this paper, systematic literature review is adapted as a methodology to explore software architectural elements that provides better performance and simplicity in requirement engineering. We analyzed, reviewed and listed the strategies, tools & techniques along with state-of-the-art mechanisms, pros and cons and application areas. Architectural components that are already implemented in the requirement elicitation process for effective software architectural design are briefly analyzed. Purpose of the paper is to explore and discuss the elements that make software architecture more integral and flexible for traceability of requirements. Another purpose is to identify relation between the software requirements and architecture along with exploring the components to bridge gap between requirements and architecture by critically evaluating industrially and academically proposed methods, tools and frameworks. We also highlighted the open research challenges of Software architecture in requirement elicitation for better software development. In the later section, a resource bank is created acting as a valuable model that encompasses targeted relevant groups, sub-groups with latest software architecture tools & techniques, methods and framework sources to facilitate effective requirement engineering."
Kutt2023,Krzysztof Kutt and Grzegorz J. Nalepa,Loki – the semantic wiki for collaborative knowledge engineering,Expert Systems with Applications,224,,2023,10.1016/j.eswa.2023.119968,09574174,"We present Loki, a semantic wiki designed to support the collaborative knowledge engineering process with the use of software engineering methods. Designed as a set of DokuWiki plug-ins, it provides a variety of knowledge representation methods, including semantic annotations, Prolog clauses, and business processes and rules oriented to specific tasks. Knowledge stored in Loki can be retrieved via SPARQL queries, in-line Semantic MediaWiki-like queries, or Prolog goals. Loki includes a number of useful features for a group of experts and knowledge engineers developing the wiki, such as knowledge visualization, ontology storage, or code hint and completion mechanism. Reasoning unit tests are also introduced to validate knowledge quality. The paper is complemented by the formulation of the collaborative knowledge engineering process and the description of experiments performed during Loki development to evaluate its functionality. Loki is available as free software at https://loki.re."
Quansah2023,Andrew Quansah and Asiamah Emmanuel and Bright Kyeremanteng and Esther Ntow Kesse,Requirement engineering problems impacting the quality of software in Sub-Saharan Africa,Indonesian Journal of Electrical Engineering and Computer Science,30,1,2023,10.11591/ijeecs.v30.i1.pp350-355,25024760,"Poor software quality has led to tremendous financial losses, necessitating the goal of this study. This study aimed to find out the major cause of poor quality of software and propose solutions to mitigate the problem. Histogram analysis was conducted using data from software development firms' online applications used to track all defects and issues for each project, which are logged under a unique project ID. The requirement engineering stage was found to produce the most problems that directly or indirectly impact software quality. The capability maturity model integration, prototyping, ISO 9001, Walkthroughs, and Formal Inspections were proposed as solutions that could be used to mitigate the software quality problems that arise from the requirement engineering stage in the software development life cycle."
Yucalar2020,Fatih Yucalar and Akin Ozcift and Emin Borandag and Deniz Kilinc,Multiple-classifiers in software quality engineering: Combining predictors to improve software fault prediction ability,"Engineering Science and Technology, an International Journal",23,4,2020,10.1016/j.jestch.2019.10.005,22150986,"Software development projects require a critical and costly testing phase to investigate efficiency of the resultant product. As the size and complexity of project increases, manual prediction of software defects becomes a time consuming and costly task. An alternative to manual defect prediction is the use of automated predictors to focus on faulty modules and let the software engineer to examine the defective part with more detail. In this aspect, improved fault predictors will always find a software quality application project to be applied on. There are many base predictors tested-designed for this purpose. However, base predictors might be combined with an ensemble strategy to further improve to increase their performance, particularly fault-detection abilities. The aim of this study is to demonstrate fault-prediction performance of ten ensemble predictors compared to baseline predictors empirically. In our experiments, we used 15 software projects from PROMISE repository and we evaluated the fault-detection performance of algorithms in terms of F-measure (FM) and Area under the Receiver Operating Characteristics (ROC) Curve (AUC). The results of experiments demonstrated that ensemble predictors might improve fault detection performance to some extent."
Rico2021,Sergio Rico and Elizabeth Bjarnason and Emelie Engström and Martin Höst and Per Runeson,A case study of industry–academia communication in a joint software engineering research project,Journal of Software: Evolution and Process,,,2021,10.1002/smr.2372,20477481,"Empirical software engineering research relies on good communication with industrial partners. Conducting joint research both requires and contributes to bridging the communication gap between industry and academia (IA) in software engineering. This study aims to explore communication between the two parties in such a setting. To better understand what facilitates good IA communication and what project outcomes such communication promotes, we performed a case study, in the context of a long-term IA joint project, followed by a validating survey among practitioners and researchers with experience of working in similar settings. We identified five facilitators of IA communication and nine project outcomes related to this communication. The facilitators concern the relevance of the research, practitioners' attitude and involvement in research, frequency of communication and longevity of the collaboration. The project outcomes promoted by this communication include, for researchers, changes in teaching and new scientific venues, and for practitioners, increased awareness, changes to practice, and new tools and source code. Besides, both parties gain new knowledge and develop social-networks through IA communication. Our study presents empirically based insights that can provide advise on how to improve communication in IA research projects and thus the co-creation of software engineering knowledge that is anchored in both practice and research."
Felderer2020,Michael Felderer and Wilhelm Hasselbring and Heiko Koziolek and Florian Matthes and Lutz Prechelt and Ralf Reussner and Bernhard Rumpe and Ina Schaefer,Ernst Denert Award for software engineering 2019: Practice meets foundations,Ernst Denert Award for Software Engineering 2019: Practice Meets Foundations,,,2020,10.1007/978-3-030-58617-1,,"This open access book provides an overview of the dissertations of the five nominees for the Ernst Denert Award for Software Engineering in 2019. The prize, kindly sponsored by the Gerlind & Ernst Denert Stiftung, is awarded for excellent work within the discipline of Software Engineering, which includes methods, tools and procedures for better and efficient development of high quality software. An essential requirement for the nominated work is its applicability and usability in industrial practice. The book contains five papers describing the works by Sebastian Baltes (U Trier) on Software Developers'Work Habits and Expertise, Timo Greifenberg's thesis on Artefaktbasierte Analyse modellgetriebener Softwareentwicklungsprojekte, Marco Konersmann's (U Duisburg-Essen) work on Explicitly Integrated Architecture, Marija Selakovic's (TU Darmstadt) research about Actionable Program Analyses for Improving Software Performance, and Johannes Späth's (Paderborn U) thesis on Synchronized Pushdown Systems for Pointer and Data-Flow Analysis - which actually won the award. The chapters describe key findings of the respective works, show their relevance and applicability to practice and industrial software engineering projects, and provide additional information and findings that have only been discovered afterwards, e.g. when applying the results in industry. This way, the book is not only interesting to other researchers, but also to industrial software professionals who would like to learn about the application of state-of-the-art methods in their daily work."
Chen2021,Chung Yang Chen and Kuang Yen Tai and Sin Sian Chong,Quality Evaluation of Structural Design in Software Reverse Engineering: A Focus on Cohesion,IEEE Access,9,,2021,10.1109/ACCESS.2021.3102295,21693536,"Software reverse engineering (SRE) plays a crucial role in contemporary software environments. Software developers may implement a system first then use SRE tools to generate design content such as the Unified Modeling Language (UML) diagrams. In the literature of SRE, studies majorly focus on how precisely the conversion can reflect the system; there is, however, little or no research that further looks into the quality of the converted results. Therefore, this paper presents an online knowledge-based ontological SRE system, OntRECoh, for quality evaluation of converted UML structural models. OntRECoh features a domain-specific knowledge base that focuses on cohesion design and a rule-based inference engine for computing the cohesion scores of Java-based implemented systems and providing improvement recommendations through its Web-based interface. Furthermore, OntRECoh includes both static and dynamic cohesion measures from both the design and the implementation aspects, for the evaluation to be more comprehensive and synthetic in the SRE context."
Perez2023,Quentin Perez and Christelle Urtado and Sylvain Vauttier,Dataset of open-source software developers labeled by their experience level in the project and their associated software metrics,Data in Brief,46,,2023,10.1016/j.dib.2022.108842,23523409,"Developers are extracted from 17 open-source projects from GitHub. Projects are chosen that use the java programming language, the Spring framework and Maven/Gradle build tools. Along with these developers, 24 software engineering metrics are extracted for each of them. These metrics are either calculated by analyzing the source code or relative to project management metadata. Each of these developers then are manually searched for in professional social media such as LinkedIn or Twitter to be labeled with their experience level in their project. Outliers are statistically detected and manually re-assigned when needed. The resulting dataset contains 703 anonymized developers qualified by their 24 project-related software engineering metrics and labeled for their experience. It is suitable for empirical software engineering studies that need to connect developers’ level of experience to tangible software engineering metrics."
Alawairdhi2020,Mohammed Alawairdhi,An Empirical Assessment of Software Engineering Curriculum in Developing Economies and its Professional Relevance,,,,2020,10.1145/3383923.3383931,,"The role or professionally trained and skilled human resource for a self-sustained IT infrastructure in any economy is very significant. The growth of IT industry in several emerging economies in last couple of decades owes mainly to mature curriculum in IT, Software Engineering and related curriculum. In one of my previous research, I had evaluated and analyzed the impact of software engineering curriculum from the perspective of students. In this work, we have expanded upon the same topic by conducting analysis of feedback from professionals and graduating students in various universities of kingdom of Saudi Arabia. The work is aimed to explore what professional areas of current software engineering curriculum are driving the professional market for students and what need further strengthening. The research as yielded some very interesting results which would be discussed in the following sections"
Assyne2022,Nana Assyne and Hadi Ghanbari and Mirja Pulkkinen,The essential competencies of software professionals: A unified competence framework,Information and Software Technology,151,,2022,10.1016/j.infsof.2022.107020,09505849,"Context: Developing high-quality software requires skilled software professionals equipped with a set of basic and essential software engineering competencies (SEC). These competencies and the satisfaction levels derived from them change over a project's lifecycle, or as software professionals move from one project to another. Objective: Previous studies suggest a lack of means enabling SEC stakeholders to identify and assess competencies suitable for different projects. Additionally, previous research has mainly portrayed SEC to be static and overlooked their evolution over time and across projects. We investigate how we could effectively identify and match the competencies of software professionals necessary for different projects. Method: We follow a mixed-method approach to iteratively develop and evaluate a framework for identifying and managing SEC. In so doing, we use the results of an extensive literature review, focus group discussions with experts from academia and industry, and data collected through interviews with 138 individuals with a supervisory role in the software industry. Results: Drawing on the Kano model and Competency Framework for Software Engineers, we propose a Unified Competence Gate for Software Professionals (UComGSP), a framework for identifying and managing SEC. The UComGSP consists of 62 hard competencies, 63 soft competencies, and 25 essential SEC competencies. Additionally, we propose three stakeholders’ satisfaction levels for SEC assessment: basic, performance, and delighter. Furthermore, based on empirical observation, we report 27 competencies not mentioned in the reviewed literature; 11 of them are considered essential competencies. Conclusion: Competence development involves different stakeholders, including software professionals, educators, and the software industry. The UComGSP framework enables SEC stakeholders to (i) identify SE competencies, (ii) identify the essential SEC, and (iii) assess the satisfaction levels that can be derived from different competencies. Future research is needed to evaluate the effectiveness of the proposed framework across software development projects."
Adil2022,Mahum Adil and Ilenia Fronza and Claus Pahl,Software Design and Modeling Practices in an Online Software Engineering Course: The Learners’ Perspective,,2,,2022,10.5220/0010978000003182,21845026,"Background. Global Software Engineering (GSE) education is an established practice in academia. Several methods and tools support communication and programming activities, but earlier development stages, such as software design and modeling practices, are less explored. Aim. The goal of this work is to analyze the learners’ perspective during an online Software Engineering course. In particular, we focus on planning/organization activities and socio-technical challenges during the software design and modeling process. Method. We used a mixed-method approach to collect data from 30 undergraduate students enrolled in an online Software Engineering course. We combined questionnaires and interviews to analyze four GSE elements (i.e., communication practices, team collaboration, task allocation and distribution, and usage of collaboration tools). Moreover, we analyzed the socio-technical challenges faced by the teams. Results. Brainstorming is the most common practice used for planning software design and modeling activities. According to students, the usage of variant design notation is among the technical challenges. Despite the challenges, students would prefer to continue working in distributed teams. Conclusions. The result shares the lessons learned that can be helpful to build best practices for managing software design and modeling activities in GSE project-based courses. It includes the need to define standard architectural terminologies, standard list of collaboration tools, early identification of architectural artifact dependencies, frequent design reviews, and face-to-face kick-off meetings."
Treude2022,Christoph Treude,Taming Multi-Output Recommenders for Software Engineering,,,,2022,10.1145/3551349.3559557,,"Recommender systems are a valuable tool for software engineers. For example, they can provide developers with a ranked list of files likely to contain a bug, or multiple auto-complete suggestions for a given method stub. However, the way these recommender systems interact with developers is often rudimentary - a long list of recommendations only ranked by the model's confidence. In this vision paper, we lay out our research agenda for re-imagining how recommender systems for software engineering communicate their insights to developers. When issuing recommendations, our aim is to recommend diverse rather than redundant solutions and present them in ways that highlight their differences. We also want to allow for seamless and interactive navigation of suggestions while striving for holistic end-to-end evaluations. By doing so, we believe that recommender systems can play an even more important role in helping developers write better software."
Khasanah2022,Fauziah Nikmatul Khasanah,Application of Hash Sha-256 Algorithm in Website-Based Sales Software Engineering,Journal of Applied Data Sciences,3,1,2022,10.47738/jads.v3i1.50,27236471,"Rapid technological developments can spur changes in the cycle of human activities, one of which is software engineering activities that continue to develop in accordance with technological developments, the development of software engineering activities is also developing methods of data security to withstand attacks from irresponsible parties. answer. This research was conducted to analyze the performance and robustness of the sha-256 data security method with ciphertext customization. The steps that the researchers took in conducting the analysis were collecting theory and case examples, designing programs, implementing programs, testing and saving the results. Based on this process, it can be concluded that ciphertext customization on sha-256 is needed to strengthen security and resistance to attacks from irresponsible parties, besides that the performance of sha-256 calculations with customization on ciphertext and without ciphertext is not too much different where only 65 ms difference based on the results of performance testing that researchers did."
Kovaleva2022,Yekaterina Kovaleva and Ari Happonen and Eneli Kindsiko,"Designing Gender-neutral Software Engineering Program. Stereotypes, Social Pressure, and Current Attitudes Based on Recent Studies",,,,2022,10.1145/3524501.3527600,,"The research focuses on designing gender-neutral Software Engineering programs as a tool to reach gender balance in this domain. There is a general lack of female students and subsequently employees in Computer Science, Engineering, Mathematics, and Physics. The attempts of attracting females in technologies have a long history and most topic related studies agree that the main reason for the low ratio of women in STEM is a result of gender stereotypes. This article reviews how gender stereotypes affect students' interest in Software Engineering. The reason we considered gender-neutrality as a tool to achieve gender balance in Software Engineering lies in the fact that both genders experience the pressure of the stereotypes. Studies show that men are not likely to choose feminine products/jobs. Thus, making the educational program more attractive for females could discourage some males from enrolling. Our work considers different approaches to archive gender-neutrality and we present recommendations for gender-neutral program designs, which combine the considered approaches to gender-neutrality. CCS CONCEPTS • Social and professional topics → User characteristics → Gender • Applied computing Education"
Huda2022,Shamsul Huda and Sultan Alyahya and Lei Pan and Hmood Al-Dossari,Combining Innovative Technology and Context based Approaches in Teaching Software Engineering,International Journal of Advanced Computer Science and Applications,13,10,2022,10.14569/IJACSA.2022.0131037,21565570,"Sustainability in learning is very essential for a sustainable future which largely depends on education. Sustainable learning requires learners to increase and rebuild base-knowledge as the circumstances change and get more complex. This becomes very obvious particularly for information technology (IT) discipline where technology is rapidly changing and practice getting more complicated. Sustainability enables students to use their learning from formal education into practice, provide hands on experience (HOE) and help them rebuild their knowledge base in complex situations. This is also essential to achieve a high graduate outcome rate (GOR) which helps the education sector to become sustainable. In the existing policies and frameworks, institutions are moving towards more off-campus learning and less face-to-face learning. As a result, a downward trend is experienced in students’ engagement across IT discipline. This affects students’ ability in achieving HOE and appears to be one of the reasons of low GOR which poses a threat to the sustainability in the education sector for both stakeholder and learners’ perspectives. This paper presents a combined approach of context-based teaching with incorporation of innovative technology to engage students and achieve a better HOE towards sustainability in learning. The proposed approach was adopted in a software engineering course taught at School of IT at Deakin University, Australia. Students were provided context-based teaching material and industry standard software engineering tools for practice to achieve HOE. Students evaluation and assessment results reports that proposed approach was significantly impacted positively to engage the students in classes towards improved sustainable learning."
Sanchez-Gordon2020,Mary Sanchez-Gordon and Ricardo Colomo-Palacios,Factors influencing Software Engineering Career Choice of Andean Indigenous,,,,2020,10.1145/3377812.3390899,02705257,"A diverse workforce is not just ""nice to have"", it is a reflection of a changing world. Such a diverse workforce brings high value to organizations and it is essential for developing the national technological innovation, economic vitality, and global competitiveness. Despite the importance of diversity in the broad field of computing, there is not only a comparatively low representation of women but also other underrepresented minorities, such as indigenous people. To gain insights about their career choice, we conducted 10 interviews with Andean indigenous. The findings reveal that seven factors (social support, exposure to digital technology, autonomy of use, purpose of use, digital skill, identity, and work ethic) help to understand how and why indigenous people choose a career related to Software Engineering. This exploratory study also contributes to challenge common stereotypes and perceptions about indigenous people as low-qualified workers, academically untalented, and unmotivated."
Chow2021,Scott P. Chow and Tanay Komarlu and Phillip T. Conrad,Teaching Testing with Modern Technology Stacks in Undergraduate Software Engineering Courses,,,,2021,10.1145/3430665.3456352,1942647X,"Students' experience with software testing in undergraduate computing courses is often relatively shallow, as compared to the importance of the topic. This experience report describes introducing industrial-strength testing into CMPSC 156, an upper division course in software engineering at UC Santa Barbara. We describe our efforts to modify our software engineering course to introduce rigorous test-coverage requirements into full-stack web development projects, requirements similar to those the authors had experienced in a professional software development setting. We present student feedback on the course and coverage metrics for the projects. We reflect on what about these changes worked (or didn't), and provide suggestions for other instructors that would like to give their students a deeper experience with software testing in their software engineering courses."
DellAnna2023,Davide Dell’Anna and Fatma Başak Aydemir and Fabiano Dalpiaz,Evaluating classifiers in SE research: the ECSER pipeline and two replication studies,Empirical Software Engineering,28,1,2023,10.1007/s10664-022-10243-1,15737616,"Context: Automated classifiers, often based on machine learning (ML), are increasingly used in software engineering (SE) for labelling previously unseen SE data. Researchers have proposed automated classifiers that predict if a code chunk is a clone, if a requirement is functional or non-functional, if the outcome of a test case is non-deterministic, etc. Objective: The lack of guidelines for applying and reporting classification techniques for SE research leads to studies in which important research steps may be skipped, key findings might not be identified and shared, and the readers may find reported results (e.g., precision or recall above 90%) that are not a credible representation of the performance in operational contexts. The goal of this paper is to advance ML4SE research by proposing rigorous ways of conducting and reporting research. Results: We introduce the ECSER (Evaluating Classifiers in Software Engineering Research) pipeline, which includes a series of steps for conducting and evaluating automated classification research in SE. Then, we conduct two replication studies where we apply ECSER to recent research in requirements engineering and in software testing. Conclusions: In addition to demonstrating the applicability of the pipeline, the replication studies demonstrate ECSER’s usefulness: not only do we confirm and strengthen some findings identified by the original authors, but we also discover additional ones. Some of these findings contradict the original ones."
Chaipunyathat2022,Ajchareeya Chaipunyathat and Nalinpat Bhumpenpein,"Communication, culture, competency, and stakeholder that contribute to requirement elicitation effectiveness",International Journal of Electrical and Computer Engineering,12,6,2022,10.11591/ijece.v12i6.pp6472-6485,20888708,"In the context of software development, requirement engineering is one of the crucial phases that leads to software project success or failure. According to several disruptive changes in the software engineering landscape as well as the world's challenge of virus pandemic, the provision of practical and innovative software applications is required. Therefore, issues resolution in requirement elicitation is potentially one of the key success factors resulting in enhanced quality of system requirement. The authors have striven to create new ways of requirement elicitation according to factor effects of communication, culture, competency, and stakeholder, by incorporating tools, processes, methods, and techniques to solve the problems comprehensively, and then proposed an adaptive and applicable conceptual framework. To illustrate these effects, the authors performed a literature review from the past 8 years, and then data analysis from interviews of 27 practitioners, observations and focus groups of software development in real-life projects."
AkohAtadoga2024,Akoh Atadoga and Uchenna Joseph Umoga and Oluwaseun Augustine Lottu and Enoch Oluwademilade Sodiya,"Tools, techniques, and trends in sustainable software engineering: A critical review of current practices and future directions",World Journal of Advanced Engineering Technology and Sciences,11,1,2024,10.30574/wjaets.2024.11.1.0051,,"The quest for sustainability has extended its reach into the realm of software engineering, prompting an exploration of tools, techniques, and emerging trends to mitigate the environmental impact of software development and operation. This review provides a critical review of current practices and future directions in sustainable software engineering. In recent years, the software industry has recognized the need to address the environmental footprint of software systems, considering factors such as energy consumption, resource utilization, and carbon emissions. Consequently, a plethora of tools and techniques have emerged to support sustainable software development processes. These range from energy-efficient programming languages and frameworks to eco-friendly software architectures and design patterns. Moreover, methodologies such as Green Software Engineering (GSE) and Sustainable Software Development (SSD) have gained traction, emphasizing the integration of sustainability considerations throughout the software development lifecycle. By adopting practices like green requirements engineering, energy-aware testing, and eco-design principles, organizations can optimize their software systems for reduced environmental impact without compromising functionality or performance. Furthermore, trends in sustainable software engineering extend beyond traditional development practices. The rise of cloud computing, edge computing, and Internet of Things (IoT) technologies presents both challenges and opportunities for sustainability. Techniques such as serverless computing and containerization offer potential benefits in terms of resource efficiency and scalability, while also introducing new considerations regarding data center energy consumption and electronic waste management. Looking ahead, the future of sustainable software engineering is marked by innovation and collaboration. Emerging technologies such as artificial intelligence (AI) and blockchain hold promise for optimizing resource allocation, enhancing energy efficiency, and fostering transparency in sustainability efforts. Additionally, interdisciplinary collaboration between software engineers, environmental scientists, and policymakers will be essential in shaping a more sustainable digital ecosystem. The journey towards sustainable software engineering involves a multifaceted approach encompassing tools, techniques, and ongoing adaptation to evolving trends. By critically evaluating current practices and embracing future directions, the software industry can contribute to a more environmentally responsible and resilient future."
Marwati2021,Arum Marwati and Ade Wahyudin and Ardian Setio Utomo and Noor Iza and Elfa Nuzila Halwa,Mendukung Transformasi Digital melalui Penyusunan Program Studi Software Engineering,Jurnal Penelitian dan Pengembangan Pendidikan,5,3,2021,10.23887/jppp.v5i3.39242,1979-7109,"STMM sebagai institusi di bidang perguruan tinggi memiliki peran dalam menjawab tantangan tersebut dengan menghasilkan lulusan-lulusan yang cakap dan unggul di bidang digital. Tujuan penelitian ini untuk dapat menyusun program studi baru yang dapat menghasilkan SDM unggul di bidang digital. Penelitian ini dilakukan dengan pendekatan kualitatif deskriptif menggunakan studi kasus. Penelitian dilakukan dengan beberapa tahap, yaitu: analisis pasar, analisis kebutuhan industri, analisis internal STMM, penyusunan prodi baru hasil yang sesuai dengan Kerangka Kualifikasi Nasional Indonesia (KKNI) dan Standar Nasional Pendidikan Tinggi (SNPT) dilanjutkan dengan analisis kebutuhan sarana dan prasarana prodi baru. Kajian ini dilakukan dengan desk research dan focus group discussion dengan pimpinan dan para dosen. Data yang telah terkumpul dianalisis menggunakan analisis kualitatif deskriptif. Hasil penelitian ini adalah analisis awal untuk menyusun pembentukan program studi baru di STMM Yogyakarta. Kajian ini berisi visi dan misi, tujuan, sasaran, profil lulusan, standar kompetensi lulusan, bahan kajian, dan materi pembelajaran prodi baru. Hasil penelitian dapat menjadi bahan kajian untuk meningkatkan peran STMM dalam memenuhi kebutuhan SDM yang terampil digital. Implikasi penelitian ini adalah Program studi Software Engineering diharapkan mampu mencetak SDM yang memiliki keterampilan digital dan berkontribusi untuk pembangunan Indonesia maupun global agar potensi digital Indonesia tidak tertinggal."
Zyrianov2022,Vlas Zyrianov and Cole S. Peterson and Drew T. Guarnera and Joshua Behler and Praxis Weston and Bonita Sharif and Jonathan I. Maletic,Deja Vu: semantics-aware recording and replay of high-speed eye tracking and interaction data to support cognitive studies of software engineering tasks—methodology and analyses,Empirical Software Engineering,27,7,2022,10.1007/s10664-022-10209-3,15737616,"The paper introduces a fundamental technological problem with collecting high-speed eye tracking data while studying software engineering tasks in an integrated development environment. The use of eye trackers is quickly becoming an important means to study software developers and how they comprehend source code and locate bugs. High quality eye trackers can record upwards of 120 to 300 gaze points per second. However, it is not always possible to map each of these points to a line and column position in a source code file (in the presence of scrolling and file switching) in real time at data rates over 60 gaze points per second without data loss. Unfortunately, higher data rates are more desirable as they allow for finer granularity and more accurate study analyses. To alleviate this technological problem, a novel method for eye tracking data collection is presented. Instead of performing gaze analysis in real time, all telemetry (keystrokes, mouse movements, and eye tracker output) data during a study is recorded as it happens. Sessions are then replayed at a much slower speed allowing for ample time to map gaze point positions to the appropriate file, line, and column to perform additional analysis. A description of the method and corresponding tool, Deja Vu, is presented. An evaluation of the method and tool is conducted using three different eye trackers running at four different speeds (60 Hz, 120 Hz, 150 Hz, and 300 Hz). This timing evaluation is performed in Visual Studio, Eclipse, and Atom IDEs. Results show that Deja Vu can playback 100% of the data recordings, correctly mapping the gaze to corresponding elements, making it a well-founded and suitable post processing step for future eye tracking studies in software engineering. Finally, a proof of concept replication analysis of four tasks from two previous studies is performed. Due to using the Deja Vu approach, this replication resulted in richer collected data and improved on the number of distinct syntactic categories that gaze was mapped on in the code."
Ali2023,Josh Mahmood Ali,AI-driven software engineering,Advances in Engineering Innovation,3,1,2023,10.54254/2977-3903/3/2023030,2977-3903,"The intersection of artificial intelligence (AI) and software engineering marks a transformative phase in the technology industry. This paper delves into AI-driven software engineering, exploring its methodologies, implications, challenges, and benefits. Drawing from data sources such as GitHub and Bitbucket and insights from industry experts, the study offers a comprehensive view of the current landscape. While the results indicate a promising uptrend in the integration of AI techniques in software development, challenges like model interpretability, ethical concerns, and integration complexities emerge as significant. Nevertheless, the transformative potential of AI within software engineering is profound, ushering in new paradigms of efficiency, innovation, and user experience. The study concludes by emphasizing the need for further research, better tooling, ethical guidelines, and education to fully harness the potential of AI-driven software engineering."
Razali2023,Rozilawati Razali and Humairath Abu Bakar,Combining Phenomenology and Grounded Theory in Software Engineering: An Experience,,2023-September,,2023,10.34190/ecrm.22.1.1456,20490976,"Phenomenology and grounded theory are two prominent qualitative methods, particularly used in social sciences research. Phenomenology is carried out to understand the individuals' actual experience regarding a phenomenon. The method describes ""what""individuals experience and ""how""they experience it. The focus is on the meaning of the exposures experienced by individuals regarding the phenomenon. Grounded theory on the other hand allows researchers to explore a phenomenon in depth with individuals, by which a theory is then generated. The goal is to go beyond the understanding of phenomenon by producing a theory that describes comprehensively the problem being studied. Although these two methods are initiated by similar motivations, namely to understand a phenomenon, they however employ slightly different approaches during execution. The differences make them fit complementary together to produce a more concrete and holistic outcome. To research that occasionally use qualitative methods such as Software Engineering, these methods bring new and multifaceted experience. Software Engineering research opts for qualitative methods to promote understanding, as many phenomena in the field have yet to be understood by its community. In that respect, grounded theory is becoming quite a norm in Software Engineering research in recent years, phenomenology however is relatively sporadic. This paper shares the experience of employing as well as combining phenomenology and grounded theory in Software Engineering research. The sharing is intended to inspire future research in twofold: more technical fields such as Software Engineering to employ qualitative methods in research; and leveraging the benefits of combining two qualitative methods complementarily in one study."
Edan2022,Naktal Moaid Edan and Basim Mahmood,Usage one of Reverse Engineering Application (Node.js Web) to Construct Software Engineering Documents,Technium: Romanian Journal of Applied Sciences and Technology,4,4,2022,10.47577/technium.v4i4.6381,,"In software engineering, documentation is considered the best way to distinguish the main functions of the software. The lack of software documentation may lead to consuming time and effort during the software design phase, which finally affects the quality of software. For instance, without proper documentation, the software maintainability index would never get a reasonable mark. The research has been progressive originally as a resilient concept, once the concept is proved to be developable in the Software Development Life Cycle (SDLC), the finishing product will be started to improve with satisfactory documentation. Also, software responsibility has caused complications, such as high costs of software repairs and inefficient process of knowledge transmission between developers. As a Software Reverse Engineering (SRE) method, this work uses a combination of an automatic class diagram creation tool with some manual tweaks, as well as source code analysis and comfortable interviews, to recover the product's design principles and requirement use cases. As a result, two documents will be produced: a Software Requirements Specification document and a Software Design Document"
Autili2021,Marco Autili and Ivano Malavolta and Alexander Perucci and Gian Luca Scoccia and Roberto Verdecchia,"Software engineering techniques for statically analyzing mobile apps: research trends, characteristics, and potential for industrial adoption",Journal of Internet Services and Applications,12,1,2021,10.1186/s13174-021-00134-x,18690238,"Mobile platforms are rapidly and continuously changing, with support for new sensors, APIs, and programming abstractions. Static analysis is gaining a growing interest, allowing developers to predict properties about the run-time behavior of mobile apps without executing them. Over the years, literally hundreds of static analysis techniques have been proposed, ranging from structural and control-flow analysis to state-based analysis.In this paper, we present a systematic mapping study aimed at identifying, evaluating and classifying characteristics, trends and potential for industrial adoption of existing research in static analysis of mobile apps. Starting from over 12,000 potentially relevant studies, we applied a rigorous selection procedure resulting in 261 primary studies along a time span of 9 years. We analyzed each primary study according to a rigorously-defined classification framework. The results of this study give a solid foundation for assessing existing and future approaches for static analysis of mobile apps, especially in terms of their industrial adoptability.Researchers and practitioners can use the results of this study to (i) identify existing research/technical gaps to target, (ii) understand how approaches developed in academia can be successfully transferred to industry, and (iii) better position their (past and future) approaches for static analysis of mobile apps."
CostaSilva2022,Camila Costa Silva and Matthias Galster and Fabian Gilson,A qualitative analysis of themes in instant messaging communication of software developers,Journal of Systems and Software,192,,2022,10.1016/j.jss.2022.111397,01641212,"Software developers use instant messaging (e.g., Slack, Gitter) to collaboratively discuss software engineering problems and solutions. This communication takes place in chat rooms that generally contain a description of the main topic of discussion and the messages exchanged. To analyze whether and how the knowledge accumulated in these chat rooms is relevant to other developers, we first need to understand the themes discussed in these chat rooms. In this paper, we used thematic analysis to manually identify software engineering themes in the description of 87 chat rooms of Gitter, an instant messaging tool for software developers. Then, we checked whether these themes also occur in 184 public chat rooms of Slack, another instant messaging tool. We identified 47 themes in Gitter chat rooms, and regarding the applicability of themes, we could relate 36 of our themes to 173 Slack chat rooms. Our results indicate that, in the context of our study, chat rooms in developer instant messaging communication are mostly about software development technologies and practices rather than development processes. Furthermore, most chat rooms are topic- rather than project-related (e.g., a chat room used by developers of a particular software development project)."
Kemell2023,Kai Kristian Kemell and Anh Nguyen-Duc and Mari Suoranta and Pekka Abrahamsson,StartCards — A method for early-stage software startups,Information and Software Technology,160,,2023,10.1016/j.infsof.2023.107224,09505849,"Context: Software startups are important drivers of economy on a global scale, and have become associated with innovation and high growth. However, the overwhelming majority of startups ends in failure. Many of these startup failures ultimately stem from software engineering issues, and requirements engineering (RE) ones in particular. Despite the emphasis placed on the importance of RE activities in the startup context, many startups continue to develop software without a clear market or customer, having never had meaningful contact with their would-be customer. Objective: We develop a method aimed at early-stage startups that is intended to help startups through the initial stages of the startup process: StartCards. The method emphasizes the importance of idea and product validation activities in particular in order to tackle anti-patterns related to (a lack of) RE in startups. This method is based on existing literature, both grey and academic literature. Method: StartCards was developed using the Canonical Action Research (CAR) approach, over the course of 4 AR cycles. During the AR process, the method was used by 44 student startup teams in a practical course setting. Data from the use of the method was collected through self-reporting in the form of modified learning diaries, mentoring meetings with the startup teams, and a qualitative survey. Results: We consider the current version of StartCards useful for early-stage startups based on the data we have collected. The method can also be used as a pedagogical tool in startup education. Conclusions: The paper presents the first published version of the method. While work on the method continues, the method is deemed ready for use."
Kstner2020,Christian Kästner and Eunsuk Kang,Teaching software engineering for AI-enabled systems,,,,2020,10.1145/3377814.3381714,02705257,"Software engineers have significant expertise to offer when building intelligent systems, drawing on decades of experience and methods for building systems that are scalable, responsive and robust, even when built on unreliable components. Systems with artificialintelligence or machine-learning (ML) components raise new challenges and require careful engineering. We designed a new course to teach software-engineering skills to students with a background in ML. We specifically go beyond traditional ML courses that teach modeling techniques under artificial conditions and focus, in lecture and assignments, on realism with large and changing datasets, robust and evolvable infrastructure, and purposeful requirements engineering that considers ethics and fairness as well. We describe the course and our infrastructure and share experience and all material from teaching the course for the first time."
Ren2020,Wei Ren and Stephen Barrett and Sourojit Das,Toward Gamification to Software Engineering and Contribution of Software Engineer,,,,2020,10.1145/3380625.3380628,,"We wish to change developers' behaviors to increase software engineering efficiency and the previous research shows that gamification method can change user's behaviors. Gamification has become a popular topic in many areas including software engineering. We sum up some pairs of gamification features which can influence people's specific behavior like engagement. Thus, we build a general gamification processing model for software development life cycle. Also, we built a metrics model of software engineers' contribution based on gamification processing model for gamifying software engineer's contribution. The next step, we will gamify the contribution of software engineers to change engineer's behaviors for converting junior software engineers to senior software engineers."
Kirk2022,Diana Kirk,Exploring Task Equivalence for Software Engineering Practice Adaptation and Replacement,,,,2022,10.1145/3563835.3567656,,"Practice adaptation in the domain of software engineering (SE) is ubiquitous and well-studied. The main rationale for tailoring lies in the complexity of the environments within which SE projects are implemented. This complexity means that the practices that make up software methodologies are often not fit-for-purpose as-is and must be replaced or adapted to suit local context. However, there is a risk that a practice may be changed in an inappropriate way, with unintended side-effects. For example, altering a practice to reduce documentation may result in a failure to meet standards expectations or the expectations of downstream practices. In this paper, we describe a study that explored the notion of practice equivalence. Our goal was to identify aspects of a practice that are 'similar' and might support adaptation. We found that adapted practices must be within the same software function, perform the same action, be compatible in terms of creativity, reasoning and perspectives and have a compatible interface type. Our contributions are the consideration of an as-yet unexplored area of software process that we hope will generate discussion and future research studies and the creation of a strawman framework that will be a starting point for further exploration and evaluation."
Trasobares2022,Jose Ignacio Trasobares and África Domingo and Lorena Arcega and Carlos Cetina,Evaluating the benefits of software product lines in game software engineering,,A,,2022,10.1145/3546932.3546998,,"Video game development is one of the fastest-growing industries in the world. The use of software product lines (SPLs) has proven to be effective in developing different types of software at a lower cost, in less time, and with higher quality. There are recent research efforts that propose to apply SPLs in the domain of video games. Video games present characteristics that differentiate their development from the development of classic software; for example, game developers perceive more difficulties than other non-game developers when reusing code. In this paper, we evaluate if the adoption of an SPL in game software engineering (GSE) can generate the same benefits as in classic software engineering (CSE) considering the case study of Kromaia. As in other disciplines dealing with human behaviour, empirical research allows for building a reliable knowledge base in software engineering. We present an experiment comparing two development approaches, Clone and Own (CaO) and an SPL in terms of correctness, efficiency, and satisfaction when subjects develop elements of a commercial video game. The results indicate that the elements developed using the SPL are more correct than those developed with CaO but do not indicate significant improvement in efficiency or satisfaction. Our findings suggest that SPLs in GSE may play a different role than the one they have played for decades in CSE. Specifically, SPLs can be relevant to generating new video game content or to balancing video game difficulty."
Dagan2023,Ella Dagan and Anita Sarma and Alison Chang and Sarah D'Angelo and Jill Dicker and Emerson Murphy-Hill,"Building and Sustaining Ethnically, Racially, and Gender Diverse Software Engineering Teams: A Study at Google",,,,2023,10.1145/3611643.3616273,,"Teams that build software are largely demographically homogeneous. Without diversity, homogeneous perspectives dominate how, why, and for whom software is designed. To understand how teams can successfully build and sustain diversity, we interviewed 11 engineers and 9 managers from some of the most gender and racially diverse teams at Google, a large software company. Qualitatively analyzing the interviews, we found shared approaches to recruiting, hiring, and promoting an inclusive environment, all of which create a positive feedback loop. Our findings produce actionable practices that every member of the team can take to increase diversity by fostering a more inclusive software engineering environment."
TOOLU2020,Mansur Alp TOÇOĞLU,Sentiment Analysis for Software Engineering Domain in Turkish,Sakarya University Journal of Computer and Information Sciences,3,3,2020,10.35377/saucis.03.03.769969,,"The focus of this study is to provide a model to be used for the identification of sentiments of comments about education and profession life of software engineering in social media and microblogging sites. Such a pre-trained model can be useful to evaluate students’ and software engineers’ feedbacks about software engineering. This problem is considered as a supervised text classification problem, which thereby requires a dataset for the training process. To do so, a survey is conducted among students of a software engineering department. In the classification phase, we represent the corpus by using conventional and word-embedding text representation schemes and yield accuracy, recall and precision results by using conventional supervised machine learning classifiers and well-known deep learning architectures. In the experimental analysis, first we focus on achieving classification results by using three conventional text representation schemes and three N-gram models in conjunction with five classifiers (i.e., naïve bayes, k-nearest neighbor algorithm, support vector machines, random forest and logistic regression). In addition, we evaluate the performances of three ensemble learners and three deep learning architectures (i.e. convolutional neural network, recurrent neural network, and long short-term memory). The empirical results indicate that deep learning architectures outperform conventional supervised machine learning classifiers and ensemble learners."
Iacob2020,Claudia Iacob and Shamal Faily,"The impact of undergraduate mentorship on student satisfaction and engagement, teamwork performance, and team dysfunction in a software engineering group project",,,,2020,10.1145/3328778.3366835,,"Mentorship schemes in software engineering education usually involve professional software engineers guiding and advising teams of undergraduate students working collaboratively to develop a software system. With or without mentorship, teams run the risk of experiencing team dysfunction: a situation where lack of engagement, internal conicts, and/or poor team management lead to dierent assessment outcomes for individual team members and overall frustration and dissatisfaction within the team. The paper describes a mentorship scheme devised as part of a 33 week software engineering group project course, where the mentors were undergraduate students who had recently completed the course successfully and possessed at least a year's experience as professional software engineers. We measure and discuss the impact the scheme had on: (1) student satisfaction and engagement, (2) team performance, and (3) team dysfunction."
Maguire2020,Joseph Maguire and Quintin Cutts and Steve Draper,Demystifying and Decluttering Participation in Software Engineering Education Programmes,,,,2020,10.1145/3341525.3393983,1942647X,"Academics and employers can partner to deliver professional software engineering education via work-based learning (WBL) programmes. These programmes have the potential to engage and motivate under-represented groups, including those that would not normally engage in higher education. However, challenges still exist in supporting such individuals in participating in WBL programmes. Consequently, we discuss a project on broadening participation in WBL software engineering to support individuals from under-represented groups to participate in software engineering education."
Zhi2023,Qiang Zhi and Li Gong and Jianguo Ren and Meiyu Liu and Zhengshu Zhou and Shuichiro Yamamoto,Element quality indicator: A quality assessment and defect detection method for software requirement specification,Heliyon,9,5,2023,10.1016/j.heliyon.2023.e16469,24058440,"A software requirements specification (SRS) provides a detailed description of the requirements of a software system that is to be developed. The Element Quality Indicator (EQI) is a novel method to detect defects and assess the quality of an SRS. It does not hinge on review guidelines and is based on the SRS element questions extraction method (EQEM). In this study, we optimized the EQI and carried out a systematic and comprehensive experiment to validate and evaluate its effectiveness. The controlled experiment, which included 60 software engineering students, found that 100% of the subjects identified defects in the SRS using EQI. Moreover, the results indicated that the average number of defects detected with EQI was greater than that of the classical review technique - perspective-based reading. Furthermore, the controlled experiment demonstrated that EQI provides a comparatively objective and accurate evaluation of the quality of the SRS and markedly diminishes the bias in understanding software requirements due to the ambiguity of natural language."
Georgiev2023,Dilyan Georgiev,Exploring Knowledge Management From a Software Engineering Perspective,,2,,2023,10.34190/eckm.24.2.1497,20488971,"Context: The knowledge domain of software engineering (SE) gradually expands due to fast emerging technologies and complex organisational processes. The software industry represents a special focus of interest from a Knowledge Management (KM) perspective, as it is characterized by high employee turnover, a high level of outsourced projects, and an accelerated pace of technology innovations. Knowledge management can substantially influence SE knowledge's life cycle, contributing to better structuring of organisational processes of knowledge creation, knowledge transfer, and knowledge sharing. Reflecting on the trends for SE automation with artificial intelligence tools, KM approaches can enable companies and professionals to reconsider the future of SE work. Purpose: The present research aims to identify and investigate how implementing knowledge management principles and processes within the software engineering domain can facilitate the future transformation of software-related jobs. Based on a conceptual model, built upon the literature review, the paper analyses the quantitative data, collected from 91 software experts. Evaluating the impact of both organizational and domain types of SE knowledge, the discussion evaluates the unique value-adding SE processes along with models of how to explore AI applications to automate the SE domain. Results: The outcomes of the analysis identify the models of structuring knowledge management processes in SE both within the technological domain and organisational layers. The discussion section establishes suitable KM methods and approaches for supporting SE job transformation. It identifies the main knowledge management integration practices, demonstrating how combining KM with AI-based innovations may change the SE knowledge flow, improving substantially SE activities like onboarding new team members, monitoring software documentation, version control, and error tracking."
Mendez2020,Daniel Mendez and Daniel Graziotin and Stefan Wagner and Heidi Seibold,Open Science in Software Engineering,,,,2020,10.1007/978-3-030-32489-6_17,,"Open science describes the movement of making any research artefact available to the public and includes, but is not limited to, open access, open data, and open source. While open science is becoming generally accepted as a norm in other scientific disciplines, in software engineering, we are still struggling in adapting open science to the particularities of our discipline, rendering progress in our scientific community cumbersome. In this chapter, we reflect upon the essentials in open science for software engineering including what open science is, why we should engage in it, and how we should do it. We particularly draw from our experiences made as conference chairs implementing open science initiatives and as researchers actively engaging in open science to critically discuss challenges and pitfalls, and to address more advanced topics such as how and under which conditions to share preprints, what infrastructure and licence model to cover, or how do it within the limitations of different reviewing models, such as double-blind reviewing. Our hope is to help establishing a common ground and to contribute to make open science a norm also in software engineering."
Fulcini2023,Tommaso Fulcini and Marco Torchiano,Is ChatGPT Capable of Crafting Gamification Strategies for Software Engineering Tasks,,,,2023,10.1145/3617553.3617887,,"Gamification has gained significant attention in the last decade for its potential to enhance engagement and motivation in various domains. During the last year ChatGPT, a state-of-The-Art large language model has received even more attention both in the field of scientific research and in common use by individuals or companies. In this study, we investigate the possibility of adopting ChatGPT as a tool for designing gamification platforms in the Software Engineering domain. Leveraging the capabilities of ChatGPT, we assess how good is it at generating effective suggestions and ideas for designers or developers. To evaluate ChatGPT's potential as a gamification platform creator we narrowed the context to one particular Software Engineering activity, asking for possible aspects of the activity to be gamified. Each proposed aspect was subsequently unraveled by ChatGPT both asking in a shared and separate context, first following the conversational nature of the model, then applying a validated design framework. The study assesses ChatGPT's ability to select and integrate game elements to build a thriving gamification environment by framing the design of the platform to a state-of-The-Art conceptual framework. To evaluate the goodness of the design choices made we relied both on the Octalysis framework and on personal experience. The findings of the papers show that ChatGPT can only create simple playful experiences not very effective. Although, by instructing the model with more specific desired mechanics and dynamics, it is possible to guide it toward the application of the ideas suggested. We argue that ChatGPT is not capable of building a gamified environment on its own, but it could still be used to build the foundation of a gamification platform as long as the designers refine and rough out the advice gained from a user-centered solution."
Abirami2023,A. M. Abirami and S. Pudumalar and S. Thiruchadai Pandeeswari,Designing an Effective Teaching Learning Environment for an Undergraduate Software Engineering Course,International Journal of Adult Education and Technology,14,1,2023,10.4018/ijaet.321655,2643-7996,"Software engineering is a core theory course offered in undergraduate engineering programmes which deals with various systematic approaches, methods, and tools that can be employed for designing, developing, testing, and maintaining quality software applications. It is one of the challenging courses for the teaching faculty. After graduation, the students are expected to practice software engineering principles and practices in their prospective projects. In order to satisfy the industrial requirement, and to create industry ready software professionals, software engineering course is offered as theory cum practical course. This article focuses on setting up an effective learning environment for learning software engineering courses. The course is blended with traditional classroom teaching methodology along with a set of pedagogical practices, active learning strategies, ICT tools for content delivery and assessment. This enhanced approach in teaching learning methodology improves student learning outcomes, which in turn helps them to adopt corporate practices more easily."
Akilal2022,Abdellah Akilal and M. Tahar Kechadi,An improved forensic-by-design framework for cloud computing with systems engineering standard compliance,Forensic Science International: Digital Investigation,40,,2022,10.1016/j.fsidi.2021.301315,26662817,"“Forensic-by-design” is an emergent and ambitious paradigm that extends the Digital Forensic Readiness (DFR) perspective. Similar to Security-by-design, this new vision advocates the integration of Forensic requirements into the system's design and development stages to get “Forensic-ready” systems. While it seems promising, we hypothesize that: (a) this new alternative is not effective for some open boundaries systems, and (b) this strategy is not fully aligned with the Systems and software Engineering (SE) standards. A six phases research methodology based on systematic literature review, mapping, and analysis was adopted. Our results confirm indeed the stated hypothesis, identify missing key factors, and point out potential omissions. A new System and software Engineering driven Forensic-by-design framework, with an emphasis on Cloud computing systems, is therefore proposed."
Resan2020,Zainab Mohammed Resan and Muayad Sadik Croock,Software engineering model based smart indoor localization system using deep-learning,Telkomnika (Telecommunication Computing Electronics and Control),18,4,2020,10.12928/TELKOMNIKA.V18I4.14318,23029293,"During the last few years, the allocation of objects or persons inside a specific building is highly required. It is well known that the global positioning system (GPS) cannot be adopted in indoor environment due to the lack of signals. Therefore, it is important to discover a new way that works inside. The proposed system uses the deep learning techniques to classify places based on capturing images. The proposed system contains two parts: software part and hardware part. The software part is built based on software engineering model to increase the reliability, flexibility, and scalability. In addition, this part, the dataset is collected using the Raspberry Pi III camera as training and validating data set. This dataset is used as an input to the proposed deep learning model. In the hardware part, Raspberry Pi III is used for loading the proposed model and producing prediction results and a camera that is used to collect the images dataset. Two wheels' car is adopted as an object for introducing indoor localization project. The obtained training accuracy is 99.6% for training dataset and 100% for validating dataset."
Biable2022,Seblewongel Esseynew Biable and Nuno Manuel Garcia and Dida Midekso and Nuno Pombo,Ethical Issues in Software Requirements Engineering,Software,1,1,2022,10.3390/software1010003,,"Context: Ethics have broad applications in different fields of study and different contexts. Like other fields of study, ethics have a significant impact on the decisions made in computing concerning software artifact production and its processes. Hence, in this research, ethics is considered in the context of requirements engineering during the software development process. Objective: The aim of this paper is to discuss the investigation results regarding ethical problems of requirements engineering processes by taking sample software developing companies and exposing existing research gaps. Method: This research uses interviewing, focus group discussions, purposive sampling, and qualitative analysis research methods. Result: This research finds an absence of industry practices, professional responsibility code of conduct standards, and other guidelines within companies when integrating ethical concerns of software during requirements engineering. It also indicates that almost all companies have no identification methods and checking mechanisms for ethical concern considerations. Furthermore, the major identified ethical concerns are classified into six categories as requirements identification problems, quality-related problems, carrying out unpermitted activities, unwillingness to give requirements, knowledge gaps and lack of legal grounds/rules for accountability. Conclusion: From the findings of this research, it can be concluded that, in the case software companies, there is no specific method for identifying ethical concerns. Additionally, there are no standards and guidelines used within the companies. This implies the need to overcome the existing and emerging ethical issues of requirements engineering."
Gtz2020,Sebastian Götz and Andreas Fehn and Frank Rohde and Thomas Kühn,Model-driven software engineering for construction engineering: Quo vadis?,Journal of Object Technology,19,2,2020,10.5381/JOT.2020.19.2.A2,16601769,"Models are an inherent part of the construction industry, which leverages from the steady advancements in information and communication technology. One of these advancements is Building Information Modeling (BIM), which denotes the move from 2D drawings to having semantically rich models of the objects subject to construction. Additionally, the way stakeholders collaborate in construction projects and their organization is revisited. This is commonly denoted as Integrated Project Delivery (IPD). Both BIM and IPD originate from the basic principles of Lean Construction, the vision to minimize waste, increase value, and continuous improvement. The application of Model-driven Software Engineering (MDSE) to BIM is a natural choice. Although several approaches utilizing MDSE for BIM have been proposed, so far no structured overview of the current state of the art has been conducted. Such an overview is vitally needed, because the existing literature is fragmented among multiple research areas. Consequently, in this paper, we present a systematic literature review on the application of MDSE to BIM, IPD and Lean Construction resulting in a systematically derived taxonomy, which we used to classify 97 papers published between 2008 and 2018. Based on the taxonomy, we provide an analysis of the classified research showing (a) where the discourse on model-driven construction engineering currently is, (b) the state of the art of model-driven techniques in construction engineering and (c) open research challenges."
DiRocco2021,Juri Di Rocco and Davide Di Ruscio and Claudio Di Sipio and Phuong T. Nguyen and Riccardo Rubei,Development of recommendation systems for software engineering: the CROSSMINER experience,Empirical Software Engineering,26,4,2021,10.1007/s10664-021-09963-7,15737616,"To perform their daily tasks, developers intensively make use of existing resources by consulting open source software (OSS) repositories. Such platforms contain rich data sources, e.g., code snippets, documentations, and user discussions, that can be useful for supporting development activities. Over the last decades, several techniques and tools have been promoted to provide developers with innovative features, aiming to bring in improvements in terms of development effort, cost savings, and productivity. In the context of the EU H2020 CROSSMINER project, a set of recommendation systems has been conceived to assist software programmers in different phases of the development process. The systems provide developers with various artifacts, such as third-party libraries, documentation about how to use the APIs being adopted, or relevant API function calls. To develop such recommendations, various technical choices have been made to overcome issues related to several aspects including the lack of baselines, limited data availability, decisions about the performance measures, and evaluation approaches. This paper is an experience report to present the knowledge pertinent to the set of recommendation systems developed through the CROSSMINER project. We explain in detail the challenges we had to deal with, together with the related lessons learned when developing and evaluating these systems. Our aim is to provide the research community with concrete takeaway messages that are expected to be useful for those who want to develop or customize their own recommendation systems. The reported experiences can facilitate interesting discussions and research work, which in the end contribute to the advancement of recommendation systems applied to solve different issues in Software Engineering."
Alchokr2022,Rand Alchokr and Jacob Kruger and Yusra Shakeel and Gunter Saake and Thomas Leich,On Academic Age Aspect and Discovering the Golden Age in Software Engineering,,,,2022,10.1145/3528579.3529175,,"Background: Physical aspects are essential human factors that play a key role in a researcher's career and development. Aging is one of the most important physical aspects that can impact the productivity of a researcher (e.g., in terms of publications). In parallel, aging adds experience and proficiency on the scientific research work, such as assuring the quality and reliability of research. Objective: We aim to understand the impact of aging and the academic age on research publications productivity of research software engineers - the people actively developing software or conducting research in an academic research environment - and explore their Golden Age aspect. Method: We performed a first study on the age distribution of researchers who have published at three famous and prestigious software-engineering conferences: ASE, ESEC/FSE, and ICSE, including 4,620 research-track papers and their 7,337 authors. Results: The results suggest that the academic productivity is maximized at year 15 (Golden Age) and it is held roughly constant for further 15 years before it declines. The results also find, that half authors disappear after their first publication year, reflecting dropout rates that academia suffers from. Conclusion: Through this pilot study, we share insights on the age distribution, and thus representation, of software-engineering researchers at major conferences and try to understand whether certain groups of researchers are over- or underrepresented."
Chang2022,Hung Fu Chang and Mohammad Shokrolah Shirazi,Adapting Scrum for Software Capstone Courses,Informatics in Education,21,4,2022,10.15388/infedu.2022.25,16485831,"Scrum is a widely-used framework in industry, so many schools apply it to their software engineering courses, particularly capstone courses. Due to the differences between students and industrial professionals, changing Scrum is necessary to fit capstone projects. In this paper, we suggest a decision-making process to assist instructors in developing a strategy to adapt Scrum for their course. This framework considers critical differences, such as student’s workloads and course schedules, and keeps the Agile principles and Scrum events. To evaluate the adapted Scrum, we investigated student’s learning experiences, satisfaction, and performance by quantitatively analyzing user story points and source codes and qualitatively studying instructor’s evaluations, student’s feedback, and Sprint Retrospective notes. Our two case studies about adapted Scrum showed that having daily stand-up meetings in every class was not helpful, student’s satisfaction positively correlated to the difficulty of the task they tackled, and the project provided good learning experiences."
Khalil2022,Zeinab Abou Khalil and Stefano Zacchiroli,The General Index of Software Engineering Papers,,,,2022,10.1145/3524842.3528494,,"We introduce the General Index of Software Engineering Papers, a dataset of fulltext-indexed papers from the most prominent scientific venues in the field of Software Engineering. The dataset includes both complete bibliographic information and indexed n-grams (sequence of contiguous words after removal of stopwords and non-words, for a total of 577 276 382 unique n-grams in this release) with length 1 to 5 for 44 581 papers retrieved from 34 venues over the 1971-2020 period. The dataset serves use cases in the field of meta-research, allowing to introspect the output of software engineering research even when access to papers or scholarly search engines is not possible (e.g., due to contractual reasons). The dataset also contributes to making such analyses reproducible and independently verifiable, as opposed to what happens when they are conducted using 3rd-party and non-open scholarly indexing services. The dataset is available as a portable Postgres database dump and released as open data."
Serrano2022,Manuel A. Serrano and José A. Cruz-Lemus and Ricardo Perez-Castillo and Mario Piattini,Quantum Software Components and Platforms: Overview and Quality Assessment,ACM Computing Surveys,55,8,2022,10.1145/3548679,15577341,"Quantum computing is the latest revolution in computing and will probably come to be seen as an advance as important as the steam engine or the information society. In the last few decades, our understanding of quantum computers has expanded and multiple efforts have been made to create languages, libraries, tools, and environments to facilitate their programming. Nonetheless, quantum computers are complex systems at the bottom of a stack of layers that programmers need to understand. Hence, efforts towards creating quantum programming languages and computing environments that can abstract low-level technology details have become crucial steps to achieve a useful quantum computing technology. However, most of these environments still lack many of the features that would be desirable, such as those outlined in The Talavera Manifesto for Quantum Software Engineering and Programming. For advancing quantum computing, we will need to develop quantum software engineering techniques and tools to ensure the feasibility of this new type of quantum software. To contribute to this goal, this paper provides a review of the main quantum software components and platformss. We also propose a set of quality requirements for the development of quantum software platforms and the conduct of their quality assessment."
Wohlin2021,Claes Wohlin and Efi Papatheocharous and Jan Carlson and Kai Petersen and Emil Alégroth and Jakob Axelsson and Deepika Badampudi and Markus Borg and Antonio Cicchetti and Federicio Ciccozzi and Thomas Olsson and Séverine Sentilles and Mikael Svahnberg and Krzysztof Wnuk and Tony Gorschek,Towards evidence-based decision-making for identification and usage of assets in composite software: A research roadmap,Journal of Software: Evolution and Process,33,6,2021,10.1002/smr.2345,20477481,"Software engineering is decision intensive. Evidence-based software engineering is suggested for decision-making concerning the use of methods and technologies when developing software. Software development often includes the reuse of software assets, for example, open-source components. Which components to use have implications on the quality of the software (e.g., maintainability). Thus, research is needed to support decision-making for composite software. This paper presents a roadmap for research required to support evidence-based decision-making for choosing and integrating assets in composite software systems. The roadmap is developed as an output from a 5-year project in the area, including researchers from three different organizations. The roadmap is developed in an iterative process and is based on (1) systematic literature reviews of the area; (2) investigations of the state of practice, including a case survey and a survey; and (3) development and evaluation of solutions for asset identification and selection. The research activities resulted in identifying 11 areas in need of research. The areas are grouped into two categories: areas enabling evidence-based decision-making and those related to supporting the decision-making. The roadmap outlines research needs in these 11 areas. The research challenges and research directions presented in this roadmap are key areas for further research to support evidence-based decision-making for composite software."
BoayeBelle2022,Alvine Boaye Belle and Yixi Zhao,Evidence-Based Software Engineering: A Checklist-Based Approach to Assess the Abstracts of Reviews Self-Identifying as Systematic Reviews,Applied Sciences (Switzerland),12,18,2022,10.3390/app12189017,20763417,"A systematic review allows synthesizing the state of knowledge related to a clearly formulated research question as well as understanding the correlations between exposures and outcomes. A systematic review usually leverages explicit, reproducible, and systematic methods that allow reducing the potential bias that may arise when conducting a review. When properly conducted, a systematic review yields reliable findings from which conclusions and decisions can be made. Systematic reviews are increasingly popular and have several stakeholders to whom they allow making recommendations on how to act based on the review findings. They also help support future research prioritization. A systematic review usually has several components. The abstract is one of the most important parts of a review because it usually reflects the content of the review. It may be the only part of the review read by most readers when forming an opinion on a given topic. It may help more motivated readers decide whether the review is worth reading or not. But abstracts are sometimes poorly written and may, therefore, give a misleading and even harmful picture of the review’s contents. To assess the extent to which a review’s abstract is well constructed, we used a checklist-based approach to propose a measure that allows quantifying the systematicity of review abstracts i.e., the extent to which they exhibit good reporting quality. Experiments conducted on 151 reviews published in the software engineering field showed that the abstracts of these reviews had suboptimal systematicity."
Tavares2021,André Afonso Tavares and Caroline Müller Bitencourt,Dialogue between law and software engineering for a new transparency paradigm: digital social control,Revista Eurolatinoamericana de Derecho Administrativo,8,1,2021,10.14409/redoeda.v8i1.9676,2362583X,"This work reflects, based on the dialogue between Law and Software Engineering, about the current paradigm of public transparency, so that electronic portals start to produce communication between citizens and governments, and, therefore, make it materially possible the exercise of social control. Thus, the problem faced here will seek to respond in a propositional way in which way and through which mechanisms the dialogue between the duties of public transparency and software engineering can contribute to the production of communication between the Public Administration and its citizens to make the exercise of social control through transparency portals. The hypothesis is that from the dialogue between law and software engineering, using programming languages, business intelligence techniques, big data, machine learning, artificial intelligence applied to transparency portals and data in format open access made available by the Public Administration, as determined by the Access to Information Law, communication between citizens and government can be made possible through the access made possible by the advent of the internet and the development of various instruments, such as the compilation and production of automated reports, tables, references and immediate access to documents and data of a public nature. The dialectic method was used in this research. Through the use of data provided by the Open Data project (https://dadosabertos.poa.br/), from the capital Porto Alegre / RS, for the present work, data extraction related to bids and administrative contracts (Licitacon) made by the mentioned public entity and made available in commaseparatedvalues - csv format. It was possible to verify 21.946 records of bids carried out and produce some graphs that facilitate the understanding of the data. In addition to open data, it is necessary to structure and expand the Brazilian public administration itself through software engineering, based on what is currently called digital government."
Tuape2022,Micheal Tuape and Victoria T. Hasheela-Mufeti and Jussi Kasurinen,Theory on Non-Technical Characteristics Affecting Process Adoption in Small Software Companies: A Grounded Theory Study,IEEE Access,10,,2022,10.1109/ACCESS.2022.3209673,21693536,"Despite the inefficiency of software processes and products, small software companies (SSCs) enjoy a promising future. These companies are known to have few employees, creating the inefficiency that makes it challenging for the SSCs to adopt effective software practices. This subsequently introduces additional complexity, affecting software engineering processes' adoption. Using the Glaserian Grounded Theory, we conducted interviews (N = 18) with participants from SSCs intensively engaged in software development from four countries. We looked for the common traits that are identifiable as antecedents to the number of employees in a company to affect the adoption of software engineering processes. From the participants' experience, five non-technical characteristics (Risk, Competitive advantage, Resilience, Innovative capacity, and Management ability) emerged, complementing the number of employees to affect the process during software practice in SSCs. By the end of this study, we developed five hypotheses for predicting and explaining the adoption of software engineering processes by small software companies."
Abdalla2021,Reem Abdalla and Alok Mishra,Agent-Oriented Software Engineering Methodologies: Analysis and Future Directions,Complexity,2021,,2021,10.1155/2021/1629419,10990526,"The Internet of Things (IoT) facilitates in building cyber-physical systems, which are significant for Industry 4.0. Agent-based computing represents effective modeling, programming, and simulation paradigm to develop IoT systems. Agent concepts, techniques, methods, and tools are being used in evolving IoT systems. Over the last years, in particular, there has been an increasing number of agent approaches proposed along with an ever-growing interest in their various implementations. Yet a comprehensive and full-fledged agent approach for developing related projects is still lacking despite the presence of agent-oriented software engineering (AOSE) methodologies. One of the moves towards compensating for this issue is to compile various available methodologies, ones that are comparable to the evolution of the unified modeling language (UML) in the domain of object-oriented analysis and design. These have become de facto standards in software development. In line with this objective, the present research attempts to comprehend the relationship among seven main AOSE methodologies. More specifically, we intend to assess and compare these seven approaches by conducting a feature analysis through examining the advantages and limitations of each competing process, structural analysis, and a case study evaluation method. This effort is made to address the significant characteristics of AOSE approaches. The main objective of this study is to conduct a comprehensive analysis of selected AOSE methodologies and provide a proposal of a draft unified approach that drives strengths (best) of these methodologies towards advancement in this area."
Tian2022,Jingbai Tian and Jianghao Yin and Liang Xiao,Software Requirements Engineer's Ability Assessment Method Based on Empirical Software Engineering,Wireless Communications and Mobile Computing,2022,,2022,10.1155/2022/3617140,15308677,"With the expansion of the scale and complexity of modern software systems, the failure rate of software projects remains high. One of the main reasons for the failure of software projects is the defects in processing software requirements. This paper proposes a software requirements engineer's ability assessment method based on empirical software engineering to measure the matching degree between the software requirements engineer's ability and industry expectations. First, collect the recruitment information of software requirements engineers from mainstream recruitment websites. Through natural language processing, the words related to the abilities of the software requirements engineer are counted. These words are summarized in the requirements acquisition, requirements analysis, and other SRE activities, then the industry expectations for various abilities are obtained. Later on, the authors collect the teaching settings of representative SRE courses, reflecting the software requirements engineer's ability to learn the course. After that, this article defines the ratio of the industry expectation weight to the weight of each SRE activity in teaching as the software requirements engineer's ability coefficient, which can intuitively reflect the matching degree between the software requirements engineer's ability and industry expectations. Finally, take the national first-class undergraduate course ""SRE""of Jinling Institute of Technology as an example to verify the method's practicality to a certain extent."
Kosiuczenko2021,Piotr Kosiuczenko,Software engineering techniques for the extraction of ontology of historical geographic information,IBIMA Business Review,2021,,2021,10.5171/2021.265538,19473788,"Unorganized processing textual information in large files is time consuming and error-prone, especially if the texts are ununiform. Investigating past and historical data concerning geography and economics can be facilitated a lot when the data is stored in a data base or has a proper form. There was a lot of research concerning the transformation of textual data to an ontology in a methodical way. In this paper, a method is presented for the extraction of geographic and economic information from a historic Lexicon. The method is based on software engineering techniques and follows an iterative cycle. It results in a well-defined ontology specified using UML class diagrams which can be queried using Object Constraint Language. An evaluation of the model is presented. This research facilitates the comparison of various historical stages of development. Thus, it helps in assessing regional development, qualitative and structural changes in the regional economy, ownership, infrastructure as well as living standard transformation."
Kalu2023,Kelechi Kalu and Taylor R. Schorlemmer and Sophie Chen and Kyle A. Robinson and Erik Kocinare and James C. Davis,Reflecting on the Use of the Policy-Process-Product Theory in Empirical Software Engineering,,,,2023,10.1145/3611643.3613075,,"The primary theory of software engineering is that an organization's Policies and Processes influence the quality of its Products. We call this the PPP Theory. Although empirical software engineering research has grown common, it is unclear whether researchers are trying to evaluate the PPP Theory. To assess this, we analyzed half (33) of the empirical works published over the last two years in three prominent software engineering conferences. In this sample, 70% focus on policies/processes or products, not both. Only 33% provided measurements relating policy/process and products. We make four recommendations: (1) Use PPP Theory in study design; (2) Study feedback relationships; (3) Diversify the studied feed-forward relationships; and (4) Disentangle policy and process. Let us remember that research results are in the context of, and with respect to, the relationship between software products, processes, and policies."
Santos2024,Anne Caroline Melo Santos and Methanias Colaço Júnior and Edna de Carvalho Andrade,Multimedia resources as a support for requirements engineering and software maintenance,Journal of Software: Evolution and Process,36,3,2024,10.1002/smr.2327,20477481,"Textual documentations are frequently used in the software development process to outline features and behaviors of an application. For some people, textual descriptions may not be enough to understand what is being developed. In this scenario, multimedia resources appear as an option for software documentation, providing other ways to observe and interpret information. Objective: To identify and characterize the approaches and techniques which promote the use of multimedia in requirements engineering (RE) to support software development and maintenance. Method: A systematic mapping was conducted to find the primary studies in the literature and collect evidence for directing future research. Results: Only 27.66% of the approaches found validated their solutions through controlled experiments, showing the need to increase the use of scientific method in this area, with replications of studies that will allow to evaluating if other researchers independently will come up with the same results. In this context, the approaches/techniques identified were TRECE, MURMER, Wiki System Multimedia, Storytelling, Virtual World Environment, VisionCatcher, PRESTO4U, ReqVidA, CrowdRE, AVW, The Software Cinema Technique, Dolli Project, UTOPIA, and approaches without explicit names, which, as a rule, use multimedia resources as an additional support. Conclusions: There was a favorable consensus regarding the use of multimedia in RE. The selected studies demonstrated to be favorable to the adoption of media to persist and store the requirements of a system. Moreover, multimedia resources can improve the process of understanding the code and decrease evolution and maintenance costs. General terms are design, documentation, experimentation, human factors, multimedia, reliability, software engineering, verification, and security."
Shaw2022,Mary Shaw and Liming Zhu,Can Software Engineering Harness the Benefits of Advanced AI?,IEEE Software,39,6,2022,10.1109/MS.2022.3203200,19374194,Artificial intelligence (AI) has allowed us to build systems beyond anything deemed possible earlier. Can we evolve existing techniques in software engineering to meet the needs of AI enabled systems or do we need to build unique and novel tools to do so?
Nasrullah2021,Muhammad Nasrullah and Nisa Dwi Angresti and Sayekti Harits Suryawan and Faizal Mahananto,Requirement Engineering terhadap Virtual Team pada Proyek Software Engineering,Journal of Advances in Information and Industrial Technology,3,1,2021,10.52435/jaiit.v3i1.79,2716-1935,"Seiring perkembangan teknologi, virtual team dapat menjadi solusi untuk sebuah proyek software engineering, karena virtual team tidak terbatas oleh letak geografis dan waktu. Namun, untuk menentukan kebutuhan pengguna dari virtual team ini menemui beberapa kendala, karena para stakeholder yang tidak saling bertatap muka, sehingga sulit untuk saling mendapatkan feeling antara stakeholder. Salah satu teknik yang digunakan untuk penggalian kebutuhan pada virtual team ini adalah teknik kolaborasi. Kolaborasi dilakukan dengan memanfaatkan berbagai media/teknologi. Tujuan literature review ini adalah mengulas cara-cara penggalian kebutuhan pada virtual team dan mengulas tantangan apa saja yang dihadapi untuk penggalian kebutuhan pada virtual team. Metode yang digunakan dalam penulisan makalah adalah studi literatur dari berbagai sumber pustaka yang relevan. Hasil studi litetatur ini adalah sebuah pemaparan tentang cara penentuan kebutuhan terhadap virtual team dan tantangan yang dihadapi dalam penggalian kebutuhan pada virtual team. Penentuan kebutuhan pada virtual team dilakukan dengan cara pendekatan user-centered design dan wawancara online."
Davis2022,James C. Davis and Paschal Amusuo and Joseph R. Bushagour,Experience Paper: A First Offering of Software Engineering,,,,2022,10.1145/3524487.3527357,,"This paper describes our first offering of a project-based software engineering course for undergraduate seniors. The course was given to 72 undergraduates, mostly seniors majoring in computer engineering. Our project taught the full engineering cycle with a narrative based on supporting the re-use of software. In two parts spanning 13 weeks, successful teams deployed a web service. We identify lessons learned and opportunities for improvement."
Karita2021,Leila Karita and Brunna Caroline Mourão and Luana Almeida Martins and Larissa Rocha Soares and Ivan Machado,Software industry awareness on sustainable software engineering: a Brazilian perspective,Journal of Software Engineering Research and Development,9,,2021,10.5753/jserd.2021.742,,"Sustainable computing is a rapidly growing research area spanning several areas of computer science. In the software engineering field, the topic has received increasing attention in recent years, with several studies addressing a range of concerns. However, few studies have demonstrated the awareness of software practitioners about the underlying concepts of sustainability in the software development practice. In this effect, this study aims to provide some evidence regarding the practitioners’ perception about the adoption of sustainability in software development, under four main perspectives: economic, social, environmental, and technical. In previous work, we carried out a preliminary survey study with twenty-five software engineers who work in a range of domains. The yielded results indicate an overall lack of knowledge about the topic, in particular, related to concepts about sustainable software. In this study, we extend the survey and reached a number of ninety-seven respondents. The novel results confirmed the evidence raised in the original survey that sustainability in the context of software is a new subject for software practitioners. However, professionals have shown interest in it. There is a general understanding that sustainability should be treated as a quality attribute. Among the observed perspectives, we generated an initial theory, which shows that software practitioners know the subject around ‘Green in Software’, even unconsciously. This study contributes to the green and sustainable software engineering field by bringing evidence on comprehending how the software industry understands the adoption of sustainability in the software development process."
Varava2021,I. P. Varava and A. P. Bohinska and T. A. Vakaliuk and I. S. Mintii,Soft Skills in Software Engineering Technicians Education,,1946,1,2021,10.1088/1742-6596/1946/1/012012,17426596,"The research work analyzes the problem of forming software engineering technicians' soft skills, approaches to defining the list, detail and the maturity level of software engineering technicians' soft skills and basic ways of their formation, in particular: development of new academic programs or individual innovative courses, the problem-oriented approach, the AGILE approach, gamification of learning, development of interdisciplinary courses, etc. The obtained data enable developing methodological recommendations to form software engineering technicians' soft skills when studying the humanities and social sciences and experimentally investigating their efficiency."
Ozkaya2021,Ipek Ozkaya,A Watershed Moment for Search-Based Software Engineering,IEEE Software,38,4,2021,10.1109/MS.2021.3075108,19374194,"Software engineering is about understanding and making the right tradeoffs. Many software engineering tasks-such as test case generation, design, sprint planning, and refactoring-boil down to understanding tradeoffs including concretely expressing the attributes to optimize and elements and decisions that will maximize their intended outcomes. Consequently, many software engineering problems can be formulated as search problems. Driven by these observations, Harman and Jones in 2001 emphasized the importance of concentrated research on the application of search-based techniques in software engineering and coined the subfield of search-based software engineering (SBSE) in software engineering research.1 During the two decades that followed, SBSE has seen a significant amount of increased research where metaheuristic algorithms are used to create recommendations for software engineering tasks."
Ferrara2024,Carmine Ferrara and Giulia Sellitto and Filomena Ferrucci and Fabio Palomba and Andrea De Lucia,Fairness-aware machine learning engineering: how far are we?,Empirical Software Engineering,29,1,2024,10.1007/s10664-023-10402-y,15737616,"Machine learning is part of the daily life of people and companies worldwide. Unfortunately, bias in machine learning algorithms risks unfairly influencing the decision-making process and reiterating possible discrimination. While the interest of the software engineering community in software fairness is rapidly increasing, there is still a lack of understanding of various aspects connected to fair machine learning engineering, i.e., the software engineering process involved in developing fairness-critical machine learning systems. Questions connected to the practitioners’ awareness and maturity about fairness, the skills required to deal with the matter, and the best development phase(s) where fairness should be faced more are just some examples of the knowledge gaps currently open. In this paper, we provide insights into how fairness is perceived and managed in practice, to shed light on the instruments and approaches that practitioners might employ to properly handle fairness. We conducted a survey with 117 professionals who shared their knowledge and experience highlighting the relevance of fairness in practice, and the skills and tools required to handle it. The key results of our study show that fairness is still considered a second-class quality aspect in the development of artificial intelligence systems. The building of specific methods and development environments, other than automated validation tools, might help developers to treat fairness throughout the software lifecycle and revert this trend."
Presler-Marshall2022,Kai Presler-Marshall and Sarah Heckman and Kathryn T. Stolee,What Makes Team[s] Work? A Study of Team Characteristics in Software Engineering Projects,,1,,2022,10.1145/3501385.3543980,,"Teaming is a core component in practically all professional software engineering careers, and as such, is a key skill taught in many undergraduate Computer Science programs. However, not all teams manage to work together effectively, and in education, this can deprive some students of successful teaming experiences. In this work, we seek to gain insights into the characteristics of successful and unsuccessful undergraduate student teams in a software engineering course. We conduct semi-structured interviews with 18 students who have recently completed a team-based software engineering course to understand how they worked together, what challenges they faced, and how they tried to overcome these challenges. Our results show that common problems include communicating, setting and holding to deadlines, and effectively identifying tasks and their relative difficulty. Additionally, we find that self-reflection on what is working and not working or external motivators such as grades help some, but not all, teams overcome these challenges. Finally, we conclude with recommendations for educators on successful behaviours to steer teams towards, and recommendations for researchers on future work to better understand challenges that teams face."
Yousef2022,Yousef A. Yousef and Abdelrafe Elzamly and Mohamed Doheir and Noorayisahbe Mohd Yaacob,Assessing Soft Skills for Software Requirements Engineering Processes,Journal of Computing and Information Technology,29,4,2022,10.20532/CIT.2021.1005397,18463908,"Software requirement engineering (SRE) is the process of establishing, documenting, and maintaining software requirements. The goal of this research is to investigate the importance of soft skills in SRE. The data collection was performed through an online questionnaire. Descriptive statistics, principal components analysis (PCA), and stepwise regression techniques were used to analyze the data. A comprehensive review determined the 31 soft skills associated with SRE. There were 122 software development experts in Gaza who participated in the survey. The PCA analysis extracted six factors, named problem-solving, learning willingness, commitment, pressure resilience, critical thinking, and interpersonal skills. The analysis discovered that the level of SRE practice in Gaza is 73.71%. Furthermore, it was determined that 89.2% of respondents have critical thinking skills, and 85% have problem-solving and commitment skills. The result shows that all soft skills factors have strong links to SRE. However, only four soft skills (problem-solving, willingness to learn, pressure tolerance, and critical thinking) were found to have an impact on SRE. Considering these findings, we recommend focusing on the development of soft skills, especially problem-solving and willingness to learn skills, for the team analyzing software system requirements."
Rosli2022,Marshima Mohd Rosli and Nor Shahida Mohamad Yusop,Evaluating the effectiveness of data quality framework in software engineering,International Journal of Electrical and Computer Engineering,12,6,2022,10.11591/ijece.v12i6.pp6410-6422,20888708,"The quality of data is important in research working with data sets because poor data quality may lead to invalid results. Data sets contain measurements that are associated with metrics and entities; however, in some data sets, it is not always clear which entities have been measured and exactly which metrics have been used. This means that measurements could be misinterpreted. In this study, we develop a framework for data quality assessment that determines whether a data set has sufficient information to support the correct interpretation of data for analysis in empirical research. The framework incorporates a dataset metamodel and a quality assessment process to evaluate the data set quality. To evaluate the effectiveness of our framework, we conducted a user study. We used observations, a questionnaire and think aloud approach to provide insights into the framework through participant thought processes while applying the framework. The results of our study provide evidence that most participants successfully applied the definitions of dataset category elements and the formal definitions of data quality issues to the datasets. Further work is needed to reproduce our results with more participants, and to determine whether the data quality framework is generalizable to other types of data sets."
Staegemann2022,Daniel Staegemann and Matthias Volk and Maneendra Perera and Christian Haertel and Matthias Pohl and Christian Daase and Klaus Turowski,A Literature Review on the Challenges of Applying Test-Driven Development in Software Engineering,Complex Systems Informatics and Modeling Quarterly,2022,31,2022,10.7250/csimq.2022-31.02,22559922,"Due to the ongoing trend of digitalization, the importance of software for today’s society is continuously increasing. Naturally, there is also a huge interest in improving its quality, which led to a highly active research community dedicated to this aim. Consequently, a plethora of propositions, tools, and methods emerged from the corresponding efforts. One of the approaches that have become highly prominent is the concept of test-driven development (TDD) that increases the quality of created software by restructuring the development process. However, such a big change to the followed procedures is usually also accompanied by major challenges that pose a risk for the achievement of the set targets. In order to find ways to overcome them, or at least to mitigate their impact, it is necessary to identify them and to subsequently raise awareness. Furthermore, since the effect of TDD on productivity and quality is already extensively researched, this work focuses only on issues besides these aspects. For this purpose, a literature review is presented that focuses on the challenges of TDD. In doing so, challenges that can be attributed to the three categories of people, software, and process are identified and potential avenues for future research are discussed."
Ahmed2022,Toufique Ahmed and Premkumar Devanbu,Multilingual training for Software Engineering,,2022-May,,2022,10.1145/3510003.3510049,02705257,"Well-trained machine-learning models, which leverage large amounts of open-source software data, have now become an interesting approach to automating many software engineering tasks. Several SE tasks have all been subject to this approach, with performance gradually improving over the past several years with better models and training methods. More, and more diverse, clean, labeled data is better for training; but constructing good-quality datasets is time-consuming and challenging. Ways of augmenting the volume and diversity of clean, labeled data generally have wide applicability. For some languages (e.g., Ruby) labeled data is less abundant; in others (e.g., JavaScript) the available data maybe more focused on some application domains, and thus less diverse. As a way around such data bottlenecks, we present evidence suggesting that human-written code in different languages (which performs the same function), is rather similar, and particularly preserving of identifier naming patterns; we further present evidence suggesting that identifiers are a very important element of training data for software engineering tasks. We leverage this rather fortuitous phenomenon to find evidence that available multilingual training data (across different languages) can be used to amplify performance. We study this for 3 different tasks: code summarization, code retrieval, and function naming. We note that this data-augmenting approach is broadly compatible with different tasks, languages, and machine-learning models."
Wolf2020,Christine T. Wolf and Drew Paine,Sensemaking Practices in the Everyday Work of AI/ML Software Engineering,,,,2020,10.1145/3387940.3391496,,"This paper considers sensemaking as it relates to everyday software engineering (SE) work practices and draws on a multi-year ethnographic study of SE projects at a large, global technology company building digital services infused with artificial intelligence (AI) and machine learning (ML) capabilities. Our findings highlight the breadth of sensemaking practices in AI/ML projects, noting developers' efforts to make sense of AI/ML environments (e.g., algorithms/methods and libraries), of AI/ML model ecosystems (e.g., pre-trained models and ""upstream""models), and of business-AI relations (e.g., how the AI/ML service relates to the domain context and business problem at hand). This paper builds on recent scholarship drawing attention to the integral role of sensemaking in everyday SE practices by empirically investigating how and in what ways AI/ML projects present software teams with emergent sensemaking requirements and opportunities."
Laplante2024,Phil Laplante,Deception and Intuition in Software Engineering,Computer,57,1,2024,10.1109/MC.2023.3328508,15580814,"Deception poses threats but also opportunities in software engineering, and we must learn to recognize and manage these. Our intuition can save us from these threats and unlock the opportunities, but takes time and practice."
Buhnova2023,Barbora Buhnova and David Halasz and Danish Iqbal and Hind Bangui,Survey on Trust in Software Engineering for Autonomous Dynamic Ecosystems,,,,2023,10.1145/3555776.3577702,,"Software systems across various application domains are undergoing a major shift, from static systems of systems to dynamic ecosystems characterized by largely autonomous software agents, engaging in mutual coalitions and partnerships to complete complex collaborative tasks. One of the key challenges facing software engineering along with this shift, is our preparedness to leverage the concept of mutual trust building among the dynamic system components, to support safe collaborations with (possibly malicious or misbehaving) components outside the boundaries of our control. To support safe evolution towards dynamic software ecosystems, this paper examines the current progress in the research on trust in software engineering across various application domains. To this end, it presents a survey of existing work in this area, and suggests the directions in which further research is needed. These directions include the research of social metrics supporting trust assessment, fine-grained quantification of trust-assessment results, and opening the discussion on governance mechanisms responsible for trust-score management and propagation across the integrated software ecosystems."
Malo-Peris2022,Pedro Malo-Perisé and José Merseguer,The “Socialized Architecture”: A Software Engineering Approach for a New Cloud,Sustainability (Switzerland),14,4,2022,10.3390/su14042020,20711050,"Today, the cloud means a revolution within the Internet revolution. However, an oligopoly sustaining the cloud may not be the best solution, since ethical problems such as privacy or even transferring data sovereignty could eventually happen. Our research, coined as the ""socialized architecture,"" presents a novel disruptive approach to completely transform the cloud as we know it today. The approach follows ideas already working in the field of volunteer computing, since it tries to socialize spare computing power in the infraused hardware that institutions and normal people own. However, our solution is completely different to current ones, since it does not create hyper-specialized muscles in client machines. The solution is new since it proposes a software engineering approach for developing “socialized services”, which, leveraging an asynchronous interaction model, creates a network of lightweight microservices that can be dynamically allocated and replicated through the network. The use of state-of-the-art patterns, such as Command Query Responsibility Segregation, helps to isolate domain events and persistence needs, while an API Gateway addresses communication. All previous ideas were tested through a complete and functional proof of concept, which is a prototype called Circle implementing a social network. Circle has been useful to expose problems that need to be addressed. The results of the assessment confirm, in our view, that it is worth to start this new field of work."
Job2021,Minimol Anil Job,Automating and Optimizing Software Testing using Artificial Intelligence Techniques,International Journal of Advanced Computer Science and Applications,12,5,2021,10.14569/IJACSA.2021.0120571,21565570,"The final product of software development process is a software system and testing is one of the important stages in this process. The success of this process can be determined by how well it accomplishes its goal. Due to the advancement of technology, various software testing tools have been introduced in the software engineering discipline. The use of software is increasing day-by-day and complexity of software functions are challenging and there is need to release the software within the short quality evaluation period, there is a high demand in adopting automation in software testing. Emergence of automatic software testing tools and techniques helps in quality enhancement and reducing time and cost in the software development activity. Artificial Intelligence (AI) techniques are widely applied in different areas of Software engineering (SE). Application of AI techniques can help in achieving good performance in software Testing and increase the productivity of the software development firms. This paper briefly presents the state of the art in the field of software testing by applying AI techniques in software testing."
Ritonummi2023,Saima Ritonummi and Valtteri Siitonen and Markus Salo and Henri Pirkkalainen and Anu Sivunen,Flow Experience in Software Engineering,,,,2023,10.1145/3611643.3616263,,"Software engineering (SE) requires high analytical skills and creativity, which makes it an excellent context for experiencing flow. Although previous work in the SE context has identified how positive affect and development tools can support the flow experience, there is still much to uncover about the characteristics of software developers' flow experiences. To address this gap in knowledge, we conducted a qualitative critical incident technique (CIT) questionnaire (n = 401) on the flow-facilitating factors and characteristics of flow in the SE context. The most important flow-facilitating factors in developers' work included optimal challenge, high motivation, positive developer experience (DX), and no distractions or interruptions. The flow experiences were characterized by absorption, effortless control, intrinsic reward, and high performance. Our study identifies the features of flow commonly addressed in flow research; however, it also highlights how IT use, especially development tools that provide positive DX, as well as being able to work without excessive distractions and interruptions are important facilitators of developers' flow."
Surez2024,Julio Suárez and Aurora Vizcaíno,"Stress, motivation, and performance in global software engineering",Journal of Software: Evolution and Process,36,5,2024,10.1002/smr.2600,20477481,"The objective of this study is to analyze the current perspective as regards knowledge related to what causes stress or motivates developers, how these two aspects are related to each other, and how this in turn affects their performance in the sphere of Global Software Development and how these can be controlled. This paper presents the results obtained after conducting a systematic mapping study of literature in order to analyze how stress, motivation, and performance affect the project members in Global Software Development teams. We carried out a systematic mapping of published studies dealing with stress, motivation, and performance in global software engineering. A total of 118 papers dealing with this subject were found. The literature analyzed provided a relatively significant quantity of data referring to the impact that the characteristics of distributed software development projects have on the performance and productivity of teams, along with the actions taken to improve that performance. However, when focusing on the analysis of the impact of this type of projects on team members' motivation, and on the actions that can be taken to improve that motivation, we discovered that the number of works decreases considerably and that works referring to the impact of this kind of development on developers' stress were virtually non-existent, as were those concerning ways in which to improve that stress. We are, therefore, of the opinion that it is necessary to carry out in-depth research into the aspects of working in distributed teams that may have a negative impact on developers' levels of motivation and stress, along with what could be beneficial in order to improve levels of motivation and decrease levels of stress."
Renz2022,Kai Renz,Using OpenSky Data for Teaching Software Engineering to Undergraduates †,Engineering Proceedings,28,1,2022,10.3390/engproc2022028003,26734591,"In the past year, the author has been using the OpenSky ressources for teaching purposes in the context of Software-Engineering courses for undergraduates at the Darmstadt University of Applied Sciences (Germany). Challenges in using the OpenSky-Online-API during hands-on sessions with groups of students could be found in the following areas: restrictions on the update frequency for data-retrieval calls when using the same IP address from the university network; necessity to create logins for accessing data for a specific timestamp; and a lack of student understanding of basic flight-data terminology, including geographic location. The OpenSky-Network data was used to teach Software-Engineering topics with the focus on professional software tests including UI-Testing with an OpenLayer visualization, testing of network connectivity using a circuit breaker, and the usage of mocks and stubs to obtain independent integration tests. To overcome the technological limitations of the data retrieval, a simple flight-data proxy was implemented to be used instead of the actual OpenSky website. This paper gives some insights into the specific testing topics covered by the course and the usage of OpenSky flight data. Some future improvements are suggested so that undergraduate students can further get to know the interesting field of flight-data processing while learning essential techniques for professional software testing."
Alhubaishy2020,Abdulaziz Alhubaishy,Factors influencing computing students⇔ readiness to online learning for understanding software engineering foundations in Saudi Arabia,International Journal of Advanced Computer Science and Applications,11,12,2020,10.14569/IJACSA.2020.0111286,21565570,"—The spread of Coronavirus disease (COVID-19) has enforced most universities/institutions over the world to transform their educational models (face-to-face and blended) bearing in mind the online educational environments as a temporary substitute. Consequently, all universities/institutions in Saudi Arabia have requested their students to continue the learning process using online environments. This transition has provided an opportunity to deeply investigate possible challenges as well as factors that influence the adoption of online learning as a future educational model for undergraduate students. This research measures the current undergraduate students’ readiness for online learning and investigates factors that influence their level of readiness. Firstly, the research proposes the adoption of a validated multidimensional instrument to measure undergraduate students’ readiness for online learning in different universities. Secondly, the research elaborates the findings by an in-depth study that highlights the main obstacles that hinder computing students’ readiness to learn Software Engineering (SE) foundations using online learning. The research adopts survey research to measure students’ readiness and analyzes the data to extract the readiness levels of different dimensions of the adopted instrument. Furthermore, interviews were conducted to specify the influential factors on computing students’ readiness levels regarding learning SE foundations. Results show that students’ readiness level for online learning is within the acceptable range while some improvements are needed. Furthermore, the study found that students’ cognition, willingness, ignorance, and the amount of assistant and help they receive play a significant role in the success/failure of the adoption of learning SE foundations through online environment."
Hoy2023,Zoe Hoy and Mark Xu,Agile Software Requirements Engineering Challenges-Solutions—A Conceptual Framework from Systematic Literature Review,Information (Switzerland),14,6,2023,10.3390/info14060322,20782489,"Agile software requirements engineering processes enable quick responses to reflect changes in the client’s software requirements. However, there are challenges associated with agile requirements engineering processes, which hinder fast, sustainable software development. Research addressing the challenges with available solutions is patchy, diverse and inclusive. In this study, we use a systematic literature review coupled with thematic classification and gap mapping analysis to examine extant solutions against challenges; the typologies/classifications of challenges faced with agile software development in general and specifically in requirements engineering and how the solutions address the challenges. Our study covers the period from 2009 to 2023. Scopus—the largest database for credible academic publications was searched. Using the exclusion criteria to filter the articles, a total of 78 valid papers were selected and reviewed. Following our investigation, we develop a framework that takes a three-dimensional view of agile requirements engineering solutions and suggest an orchestrated approach balancing the focus between the business context, project management and agile techniques. This study contributes to the theoretical frontier of agile software requirement engineering approaches and guidelines for practice."
Bondi2023,André Benjamin Bondi and Lu Xiao,Early Progress on Enhancing Existing Software Engineering Courses to Cultivate Performance Awareness,,,,2023,10.1145/3578245.3584352,,"Software engineering and computer science courses are frequently focused on particular areas in a way that neglects such cross-cutting quality attributes as performance, reliability, and security. We will describe the progress we have made in developing enhancements to some of our existing software engineering courses to draw attention and even lay the foundations of an awareness of performance considerations in the software development life cycle. In doing so, we wish to make performance considerations integral to the software engineering mindset while avoiding the need to remove current material from our existing courses. This work is part of an NSF-funded project for undergraduate curriculum development."
SernaM2020,Edgar Serna M. and Alexei Serna A.,Process and progress of requirement formalization in software engineering,Ingeniare,28,3,2020,10.4067/S0718-33052020000300411,07183305,"Since the middle of the last century was initiated the research in formal methods and was presented proposals and methodologies to apply them in software development. The idea was overcome the diagnosed software crisis through the materialization of the life cycle of this product development. In this article is presented the results of a revision of the literature, about progress and develop the requirement formalization. The conclusion is that both are slow: there is not enough interest in the industry, the academy does not train in formal methods, and there is no sponsorship for this research field, professionals fear mathematics and traditional methodologies of software engineering are still the most used in development teams. Due to the quality efficiency, software security and reliability, it is necessary to reactivate the research and experimentation with all the formal methods, because the hope is that mathematics will be the toll with which the crisis promulgated in the 60s is overcome."
Al-Obaidy2024,Hadeel Al-Obaidy and Aysha Ebrahim and Ali Aljufairi and Ahmed Mero and Omar Eid,Software Engineering for Developing a Cloud Computing Museum-Guide System,International Journal of Cloud Applications and Computing,14,1,2024,10.4018/IJCAC.339200,21561826,"The aim of this article proposes an innovative solution for developing a museum-guide system, which employs a voice-activated assistant paired with 3-D hologram displays, that utilizes Amazon web services (AWS) to enhance the visitor experience at the Bahrain National Museum. The proposed system uses software engineering as a service (SaaS) and involves an agile development process model with microservice architecture that adapts cloud computing capabilities to provide scalability, reliability, and maintainability. The proposed system enhances the existing museum infrastructure and databases through a flexible, API-based architecture. The proposed system is highly adaptable and flexible in different desirable aspects of user experience goals. The implementation results proved that the system is highly reliable, adaptable, and efficient and has the potential to improve the user experience by transforming the way museum visitors explore and interact with user interfaces of the museum-guide system."
Usman2023,Muhammad Usman and Nauman Bin Ali and Claes Wohlin,A Quality Assessment Instrument for Systematic Literature Reviews in Software Engineering,E-Informatica Software Engineering Journal,17,1,2023,10.37190/e-Inf230105,20844840,"Background: Systematic literature reviews (SLRs) have become a standard practice as part of software engineering (SE) research, although their quality varies. To build on the reviews, both for future research and industry practice, they need to be of high quality. Aim: To assess the quality of SLRs in SE, we put forward an appraisal instrument for SLRs. Method: A well-established appraisal instrument from research in healthcare was used as a starting point to develop the instrument. It is adapted to SE using guidelines, checklists, and experiences from SE. The first version was reviewed by four external experts on SLRs in SE and updated based on their feedback. To demonstrate its use, the updated version was also used by the authors to assess a sample of six selected systematic literature studies. Results: The outcome of the research is an appraisal instrument for quality assessment of SLRs in SE. The instrument includes 15 items with different options to capture the quality. The instrument also supports consolidating the items into groups, which are then used to assess the overall quality of an SLR. Conclusion: The presented instrument may be helpful support for an appraiser in assessing the quality of SLRs in SE."
Ambros-Antemate2021,Jorge Fernando Ambros-Antemate and María Del Pilar Beristain-Colorado and Marciano Vargas-Treviño and Jaime Gutiérrez-Gutiérrez and Pedro Antonio Hernández-Cruz and Itandehui Belem Gallegos-Velasco and Adriana Moreno-Rodríguez,Software engineering frameworks used for serious games development in physical rehabilitation: Systematic review,JMIR Serious Games,9,4,2021,10.2196/25831,22919279,"Background: Serious games are a support in the rehabilitation process for treating people with physical disabilities. However, many of these serious games are not adapted to the patient’s needs because they are not developed with a software engineering framework with a set of activities, actions, and tasks that must be executed when creating a software product. Better serious games for rehabilitation will be developed if the patient and therapist requirements are identified, the development is planned, and system improvements and feedback are involved. The goal is that the serious game must offer a more attractive environment, while maintaining patient interest in the rehabilitation process. Objective: This paper submits the results of a systematic review of serious games in physical rehabilitation identifying the benefits of using a software engineering framework. Methods: A systematic research was conducted using PubMed, PEDro (Physiotherapy Evidence Database), IEEE Xplore, ScienceDirect, ACM Digital Library, Mary Ann Liebert, Taylor & Francis Online, Wiley Online Library, and Springer databases. The initial search resulted in 701 papers. After assessing the results according to the inclusion criteria, 83 papers were selected for this study. Results: From the 83 papers reviewed, 8 used a software engineering framework for its development. Most of them focused their efforts on 1 or more aspects, such as data acquisition and processing, game levels, motivation, therapist supervision. Conclusions: This systematic review proves that most of the serious games do not use a software engineering framework for their development. As a result, development systems overlook several aspects and do not have a standardized process, eventually omitting important implementation aspects, which impact the patient’s recovery time."
Sharma2024,Tushar Sharma and Maria Kechagia and Stefanos Georgiou and Rohit Tiwari and Indira Vats and Hadi Moazen and Federica Sarro,A survey on machine learning techniques applied to source code,Journal of Systems and Software,209,,2024,10.1016/j.jss.2023.111934,01641212,"The advancements in machine learning techniques have encouraged researchers to apply these techniques to a myriad of software engineering tasks that use source code analysis, such as testing and vulnerability detection. Such a large number of studies hinders the community from understanding the current research landscape. This paper aims to summarize the current knowledge in applied machine learning for source code analysis. We review studies belonging to twelve categories of software engineering tasks and corresponding machine learning techniques, tools, and datasets that have been applied to solve them. To do so, we conducted an extensive literature search and identified 494 studies. We summarize our observations and findings with the help of the identified studies. Our findings suggest that the use of machine learning techniques for source code analysis tasks is consistently increasing. We synthesize commonly used steps and the overall workflow for each task and summarize machine learning techniques employed. We identify a comprehensive list of available datasets and tools useable in this context. Finally, the paper discusses perceived challenges in this area, including the availability of standard datasets, reproducibility and replicability, and hardware resources. Editor's note: Open Science material was validated by the Journal of Systems and Software Open Science Board."
Jaccheri2021,Letizia Jaccheri and Zamira Kholmatova and Giancarlo Succi,Systematizing the Meta-Analytical Process in Software Engineering,,,,2021,10.1145/3501774.3501775,,"The generalization of knowledge is a necessary part of every scientific field. Meta-analysis is already advocated as a tool for generalization in different areas such as medicine, psychology, business, and this process is already standardized for them. Software engineering started using meta-analysis as a tool for aggregating results from families of experiments, but not so long for generalization of results coming from different studies, and for this purpose, the meta-analytical approach is not yet clarified. In this paper, we attempt to systematize the application of meta-analysis as a secondary study to the software engineering field suggesting our preliminary protocol. To see the reliability of the proposed protocol we conducted several studies using it. Following even uniform protocol with these studies, we identified the issues preventing the wide usage of meta-analysis in software engineering and proposed our solutions for them."
Buffardi2020,Kevin Buffardi,Assessing individual contributions to software engineering projects with git logs and user stories,,,,2020,10.1145/3328778.3366948,,"Software Engineering courses often incorporate large-scale projects with collaboration between students working in teams. However, it is difficult to objectively assess individual students when their projects are a product of collaborative efforts. This study explores measurements of individuals' contributions to their respective teams. I analyzed ten Software Engineering team projects (n=42) and evaluations of individual contributions using automated evaluation of the version control system history (Git logs) and user stories completed on their project management (Kanban) boards. Unique insights from meta-data within the Git history and Kanban board user stories reveal complicated relationships between these measurements and traditional assessments, such as peer review and subjective instructor evaluation. From the results, I suggest supplementing and validating traditional assessments with insights from individuals' commit history and user story contributions."
Striuk2022,Andrii M. Striuk and Serhiy O. Semerikov and Hanna M. Shalatska and Vladyslav P. Holiver,Software requirements engineering training: problematic questions,,3077,,2022,,16130073,"The key problems of training Requirement Engineering and the following ways to overcome the contradiction between the crucial role of Requirement Engineering in industrial software development and insufficient motivation to master it in the process of Software Engineering specialists professional training were identified based on a systematic research analysis on the formation of the ability of future software engineers to identify, classify and formulate software requirements: use of activity and constructivist approaches, game teaching methods in the process of modeling requirements; active involvement of stakeholders in identifying, formulating and verifying requirements at the beginning of the project and evaluating its results at the end; application of mobile technologies for training of geographically distributed work with requirements; implementation of interdisciplinary cross-cutting Software Engineering projects; involvement of students in real projects; stimulating the creation of interdisciplinary and age-old student project teams."
Lakshman2022,Shashank Bangalore Lakshman and Nasir U. Eisty,Software Engineering Approaches for TinyML based IoT Embedded Vision: A Systematic Literature Review,,,,2022,10.1145/3528227.3528569,,"Internet of Things (IoT) has catapulted human ability to control our environments through ubiquitous sensing, communication, computation, and actuation. Over the past few years, IoT has joined forces with Machine Learning (ML) to embed deep intelligence at the far edge. TinyML (Tiny Machine Learning) has enabled the deployment of ML models for embedded vision on extremely lean edge hardware, bringing the power of IoT and ML together. However, TinyML powered embedded vision applications are still in a nascent stage, and they are just starting to scale to widespread real-world IoT deployment. To harness the true potential of IoT and ML, it is necessary to provide product developers with robust, easy-to-use software engineering (SE) frameworks and best practices that are customized for the unique challenges faced in TinyML engineering. Through this systematic literature review, we aggregated the key challenges reported by TinyML developers and identified state-of-art SE approaches in large-scale Computer Vision, Machine Learning, and Embedded Systems that can help address key challenges in TinyML based IoT embedded vision. In summary, our study draws synergies between SE expertise that embedded systems developers and ML developers have independently developed to help address the unique challenges in the engineering of TinyML based IoT embedded vision."
Galster2023,Matthias Galster and Antonija Mitrovic and Sanna Malinen and Jay Holland and Pasan Peiris,Soft skills required from software professionals in New Zealand,Information and Software Technology,160,,2023,10.1016/j.infsof.2023.107232,09505849,"Context: Soft skills (e.g., communication) significantly contribute to software project success. Objective: We aim to understand (a) what are relevant soft skills in software engineering, (b) how soft skills relate to characteristics of hiring organizations, and (c) how reliably we can automatically identify soft skills in job adverts to support their continuous analysis. We focus on soft skills required by organizations in New Zealand, a country with a small but growing software sector characterized by a skills shortage, reliance on offshoring, and embedded in a bi-cultural context. Method: We manually analyzed 530 job adverts from New Zealand's largest portal for technology-related positions. We identified soft skills following an inductive approach, i.e., without pre-defined soft skills. We complemented the manual analysis with an automated analysis using Flexiterm (an approach for term recognition). Results: We found explicit references to soft skills in 82% of adverts. Adverts from recruitment agencies (compared to hiring companies) included fewer soft skills. We identified 17 soft skills and proposed a contextualized software engineering description. Communication-related skills are most in demand. Soft skills related to broader human or societal values (e.g., empathy, cultural awareness) or distributed development are not common. Soft skills do not depend on company size or core business and domain of companies, or whether a company operates globally. Automatically identifying soft skills in adverts is error-prone. Conclusions: Employers explicitly ask for soft skills. Our findings support previous studies that highlight the importance of communication. On the other hand, identified soft skills only partially overlap with those reported in other skills classifications. Characteristics specific to New Zealand do not impact the demand for soft skills. Our findings benefit researchers in human aspects of software engineering and to those responsible for staff, curricula and professional development."
Krunic2023,Momcilo V. Krunic,Documentation as Code in Automotive System/Software Engineering,Elektronika ir Elektrotechnika,29,4,2023,10.5755/j02.eie.33843,20295731,"Documentation as Code (DaC) is an approach that applies the principles of software development to the production of technical documentation. Using modern tools, DaC enables software engineers to treat documentation as a first-class citizen in the development process, alongside code and tests. In this paper, we discuss the advantages of DaC in system and software engineering, including improved accuracy, traceability, and maintainability. In the automotive industry, DaC has been used to document various aspects of vehicle development, such as requirements, design, testing, and compliance. This paper provides an overview of the state-of-the-art in DaC in the automotive industry and discusses the potential benefits and challenges of using this approach. In addition, case studies and examples of how DaC has been used in the automotive industry to improve the quality and maintainability of documentation are presented. This research has been conducted with more than 150 engineers actively contributing to DaC on the project for more than a year within a company, so the scalability of the presented solution has been tested. Finally, a set of guidelines is provided for teams to follow when adopting DaC to ensure successful implementation."
Fawzy2022,Dina Fawzy and Sherin M. Moussa and Nagwa L. Badr,The Internet of Things and Architectures of Big Data Analytics: Challenges of Intersection at Different Domains,IEEE Access,10,,2022,10.1109/ACCESS.2022.3140409,21693536,"The current exponential advancements in the Internet of Things (IoT) technologies pave a vast intelligent computing platform by integrating smart objects with sensing, processing and communication capabilities. The core element of IoT is the complex big data generated from different interconnected sources at real-time, presenting divergent processing and analysis challenges. Best practices in software engineering have been continuously addressed in IoT technologies to handle such big data efficiently at different domains. Despite of the massive studies dedicated for IoT, no explicit processing architecture is proposed based on real investigation of software engineering concepts and big data analytics characteristics in IoT. This paper provides a systematic literature review for the current state-of-the-art of IoT systems in different domains. The study investigates the current techniques and technologies that serve IoT systems from the big data analytics and software engineering perspectives, revealing a matrix for the specific IoT data features and their encountered challenges and gaps for each domain. The review deduces a proposed domain-independent software architecture for big IoT data analytics, maintaining various IoT data processing challenges, including data scalability, timeliness, heterogeneity, inconsistency, confidentiality and correlations. Finally, the main research gaps are emphasized for future considerations."
Barbosa2020,Luis S. Barbosa,Software engineering for 'quantum advantage',,,,2020,10.1145/3387940.3392184,,"Software is a critical factor in the reliability of computer systems. While the development of hardware is assisted by mature science and engineering disciplines, software science is still in its infancy. This situation is likely to worsen in the future with quantum computer systems. Actually, if quantum computing is quickly coming of age, with potential groundbreaking impacts on many different fields, such benefits come at a price: quantum programming is hard and finding new quantum algorithms is far from straightforward. Thus, the need for suitable formal techniques in quantum software development is even bigger than in classical computation. A lack of reliable approaches to quantum computer programming will put at risk the expected quantum advantage of the new hardware. This position paper argues for the need for a proper quantum software engineering discipline benefiting from precise foundations and calculi, capable of supporting algorithm development and analysis."
Bambazek2023,Peter Bambazek and Iris Groher and Norbert Seyff,Requirements engineering for sustainable software systems: a systematic mapping study,Requirements Engineering,28,3,2023,10.1007/s00766-023-00402-1,1432010X,"Various approaches toward the development of sustainable software systems have been proposed by the requirements engineering community over the last decade. We conducted a systematic mapping study, analyzed 55 publications, and identified 29 approaches that have been published since the year 2000. We analyzed how the approaches evolved over time and how the publications and authors are influenced by each other. Furthermore, the approaches are analyzed in terms of their supported requirements engineering activities, along with the evidence provided in the publications. Additionally, we also analyzed which sustainability definitions have been used, if an iterative application of the approaches is discussed, and if the approaches also provide a tool-support for practitioners. We noticed an increase of publications on requirements engineering approaches toward sustainability in the last years, whereas a majority discuss sustainability based on the same multi-dimensional concept. Although different case studies have been already conducted, we noticed a lack of an industrial application. Our main findings concern the need of an evaluation on how the proposed requirements engineering approaches can also be applied in agile software development processes. Additionally, we also promote the development of supporting software tools to support practitioners in adapting the proposed approaches."
Zhang2020,He Zhang and Xin Zhou and Xin Huang and Huang Huang and Muhammad Ali Babar,An evidence-based inquiry into the use of grey literature in software engineering,,,,2020,10.1145/3377811.3380336,02705257,"Context: Following on other scientific disciplines, such as health sciences, the use of Grey Literature (GL) has become widespread in Software Engineering (SE) research. Whilst the number of papers incorporating GL in SE is increasing, there is little empirically known about different aspects of the use of GL in SE research. Method: We used a mixed-methods approach for this research. We carried out a Systematic Literature Review (SLR) of the use of GL in SE, and surveyed the authors of the selected papers included in the SLR (as GL users) and the invited experts in SE community on the use of GL in SE research. Results: We systematically selected and reviewed 102 SE secondary studies that incorporate GL in SE research, from which we identified two groups based on their reporting: 1) 76 reviews only claim their use of GL; 2) 26 reviews report the results by including GL.We also obtained 20 replies from the GL users and 24 replies from the invited SE experts. Conclusion: There is no common understanding of the meaning of GL in SE. Researchers define the scopes and the definitions of GL in a variety of ways.We found five main reasons of using GL in SE research. The findings have enabled us to propose a conceptual model for how GL works in SE research lifecycle. There is an apparent need for research to develop guidelines for using GL in SE and for assessing quality of GL. The current work can provide a panorama of the state-of-the-art of using GL in SE for the follow-up research, as to determine the important position of GL in SE research."
Raibulet2022,Claudia Raibulet and Francesca Arcelli Fontana and Ilaria Pigazzini,Hints on Designing and Running Project-based Exams for a Software Engineering Course,,,,2022,10.1145/3524487.3527355,,"In this paper, we share our experience in designing and running a project-based exam for a Software Engineering course at the undergraduate level. We underline the teaching objectives and content of the course which aim to prepare students for the industrial environment, e.g., various design perspectives on the project, software quality assurance and evaluation, tools for development and software quality assessment, project management and team work. We present our project-based exam and summarize hints on its design, organization, and evaluation. Project examples are also introduced."
Liu2022,Can Liu and Sumaya Sanober and Abu Sarwar Zamani and L. Rama Parvathy and Rahul Neware and Abdul Wahab Rahmani,Defect Prediction Technology in Software Engineering Based on Convolutional Neural Network,Security and Communication Networks,2022,,2022,10.1155/2022/5058461,19390122,"Software defect prediction has become a significant study path in the field of software engineering in order to increase software reliability. Program defect predictions are being used to assist developers in identifying potential problems and optimizing testing resources to enhance program dependability. As a consequence of this strategy, the number of software defects may be predicted, and software testing resources are focused on the software modules with the most problems, allowing the defects to be addressed as soon as feasible. The author proposes a research method of defect prediction technology in software engineering based on convolutional neural network. Most of the existing defect prediction methods are based on the number of lines of code, module dependencies, stack reference depth, and other artificially extracted software features for defect prediction. Such methods do not take into account the underlying semantic features in software source code, which may lead to unsatisfactory prediction results. The author uses a convolutional neural network to mine the semantic features implicit in the source code and use it in the task of software defect prediction. Empirical studies were conducted on 5 software projects on the PROMISE dataset and using the six evaluation indicators of Recall, F1, MCC, pf, gm, and AUC to verify and analyze the experimental results showing that the AUC values of the items varied from 0.65 to 0.86. Obviously, software defect prediction experimental results obtained using convolutional neural networks are still ideal. Defect prediction model in software engineering based on convolutional neural network has high prediction accuracy."
Habibullah2024,Khan Mohammad Habibullah and Hans Martin Heyn and Gregory Gay and Jennifer Horkoff and Eric Knauss and Markus Borg and Alessia Knauss and Håkan Sivencrona and Polly Jing Li,Requirements and software engineering for automotive perception systems: an interview study,Requirements Engineering,29,1,2024,10.1007/s00766-023-00410-1,1432010X,"Driving automation systems, including autonomous driving and advanced driver assistance, are an important safety-critical domain. Such systems often incorporate perception systems that use machine learning to analyze the vehicle environment. We explore new or differing topics and challenges experienced by practitioners in this domain, which relate to requirements engineering (RE), quality, and systems and software engineering. We have conducted a semi-structured interview study with 19 participants across five companies and performed thematic analysis of the transcriptions. Practitioners have difficulty specifying upfront requirements and often rely on scenarios and operational design domains (ODDs) as RE artifacts. RE challenges relate to ODD detection and ODD exit detection, realistic scenarios, edge case specification, breaking down requirements, traceability, creating specifications for data and annotations, and quantifying quality requirements. Practitioners consider performance, reliability, robustness, user comfort, and—most importantly—safety as important quality attributes. Quality is assessed using statistical analysis of key metrics, and quality assurance is complicated by the addition of ML, simulation realism, and evolving standards. Systems are developed using a mix of methods, but these methods may not be sufficient for the needs of ML. Data quality methods must be a part of development methods. ML also requires a data-intensive verification and validation process, introducing data, analysis, and simulation challenges. Our findings contribute to understanding RE, safety engineering, and development methodologies for perception systems. This understanding and the collected challenges can drive future research for driving automation and other ML systems."
Ju2020,An Ju and Adnan Hemani and Yannis Dimitriadis and Armando Fox,What agile processes shouldwe use in software engineering course projects?,,,,2020,10.1145/3328778.3366864,,"While project-based software engineering courses aim to provide learning opportunities grounded in professional processes, it is not always possible to replicate every process in classrooms due to course constraints. Previous studies observed how students react to various processes and gave retroactive recommendations. In this study, we instead combine a field study on professional Agile (eXtreme Programming, XP) teams and an established team process taxonomy to proactively select team processes to incorporate in a project-based software engineering course. With collected knowledge from the field study, we choose three XP processes to augment the design of a mature software engineering project course. We choose processes that are 1) considered important by professionals, and 2) complete with respect to coverage of the taxonomy's main categories.We then compare the augmented course design with the original design in a case study. Our results suggest that 1) even without extra resources, adding these new processes does not interfere with learning opportunities for XP processes previously existing in the course design; 2) student teams experience similar benefits from these new processes as professional teams do, and students appreciate the usefulness and value of the processes. In other words, our approach allows instructors to make conscious choices of XP processes that improve student learning outcomes while exposing students to a more complete set of processes and thus preparing them better for professional careers. Course designers with limited resources are encouraged to use our methodology to evaluate and improve the designs of their own project-based courses."
Shneiderman2020,Ben Shneiderman,"Bridging the gap between ethics and practice: Guidelines for reliable, safe, and trustworthy human-centered AI systems",ACM Transactions on Interactive Intelligent Systems,10,4,2020,10.1145/3419764,21606463,"This article attempts to bridge the gap between widely discussed ethical principles of Human-centered AI (HCAI) and practical steps for effective governance. Since HCAI systems are developed and implemented in multiple organizational structures, I propose 15 recommendations at three levels of governance: team, organization, and industry. The recommendations are intended to increase the reliability, safety, and trustworthiness of HCAI systems: (1) reliable systems based on sound software engineering practices, (2) safety culture through business management strategies, and (3) trustworthy certification by independent oversight. Software engineering practices within teams include audit trails to enable analysis of failures, software engineering workflows, verification and validation testing, bias testing to enhance fairness, and explainable user interfaces. The safety culture within organizations comes from management strategies that include leadership commitment to safety, hiring and training oriented to safety, extensive reporting of failures and near misses, internal review boards for problems and future plans, and alignment with industry standard practices. The trustworthiness certification comes from industry-wide efforts that include government interventions and regulation, accounting firms conducting external audits, insurance companies compensating for failures, nongovernmental and civil society organizations advancing design principles, and professional organizations and research institutes developing standards, policies, and novel ideas. The larger goal of effective governance is to limit the dangers and increase the benefits of HCAI to individuals, organizations, and society."
Marijan2022,Dusica Marijan and Sagar Sen,Industry-Academia Research Collaboration and Knowledge Co-creation: Patterns and Anti-patterns,ACM Transactions on Software Engineering and Methodology,31,3,2022,10.1145/3494519,15577392,"Increasing the impact of software engineering research in the software industry and the society at large has long been a concern of high priority for the software engineering community. The problem of two cultures, research conducted in a vacuum (disconnected from the real world), or misaligned time horizons are just some of the many complex challenges standing in the way of successful industry-Academia collaborations. This article reports on the experience of research collaboration and knowledge co-creation between industry and academia in software engineering as a way to bridge the research-practice collaboration gap. Our experience spans 14 years of collaboration between researchers in software engineering and the European and Norwegian software and IT industry. Using the participant observation and interview methods, we have collected and afterwards analyzed an extensive record of qualitative data. Drawing upon the findings made and the experience gained, we provide a set of 14 patterns and 14 anti-patterns for industry-Academia collaborations, aimed to support other researchers and practitioners in establishing and running research collaboration projects in software engineering."
Steinmacher2023,Igor Steinmacher and Paul Clarke and Eray Tuzun and Ricardo Britto,Editorial: Best papers of the 14th International Conference on Software and System Processes (ICSSP 2020) and 15th International Conference on Global Software Engineering (ICGSE 2020),Journal of Software: Evolution and Process,35,5,2023,10.1002/smr.2544,20477481,"Today's software industry is global, virtual, and depending more than ever on strong and reliable processes. Stakeholders and infrastructure are distributed across the globe, posing challenges that go beyond those with co-located teams and servers. Software Engineering continues to be a complex undertaking, with projects challenged to meet expectations, especially regarding costs. We know that Software Engineering is an ever-changing discipline, with the result that firms and their employees must regularly embrace new methods, tools, technologies, and processes. In 2020, the International Conference on Global Software Engineering (ICGSE) and the International Conference on Systems and Software Processes (ICSSP) joined forces aiming to create a holistic understanding of the software landscape both from the perspective of human and infrastructure distribution and also the processes to support software development. Unfortunately, these challenges have become even more personal to many more in 2020 due to the disruption introduced by the COVID-19 pandemic, which forced both conferences to be held virtually. As an outcome of the joint event, we selected a set of the best papers from the two conferences, which were invited to submit extended versions to this Special Issue in the Journal of Software: Maintenance and Evolution. Dedicated committees were established to identify the best papers. Eight papers were invited and ultimately, seven of these invited papers have made it into this Special Issue."
Calefato2022,Fabio Calefato and Filippo Lanubile,Using Personality Detection Tools for Software Engineering Research: How Far Can We Go,ACM Transactions on Software Engineering and Methodology,31,3,2022,10.1145/3491039,15577392,"Assessing the personality of software engineers may help to match individual traits with the characteristics of development activities such as code review and testing, as well as support managers in team composition. However, self-Assessment questionnaires are not a practical solution for collecting multiple observations on a large scale. Instead, automatic personality detection, while overcoming these limitations, is based on off-The-shelf solutions trained on non-Technical corpora, which might not be readily applicable to technical domains like software engineering. In this article, we first assess the performance of general-purpose personality detection tools when applied to a technical corpus of developers' e-mails retrieved from the public archives of the Apache Software Foundation. We observe a general low accuracy of predictions and an overall disagreement among the tools. Second, we replicate two previous research studies in software engineering by replacing the personality detection tool used to infer developers' personalities from pull-request discussions and e-mails. We observe that the original results are not confirmed, i.e., changing the tool used in the original study leads to diverging conclusions. Our results suggest a need for personality detection tools specially targeted for the software engineering domain."
Sriraman2023,Gopalakrishnan Sriraman and Shriram Raghunathan,A Systems Thinking Approach to Improve Sustainability in Software Engineering—A Grounded Capability Maturity Framework,Sustainability (Switzerland),15,11,2023,10.3390/su15118766,20711050,"Sustainability has become a critical issue for the software industry as the environmental impact of software development and use increases. To address this issue, organizations need a framework for developing and accessing sustainable software practices. In this study, we reviewed the existing literature, models, and practices in this domain as well as carried out surveys and interviews to understand the reality, practices, and challenges on the ground. We identified a set of research questions to discover why sustainability is important, what actions can be taken to improve it, and how and when they can be implemented. This study highlighted the limitations of existing models and the need to address the research gaps. Existing research is limited by a narrow focus on specific domains such as the environment and energy or a specific phase of software development. We aim to address these through the proposed comprehensive software sustainability capability framework (SSCF), which provides a “ready reckoner”, allowing any organization to assess their current software sustainability and the capabilities and metrics they could focus on to improve sustainable software maturity. This paper also provides detailed assessment criteria, metrics, and a roadmap that can be used by any software organization to enhance their sustainability."
Graziotin2022,Daniel Graziotin and Per Lenberg and Robert Feldt and Stefan Wagner,Psychometrics in Behavioral Software Engineering: A Methodological Introduction with Guidelines,ACM Transactions on Software Engineering and Methodology,31,1,2022,10.1145/3469888,1049-331X,"A meaningful and deep understanding of the human aspects of software engineering (SE) requires psychological constructs to be considered. Psychology theory can facilitate the systematic and sound development as well as the adoption of instruments (e.g., psychological tests, questionnaires) to assess these constructs. In particular, to ensure high quality, the psychometric properties of instruments need evaluation. In this article, we provide an introduction to psychometric theory for the evaluation of measurement instruments for SE researchers. We present guidelines that enable using existing instruments and developing new ones adequately. We conducted a comprehensive review of the psychology literature framed by the Standards for Educational and Psychological Testing. We detail activities used when operationalizing new psychological constructs, such as item pooling, item review, pilot testing, item analysis, factor analysis, statistical property of items, reliability, validity, and fairness in testing and test bias. We provide an openly available example of a psychometric evaluation based on our guideline. We hope to encourage a culture change in SE research towards the adoption of established methods from psychology. To improve the quality of behavioral research in SE, studies focusing on introducing, validating, and then using psychometric instruments need to be more common."
Alam2022,Sana Alam and Shehnila Zardari and Muneera Bano,Software engineering and 12 prominent sub-areas: Comprehensive bibliometric assessment on 13 years (2007–2019),IET Software,16,2,2022,10.1049/sfw2.12046,17518814,"Software Engineering (SE) as a field has grown significantly in terms of the research publications in the last few years. Criterion-based assessment of research performance of scholars, countries along with collaboration networks and links among articles are better judged using bibliometric analysis. The results proposed in this work are based on quantitative analysis and data visualisation of 150,087 scholarly articles published in last 13 years (2007–2019) across 85 research areas based on seven categories by providing a detailed and distinctive time-frame-based comparison to observe a shift in research trend in SE and its 12 distinguished sub-areas. The results of the observations proposed in this work include analysis on types of documents, yearly publication trends with results suggesting a prominent increment for the period (2015–2017) in terms of research publications, languages of research publications with findings indicating that there are articles published in languages other than English language, also, publication count-based rankings of authors, collaboration networks of countries with findings showing the supremacy of USA and China and keyword statistics."
Wohlin2021,Claes Wohlin and Austen Rainer,Challenges and recommendations to publishing and using credible evidence in software engineering,Information and Software Technology,134,,2021,10.1016/j.infsof.2021.106555,09505849,"Context: An evidence-based scientific discipline should produce, consume and disseminate credible evidence. Unfortunately, mistakes are sometimes made, resulting in the production, consumption and dissemination of invalid or otherwise questionable evidence. In the worst cases, such questionable evidence achieves the status of accepted knowledge. There is, therefore, the need to ensure that producers and consumers seek to identify and rectify such situations. Objectives: To raise awareness of the negative impact of misinterpreting evidence and of propagating that misinterpreted evidence, and to provide guidance on how to improve on the type of issues identified. Methods: We use a case-based approach to present and analyse the production, consumption and dissemination of evidence. The cases are based on the literature and our professional experience. These cases illustrate a range of challenges confronting evidence-based researchers as well as the consequences to research when invalid evidence is not corrected in a timely way. Results: We use the cases and the challenges to formulate a framework and a set of recommendations to help the community in producing and consuming credible evidence. Conclusions: We encourage the community to collectively remain alert to the emergence and dissemination of invalid, or otherwise questionable, evidence, and to proactively seek to identify and rectify it."
Miheli2023,Anže Mihelič and Tomaž Hovelja and Simon Vrhovec,"Identifying Key Activities, Artifacts and Roles in Agile Engineering of Secure Software with Hierarchical Clustering",Applied Sciences (Switzerland),13,7,2023,10.3390/app13074563,20763417,"Different activities, artifacts, and roles can be found in the literature on the agile engineering of secure software (AESS). The purpose of this paper is to consolidate them and thus identify key activities, artifacts, and roles that can be employed in AESS. To gain initial sets of activities, artifacts, and roles, the literature was first extensively reviewed. Activities, artifacts, and roles were then cross-evaluated with similarity matrices. Finally, similarity matrices were converted into distance matrices, enabling the use of Ward’s hierarchical clustering method for consolidating activities, artifacts, and roles into clusters. Clusters of activities, artifacts, and roles were then named as key activities, artifacts, and roles. We identified seven key activities (i.e., security auditing, security analysis and testing, security training, security prioritization and monitoring, risk management, security planning and threat modeling; and security requirements engineering), five key artifacts (i.e., security requirement artifacts, security repositories, security reports, security tags, and security policies), and four key roles (i.e., security guru, security developer, penetration tester, and security team) in AESS. The identified key activities, artifacts, and roles can be used by software development teams to improve their software engineering processes in terms of software security."
Alomari2020,Hakam W. Alomari and Vijayalakshmi Ramasamy and James D. Kiper and Geoff Potvin,A User Interface (UI) and User eXperience (UX) evaluation framework for cyberlearning environments in computer science and software engineering education,Heliyon,6,5,2020,10.1016/j.heliyon.2020.e03917,24058440,"Despite the widespread availability and increasing use of cyberlearning environments, there remains a need for more research about their usefulness in undergraduate education, particularly in STEM education. The process of evaluating the usefulness of a cyberlearning environment is an essential measure of its success and is useful in assisting the design process and ensuring user satisfaction. Unfortunately, there are relatively few empirical studies that provide a comprehensive test of the usefulness of cyberlearning in education. Additionally, there is a lack of standards upon whose usefulness evaluators agree. In this research, we present multiple user studies that can be used to assess the usefulness of a cyberlearning environment used in Computer Science and Software Engineering courses through testing its usability and measuring its utility using user interface and user experience evaluations. Based on these assessments, we propose an evaluation framework to evaluate cyberlearning environments. To help illustrate the framework utility and usability evaluations, we explain them through an example SEP-CyLE (Software Engineering and Programming Cyberlearning Environment). The evaluation techniques used are cognitive walkthroughs with a think-aloud protocol and a heuristic evaluation survey. We further use a network-based analysis to find the statistically significant correlated responses in the heuristic evaluation survey with regard to the students’ perceptions of using SEP-CyLE. Our goal is to improve cyberlearning practice and to emphasize the need for evaluating cyberlearning environments with respect to its designated tasks and its users using UI/UX evaluations. Our experiments demonstrated participants were able to utilize SEP-CyLE efficiently to accomplish the tasks we posed to them and to enhance their software development concepts, specifically, software testing. We discovered areas of improvement in the visibility and navigation of SEP-CyLE's current design. We provide our recommendations for improving SEP-CyLE and provide guidance and possible directions for future research on designing cyberlearning environments for computer education."
Ramulu2020,Kothuri Ramulu and B.V. Ramana Murhtyr,IMPORTANCE OF SOFTWARE QUALITY MODELS IN SOFTWARE ENGINEERING,International Journal of Engineering Technologies and Management Research,5,3,2020,10.29121/ijetmr.v5.i3.2018.192,,"The purpose of this paper is to identify the importance quality in software engineering when the projects or products are developed. The degree to which a component, system or process meets specified requirements and/or user/customer needs and expectations is the quality. The totality of functionality and features of a software product that bear on its ability to satisfy stated or implied needs is software quality. Some even say that ‘quality’ cannot be defined and some say that it can be defined but only in a particular context. Some even state confidently that ‘quality is lack of bugs’. In this paper we discuss about the quality and the quality models."
DurnToro2024,Amador Durán Toro and Pablo Fernández and Beatriz Bernárdez and Nathaniel Weinman and Aslıhan Akalın and Armando Fox,Exploring Gender Bias In Remote Pair Programming Among Software Engineering Students: The twincode Original Study And First External Replication,Empirical Software Engineering,29,2,2024,10.1007/s10664-023-10416-6,15737616,"Context: Women have historically been underrepresented in Software Engineering, due in part to the stereotyped assumption that women are less technically competent than men. Pair programming is both widely used in industry and has been shown to increase student interest in Software Engineering, particularly among women; but if those same gender biases are also present in pair programming, its potential for attracting women to the field could be thwarted. Objective: We aim to explore the effects of gender bias in pair programming. Specifically, in a remote setting in which students cannot directly observe the gender of their peers, we study whether the perception of the partner, the behavior during programming, or the style of communication of Software Engineering students differ depending on the perceived gender of their remote partner. To our knowledge, this is the first study specifically focusing on the impact of gender stereotypes and bias within pairs in pair programming. Method: We have developed an online pair-programming platform (twincode) that provides a collaborative editing window and a chat pane, both of which are heavily instrumented. Students in the control group had no information about their partner’s gender, whereas students in the treatment group could see a gendered avatar representing the other participant as a man or as a woman. The gender of the avatar was swapped between programming tasks to analyze 45 variables related to the collaborative coding behavior, chat utterances, and questionnaire responses of 46 pairs in the original study at the University of Seville, and 23 pairs in the external replication at the University of California, Berkeley. Results: We did not observe any statistically significant effect of the gender bias treatment, nor any interaction between the perceived partner’s gender and subject’s gender, in any of the 45 response variables measured in the original study. In the external replication, we observed statistically significant effects with moderate to large sizes in four dependent variables within the experimental group, comparing how subjects acted when their partners were represented as a man or a woman. Conclusions: The results in the original study do not show any clear effect of the treatment in remote pair programming among current Software Engineering students. In the external replication, it seems that students delete more source code characters when they have a woman partner, and communicate using more informal utterances, reflections and yes/no questions when they have a man partner, although these results must be considered inconclusive because of the small number of subjects in the replication, and because when multiple test corrections are applied, only the result about informal utterances remains significant. In any case, more mixed methods replications are needed in order to confirm or refute the results in the same and other Software Engineering students populations."
Cassee2022,Nathan Cassee,Sentiment in software engineering: detection and application,,,,2022,10.1145/3540250.3558908,,"In software engineering the role of human aspects is an important one, especially as developers indicate that they experience a wide range of emotions while developing software. Within software engineering researchers have sought to understand the role emotions and sentiment play in the development of software by studying issues, pull-requests and commit messages. To detect sentiment, automated tools are used, and in this doctoral thesis we plan to study the use of these sentiment analysis tools, their applications, best practices for their usage and the effect of non-natural language on their performance. In addition to studying the application of sentiment analysis tools, we also aim to study self-admitted technical debt and bots in software engineering, to understand why developers express sentiment and what they signal when they express sentiment. Through studying both the application of sentiment analysis tools and the role of sentiment in software engineering, we hope to provide practical recommendations for both researchers and developers."
Almeida2022,Cleuton Almeida and Cesar Franca,Improving the PBL method with Experiential Learning Theory in Software Engineering Teaching,,,,2022,10.1145/3528231.3536382,,"Context: Problem-Based Learning (PBL) and Experiential Learning Theory (ELT) are convergent active learning approaches widely known for their competent integration between theory and practice. Problem/Objective: However, the usual implementation of PBL leaves out the final active experimentation stage of the experiential learning cycle. In this article, we intend to systematically investigate the impacts of this last stage on the learning outcomes of software engineering students. Methods: A quasi-experiment was designed and applied in three software engineering courses of an undergraduate course, in Rio Branco-Acre / Brazil. Results: students who participated in two of the three treatment groups scored significantly higher on measures of motivation, experience and learning, which means that the PBL method contains gaps that can be significantly improved with the help of ELT, benefiting the learning outcomes of software engineering students."
Thackeray2023,Lynn Roy Thackeray and Susan L. Thackeray,A Novel Interdepartmental Approach to Teach Cross-Functional Collaboration in Software Engineering,,,,2023,10.18260/1-2--42454,21535965,"In recent years, the development of software products has become increasingly complex and involves a variety of professionals from different disciplines. Software engineers need to be able to communicate and collaborate across teams, departments, and organizations. Although interdepartmental course collaborations are not a new pedagogical approach, linking concepts from different subject areas creates a holistic learning experience that is often lacking in software engineering courses and is needed to effectively mirror industry software development. The collaborative approach to course delivery has the advantage of allowing software engineers to work together with less technical project managers to gain a broader understanding of the software industry. This experiential paper will describe two approaches implemented in technology management and software engineering courses: a novel interdepartmental active learning environment for undergraduate and graduate students and a discipline-specific application of an Agile Scrum project framework. The undergraduate course Introduction to Technology Management is a three-hour per week project-based class with the goal of introducing students to the challenges and rewards of managing complex technical projects with budget and time constraints. The graduate course Software Engineering Leadership is a three- hour per week project-based class designed for computer science graduate students to identify important roles and success in software project deliverables. The primary goals of both courses are to provide technical students with engaging activities related to developing skills like teamwork, communication, and following a development framework that involves both synchronous and asynchronous communication and collaboration with non-technical teams in a distributed environment. The traditional classroom instructional approach to teaching management and software engineering typically includes lectures, discussions, and group activities. For this study, the interdepartmental collaboration experimentation identified strategies essential for the novel coordination of efforts to align course content between two skilled disciplines. The research was designed with the intention that the course should prepare, motivate, and engage students in collaborative project outcomes that focus on project frameworks, documentation, and intentional outcomes. Course design commenced with a volunteer interdepartmental partnership between two professors within the College of Engineering and Technology and was delivered in one semester during separate times and class locations. Undergraduate and graduate students were compelled to interact for project success. The advantage of practical experience through collaboration provides students with insights into interpersonal relationships. This study focuses on the effectiveness of interdisciplinary teaching on engineering student project outcomes."
Cutrupi2023,Claudia Maria Cutrupi and Irene Zanardi and Letizia Jaccheri and Monica Landoni,Draw a Software Engineer Test - An Investigation into Children's Perceptions of Software Engineering Profession,,2023-May,,2023,10.1109/ICSE-SEIS58686.2023.00010,02705257,"Context: The gender gap is particularly affecting the software engineering community, as both academia and industry are dominated by men. Literature reports how the lack of women is a consequence of gender stereotypes around certain figures that begin in the early stages of education, affecting children's perceptions of the role they can play across scientific fields.Objective: In this study, we asked children to draw a software engineer in order to collect their perceptions and let us check whether gender stereotypes still persist.Methods: We asked a total of 371 children to draw a person who works in the software engineering field. We analyzed the drawings based on a set of parameters extracted from literature and inspected the results through a cross-sectional study.Results: Children agreed on their representations of a software engineer: 51% drew a man and 44% drew a woman, while 5% a non-recognizable figure. The main differences emerged when the data were grouped by age and gender: only 23% of eleven-year-old girls drew a woman software engineer, while 54% drew a man, and in 23% gender was non-recognizable.Conclusion: The findings revealed a favorable gender balance in children's perceptions of software engineering. They seem more willing to recognize diversity, an improvement compared with what was reported in previous studies. Children's perceptions of technology may have become more accessible as a result of the COVID-19 situation. These findings may draw positive comparisons with the current gender gap in software engineering, encouraging future developments."
Felderer2021,Michael Felderer and Ralf Reussner and Bernhard Rumpe,Software Engineering und Software-Engineering-Forschung im Zeitalter der Digitalisierung,Informatik Spektrum,44,2,2021,10.1007/s00287-020-01322-y,0170-6012,"Die Digitalisierung und der damit verbundene digitale Wandel durchdringen alle Lebensbereiche. Qualitativ hochwertige Software ist der zentrale Baustein und Treiber der Digitalisierung. Damit nimmt auch das ingenieurmäßige Erstellen von Software, das Software Engineering, eine zentrale Rolle im digitalen Wandel ein und ist dabei selbst großen Veränderungen unterworfen. Dieser Artikel versucht deshalb eine Standortbestimmung des Software Engineering und seiner Forschung im Zeitalter der Digitalisierung vorzunehmen."
Vives2022,L. Vives and K. Melendez and A. Dávila,ISO/IEC 29110 and Software Engineering Education: A Systematic Mapping Study,Programming and Computer Software,48,8,2022,10.1134/S0361768822080229,16083261,"Abstract: This article presents a study of the publications made on the ISO/IEC 29110 standard in the university context, especially from the perspective of software engineering education. ISO 29110 is a life cycle profiles for very small entities on systems and software engineering standard, published in many parts. ISO 29110, since its publication in 2011 and its continuous evolution to these days, is the subject of study in different contexts, with education being a relevant axis. Considering, that software engineering education has implications in the software industry in emerging countries, it is necessary to identify and consolidate the work done in this context. In this study, the main research question was what researches have been done at ISO 29110 in the training of software engineers? To answer this question, a systematic mapping study (SMS) was performed. In the SMS, 241 articles were obtained with search string and 17 of them became as primary study after a process selection. Based on these studies, it was possible to determine that the software engineering Basic profile of ISO 29110 and its processes (Project Management and Software Implementation) have been the most studied. Besides, it was identified that project-oriented learning and gamification techniques have been the most used ISO 29110 learning strategies in the training of future software industry professionals."
Samoaa2022,Hazem Peter Samoaa and Firas Bayram and Pasquale Salza and Philipp Leitner,A systematic mapping study of source code representation for deep learning in software engineering,IET Software,16,4,2022,10.1049/sfw2.12064,17518814,"The usage of deep learning (DL) approaches for software engineering has attracted much attention, particularly in source code modelling and analysis. However, in order to use DL, source code needs to be formatted to fit the expected input form of DL models. This problem is known as source code representation. Source code can be represented via different approaches, most importantly, the tree-based, token-based, and graph-based approaches. We use a systematic mapping study to investigate i detail the representation approaches adopted in 103 studies that use DL in the context of software engineering. Thus, studies are collected from 2014 to 2021 from 14 different journals and 27 conferences. We show that each way of representing source code can provide a different, yet orthogonal view of the same source code. Thus, different software engineering tasks might require different (combinations of) code representation approaches, depending on the nature and complexity of the task. Particularly, we show that it is crucial to define whether the DL approach requires lexical, syntactical, or semantic code information. Our analysis shows that a wide range of different representations and combinations of representations (hybrid representations) are used to solve a wide range of common software engineering problems. However, we also observe that current research does not generally attempt to transfer existing representations or models to other studies even though there are other contexts in which these representations and models may also be useful. We believe that there is potential for more reuse and the application of transfer learning when applying DL to software engineering tasks."
Udousoro2020,Isonkobong Udousoro,Effective Requirement Engineering Process Model in Software Engineering,Software Engineering,8,1,2020,10.11648/j.se.20200801.11,2376-8029,"Requirement Engineering is regarded as one of the major stages in software development and management. The aim of requirement engineering is to analyse, investigate, document and check the services and also the constraints of the software system that is being developed. The world of requirement management has increasingly developed over the years and has become the cornerstone for any software development to be successful, therefore it is vital for every organization to consider and pay more attention to the requirement engineering if they intend to build a quality software product that will satisfy the users. Different viewpoints, objectives, roles and responsibilities are all incorporated into requirements engineering which makes it a difficult and complex process in software engineering. Many researchers have come up with different optimized approaches to the requirement engineering. This paper reviewed relevant literature from Elsevier, Emerald, IEEE, ProQuest and Google Scholar databases. Requirement engineering processes are further outlined and explained which include requirement elicitation and development, validation and verification etc. In conclusion, the paper recommends effective requirements engineering process to adopt depending on the industry's goal."
Matthies2023,Christoph Matthies and Robert Heinrich and Rebekka Wohlrab,Investigating Software Engineering Artifacts in DevOps Through the Lens of Boundary Objects,,,,2023,10.1145/3593434.3593441,,"Software engineering artifacts are central to DevOps, enabling the collaboration of teams involved with integrating the development and operations domains. However, collaboration around DevOps artifacts has yet to receive detailed research attention. We apply the sociological concept of Boundary Objects to describe and evaluate the specific software engineering artifacts that enable a cross-disciplinary understanding. Using this focus, we investigate how different DevOps stakeholders can collaborate efficiently using common artifacts. We performed a multiple case study and conducted twelve semi-structured interviews with DevOps practitioners in nine companies. We elicited participants' collaboration practices, focusing on the coordination of stakeholders and the use of engineering artifacts as a means of translation. This paper presents a consolidated overview of four categories of DevOps Boundary Objects and eleven stakeholder groups relevant to DevOps. To help practitioners assess cross-disciplinary knowledge management strategies, we detail how DevOps Boundary Objects contribute to four areas of DevOps knowledge and propose derived dimensions to evaluate their use."
Farooq2022,Muhammad Shoaib Farooq and Mishaal Ahmed and Muhammad Emran,"A Survey on Blockchain Acquainted Software Requirements Engineering: Model, Opportunities, Challenges, and Future Directions",IEEE Access,10,,2022,10.1109/ACCESS.2022.3171408,21693536,"Requirements are the basis of software development practices. Ambiguities in requirements lead a project to a point of failure or penalize it with a high budget and time for defect traceability. The ever-growing demand for advanced computing systems has increased the complexity of Software Requirements Engineering (SRE) practices. Blockchain systems require specialized SRE practices as the issues of Requirement Traceability (RT), developer/client confidentiality, and Requirement Negotiation (RN) typically exist in conventional approaches, which require more improvement. Moreover, blockchain technology incorporates the capacity to function as an infrastructure for the SRE framework providing transparency, security, and reliability. Even though the significance of studying blockchain in the context of SRE is evident, it is still in its infancy. None of the previous studies surveyed this domain to the best of our knowledge. We aim to summarize the scholarly contributions of blockchain acquainted SRE from 2015 to 2021 and to provide academia and practitioners with in-depth knowledge about this domain. In this article, we have provided a novel comprehensive review of the aspects of blockchain-acquainted SRE practices. We have presented SRE-based quality improvement factors and outlined the need for blockchain technology in this domain. Furthermore, we have classified SRE practices based on blockchain engineering. In addition, we have proposed a generic SRE model built on blockchain infrastructure along with its workflows. Similarly, we have provided implementation guidelines for the future development guidance of SRE applications built on blockchain technology. Finally, we have presented the current research challenges and provided future directions based on blockchain acquainted SRE."
WahyuSejati2023,Wahyu Sejati and Ankur Singh Bist and Amirsyah Tambunan,Pengembangan Analisis Sentimen dalam Rekayasa Software Engineering menggunakan tinjauan literatur sistematis,"Jurnal MENTARI: Manajemen, Pendidikan dan Teknologi Informasi",2,1,2023,10.33050/mentari.v2i1.377,2963-4423,"Pengembangan Software Engineering membutuhkan kolaborasi banyak orang dengan beragam peran. Menurut penelitian, elemen sosial dalam tim pengembangan sangat penting untuk menyelesaikan proyek dengan sukses dan memuaskan. Telah dibuktikan bahwa suasana hati tim, memiliki dampak yang signifikan. Oleh karena itu, manajer proyek atau pemimpin proyek tertarik untuk meneliti situasi yang menimbulkan suasana hati negatif untuk membuat intervensi yang sesuai. Salah satu alat yang digunakan dalam situasi ini adalah analisis sentimen, yang menawarkan cara untuk mengukur suasana hati dari pertukaran tekstual. Makalah ini menyajikan temuan dari tinjauan menyeluruh terhadap literatur tentang teknologi analisis sentimen yang dikembangkan atau digunakan dalam Software Engineering. Penelitian ini mengumpulkan data dari 80 makalah tentang berbagai subjek, termasuk domain aplikasi, tujuan penggunaan, kumpulan data, pendekatan untuk mengembangkan alat analisis sentimen, dan kesulitan yang dihadapi para peneliti saat menggunakan analisis sentimen dalam konteks proyek Software Engineering. Berdasarkan temuan penelitian, inisiatif rekayasa perangkat lunak sumber terbuka yang menggunakan analisis sentimen sering kali menggunakan teknik berbasis mesin vektor pendukung. Terlepas dari kenyataan bahwa analisis sentimen sering digunakan dalam Software Engineering, masih ada beberapa masalah yang perlu diperbaiki. Misalnya, sulit untuk mengidentifikasi ironi atau sarkasme dalam teks. Hal ini menunjukkan jenis penelitian yang harus dilakukan untuk mengembangkan cara yang lebih baik dalam menerapkan analisis sentimen dalam konteks proyek Software Engineering. Secara keseluruhan, penelitian ini meningkatkan pemahaman kita tentang pengembangan dan penerapan alat analisis sentimen dalam Software Engineering. Penelitian ini dapat meningkatkan keberhasilan proyek Software Engineering secara signifikan karena pengetahuan yang lebih baik tentang elemen sosial dan implementasi teknologi analisis sentimen yang efektif."
Hooshyar2023,Hosna Hooshyar and Eduardo Guerra and Jorge Melegati and Dron Khanna and Abdullah Aldaeej and Gerardo Matturro and Luciana Zaina and Des Greer and Usman Rafiq and Rafael Chanin and Xiaofeng Wang and Juan Garbajosa and Pekka Abrahamsson and Foutse Khomh and Anh Nguyen-Duc,Impact in Software Engineering Activities After One Year of COVID-19 Restrictions for Startups and Established Companies,IEEE Access,11,,2023,10.1109/ACCESS.2023.3279917,21693536,"The restrictions imposed by the COVID-19 pandemic required software development teams to adapt, being forced to work remotely and adjust the software engineering activities accordingly. In the studies evaluating these effects, a few have assessed the impact on software engineering activities from a broader perspective and after a period of time when teams had time to adjust to the changes. No studies have been found comparing software startups and established companies either. This paper aims to investigate the impacts of COVID-19 on software development activities after one year of the pandemic restrictions, comparing the results between startups and established companies. Our approach was to design a cross-sectional survey and distribute it online among software development companies worldwide. The participants were asked about their perception of COVID-19's pandemic impact on different software engineering activities: requirements engineering, software architecture, user experience design, software implementation, and software quality assurance. The survey received 170 valid answers from 29 countries, and for all the software engineering activities, we found that most respondents did not observe a significant impact. The results also showed that software startups and established companies were affected differently since, in some activities, we found a negative impact in the former and a positive impact in the latter. Regarding the time spent on each software engineering activity, most of the answers reported no change, but on those that did, the result points to an increase in time. Thus, we cannot find any relation between the change in time of effort and the reported positive or negative impact."
Soufiane2023,Ouariach Soufiane and Khaldi Maha and Khaldi Mohamed,Conceptualizing an Inductive Learning Situation in Online Learning Enabled by Software Engineering,International Journal of Advanced Computer Science and Applications,14,12,2023,10.14569/IJACSA.2023.0141210,21565570,"Our work highlights the importance of adopting a systematic and methodical software engineering approach to the development of information technology projects for e-learning. We place particular emphasis on conceptualizing pedagogical scenarios and an inductive online learning situation. To ensure effective management of the information systems development process, we applied instructional design principles and adopted the 2TUP process, a refined version of the Rational Unified Process (RUP) suitable for projects of all sizes. To provide a visual representation of the system architecture and inform instructional design decisions, we used the Unified Modeling Language (UML) to create class, use case, activity, and sequence diagrams. We aim to demonstrate the potential of a structured software engineering approach to creating effective and efficient e-learning systems by conceptualizing an inductive online learning situation and five concrete examples illustrating the system's functionality. Our work underlines the importance of using standardized modeling languages such as UML to facilitate communication between stakeholders and collaboration between instructional designers and software developers."
Lu2023,Fengwei Lu and Zhongwei Jin and Xiaolin Zhang,An Analysis of Data Mining Techniques in Software Engineering Database Design,,228,,2023,10.1016/j.procs.2023.11.007,18770509,"With the advent of Internet information technology, data mining technology has been applied in various fields of China's social and industrial development, and has promoted the quality development of the industry [1]. Nowadays, people are widely influenced by Internet computer technology, and the application of computer technology has been indispensable in life, work and study. At the same time, data mining technology arises from Internet communication technology and is used as an important technical means of operation and development by various industries, especially the application of data mining technology in university software engineering teaching is becoming more and more widespread, but there are still many unavoidable problems that require scholars to pay more attention. The article analyzes and discusses the significance of applying data mining technology in software engineering [2], and on this basis, the application path of data mining technology in software engineering database design is proposed, including: mining information, mining loopholes, mining execution records and open source. It is hoped that relevant technology users can efficiently apply data mining technology to make software engineering It is more reasonable and efficient, so that software engineering can be further developed. This paper presents the implementation and performance analysis of the teacher management decision-making system based on data mining, and introduces the implementation process of the system in detail."
Cornide-Reyes2021,Hector Cornide-Reyes and Fabian Riquelme and Rene Noel and Rodolfo Villarroel and Cristian Cechinel and Patricio Letelier and Roberto Munoz,Key Skills to Work with Agile Frameworks in Software Engineering: Chilean Perspectives,IEEE Access,9,,2021,10.1109/ACCESS.2021.3087717,21693536,"Agile frameworks continue to provide positive evidence regarding the benefits of their use in the software products. Since these methods develop professional skills in those who practice them, their knowledge and use will acquire greater demand in areas other than software development. For this reason, it is essential to recognize the key skills for agile team building. The goal of this paper is to identify the agile professional skills that the Chilean industry considers key to conform high-performance agile teams. A survey was applied to agile community professionals in Chile to validate the results of previous work and to identify relevant information regarding learning processes, techniques, and tools for working with agile frameworks. The results allowed to establish three key skills for high-performance teams with their respective levels of achievement."
Vingerhoets2021,Anne Sofie Vingerhoets and Samedi Heng and Yves Wautelet,"Using i* and UML for Blockchain Oriented Software Engineering: Strengths, Weaknesses, Lacks and Complementarity",Complex Systems Informatics and Modeling Quarterly,2021,26,2021,10.7250/csimq.2021-26.02,22559922,New blockchain-based projects do appear every day. The technology has indeed been popularized by cryptocurrencies but is now gaining interest in various domains and new types of applications are evaluated constantly. Understanding the impact of blockchain adoption on the organization and the internals of blockchain-related behavior nevertheless remains a challenge for managers but also for IT professionals. This article studies how two existing organizational and software modeling languages can be fit to document a blockchain development project in Supply Chain Management (SCM) at its earliest stages. These two frameworks are i* on the one side and the Unified Modeling Language (UML) use case and sequence diagrams on the other side. The real life project used as a case study in this application is ‘Farm-to-Fork’ where a blockchain solution for the Supply Chain (SC) of farm animals is developed. The application of the frameworks is intended to identify their strengths and weaknesses. An extension of i* is proposed to deal with blockchain privacy issues as well as laws and norms. We finally point to the complementarity of i* and UML use case and sequence diagrams in a Blockchain-Oriented Software Engineering (BOSE) context. The i* framework indeed supports early requirements to understand the impact of the project on stakeholders while UML use case and sequence diagrams support the late requirements and the design by depicting the use of blockchain and some of its behavioral mechanisms.
Azzazi2022,Ahmad Azzazi and Mohammad Shkoukani,A Knowledge-based Expert System for Supporting Security in Software Engineering Projects,International Journal of Advanced Computer Science and Applications,13,1,2022,10.14569/IJACSA.2022.0130149,21565570,"Building secure software systems requires the intersection between two engineering disciplines, software engineering and security engineering. There is a lack of a defined security mechanism for each of the software development phases, which affects the quality of the software system intensively. In this paper, the authors are proposing a framework to consider the security aspects in all the phases of the software development process from the requirements until the deployment of the software product, with three additional phases that are important to automatically produce a secure system. The framework is developed after analyzing the existing models for secure system development. The key elements of the framework are the addition of the phases like physical, training, and auditing, where they improve the level of security in software engineering projects. The authors found so a solution for the replacement of the knowledge of the security engineer through the construction of an intelligent knowledge-based system, which provides the software developer with the security rules needed in each phase of the software development lifecycle and it improves the awareness of the software developer about the security-related issues in each phase of the software development lifecycle. The framework and the expert system are tested on a variety of software projects, where a significant improvement of security in each phase of the software development process is achieved."
Ahmad2023,Khlood Ahmad and Mohamed Abdelrazek and Chetan Arora and Arbind Agrahari Baniya and Muneera Bano and John Grundy,Requirements engineering framework for human-centered artificial intelligence software systems,Applied Soft Computing,143,,2023,10.1016/j.asoc.2023.110455,15684946,"Context: Artificial intelligence (AI) components used in building software solutions have substantially increased in recent years. However, many of these solutions focus on technical aspects and ignore critical human-centered aspects. Objective: Including human-centered aspects during requirements engineering (RE) when building AI-based software can help achieve more responsible, unbiased, and inclusive AI-based software solutions. Method: In this paper, we present a new framework developed based on human-centered AI guidelines and a user survey to aid in collecting requirements for human-centered AI-based software. We provide a catalog to elicit these requirements and a conceptual model to present them visually. Results: The framework is applied to a case study to elicit and model requirements for enhancing the quality of 360° videos intended for virtual reality (VR) users. Conclusion: We found that our proposed approach helped the project team fully understand the human-centered needs of the project to deliver. Furthermore, the framework helped to understand what requirements need to be captured at the initial stages against later stages in the engineering process of AI-based software."
Nofriansyah2020,Dicky Nofriansyah and Ganefri and Ridwan,A new learning model of software engineering in vocational education,International Journal of Evaluation and Research in Education,9,3,2020,10.11591/ijere.v9i3.20482,26205440,"This research focused on the development a new learning model in Vocational Education to answer the challenges of this Industrial Revolution 4.0 era. The problem identified was the lack of learning outcomes, especially subjects oriented to software engineering for information systems students in particular and other computer science seen in the phenomenon of the inability of students to produce intelligent systems. From a series of validity, practicality, and effectiveness test results, use content validity with Aiken'V and construct validity with CFA (Confirmatory Factor Analysis) states that the model resulting from this study is stated, valid, practical and effective. This study also produced a new learning model with five syntaxes, namely (1) Define Problem and Design a Plan Project, (2) Interaction with Support System, (3) Create a Project, (4) Keep control and Monitoring Project, (4) Yield and Assessment of Project. And based on the test of the validity of the syntax of this model stated goodness-of-fit or valid."
DosSantos2020,Marcela G. Dos Santos and Bianca M. Napoleão and Fabio Petrillo and Darine Ameyed and Fehmi Jaafar,A Preliminary Systematic Mapping on Software Engineering for Robotic Systems: A Software Quality Perspective,,,,2020,10.1145/3387940.3392197,,"Robotic systems have been increasingly employed in everyday tasks. Considering that software plays a crucial point in robot systems, to investigate how software engineering concepts in a software quality perspective can improve robotic systems. In this work, we present a systematic mapping to identify and classify the state-of-art of software engineering for robotic systems in a quality software perspective. We selected and systematically analyzed a final set of 35 primary studies extracted from an automated search on Scopus digital library. This work presents three main contributions. Firstly, we organize a catalogue of research studies about software engineering, more specifically software quality applied in robotic systems. Next, we systematically analyze software quality areas used in robotic systems. Finally, we discuss insights into research opportunities and gaps in software engineering to robotic systems for future studies. As a result, we observed that there are studies in the robotic systems area, addressing in a combined way, software engineering approaches and software quality aspects. The less investigated software quality aspect is security. Due to this fact, we presented an overview of the state-of-art on blockchain applying in robotics systems. Blockchain brings opportunities for changing the ways that robots interact with humans. Finally, we identify research opportunities and gaps in software quality on robotic systems, presenting an overview for future studies."
Timperley2021,Christopher S. Timperley and Lauren Herckis and Claire Le Goues and Michael Hilton,Understanding and improving artifact sharing in software engineering research,Empirical Software Engineering,26,4,2021,10.1007/s10664-021-09973-5,15737616,"In recent years, many software engineering researchers have begun to include artifacts alongside their research papers. Ideally, artifacts, including tools, benchmarks, and data, support the dissemination of ideas, provide evidence for research claims, and serve as a starting point for future research. However, in practice, artifacts suffer from a variety of issues that prevent the realization of their full potential. To help the software engineering community realize the full potential of artifacts, we seek to understand the challenges involved in the creation, sharing, and use of artifacts. To that end, we perform a mixed-methods study including a survey of artifacts in software engineering publications, and an online survey of 153 software engineering researchers. By analyzing the perspectives of artifact creators, users, and reviewers, we identify several high-level challenges that affect the quality of artifacts including mismatched expectations between these groups, and a lack of sufficient reward for both creators and reviewers. Using Diffusion of Innovations (DoI) as an analytical framework, we examine how these challenges relate to one another, and build an understanding of the factors that affect the sharing and success of artifacts. Finally, we make recommendations to improve the quality of artifacts based on our results and existing best practices."
Budgen2022,David Budgen and Pearl Brereton,Short communication: Evolution of secondary studies in software engineering,Information and Software Technology,145,,2022,10.1016/j.infsof.2022.106840,09505849,"Context: Other disciplines commonly employ secondary studies to address the needs of practitioners and policy-makers. Since being adopted by software engineering in 2004, many have been undertaken by researchers. Objective: To assess how the role of secondary studies in software engineering has evolved. Methods: We examined a sample of 131 secondary studies published in a set of five major software engineering journals for the years 2010, 2015 and 2020. These were categorised by their type (e.g. mapping study), their research focus (quantitative/qualitative and practice/methodological), as well as the experience of the first authors. Results: Secondary studies are now a well-established research tool. They are predominantly qualitative and there is extensive use of mapping studies to profile research in particular areas. A significant number are clearly produced as part of postgraduate study, although experienced researchers also conduct many secondary studies. They are sometimes also used as part of a multi-method study. Conclusion: Existing guidelines largely focus upon quantitative systematic reviews. Based on our findings, we suggest that more guidance is needed on how to conduct, analyse, and report qualitative secondary studies."
Brstler2023,Jürgen Börstler and Nauman bin Ali and Kai Petersen,Double-counting in software engineering tertiary studies — An overlooked threat to validity,Information and Software Technology,158,,2023,10.1016/j.infsof.2023.107174,09505849,"Context: Double-counting in a literature review occurs when the same data, population, or evidence is erroneously counted multiple times during synthesis. Detecting and mitigating the threat of double-counting is particularly challenging in tertiary studies. Although this topic has received much attention in the health sciences, it seems to have been overlooked in software engineering. Objective: We describe issues with double-counting in tertiary studies, investigate the prevalence of the issue in software engineering, and propose ways to identify and address the issue. Method: We analyze 47 tertiary studies in software engineering to investigate in which ways they address double-counting and whether double-counting might be a threat to validity in them. Results: In 19 of the 47 tertiary studies, double-counting might bias their results. Of those 19 tertiary studies, only 5 consider double-counting a threat to their validity, and 7 suggest strategies to address the issue. Overall, only 9 of the 47 tertiary studies, acknowledge double-counting as a potential general threat to validity for tertiary studies. Conclusions: Double-counting is an overlooked issue in tertiary studies in software engineering, and existing design and evaluation guidelines do not address it sufficiently. Therefore, we propose recommendations that may help to identify and mitigate double-counting in tertiary studies."
Horner2020,Jack K. Horner and John F. Symons,Software engineering standards for epidemiological models,History and Philosophy of the Life Sciences,42,4,2020,10.1007/s40656-020-00347-6,17426316,"There are many tangled normative and technical questions involved in evaluating the quality of software used in epidemiological simulations. In this paper we answer some of these questions and offer practical guidance to practitioners, funders, scientific journals, and consumers of epidemiological research. The heart of our paper is a case study of the Imperial College London (ICL) covid-19 simulator, set in the context of recent work in epistemology of simulation and philosophy of epidemiology."
Rainer2021,Austen Rainer,Storytelling in human-centric software engineering research,,,,2021,10.1145/3463274.3463803,,"BACKGROUND: Software engineering is a human activity. People naturally make sense of their activities and experience through storytelling. But storytelling does not appear to have been properly studied by software engineering research. AIM: We explore the question: what contribution can storytelling make to human-centric software engineering research? METHOD: We define concepts, identify types of story and their purposes, outcomes and effects, briefly review prior literature, identify several contributions and propose next steps. RESULTS: Storytelling can, amongst other contributions, contribute to data collection, data analyses, ways of knowing, research outputs, interventions in practice, and advocacy, and can integrate with evidence and arguments. Like all methods, storytelling brings risks. These risks can be managed. CONCLUSION: Storytelling provides a potential counter-balance to abstraction, and an approach to retain and honour human meaning in software engineering."
Croock2020,Muayad Sadik Croock and Saja Dhyaa Khuder and Ayad Esho Korial and Sahar Salman Mahmood,Early detection of breast cancer using mammography images and software engineering process,Telkomnika (Telecommunication Computing Electronics and Control),18,4,2020,10.12928/TELKOMNIKA.V18I4.14718,23029293,"The breast cancer has affected a wide region of women as a particular case. Therefore, different researchers have focused on the early detection of this disease to overcome it in efficient way. In this paper, an early breast cancer detection system has been proposed based on mammography images. The proposed system adopts deep-learning technique to increase the accuracy of detection. The convolutional neural network (CNN) model is considered for preparing the datasets of training and test. It is important to note that the software engineering process model has been adopted in constructing the proposed algorithm. This is to increase the reliably, flexibility and extendibility of the system. The user interfaces of the system are designed as a website used at country side general purpose (GP) health centers for early detection to the disease under lacking in specialist medical staff. The obtained results show the efficiency of the proposed system in terms of accuracy up to more than 90% and decrease the efforts of medical staff as well as helping the patients. As a conclusion, the proposed system can help patients by early detecting the breast cancer at far places from hospital and referring them to nearest specialist center."
Perez2020,Francisca Perez and Ana C. Marcen and Raul Lapena and Carlos Cetina,Evaluating Low-Cost in Internal Crowdsourcing for Software Engineering: The Case of Feature Location in an Industrial Environment,IEEE Access,8,,2020,10.1109/ACCESS.2020.2985915,21693536,"Internal crowdsourcing in software engineering is a mechanism for recruiting engineers to carry out more efficiently software engineering tasks. However, engineers are busy resources and time is a valuable asset in industry, which hinders internal crowdsourcing in software engineering from becoming a widespread practice. In this work, we propose a low-cost variant of internal crowdsourcing for locating features in models, which limits the time that engineers can spend for providing knowledge. Our approach uses the knowledge provided by the internal crowd to automatically reformulate an initial feature description. The result is taken as input to automatically locate the relevant model fragment using Latent Semantic Indexing. We evaluate our approach using four query reformulation techniques in a real-world case study from our industrial partner. We compare the results of our approach in terms of recall, precision and F-measure with a baseline by means of statistical methods to show that the impact of the results of our approach is significant. Despite the limitation of time, the results show that low-cost in internal crowdsourcing improves significantly the results in an industrial context where engineers' availability is scarce."
Almeida2022,Fernando Almeida and Jorge Simões and Sérgio Lopes,Exploring the Benefits of Combining DevOps and Agile,Future Internet,14,2,2022,10.3390/fi14020063,19995903,"The combined adoption of Agile and DevOps enables organizations to cope with the increasing complexity of managing customer requirements and requests. It fosters the emergence of a more collaborative and Agile framework to replace the waterfall models applied to software development flow and the separation of development teams from operations. This study aims to explore the benefits of the combined adoption of both models. A qualitative methodology is adopted by including twelve case studies from international software engineering companies. Thematic analysis is employed in identifying the benefits of the combined adoption of both paradigms. The findings reveal the existence of twelve benefits, highlighting the automation of processes, improved communication between teams, and reduction in time to market through process integration and shorter software delivery cycles. Although they address different goals and challenges, the Agile and DevOps paradigms when properly combined and aligned can offer relevant benefits to organizations. The novelty of this study lies in the systematization of the benefits of the combined adoption of Agile and DevOps considering multiple perspectives of the software engineering business environment."
Fagerholm2022,Fabian Fagerholm and Michael Felderer and Davide Fucci and Michael Unterkalmsteiner and Bogdan Marculescu and Markus Martini and Lars Göran Wallgren Tengberg and Robert Feldt and Bettina Lehtelä and Balázs Nagyváradi and Jehan Khattak,Cognition in Software Engineering: A Taxonomy and Survey of a Half-Century of Research,ACM Computing Surveys,54,11s,2022,10.1145/3508359,15577341,"Cognition plays a fundamental role in most software engineering activities. This article provides a taxonomy of cognitive concepts and a survey of the literature since the beginning of the Software Engineering discipline. The taxonomy comprises the top-level concepts of perception, attention, memory, cognitive load, reasoning, cognitive biases, knowledge, social cognition, cognitive control, and errors, and procedures to assess them both qualitatively and quantitatively. The taxonomy provides a useful tool to filter existing studies, classify new studies, and support researchers in getting familiar with a (sub) area. In the literature survey, we systematically collected and analysed 311 scientific papers spanning five decades and classified them using the cognitive concepts from the taxonomy. Our analysis shows that the most developed areas of research correspond to the four life-cycle stages, software requirements, design, construction, and maintenance. Most research is quantitative and focuses on knowledge, cognitive load, memory, and reasoning. Overall, the state of the art appears fragmented when viewed from the perspective of cognition. There is a lack of use of cognitive concepts that would represent a coherent picture of the cognitive processes active in specific tasks. Accordingly, we discuss the research gap in each cognitive concept and provide recommendations for future research."
Carvalho2024,Luiz Paulo Carvalho and José Antonio Suzano and Thais Batista and Flávia Maria Santoro and Jonice Oliveira,Ethics: What is the Brazilian Software Engineering Research Scenario?,Journal of Software Engineering Research and Development,12,1,2024,10.5753/jserd.2024.3395,,"Background: Ethics is the theory or science of the moral behavior of humans in society. Traditionally, we value “unethical” actions that go against determining morality in a specific context. One of the sub-domains of Ethics is Computational Ethics, which deals with ethical dilemmas that are strictly related to computational issues. Dilemmas in this area involve privacy, improper access, intellectual property, digital norms and laws, power, socio-technical aspects (such as gender discrimination), and robotics, among others. In this context, “Software Engineering” and “Software” are different objects. Engineering is an act, a practice, as also coding, programming, and software reuse. As with any act, moral and subsequent ethical considerations are appropriate. We characterize software as an object of concrete reality, as a sociotechnical system formed by a technical artifact, human aspect, and procedural aspect. This assumption will form the main base discussion of ethics and morals in Software Engineering in this paper. Objective: The goal of this paper is to unveil the Brazilian Software Engineering ethics panorama. Method: We follow the rigor of, and inspired by, a Systematic Literature Review (SLR) protocol to answer the question: how does ethics explicitly permeate the Brazilian Software Engineering publications between the last thirteen years (2010 and 2022)? Results: After analyzing 1529 papers through the research protocol, 175 (≈11%) presented some explicit occurrence of ethical aspects. The occurrence was relevant in only 7 papers (≈0.4%), exposing a shallow scenario on ethical or moral aspects. Conclusions: If Ethics is a topic considered important to deliberate, research or discuss, this did not occur significantly in the Brazilian Software Engineering research scenario since 2010. With this result in mind, we discussed parallel terms and concepts to enrich the contribution of the qualitative synthesis."
Rainer2022,Austen Rainer and Claes Wohlin,Recruiting credible participants for field studies in software engineering research,Information and Software Technology,151,,2022,10.1016/j.infsof.2022.107002,09505849,"Context: Software practitioners are a primary provider of information for field studies in software engineering. Research typically recruits practitioners through some kind of sampling. But sampling may not in itself recruit the “right” participants. Objective: To assess existing guidance on participant recruitment, and to propose and illustrate a framework for recruiting professional practitioners as credible participants in field studies of software engineering. Methods: We review existing guidelines, checklists and other advisory sources on recruiting participants for field studies. We develop a framework, partly based on our prior research and on the research of others. We search for and select three exemplar studies (a case study, an interview study and a survey study) and use those to illustrate the framework. Results: Whilst existing guidance recognises the importance of recruiting participants, there is limited guidance on how to recruit the “right” participants. The framework suggests the conceptualisation of participants as “research instruments” or, alternatively, as a sampling frame for items of interest. The exemplars suggest that at least some members of the research community are aware of the need to carefully recruit the “right” participants. Conclusions: The framework is intended to encourage researchers to think differently about the involvement of practitioners in field studies of software engineering. Also, the framework identifies a number of characteristics not explicitly addressed by existing guidelines."
SadikCroock2021,Muayad Sadik Croock and Zahraa Abbas Hassan and Saja Dhyaa Khuder,Adaptive key generation algorithm based on software engineering methodology,International Journal of Electrical and Computer Engineering,11,1,2021,10.11591/ijece.v11i1.pp589-595,20888708,"Recently, the generation of security keys has been considered for guaranteeing the strongest of them in terms of randomness. In addition, the software engineering methodologies are adopted to ensure the mentioned goal is reached. In this paper, an adaptive key generation algorithm is proposed based on software engineering techniques. The adopted software engineering technique is self-checking process, used for detecting the fault in the underlying systems. This technique checks the generated security keys in terms of validity based on randomness factors. These factors include the results of National Institute of Standard Test (NIST) tests. In case the randomness factors are less than the accepted values, the key is regenerated until obtaining the valid one. It is important to note that the security keys are generated using shift register and SIGABA technique. The proposed algorithm is tested over different case studies and the results show the effective performance of it to produce well random generated keys."
Carleton2022,Anita Carleton and Forrest Shull and Erin Harper,Architecting the Future of Software Engineering,Computer,55,9,2022,10.1109/MC.2022.3187912,15580814,"To meet future demand for software capabilities, improvements in critical software engineering technologies are needed. This article presents a research road map and call to action that highlights the need for continual investment in software engineering research."
Fatima2020,Nargis Fatima and Sumaira Nazir and Suriayati Chuprat,Knowledge sharing factors for modern code review to minimize software engineering waste,International Journal of Advanced Computer Science and Applications,11,1,2020,10.14569/ijacsa.2020.0110160,21565570,"Software engineering activities, for instance, Modern Code Review (MCR) produce quality software by identifying the defects from the code. It involves social coding and provides ample opportunities to share knowledge among MCR team members. However, the MCR team is confronted with the issue of waiting waste due to poor knowledge sharing among MCR team members. As a result, it delays the project delays and increases mental distress. To minimize the waiting waste, this study aims to identify knowledge sharing factors that impact knowledge sharing in MCR. The methodology employed for this study is a systematic literature review to identify knowledge sharing factors, data coding with continual comparison and memoing techniques of grounded theory to produce a unique and categorized list of factors influencing knowledge sharing. The identified factors were then assessed through expert panel for its naming, expressions, and categorization. The study finding reported 22 factors grouped into 5 broad categories i.e. Individual, Team, Social, Facility conditions, and Artifact. The study is useful for researchers to extend the research and for the MCR team to consider these factors to enhance knowledge sharing and to minimize waiting waste."
Klotins2023,Eriks Klotins and Elliot Talbert-Goldstein,Organizational Conflicts in the Adoption of Continuous Software Engineering,,475 LNBIP,,2023,10.1007/978-3-031-33976-9_10,18651356,"Software is a critical component of nearly every product or service. Improvements in software can lead to substantial competitive advantages. At the same time, software and surrounding engineering teams have become increasingly complex. The adoption of continuous integration and delivery is a recent trend to radically improve software release speed. However, its adoption is far from straightforward. Specifically, rethinking processes, organizational culture, ways of working, and business models require buy-in from diverse stakeholders that may have conflicting objectives. Such situations are explored by organizational conflict research. This paper reports on early lessons from an ongoing research project in continuous software engineering, specifically investigating adoption challenges from an organizational conflict perspective. We identify catalysts, symptoms, and outcomes of organizational conflicts hindering the adoption process. We conclude that predictable conflicts emerge when adopting continuous engineering. Engineers, managers, and other teams can proactively prepare for and allocate resources to resolve them. Proper analysis and management can help avoid wasted time, impeding processes, and frustration."
Daz2023,Jessica Díaz and Jorge Pérez and Carolina Gallardo and Ángel González-Prieto,Applying Inter-Rater Reliability and Agreement in collaborative Grounded Theory studies in software engineering,Journal of Systems and Software,195,,2023,10.1016/j.jss.2022.111520,01641212,"Context: The qualitative research on empirical software engineering that uses Grounded Theory is increasing (GT). The trustworthiness, rigor, and transparency of GT qualitative data analysis can benefit, among others, when multiple analysts juxtapose diverse perspectives and collaborate to develop a common code frame based on a consensual and consistent interpretation. Inter-Rater Reliability (IRR) and/or Inter-Rater Agreement (IRA) are commonly used techniques to measure consensus, and thus develop a shared interpretation. However, minimal guidance is available about how and when to measure IRR/IRA during the iterative process of GT, so researchers have been using ad hoc methods for years. Objective: This paper presents a process for systematically measuring IRR/IRA in GT studies, when appropriate, which is grounded in a previous systematic mapping study on collaborative GT in the field of software engineering. Methods: Meta-science guided us to analyze the issues and challenges of collaborative GT and formalize a process to measure IRR/IRA in GT. Results: This process guides researchers to incrementally generate a theory while ensuring consensus on the constructs that support it, improving trustworthiness, rigor, and transparency, and promoting the communicability, reflexivity, and replicability of the research. Conclusion: The application of this process to a GT study seems to support its feasibility. In the absence of further confirmation, this would represent the first step in a de facto standard to be applied to those GT studies that may benefit from IRR/IRA techniques."
Ko2021,Hatice Koç and Ali Mert Erdoğan and Yousef Barjakly and Serhat Peker,UML Diagrams in Software Engineering Research: A Systematic Literature Review,,,,2021,10.3390/proceedings2021074013,,"Software engineering is a discipline utilizing Unified Modelling Language (UML) diagrams, which are accepted as a standard to depict object-oriented design models. UML diagrams make it easier to identify the requirements and scopes of systems and applications by providing visual models. In this manner, this study aims to systematically review the literature on UML diagram utilization in software engineering research. A comprehensive review was conducted over the last two decades, spanning from 2000 to 2019. Among several papers, 128 were selected and examined. The main findings showed that UML diagrams were mostly used for the purpose of design and modeling, and class diagrams were the most commonly used ones."
Croock2022,Muayad Sadik Croock,Keyboard Encryption Algorithm Based on Software Engineering Security,International Journal of Computing and Digital Systems,11,1,2022,10.12785/ijcds/1101106,2210142X,"It is well known that nowadays the security issues in the information system are increased in noticed way. These issues include the hardware and software sides, in terms of data or information as well as the devices. One of the most important parts that can be used to tackle these issues is the keyboard. The attacks on the keyboard, such as key stroke and key logging, need to be tackled. In this paper, an encryption algorithm for keyboard is proposed to guarantee the security in the output keyboard from attackers. The proposed algorithm is built based on software engineering model in terms of structure and formulation. This is to increase the flexibility of development, scalability to involve more number of users (keyboards), and reliability of employing in real-time system due to its light and efficiency. The proposed algorithm uses the interleaving and XOR processes in encryption, where the interleaver uses random seeds in generating specific indexing orders. It is tested among numerous case studies including the randomness of the generated indexing orders that are passed through different tests. The efficiency of the proposed algorithm is tested over many experiments to obtain more than 98%. Moreover, the consumed time for encryption and decryption processes is checked as well to obtain that the decryption process takes less time."
Abbasi2021,Khurrum Mustafa Abbasi and Tamim Ahmed Khan and Irfan ul Haq,Modeling-framework for model-based software engineering of complex Internet of things systems,Mathematical Biosciences and Engineering,18,6,2021,10.3934/mbe.2021458,15510018,"Internet of things (IoT) systems are composed of variety of units from different domains. While developing a complete IoT system, different professionals from different domains may have to work in collaboration. In this paper we provide a framework which allows using discrete and continuous time modeling and simulation approaches in combination for IoT systems. The proposed framework demonstrates on how to model Ad-hoc and general IoT systems for software engineering purpose. We demonstrate that model-based software engineering on one hand can provide a common platform to overcome communication gaps among collaborating stakeholders whereas, on the other hand can model and integrate heterogeneous components of IoT systems. While modeling heterogeneous IoT systems, one of the major challenges is to apply continuous and discrete time modeling on intrinsically varying components of the system. Another difficulty may be how to compose these heterogeneous components into one whole system. The proposed framework provides a road-map to model discrete, continuous, Ad-hoc, general systems along with composition mechanism of heterogeneous subsystems. The framework uses a combination of Agent-based modeling, Aspect-oriented modeling, contract-based modeling and services-oriented modeling concepts. We used this framework to model a scenario example of a service-oriented IoT system as proof of concept. We analyzed our framework with existing systems and discussed it in details. Our framework provides a mechanism to model different viewpoints. The framework also enhances the completeness and consistency of the IoT software models."
Fatima2020,Nargis Fatima and Sumaira Nazir and Suriayati Chuprat,Knowledge sharing framework for modern code review to diminish software engineering waste,International Journal of Advanced Computer Science and Applications,11,6,2020,10.14569/IJACSA.2020.0110656,21565570,"Modern Code Review (MCR) is a quality assurance technique that involves massive interactions between team members of MCR. Presently team members of MCR are confronting with the problem of waiting waste production, which results in their psychological distress and project delays. Therefore, the MCR team needs to have effective knowledge sharing during MCR activities, to avoid the circumstances that lead the team members to the waiting state. The objective of this study is to develop the knowledge sharing framework for MCR team to reduce waiting waste. The research methodology used for this study is the Delphi survey. The conducted Delphi survey intended to produce the finalized list of knowledge sharing factors and to recognize and prioritize the most influencing knowledge sharing factor for MCR activities. The study results reported 22 knowledge sharing factors, 135 sub-factor, and 5 categories. Grounded on the results of the Delphi survey the knowledge sharing framework for MCR has been developed. The study is beneficial for software engineering researchers to outspread the research. It can also help the MCR team members to consider the designed framework to increase knowledge sharing and diminish waiting waste."
Melegati2020,Jorge Melegati and Xiaofeng Wang,Case survey studies in software engineering research,,,,2020,10.1145/3382494.3410683,19493789,"Background: Given the social aspects of Software Engineering (SE), in the last twenty years, researchers from the field started using research methods common in social sciences such as case study, ethnography, and grounded theory. More recently, case survey, another imported research method, has seen its increasing use in SE studies. It is based on existing case studies reported in the literature and intends to harness the generalizability of survey and the depth of case study. However, little is known on how case survey has been applied in SE research, let alone guidelines on howto employ it properly. Aims: This article aims to provide a better understanding of how case survey has been applied in Software Engineering research. Method: To address this knowledge gap, we performed a systematic mapping study and analyzed 12 Software Engineering studies that used the case survey method. Results: Our findings show that these studies presented a heterogeneous understanding of the approach ranging from secondary studies to primary inquiries focused on a large number of instances of a research phenomenon. They have not applied the case survey method consistently as defined in the seminal methodological papers. Conclusions: We conclude that a set of clearly defined guidelines are needed on how to use case survey in SE research, to ensure the quality of the studies employing this approach and to provide a set of clearly defined criteria to evaluate such work."
Ma2024,Li Ma and Lei Huang,RESEARCH ON THE APPLICATION OF PROJECT TEACHING METHOD IN THE NEW MODEL OF SOFTWARE ENGINEERING COURSE,Scalable Computing,25,1,2024,10.12694/scpe.v25i1.2287,18951767,"Software engineering course is one of the core courses of computer science. The students trained in the current teaching mode can no longer meet the market demand for high-technology talents. Based on this, the research attempts to optimize the traditional software engineering teaching mode by using the project teaching method (PTM). According to the basic concept of PTM and course characteristics, the reform path of PTM in software engineering course is explored in the experiment. Then in the experiment, the indicators that affect teaching reform effect is selected and a evaluation model is built. And GA-BP algorithm is used to evaluate the effect of the evaluation model. To verify the performance of the built model and the final evaluation effect, the research results are tested from the fitness of the algorithm, error performance, prediction in the data set and other aspects. GA-BP algorithm converged when it iterated to the 18th generation, and the final fitness value was 0.61. The average error square value of GA-BP was 0.35 and the minimum error square sum of GA-BP was 0.48. Its prediction accuracy in test set and training set was 93.4% and 94.1% respectively. The maximum prediction error in the training sample was only 0.015, and the performance of the above data was better than the other three algorithms. To sum up, applying PTM to software engineering curriculum reform can achieve better teaching results."
Costa2020,Alexandre Costa and Felipe Ramos and Mirko Perkusich and Emanuel Dantas and Ednaldo Dilorenzo and Ferdinandy Chagas and Andre Meireles and Danyllo Albuquerque and Luiz Silva and Hyggo Almeida and Angelo Perkusich,Team Formation in Software Engineering: A Systematic Mapping Study,IEEE Access,8,,2020,10.1109/ACCESS.2020.3015017,21693536,"Context: Software team formation is an important project management activity. However, forming appropriate teams is a challenge for most of the companies. Objective: To analyze and synthesize the state of the art on the software team formation research. Additionally, we aim to organize the identified body of knowledge in software team formation as a taxonomy. Method: Using a Snowballing-based systematic mapping study, 51 primary studies, out of 2516, were identified and analyzed. We classified the studies considering the research methods used, their overall quality, and the characteristics of the formed teams and the proposed solutions. Results: The majority of the studies use search and optimization techniques in their approaches. Also, technical attributes are the most frequent type considered to build individuals' profiles during the team formation process. Furthermore, we proposed a taxonomy on software team formation. Conclusion: There is a predominant use of search-based approaches that combine search and optimization techniques with technical attributes. However, the adoption of non-technical attributes as complementary information is a tendency. Regarding the research gaps, we highlight the level of subjectivity in software team formation and the lack of scalability of the proposed solutions."
Shafiq2021,Saad Shafiq and Atif Mashkoor and Christoph Mayr-Dorn and Alexander Egyed,A Literature Review of Using Machine Learning in Software Development Life Cycle Stages,IEEE Access,9,,2021,10.1109/ACCESS.2021.3119746,21693536,"The software engineering community is rapidly adopting machine learning for transitioning modern-day software towards highly intelligent and self-learning systems. However, the software engineering community is still discovering new ways how machine learning can offer help for various software development life cycle stages. In this article, we present a study on the use of machine learning across various software development life cycle stages. The overall aim of this article is to investigate the relationship between software development life cycle stages, and machine learning tools, techniques, and types. We attempt a holistic investigation in part to answer the question of whether machine learning favors certain stages and/or certain techniques."
Wahyuningsih2024,Tri Wahyuningsih and Eko Sediyono and Kristoko Dwi Hartomo and Irwan Sembiring,The role of gamification implementation in improving quality and intention in software engineering learning,Journal of Education and Learning,18,1,2024,10.11591/edulearn.v18i1.20823,23029277,"Gamification can make learning more fun and engaging for students. Software engineering can utilize gamification to help students learn and improve their skills from the complexity of software engineering. This study used quantitative research to examines perceived ease of use, student satisfaction, and perceived usefulness to measure gamification quality, which can have an impact on software engineering intention, namely intention, loyalty, and participation in following and understanding software engineering materials. The data was collected based on an online questionnaire survey, 90 data were collected and then measured and analyzed using SmartPLS 3. The results showed that perceived ease of use, student satisfaction, and perceived usefulness have a significant influence on gamification quality, which also leads to a positive impact on software engineering intention. This research guides teachers and educational institutions that gamification is very successful as a learning medium to simplify complex information to be more interactive."
Yitao2023,Lin Yitao,Research on the application of data mining technology in software engineering,,228,,2023,10.1016/j.procs.2023.11.150,18770509,"The role of data mining in software engineering is obvious, but there is a lack of mining depth. In the past, engineering layout and function distribution problems in software development, and the framework construction was unreasonable. Therefore, this paper proposes a software system based on data mining technology. First, the software engineering project is divided, and the software engineering project design is carried out according to the interactive requirements to realize the preprocessing of software engineering. Then, according to the design criteria, the design collection of software engineering is formed, and the information system is analyzed in depth. MATLAB simulation shows that the design interactivity and accuracy of data mining are superior to traditional design methods under the condition that software engineering requirements are consistent."
Hofbauer2022,Markus Hofbauer and Christoph Bachhuber and Christopher Kuhn and Eckehard Steinbach,Teaching Software Engineering As Programming over Time,,,,2022,10.1145/3528231.3528353,,"Programming and software engineering differ by the aspect of time and scale. Going beyond just implementing software that fulfills requirements, software engineering also means writing code that can be maintained by multiple contributors over months, years or even decades. Due to the limited time of university projects, students mainly learn to focus on writing software that works once. In in-dustry, software lifetime is longer and the aspect of time becomes highly relevant. Professional software must be readable and modular to be maintainable. In this paper, we present an experience report on a novel university course in software engineering. The course teaches the concepts of unit testing, refactoring, and automation tools to novices with basic programming experience. We present those concepts for the example of C++,but they are applicable to any programming language. Our goal is to teach students the key con-cepts of software engineering early on, giving them the opportunity to benefit from these concepts in their further projects. We present these concepts in five plenary lectures with live coding sessions, and then student teams apply the concepts in five practical homework as-signments. All assignments contribute to a single project maintained and improved by the student groups for the duration of the course. Additionally, we present a teaching tool framework that can be used to automate tasks for student project management and examinations. Finally, we discuss the lessons learned from conducting this course for the first time. We believe this course is a valuable step towards including essential software engineering skills in the education of science and engineering students."
Ostberg2020,Jan Peter Ostberg and Daniel Graziotin and Stefan Wagner and Birgit Derntl,A methodology for psycho-biological assessment of stress in software engineering,PeerJ Computer Science,6,,2020,10.7717/PEERJ-CS.286,23765992,"Stress pervades our everyday life to the point of being considered the scourge of the modern industrial world. The effects of stress on knowledge workers causes, in short term, performance fluctuations, decline of concentration, bad sensorimotor coordination, and an increased error rate, while long term exposure to stress leads to issues such as dissatisfaction, resignation, depression and general psychosomatic ailment and disease. Software developers are known to be stressed workers. Stress has been suggested to have detrimental effects on team morale and motivation, communication and cooperation-dependent work, software quality, maintainability, and requirements management. There is a need to effectively assess, monitor, and reduce stress for software developers. While there is substantial psycho-social and medical research on stress and its measurement, we notice that the transfer of these methods and practices to software engineering has not been fully made. For this reason, we engage in an interdisciplinary endeavor between researchers in software engineering and medical and social sciences towards a better understanding of stress effects while developing software. This article offers two main contributions. First, we provide an overview of supported theories of stress and the many ways to assess stress in individuals. Second, we propose a robust methodology to detect and measure stress in controlled experiments that is tailored to software engineering research. We also evaluate the methodology by implementing it on an experiment, which we first pilot and then replicate in its enhanced form, and report on the results with lessons learned. With this work, we hope to stimulate research on stress in software engineering and inspire future research that is backed up by supported theories and employs psychometrically validated measures."
Subahi2023,Ahmad F. Subahi,BERT-Based Approach for Greening Software Requirements Engineering Through Non-Functional Requirements,IEEE Access,11,,2023,10.1109/ACCESS.2023.3317798,21693536,"The incorporation of sustainability principles during the requirements engineering phase of the development life cycle constitutes greening software requirements. This incorporation can have a variety of effects on the software design employed in modern and cutting-edge information technology (IT) systems. When sustainability principles are incorporated into requirements engineering, software design priorities can change and address current design issues such as energy and resource consumption, modularity, maintainability, and adaptability. In contrast to other green approaches that consider sustainable development, there is a further need to investigate the relationship between software development and the relevant green principles of sustainability during the requirements engineering phase. We present a new mechanism for mapping software nonfunctional requirements (NFRs) to defined dimensions of green software sustainability, consisting of two mapping steps: 1) between NFRs and sustainability dimensions; and 2) between sustainability dimensions and two clusters of green IT aspects defined in this work. The overall architecture of the promising approach is based on the use of the Bidirectional Encoder Representations from Transformers (BERT) language model with an expanded dataset. We consider transfer learning and domain-specific fine-tuning capabilities for constructing and evaluating a model specifically tailored for developing a proof of concept of the greening software requirements engineering task, as language models have recently emerged as a potent technique in the field of software engineering, with numerous applications in code analysis, automated documentation, and code generation. In addition, we test the model's performance using an extended version of the PROMISE_exp dataset after adding a new binary classification column for categorizing sustainability dimensions into two defined clusters: Eco-technical and Socioeconomic, and having a selected domain expert label the raw data. The model's efficiency is evaluated using four matrices - 1) accuracy; 2) precision; 3) recall; and 4) F1 score - across a variety of epoch and batch sizes. Our numerical results demonstrate the viability of the approach in text classification tasks via performing well in mapping NFRs to software sustainability dimensions. This acts as a proof of concept for automating the sustainability measurement of software awareness at the early development stage. In addition, the results emphasize the importance of domain-specific fine-tuning and transfer learning for obtaining high performance in classification tasks in requirements engineering."
Savary-Leblanc2023,Maxime Savary-Leblanc and Lola Burgueño and Jordi Cabot and Xavier Le Pallec and Sébastien Gérard,Software assistants in software engineering: A systematic mapping study,Software - Practice and Experience,53,3,2023,10.1002/spe.3170,1097024X,"The increasing essential complexity of software systems makes current software engineering methods and practices fall short in many occasions. Software assistants have the ability to help humans achieve a variety of tasks, including the development of software. Such assistants, which show human-like competences such as autonomy and intelligence, help software engineers do their job by empowering them with new knowledge. This article investigates the research efforts that have been conducted on the creation of assistants for software design, construction and maintenance paying special attention to the user-assistant interactions. To this end, we followed the standard systematic mapping study method to identify and classify relevant works in the state of the art. Out of the 7580 articles resulting from the automatic search, we identified 112 primary studies that present works which qualify as software assistants. We provide all the resources needed to reproduce our study. We report on the trends and goals of the assistants, the tasks they perform, how they interact with users, the technologies and mechanisms they exploit to embed intelligence and provide knowledge, and their level of automation. We propose a classification of software assistants based on interactions and present an analysis of the different automation patterns. As outcomes of our study, we provide a classification of software assistants dealing with the design, construction and maintenance phases of software development, we discuss the results, identify open lines of work and challenges and call for new innovative and rigorous research efforts in this field."
Zeb2023,Alam Zeb and Fakhrud Din and Muhammad Fayaz and Gulzar Mehmood and Kamal Z. Zamli,A Systematic Literature Review on Robust Swarm Intelligence Algorithms in Search-Based Software Engineering,Complexity,2023,,2023,10.1155/2023/4577581,10990526,"Swarm intelligence algorithms are metaheuristics inspired by the collective behavior of species such as birds, fish, bees, and ants. They are used in many optimization problems due to their simplicity, flexibility, and scalability. These algorithms get the desired convergence during the search by balancing the exploration and exploitation processes. These metaheuristics have applications in various domains such as global optimization, bioinformatics, power engineering, networking, machine learning, image processing, and environmental applications. This paper presents a systematic literature review (SLR) on applications of four swarm intelligence algorithms i.e., grey wolf optimization (GWO), whale optimization algorithms (WOA), Harris hawks optimizer (HHO), and moth-flame optimizer (MFO) in the field of software engineering. It presents an in-depth study of these metaheuristics' adoption in the field of software engineering. This SLR is mainly comprised of three phases such as planning, conducting, and reporting. This study covers all related studies published from 2014 up to 2022. The study shows that applications of the selected metaheuristics have been utilized in various fields of software engineering especially software testing, software defect prediction, and software reliability. The study also points out some of the areas where applications of these swarm intelligence algorithms can be utilized. This study may act as a guideline for researchers in improving the current state-of-the-art on generally adopting these metaheuristics in software engineering."
Stol2022,Klaas Jan Stol and Mario Schaarschmidt and Shelly Goldblit,Gamification in software engineering: the mediating role of developer engagement and job satisfaction,Empirical Software Engineering,27,2,2022,10.1007/s10664-021-10062-w,15737616,"Gamification seeks to encourage behavior of participants by borrowing elements of games, such as scoring points. Few rigorous studies exist of gamification in software organizations, and several questions have remained unanswered, for example, what might drive developers to partake, and what are the consequences of developer engagement. This article seeks to provide some answers through a rigorous empirical study at one organization that created an internal gamification platform. We develop a theoretical model that seeks to explain why developers may participate, and develop the concept of developer engagement, which we link to job satisfaction. We collected data from two sources that were linked together: developer opinion data collected through a survey, and data from the organization’s version control system. We test our theoretical model using structural equation modeling and moderation analysis, and find support for our model. These findings suggest that gamification can be an effective mechanism to engage developers within the organization, and that developer engagement is positively associated with job satisfaction, which is a key outcome that is of great interest to software organizations."
Laporte2021,Claude Y. Laporte and Mirna Munoz,Not Teaching Software Engineering Standards to Future Software Engineers-Malpractice?,Computer,54,5,2021,10.1109/MC.2021.3064438,15580814,Software engineering standards are essential sources of codified knowledge for all software engineers. Could the professors who are not teaching software engineering standards to software engineering students be accused of malpractice?
Isomttnen2023,Ville Isomöttönen and Toni Taipalus,Status indicators in software engineering group projects,Journal of Systems and Software,198,,2023,10.1016/j.jss.2023.111612,01641212,"A segment of studies on group structure and performance in software engineering (SE) project-based learning (PjBL) have focused on roles, including studies that use Belbin team roles and studies that address problematic roles such as social loafing. The present study focuses on the status, which is basically missing in SE PjBL studies, although relating to roles. The study investigates the aspects that students identified as indicators of rising or declining status in their project groups. The status theory was utilized as the framework that motivated the research and on which the results were reflected. An inductive qualitative content analysis was applied to learning reports in which students reflected on their statuses. The indicators of rising status included technical know-how, commitment, management responsibility, and idea ownership, while also group-level attributes such as a caring atmosphere and joint responsibility. The indicators of a declining status included aspects that appear as counterparts of rising status indicators, while also more refined aspects such as no one willing to be a leader or study background. The results are concluded to provide material for educating students about intra-group relations and promoting self-regulation for fruitful collaboration in groups. The authors believe that the results also initiate further PjBL research in which status theory can be utilized."
Klotins2022,Eriks Klotins and Einav Peretz-Andersson,The unified perspective of digital transformation and continuous software engineering,,,,2022,10.1145/3524614.3528626,,"Software is a key component of most products, services, industrial processes, and back-office functions. Thus, companies may gain an advantage by establishing fast feedback cycles to improve their software. Continuous software engineering (CI/CD) is being primarily studied as an engineering topic. However, the rest of the organization needs to align and be prepared to utilize the benefits of CI/CD. In this paper, we explore the overlap between CI/CD and digital transformation (DT). We study literature in both areas to develop a map of conditions, mechanisms, and outcomes. As a result, we present a unified perspective of CI/CD and DT. We found that CI/CD can be seen as an implementation of DT in a software organization. DT perspective can help to guide the adoption of CI/CD from an organizational perspective."
Olsson2021,Jesper Olsson and Erik Risfelt and Terese Besker and Antonio Martini and Richard Torkar,Measuring affective states from technical debt: A psychoempirical software engineering experiment,Empirical Software Engineering,26,5,2021,10.1007/s10664-021-09998-w,15737616,"Context: Software engineering is a human activity. Despite this, human aspects are under-represented in technical debt research, perhaps because they are challenging to evaluate. Objective: This study’s objective was to investigate the relationship between technical debt and affective states (feelings, emotions, and moods) from software practitioners. Method: Forty participants (N = 40) from twelve companies took part in a mixed-methods approach, consisting of a repeated-measures (r = 5) experiment (n = 200), a survey, and semi-structured interviews. From the qualitative data, it is clear that technical debt activates a substantial portion of the emotional spectrum and is psychologically taxing. Further, the practitioners’ reactions to technical debt appear to fall in different levels of maturity. Results: The statistical analysis shows that different design smells (strong indicators of technical debt) negatively or positively impact affective states. Conclusions: We argue that human aspects in technical debt are important factors to consider, as they may result in, e.g., procrastination, apprehension, and burnout."
Gmez-lvarez2022,María Clara Gómez-álvarez and Carlos Mario Zapata Jaramillo,A proposal of educational games classification for software engineering,Ingeniare,30,2,2022,10.4067/S0718-33052022000200239,07183305,"Games for educational purposes have been used in different areas as supplementary strategies to the traditional teaching-learning process. Software Engineering-applying methods and techniques for building high-quality software products-also includes games in the professional training process. In order to provide a tool for teacher guidance about their use in their teaching-learning process, several game classifications have been proposed. However, existing classifications consider the game purpose and the target audience, but they omit variables associated with the structure of the game-e.g., rules or materials. This paper proposes a classification of games for teaching software engineering considering purpose, scope, and fun components. This proposal defines the key features of any game for teaching software engineering and provides alternatives to professors for incorporating this teaching tool in their training process. The proposed classification was applied to a set of 17 games oriented to software engineering teaching, showing its potential as a support tool in the analysis, comparison, and selection of games for teaching this discipline."
Liu2023,Xuanzhe Liu and Diandian Gu and Zhenpeng Chen and Jinfeng Wen and Zili Zhang and Yun Ma and Haoyu Wang and Xin Jin,Rise of Distributed Deep Learning Training in the Big Model Era: From a Software Engineering Perspective,ACM Transactions on Software Engineering and Methodology,32,6,2023,10.1145/3597204,15577392,"Deep learning (DL) has become a key component of modern software. In the ""big model""era, the rich features of DL-based software (i.e., DL software) substantially rely on powerful DL models, e.g., BERT, GPT-3, and the recently emerging GPT-4, which are trained on the powerful cloud with large datasets. Hence, training effective DL models has become a vital stage in the whole software lifecycle. When training deep learning models, especially those big models, developers need to parallelize and distribute the computation and memory resources amongst multiple devices (e.g., a cluster of GPUs) in the training process, which is known as distributed deep learning training, or distributed training for short. However, the unique challenges that developers encounter in distributed training process have not been studied in the software engineering community. Given the increasingly heavy dependence of current DL-based software on distributed training, this paper aims to fill in the knowledge gap and presents the first comprehensive study on developers' issues in distributed training. To this end, we focus on popular DL frameworks that support distributed training (including TensorFlow, PyTorch, Keras, and Horovod) and analyze 1,131 real-world developers' issues about using these frameworks reported on Stack Overflow and GitHub. We construct a fine-grained taxonomy consisting of 30 categories regarding the fault symptoms and summarize common fix patterns for different symptoms. We find that: (1) many distributed-specific faults and non-distributed-specific faults inherently share the same fault symptoms, making it challenging to debug; (2) most of the fault symptoms have frequent fix patterns; (3) about half of the faults are related to system-level configurations. Based on the results, we suggest actionable implications on research avenues that can potentially facilitate the distributed training to develop DL-based software, such as focusing on the frequent and common fix patterns when designing testing or debugging tools, developing efficient testing and debugging techniques for communication configuration along with the synthesis of network configuration analysis, designing new multi-device checkpoint-and-replay techniques to help reproduction, and designing serverless APIs for cloud platforms."
Marar2024,Hazem W. Marar,Advancements in software engineering using AI,Computer Software and Media Applications,6,1,2024,10.24294/csma.v6i1.3906,,"The integration of Artificial Intelligence (AI) into the space of software engineering marks a transformative period that reshapes traditional development processes and propels the industry into a new era of innovation. This exploration delves into the multifaceted impact of AI, from its roots in early symbolic AI to the contemporary dominance of machine learning and deep learning. AI’s applications span various domains, but its significance in software engineering lies in its ability to enhance efficiency, improve software quality, and introduce novel approaches to problem-solving. From automating routine tasks to streamlining complex development workflows, AI acts as a virtual collaborator, allowing human developers to focus on higher-order thinking and creativity. This study introduces the application of AI in software engineering. reverse-engineering, and development environments. Moreover, ethical considerations, challenges, and future trends, including explainable AI, reinforcement learning, and human-AI collaboration, are presented."
Perera2020,Harsha Perera and Waqar Hussain and Jon Whittle and Arif Nurwidyantoro and Davoud Mougouei and Rifat Ara Shams and Gillian Oliver,"A study on the prevalence of human values in software engineering publications, 2015 2",,,,2020,10.1145/3377811.3380393,02705257,"Failure to account for human values in software (e.g., equality and fairness) can result in user dissatisfaction and negative socioeconomic impact. Engineering these values in software, however, requires technical and methodological support throughout the development life cycle. This paper investigates to what extent top Software Engineering (SE) conferences and journals have included research on human values in SE. We investigate the prevalence of human values in recent (2015 2018) publications in these top venues. We classify these publications, based on their relevance to di erent values, against a widely used value structure adopted from the social sciences. Our results show that: (a) only a small proportion of the publications directly consider values, classified as directly relevant publications; (b) for the majority of the values, very few or no directly relevant publications were found; and (c) the prevalence of directly relevant publications was higher in SE conferences compared to SE journals. This paper shares these and other insights that may motivate future research on human values in software engineering."
Miranskyy2022,Andriy Miranskyy and Mushahid Khan and Jean Paul Latyr Faye and Udson C. Mendes,Quantum computing for software engineering: prospects,,,,2022,10.1145/3549036.3562060,,"Quantum computers (QCs) are maturing. When QCs are powerful enough, they may be able to handle problems in chemistry, physics, and finance that are not classically solvable. However, the applicability of quantum algorithms to speed up Software Engineering (SE) tasks has not been explored. We examine eight groups of quantum algorithms that may accelerate SE tasks across the different phases of SE and sketch potential opportunities and challenges."
Renaud2023,Karen Renaud,Human-centred cyber secure software engineering,Zeitschrift für Arbeitswissenschaft,77,1,2023,10.1007/s41449-022-00346-2,0340-2444,"Software runs our modern day lives: our shopping, our transport and our medical devices. Hence, no citizen can escape the consequences of poor software engineering. A closely-aligned concern, which also touches every aspect of our lives, is cyber security. Software has to be developed with cybersecurity threats in mind, in order to design resistance and resilience into the software, given that they are often rooted in malicious human behaviour. Both software engineering and cyber security disciplines need to acknowledge and accommodate humans, not expect perfect performances. This is a position paper, delineating the extent of the challenge posed by this reality, and suggesting ways for accommodating the influence of human nature on secure software engineering. Practical Relevance : Socio-technical systems are made up of people, processes and technology. All can fail or be suboptimal. Software itself, being designed, developed and used by humans, is likely to malfunction. This could be caused by human error, or by malice. This paper highlights this reality, taking a closer look at all of the possible sources of malfunctioning technology. By doing so, I hope to infuse the management of socio-technical systems with an understanding and acknowledgement of this reality. Software steuert unser modernes Leben: unsere Einkäufe, unsere Transportmittel und unsere medizinischen Geräte. Daher kann sich kein Bürger den Folgen schlechter Softwareentwicklung entziehen. Ein eng damit verbundenes Anliegen, das auch jeden Aspekt unseres Lebens berührt, ist die Cybersicherheit, und Software muss unter Berücksichtigung von Cybersicherheitsbedrohungen entwickelt werden, um Widerstandsfähigkeit in die Software zu integrieren. Sowohl Software-Engineering als auch Cybersicherheitsdisziplinen sind für die Verwendung durch Menschen konzipiert, nicht für unfehlbare Roboter. Dies ist ein Positionspapier, das das Ausmaß der Herausforderung beschreibt, die sich aus dieser Notwendigkeit ergibt, und Wege aufzeigt, wie dem Einfluss der menschlichen Natur auf cybersicheres Software-Engineering Rechnung getragen werden kann. Praktische Relevanz : Soziotechnische Systeme bestehen aus Menschen, Prozessen und Technologie. Alle können fehlschlagen oder suboptimal sein. Software selbst, die von Menschen entworfen, entwickelt und verwendet wird, weist wahrscheinlich Fehlfunktionen auf. Dies kann durch menschliches Versagen oder durch Vorsatz verursacht werden. Dieses Papier beleuchtet diese Realität, indem es alle möglichen Ursachen für fehlerhafte Technologie genauer unter die Lupe nimmt. Auf diese Weise hoffe ich, das Management soziotechnischer Systeme mit einem Verständnis und einer Anerkennung dieser Realität zu erfüllen. "
El-Glaly2020,Yasmine N. El-Glaly,Teaching accessibility to software engineering students,,,,2020,10.1145/3328778.3366914,,"This paper describes the development process of a graduate course on accessibility that is offered as an elective for software engineering students. The paper reports on the three iterations of the course evolution as topics and pedagogy are updated from one iteration to the next. The main motivation of the course updates was to cover the educational needs of software engineering students, which are not the same as HCI or design students. Software engineering students learned better and became more engaged with the topic of accessibility when it was tied to programming and developmentbased activities. The final form of the course was evaluated using a survey, and the results showed that students found the course beneficial to their education and relevant to their career.We discuss the challenges of creating, teaching, and maintaining a course on accessibility, and we offer insights on what research is needed in this area to support accessibility educators."
Anchundia2020,Carlos E. Anchundia and Efrain R. Fonseca C,Resources for reproducibility of experiments in empirical software engineering: Topics derived from a secondary study,IEEE Access,8,,2020,10.1109/ACCESS.2020.2964587,21693536,"Background: Replication is a recurrent issue in empirical software engineering (ESE). Although it is a foundation of science, replication is hard to execute despite the many supporting tools meant to facilitate reproducibility. For example, in an experiment, which is the most used method in ESE, the number of replications is not enough compared to other sciences. Objective: In this study, we aim to identify tools that maximize reproducibility in software engineering experiments and how they are applied. Methods: We performed a Systematic Mapping Study and complementary strategies to analyze replication from three concerns (communication, knowledge management, and motivation). We analyzed more than 2,600 studies to get 40 primary studies, using a qualitative analytical tool (Atlas.ti) to create semantic maps for synthesizing our results. Result: We found that tools and practices depend on the experiment domain. Human-oriented experiments tend to use an informal mechanism that is costly and time-consuming. On the other hand, technology-oriented experiments are automated, domain-centric, and specialized so they require a learning process and are not transferable to other domains. Conclusion: Tools and practices still lack acceptation and usability among the ESE research community. Therefore, reproducibility is mostly relegated to internal replication, at which time and costs can be assumed within research groups. A focus on new alternatives should be considered to broaden replication."
Cao2024,Jingxin Cao,Construction and Application of Industry-University-Research Platform Based on Software Engineering and Internet of Things Technology in Digital Art,Computer-Aided Design and Applications,21,S11,2024,10.14733/cadaps.2024.S11.122-140,16864360,"In order to improve the practical effect of industry-university-research research, this paper combines software engineering and Internet of Things technology to build and apply the industry-university-research platform, and focuses on the analysis of system security and data processing in the operation process of industry-university-research research.Moreover, this paper combines the actual operation requirements of industry, university and research institute to propose a security situation assessment technology based on the fuzzy comprehensive evaluation method, and considers a variety of influencing factors to grasp the security status of the industry-university-research platform from multiple angles in all aspects.In addition, this paper imports the keyword co-occurrence matrix obtained by ROST software into SPSS19.0 software for cluster analysis and statistical clustering results. Finally, this paper combines expert evaluation to comprehensively evaluate the effect of the platform. The research shows that the industry-university-research platform based on software engineering and Internet of Things technology proposed in this paper meets the current actual needs of industry-university-research research."
Zizyte2022,Milda Zizyte and Trenton Tabor,Should Robotics Engineering Education Include Software Engineering Education?,,,,2022,10.1145/3526071.3527514,,"Multiple universities across the United States now offer bachelor's degrees in robotics, which aim to prepare students to work in the robotics industry. To judge how well these programs are providing software engineering training, we evaluate whether these programs teach the software engineering practices that are required for robotics software engineering. We compile an updated list of robotics bachelor's degree programs and measure whether the curriculum of each program claims to teach a specific practice. We find that some of these practices are not mentioned in the curricula, and that some are only taught implicitly in long-term project courses. These project courses vary in scope, guidance, and structure. This implies that robotics bachelor's degrees may not be preparing students to engage with the practices in the workforce."
Portela2021,Carlos Dos Santos Portela and Alexandre Marcos Lins De Vasconcelos and Sandro Ronaldo Bezerra Oliveira and Mauricio Ronny De Almeida Souza,An Empirical Study on the Use of Student-Focused Approaches in the Software Engineering Teaching,Informatics in Education,20,2,2021,10.15388/infedu.2021.13,16485831,"The software industry is not satisfied with the preparation level of newly graduated professionals in Computing undergraduate courses. There is a predominance of traditional approaches to the Software Engineering (SE) teaching which proved to be inefficient, because they focus on the content from the professor's viewpoint. This research aims to investigate if the use of student-focused approaches in the SE teaching can develop more technical competencies to apply in industry than when traditional approaches are applied. For this, an iterative model has been defined to integrate the main student-focused approaches and a controlled experiment was carried out in four undergraduate courses. The data were collected from structured interviews with students and analyzed using ANOVA. The results showed no significant statistical difference between student-focused and traditional teaching approaches in the development of SE competencies. However, these results were impacted by the motivation and commitment of the experiment students."
Shihab2022,Emad Shihab and Stefan Wagner and Marco A. Gerosa and Mairieli Wessel and Jordi Cabot,The Present and Future of Bots in Software Engineering,IEEE Software,39,5,2022,10.1109/MS.2022.3176864,19374194,
Li2022,Miqing Li and Tao Chen and Xin Yao,How to Evaluate Solutions in Pareto-Based Search-Based Software Engineering: A Critical Review and Methodological Guidance,IEEE Transactions on Software Engineering,48,5,2022,10.1109/TSE.2020.3036108,19393520,"With modern requirements, there is an increasing tendency of considering multiple objectives/criteria simultaneously in many Software Engineering (SE) scenarios. Such a multi-objective optimization scenario comes with an important issue - how to evaluate the outcome of optimization algorithms, which typically is a set of incomparable solutions (i.e., being Pareto nondominated to each other). This issue can be challenging for the SE community, particularly for practitioners of Search-Based SE (SBSE). On one hand, multi-objective optimization could still be relatively new to SE/SBSE researchers, who may not be able to identify the right evaluation methods for their problems. On the other hand, simply following the evaluation methods for general multi-objective optimization problems may not be appropriate for specific SBSE problems, especially when the problem nature or decision maker's preferences are explicitly/implicitly known. This has been well echoed in the literature by various inappropriate/inadequate selection and inaccurate/misleading use of evaluation methods. In this paper, we first carry out a systematic and critical review of quality evaluation for multi-objective optimization in SBSE. We survey 717 papers published between 2009 and 2019 from 36 venues in seven repositories, and select 95 prominent studies, through which we identify five important but overlooked issues in the area. We then conduct an in-depth analysis of quality evaluation indicators/methods and general situations in SBSE, which, together with the identified issues, enables us to codify a methodological guidance for selecting and using evaluation methods in different SBSE scenarios."
Haider2021,Michael Haider and Michael Riesch and Christian Jirauschek,Realization of best practices in software engineering and scientific writing through ready-to-use project skeletons,Optical and Quantum Electronics,53,10,2021,10.1007/s11082-021-03192-4,1572817X,"Efforts in providing high-quality scientific software are hardly rewarded, as scientific output is typically measured in terms of publications in high ranking journals. As a result, scientific software is often developed without proper documentation and support of modern software design patterns. Ready-to-use project skeletons can be employed to accelerate the development process, while at the same time taking care of the implementation of best practices in software engineering. In this work, we revisit best practices in software engineering and review existing project skeletons. Special emphasis is given on the realization of best practices. Finally, we present a new project skeleton for scientific writing in [InlineMediaObject not available: see fulltext.], which takes care of the attainment of best practices, adapted for being used in academic publications."
Williams2020,Richard A. Williams,Cybernetics of Conflict within Multi-Partner Technology and Software Engineering Programmes,IEEE Access,8,,2020,10.1109/ACCESS.2020.2995263,21693536,"Large technology and software engineering programmes, such as enterprise system programmes, are increasingly implemented through a mixture of customer and specialist third-party resources. These multi-partner working environments can be thought of as a complex social system, which oftentimes experience various forms of conflict. This can be due to competing objectives and priorities of the various organizations, along with incompatibilities of team members within the work-based social network of the implementation programme. If not brought under control, conflict can lead to complex emergent behaviours and dynamics within the wider social network, which can severely impact the likelihood of successful programme implementation of these software-intensive systems. Using social network analysis and thematic coding analysis within a case study, we show that the project management of complex software-intensive implementations requires considerable focus on control and communication across the programme-wide social network of team members, which we represent as a cybernetic system. A conceptual framework has been developed that extends extant literature around conflict in teams by framing the individual projects and the overall programme-wide implementation as cybernetic systems. The conceptual framework illustrates how a cybernetics approach to conflict within enterprise system implementations, can provide new insights into how conflict develops within project teams. Finally, we argue that the cybernetic approach allows us to develop project management interventions to mitigate the risk of conflict development, or control and regulate conflict once it has developed. We conclude by setting the agenda for future research on how conflict can be controlled within the implementation of software-intensive systems, such as enterprise systems."
Washizaki2022,Hironori Washizaki and Foutse Khomh and Yann Gael Gueheneuc and Hironori Takeuchi and Naotake Natori and Takuo Doi and Satoshi Okuda,Software-Engineering Design Patterns for Machine Learning Applications,Computer,55,3,2022,10.1109/MC.2021.3137227,15580814,"In this study, a multivocal literature review identified 15 software-engineering design patterns for machine learning applications. Findings suggest that there are opportunities to increase the patternsâÂ€Â™ adoption in practice by raising awareness of such patterns within the community."
Almomani2020,Iman Almomani and Afnan Alromi,Integrating software engineering processes in the development of efficient intrusion detection systems in wireless sensor networks,Sensors (Switzerland),20,5,2020,10.3390/s20051375,14248220,"Applying Software Engineering (SE) processes is vital to critical and complex systems including security and networking systems. Nowadays, Wireless Sensor Networks (WSNs) and their applications are found in many military and civilian systems which make them attractive to security attackers. The increasing risks and system vulnerabilities of WSNs have encouraged researchers and developers to propose many security solutions including software-based Intrusion Detection Systems (IDSs). The main drawbacks of current IDSs are due to the lack of clear, structured software development processes. Unfortunately, a substantial gap has been observed between WSN and SE research communities. Integrating SE and WSNs is an emerging topic that will be expanded as technology evolves and spreads in all life aspects. Consequently, this paper highlighted the importance of Requirement Engineering, Software Design, and Testing when developing IDSs for WSNs. Three software IDS designs were proposed in this study: Scheduling, Broadcast, and Watchdog designs. The three designs were compared in terms of consumed energy and network lifetime. Although the same IDS approach was used, but, by highlighting the design phase and implementing different designs, the network lifetime was increased by 73.6% and the consumed energy was reduced by 20% in some of the designs. This is a clear indication of how following a proper SE process could affect the performance of the IDS in WSN. Moreover, conclusions were drawn in regard to applying software engineering processes to IDSs to deliver the required functionalities, with respect to operational constraints, with an improved performance, accuracy and reliability."
Yasin2020,Affan Yasin and Rubia Fatima and Lijie Wen and Wasif Afzal and Muhammad Azhar and Richard Torkar,On using grey literature and google scholar in systematic literature reviews in software engineering,IEEE Access,8,,2020,10.1109/ACCESS.2020.2971712,21693536,"Context: The inclusion of grey literature (GL) is important to remove publication bias while gathering available evidence regarding a certain topic. The number of systematic literature reviews (SLRs) in Software Engineering (SE) is increasing but we do not know about the extent of GL usage in these SLRs. Moreover, Google Scholar is rapidly becoming a search engine of choice for many researchers but the extent to which it can find the primary studies is not known. Objective: This tertiary study is an attempt to i) measure the usage of GL in SLRs in SE. Furthermore this study proposes strategies for categorizing GL and a quality checklist to use for GL in future SLRs; ii) explore if it is feasible to use only Google Scholar for finding scholarly articles for academic research. Method: We have conducted a systematic mapping study to measure the extent of GL usage in SE SLRs as well as to measure the feasibility of finding primary studies using Google Scholar. Results and conclusions: a) Grey Literature: 76.09% SLRs (105 out of 138) in SE have included one or more GL studies as primary studies. Among total primary studies across all SLRs (6307), 582 are classified as GL, making the frequency of GL citing as 9.23%. The intensity of GL use indicate that each SLR contains 5 primary studies on average (total intensity of GL use being 5.54). The ranking of GL tells us that conference papers are the most used form 43.3% followed by technical reports 28.52%. Universities, research institutes, labs and scientific societies together make up 67.7% of GL used, indicating that these are useful sources for searching GL. We additionally propose strategies for categorizing GL and criteria for evaluating GL quality, which can become a basis for more detailed guidelines for including GL in future SLRs. b) Google Scholar Results: The results show that Google Scholar was able to retrieve 96% of primary studies of these SLRs. Most of the primary studies that were not found using Google Scholar were from grey sources."
Croock2022,Muayad Sadik Croock and Sahar Salman Mahmood,Management System of Smart Electric Vehicles Using Software Engineering Model,International Journal of Electrical and Computer Engineering Systems,13,5,2022,10.32985/Ijeces.13.5.5,18477003,"In this paper, a management system for smart electric vehicle is introduced using software engineering models and installed Sensor Network (SN). Two software engineering models are proposed to construct the information exchange and available resource management algorithms, in which the required performance of vehicles is obtained. The resource management algorithm adopts the LeNet-5 deep-learning model in choosing the best driving mode amongst suggested five modes. The dataset is achieved from the simulated SN. The results show the satisfactory performance of the electric cars in terms of information exchange and resource management. The Message Queuing Telemetry Transport (MQTT) broker server is employed for monitoring the information exchange between the sensors, actuators and controller. The delay time is measured to be less than 1 sec for transmitting 1000 message. The proposed system saves energy by 1-8 kWh and a storage capacity by 9-95 MB for driving 100 km."
Lozano2020,Silvia I. Lozano and Elizabeth Suescun and Paola Vallejo and Raul Mazo and Daniel Correa,Comparing two active learning strategies for teaching scrum in an introductory software engineering course,Ingeniare,28,1,2020,10.4067/S0718-33052020000100083,07183305,"Active learning comprises any process in which students are actively engaged in building understanding of facts, ideas, and skills through instructor-directed tasks and activities undertaken mainly during classes. In this paper we presented results from a comparison of two active learning strategies to teach the agile Scrum framework in the context of an introductory software engineering course. The comparison was carried out through a quasi-experiment in which participants were divided into two groups. The first group used the strategy of active reading on basic concepts of Scrum, while for the other group a game was used; representing two teaching strategies for active learning. The results gave indications at the population level that there are significant differences in the concepts learned by the members of both groups and ratifies the use of active learning strategies to teach Scrum. The results provided empirical evidence indicating that using various active learning strategies facilitates the retention and appropriation of concepts related to Scrum and offer teachers a point of reference about the effectiveness of these two strategies of active learning to teach Scrum concepts."
Fronza2020,Ilenia Fronza and Luis Corral and Claus Pahl,End-user software development: Effectiveness of a software engineering-centric instructional strategy,Journal of Information Technology Education: Research,19,,2020,10.28945/4580,15393585,"Aim/Purpose This work aims to introduce and evaluate an instructional strategy that aids end-users with developing their software products during intensive project-based events. Background End-users produce software in the labor market, and one of the challenges for End-User Software Engineering (EUSE) is the need to create functional software products without a formal education in software development. Methodology In this work, we present an instructional strategy to expose end-users to Ag-ile-based Software Engineering (SE) practices and enhance their ability to de-veloping high-quality software. Moreover, we introduce a SE approach for the collection of metrics to assess the effectiveness of the instructional strat-egy. We conducted two case studies to validate the effectiveness of our strat-egy; the comprehensive analysis of the outcome products evaluates the strat-egy and demonstrates how to interpret the collected metrics. Contribution This work contributes to the research and practitioner body of knowledge by leveraging SE centric concepts to design an instructional strategy to lay the foundations of SE competencies in inexperienced developers. This work pre-sents an instructional strategy to develop SE competencies through an inten-sive and time-bound structure that may be replicated. Moreover, the present work introduces a framework to evaluate these competencies from a prod-uct-centric approach, specialized for non-professional individuals. Finally, the framework contributes to understanding how to assess software quality when the software product is written in non-conventional, introductory program-ming languages. Findings The results show the effectiveness of our instructional strategy: teams were successful in constructing a working software product. However, participants did not display a good command of source code order and structure. Recommendations for Practitioners Our instructional strategy provides practitioners with a framework to lay foundations in SE competencies during intensive project-based events. Based on the results of our case studies, we provide a set of recommendations for educational practice. Recommendations for Researchers We propose an assessment framework to analyze the effectiveness of the in-structional strategy from a SE perspective. This analysis provides an overall picture of the participants' performance; other researchers could use our framework to evaluate the effectiveness of their activities, which would con-tribute to increasing the possibility of comparing the effectiveness of differ-ent instructional strategies. Impact on Society Given the number of end-user developers who create software products without a formal SE training, several professional and educational contexts can benefit from our proposed instructional strategy and assessment frame-work. Future Research Further research can focus on improving the assessment framework by in-cluding both process and product metrics to shed light on the effectiveness of the instructional strategies."
Vasiljevi2024,Jan Vasiljević and Dejan Lavbič,A Data-Driven Approach to Team Formation in Software Engineering Based on Personality Traits,Electronics (Switzerland),13,1,2024,10.3390/electronics13010178,20799292,"Collaboration among individuals with diverse skills and personalities is crucial to producing high-quality software. The success of any software project depends on the team’s cohesive functionality and mutual complementation. This study introduces a data-centric methodology for forming Software Engineering (SE) teams centred around personality traits. Our study analysed data from an SE course where 157 students in 31 teams worked through four project phases and were evaluated based on deliverables and instructor feedback. Using the Five-Factor Model (FFM) and a variety of statistical tests, we determined that teams with higher levels of extraversion and conscientiousness, and lower neuroticism, consistently performed better. We examined team members’ interactions and developed a predictive model using extreme gradient boosting. The model achieved a 74% accuracy rate in predicting inter-member satisfaction rankings. Through graphical explainability, the model underscored incompatibilities among members, notably those with differing levels of extraversion. Based on our findings, we introduce a team formation algorithm using Simulated Annealing (SA) built upon the insights derived from our predictive model and additional heuristics."
Steinmacher2023,Igor Steinmacher and Paul Clarke and Eray Tuzun and Ricardo Britto,"Editorial: Machine learning, software process, and global software engineering",Journal of Software: Evolution and Process,35,6,2023,10.1002/smr.2545,20477481,"On June 26–28, 2020, the International Conference on Software and Systems Processes (ICSSP 2020) and the International Conference on Global Software Engineering (ICGSE 2020) were held in virtual settings during the first year of the COVID pandemic. Several submissions to the joint event have been selected for inclusion in this special issue, focusing on impactful and timely contributions to machine learning (ML). At present, many in our field are enthusiastic about the potential of ML, yet some risks should not be casually overlooked or summarily dismissed. Each ML implementation is subtly different from any other implementation, and the risk profile varies greatly based on the approach adopted and the implementation context. The ICSSP/ICGSE 2020 Program Committees have encouraged submissions that explore the risks and benefits associated with ML so that the important discussion regarding ML efficacy and advocacy can be further elaborated. Four contributions have been included in this special issue."
Nogueira2024,Ana Filipa Nogueira and Mário Zenha-Rela,Process mining software engineering practices: A case study for deployment pipelines,Information and Software Technology,168,,2024,10.1016/j.infsof.2023.107392,09505849,"Context: In mature software development organizations the CI/CD pipeline is the only route to deploy software into production. While the workflow of this process seems straightforward, the reality is different since exceptions and deviations are the norm in actual industry practice. In this context, Process Mining appears as a promising technique to uncover deviations and check compliance with standardized DevOps processes, and highlight bottlenecks and potential improvement areas. Objective: This paper presents a case study designed to assess the potential of using Process Mining techniques to provide visibility into the deployment pipeline. Method: This research uses raw event data extracted from the continuous practices toolchain, which is then used to compute a comprehensive set of DevOps-specific metrics, thus supporting objective monitoring of the quality and efficiency of the deployment workflow. The study focuses on different development units in the Engineering team, each working in a distinct business context but sharing standard practices. Results: We verified that even though there are standards for the deployment pipelines, each team's workflow denotes local variations with unique points for improvement that are highly coupled to their business unit context. We observed that each team's pipeline has different temporal profiles that reflect their context and work practices. Additionally, we identified a set of deployment pipeline metrics focusing on process compliance, efficiency, and deployment stability. Conclusion: The main contributions of this paper include (1) the description of an actual application of Process Mining to the deployment pipeline of a highly complex e-commerce platform, (2) how this approach provided an objective understanding of the efficiency and quality of the development workflow, (3) how this process-centric view, combined with domain-specific DevOps metrics, supports continuous practices, and (4) how Developers can analyse their workflows by applying Process Mining while using standard tools like GitLab and PM4Py."
Shahin2022,Mojtaba Shahin and Waqar Hussain and Arif Nurwidyantoro and Harsha Perera and Rifat Shams and John Grundy and Jon Whittle,Operationalizing Human Values in Software Engineering: A Survey,IEEE Access,10,,2022,10.1109/ACCESS.2022.3190975,21693536,"Human values (e.g., pleasure, privacy, and social justice) are what a person or a society considers important. Inability to address them in software-intensive systems can result in numerous undesired consequences (e.g., financial losses) for individuals and communities. Various solutions (e.g., methodologies, techniques) are developed to help 'operationalize values in software'. The ultimate goal is to ensure building software (better) reflects and respects human values. In this survey, 'operationalizing values' is referred to as the process of identifying human values and translating them to accessible and concrete concepts so that they can be implemented, validated, verified, and measured in software. This paper provides a deep understanding of the research landscape on operationalizing values in software engineering, covering 51 primary studies. It also presents an analysis and taxonomy of 51 solutions for operationalizing values in software engineering. Our survey reveals that most solutions attempt to help operationalize values in the early phases (requirements and design) of the software development life cycle. However, the later phases (implementation and testing) and other aspects of software development (e.g., 'team organization') still need adequate consideration. We outline implications for research and practice and identify open issues and future research directions to advance this area."
Ali2020,Shaukat Ali and Paolo Arcaini and Dipesh Pradhan and Safdar Aqeel Safdar and Tao Yue,Quality Indicators in Search-based Software Engineering,ACM Transactions on Software Engineering and Methodology,29,2,2020,10.1145/3375636,15577392,"Search-Based Software Engineering (SBSE) researchers who apply multi-objective search algorithms (MOSAs) often assess the quality of solutions produced by MOSAs with one or more quality indicators (QIs). However, SBSE lacks evidence providing insights on commonly used QIs, especially about agreements among them and their relations with SBSE problems and applied MOSAs. Such evidence about QIs agreements is essential to understand relationships among QIs, identify redundant QIs, and consequently devise guidelines for SBSE researchers to select appropriate QIs for their specific contexts. To this end, we conducted an extensive empirical evaluation to provide insights on commonly used QIs in the context of SBSE, by studying agreements among QIs with and without considering differences of SBSE problems and MOSAs. In addition, by defining a systematic process based on three common ways of comparing MOSAs in SBSE, we present additional observations that were automatically produced based on the results of our empirical evaluation. These observations can be used by SBSE researchers to gain a better understanding of the commonly used QIs in SBSE, in particular, regarding their agreements. Finally, based on the results, we also provide a set of guidelines for SBSE researchers to select appropriate QIs for their particular context."
Benabdelouahab2023,Soukaina Benabdelouahab and José A. García-Berná and Chaimae Moumouh and Juan M. Carrillo-De-gea and Jaber El Bouhdidi and Yacine El Younoussi and José L. Fernández-Alemán,A Bibliometric Study on E-Learning Software Engineering Education,Journal of Universal Computer Science,29,6,2023,10.3897/jucs.87550,09486968,"Due to the substantial development of information and communications technology, the use of E-learning in higher education has become essential to boost teaching methods and enhance students' learning skills and competencies. E-learning in Software Engineering turns out to be increasingly interesting for scholars. In fact, researchers have worked to enhance modern Software Engineering education techniques to meet the required educational objectives. The aim of this article is to analyse the scientific production on E-learning Software Engineering education by conducting a bibliometric analysis of 10,603 publications, dating from 1954 to 2020 and available in the Scopus database. The results reveal some scientific production information, such as the temporal evolution of the publications, the most prolific authors, institutions and countries, as well as the languages used. Besides, the paper evaluates additional bibliometric parameters, including the authors' production, journal productivity, and scientific cooperation, among other bibliometric parameters. The subject of the current study has not been treated by any previous bibliometric studies. Our research is deeper and more specific; it covers a long period of 66 years and a large number of publications, thanks to the chosen search string containing the different spellings of the used terms. In addition, the literature is analysed using several tools such as Microsoft Excel, VOSviewer, and Python. The research findings can be used to identify the current state of E-learning Software Engineering Education, as well as to identify various research trends and the general direction of E-learning research."
Biable2023,Seblewongel E. Biable and Nuno M. Garcia and Dida Midekso,Proposed ethical framework for software requirements engineering,IET Software,17,4,2023,10.1049/sfw2.12136,17518814,"Requirements engineering is a fundamental process in software development phases. At the same time, it is a difficult phase and exposed many ethical violations. The main purpose is proposing an ethical framework for software requirements engineering that addresses the identified concerns. These concerns include problems associated with a knowledge gap, requirements identification, quality-related concerns, unwillingness to give requirements, and practicing forbidden activities. These concerns are grouped into a category as the proposed framework components. Each of the categories encompasses more than one problem domain. The proposed framework suggests resolving mechanisms as collections of clauses for each of those concerns. An expert evaluation technique is used to validate the proposed framework. The experts are purposefully selected from software industries and institutions. Questionnaires and focus group discussions were used as data-gathering tools for the validation of the proposed framework. The validity (face validity, content validity, and construct validity) and the reliability of the proposed framework were checked. The evaluation results show that the proposed framework has an acceptable range of validity and reliability. The proposed framework can be used as a guideline for software engineers to minimise the occurrence of those identified concerns during the requirements engineering process."
Morales-Trujillo2022,Miguel Ehecatl Morales-Trujillo and Matthias Galster and Fabian Gilson and Moffat Mathews,A Three-Year Study on Peer Evaluation in a Software Engineering Project Course,IEEE Transactions on Education,65,3,2022,10.1109/TE.2021.3123682,15579638,"Background: Peer evaluation in software engineering (SE) project courses enhances the learning experience of students. It also helps instructors monitor and assess both teams and individual students. Peer evaluations might influence the way individual students and teams work; therefore, the quality of the peer evaluations should be tracked through the project course. Contribution: In this article, we analyzed the quality and scoring behavior of students in peer evaluation in an undergraduate SE project course over three years. Research Questions: RQ1: What is the quality of peer evaluation of undergraduate students in a SE project course? RQ2: How do undergraduate students in an SE project course score each other? Methodology: The quality of peer evaluation (length, level of detail, etc.) and scoring of peers based on various aspects of peer evaluations of third-year students in a year-long SE project course were studied. Taking into account the grade students received at the end of the course (A, B, C, and F-calibers), peer evaluations were categorized, analyzed over time, and compared between students calibers. Findings: After analyzing 6854 peer evaluations from 193 students, it was found that the quality of peer evaluations across students was mostly consistent throughout the course. Also, it was observed that quantitative aspects of the peer evaluation were scored similarly across student calibers. However, the qualitative aspects of the peer evaluation were impacted by the caliber of students. These findings suggest that weaker students (i.e., C-caliber students) generally receive better quality peer evaluations than stronger students (i.e., A-caliber students). Finally, a preliminary analysis showed a positive connotation of emotions and sentiments found in the textual feedback delivered by students."
Lee2023,Wen Tin Lee and Chih Hsien Chen,Agile Software Development and Reuse Approach with Scrum and Software Product Line Engineering,Electronics (Switzerland),12,15,2023,10.3390/electronics12153291,20799292,"Agile methods and software product line engineering (SPLE) are widely recognized as practical approaches for delivering high-quality software, adapting to evolving stakeholder needs, and tackling complex problems. This study proposes a hybrid agile software development and reuse approach called SPLE-Scrum based on the activities of software product line engineering and Scrum. Within the SPLE process, we incorporate requirement engineering and design practices to create a reference architecture with reusable components called core assets by introducing a product management meeting. The core assets are reused to build a series of applications with various product lines. The product increments are delivered in each Sprint with the review and retrospective meetings based on Scrum lifecycle and practices. We present a case study involving a blockchain online store to demonstrate the practical application of SPLE-Scrum, highlighting the benefits of integrating Scrum and software product line engineering. The research hypotheses of the proposed approach were validated through a study of structured interviews with 5 experts and 44 software practitioners, showing that the key factors of product management, project requirements, and product architecture in the SPLE-Scrum approach have a beneficial impact on project success. The SPLE-Scrum approach provides valuable insights and practical guidance for organizations seeking to optimize their software engineering practices while incorporating agile development and software reuse capabilities."
Weber2021,Barbara Weber and Thomas Fischer and René Riedl,Brain and autonomic nervous system activity measurement in software engineering: A systematic literature review,Journal of Systems and Software,178,,2021,10.1016/j.jss.2021.110946,01641212,"In the past decade, brain and autonomic nervous system activity measurement received increasing attention in the study of software engineering (SE). This paper presents a systematic literature review (SLR) to survey the existing NeuroSE literature. Based on a rigorous search protocol, we identified 89 papers (hereafter denoted as NeuroSE papers). We analyzed these papers to develop a comprehensive understanding of who had published NeuroSE research and classified the contributions according to their type. The 47 articles presenting completed empirical research were analyzed in detail. The SLR revealed that the number of authors publishing NeuroSE research is still relatively small. The thematic focus so far has been on code comprehension, while code inspection, programming, and bug fixing have been less frequently studied. NeuroSE publications primarily used methods related to brain activity measurement (particularly fMRI and EEG), while methods related to the measurement of autonomic nervous system activity (e.g., pupil dilation, heart rate, skin conductance) received less attention. We also present details of how the empirical research was conducted, including stimuli and independent and dependent variables, and discuss implications for future research. The body of NeuroSE literature is still small. Yet, high quality contributions exist constituting a valuable basis for future studies."
Alghasham2023,Mohammed Alghasham and Mousa Alzakan and Mohammed Al-Hagery,A Review of Trending Crowdsourcing Topics in Software Engineering Highlighting Mobile Crowdsourcing and AI Utilization,International Journal of Advanced Computer Science and Applications,14,4,2023,10.14569/IJACSA.2023.0140486,21565570,"Today’s modern technologies and requirements make the utilization of crowdsourcing more viable and applicable. It is one of the problem-solving models that can be used in various domains to reduce costs and time. It is also an excellent way to find new and different ideas and solutions. This paper studies the use of crowdsourcing in software engineering and reveals adequate details to highlight its significance. A few recent literature reviews have been published to address specific topics or study general attributes of papers in crowdsourced software engineering. This paper, however, explores all recent publications related to software and crowdsourcing to find the trends and highlight mobile and AI usage in software crowdsourcing. The findings of this paper show that most research papers are in the areas of software management and software verification and validation. The results also reveal that machine learning and data mining techniques are predominant in software management crowdsourcing and software verification and validation. Furthermore, this study shows that the methods and techniques used in general crowdsourcing apply to mobile crowdsourcing except in mobile testing, where there is a need for clustering and prioritization of test reports"
Russo2021,Daniel Russo and Klaas Jan Stol,PLS-SEM for software engineering research: An introduction and survey,ACM Computing Surveys,54,4,2021,10.1145/3447580,15577341,"Software Engineering (SE) researchers are increasingly paying attention to organizational and human factors. Rather than focusing only on variables that can be directly measured, such as lines of code, SE research studies now also consider unobservable variables, such as organizational culture and trust. To measure such latent variables, SE scholars have adopted Partial Least Squares Structural Equation Modeling (PLS-SEM), which is one member of the larger SEM family of statistical analysis techniques. As the SE field is facing the introduction of new methods such as PLS-SEM, a key issue is that not much is known about how to evaluate such studies. To help SE researchers learn about PLS-SEM, we draw on the latest methodological literature on PLS-SEM to synthesize an introduction. Further, we conducted a survey of PLS-SEM studies in the SE literature and evaluated those based on recommended guidelines."
Kemell2023,Kai Kristian Kemell and Matti Saarikallio,Hybrid Work Practices and Strategies in Software Engineering-Emerging Software Developer Experiences,IEEE Access,11,,2023,10.1109/ACCESS.2023.3322934,21693536,"As a result of the COVID-19 pandemic, remote work became commonplace out of necessity, and has since remained widespread. Currently, many organizations offer the possibility of remote work while encouraging occasional on-site presence, leading to the emergence of 'hybrid work' or 'work-from-anywhere' - a blend of remote and on-site work. Hybrid work is a novel phenomenon that currently concerns the industry, as organizations are still looking for good and best practices related to work modes in the post-pandemic situation. As few studies on hybrid work in the context of Software Engineering (SE) currently exist, we take on an explorative approach to the topic in this paper. Using a qualitative case study approach, we interview 10 software developers to study (a) the factors influencing the work mode choices of software developers, and (b) how remote and on-site work could compliment each other in SE. Our findings highlight factors that influence the work mode choices of developers when they are free to choose their own work modes, in addition to providing some insights into challenges in hybrid work in SE and how to tackle them. Hybrid work can, for example, help tackle some of the issues often associated with fully remote work such as social isolation, but it also results in new challenges such as on-site days feeling unrewarding for developers if the office is largely empty. Our findings build on existing research by providing further insights into the challenges and benefits of the two work modes in hybrid contexts."
Dzhalila2023,Dzhillan Dzhalila and Daniel Siahaan and Reza Fauzan and Raka Asyrofi and Muhammad Ihsan Karimi,A Systematic Literature Review on Blockchain Technology in Software Engineering,Jurnal ELTIKOM,7,1,2023,10.31961/eltikom.v7i1.725,2598-3245,"Blockchain technology is gaining increasing interest among software developers as a distributed and decentralized ledger for tracking the origin of digital assets. However, the application of blockchain in software engineering requires further attention. In this study, we aim to address the current challenges and explore the need for specialized blockchain practices in software engineering. Through a systematic literature review, we identify the various applications of blockchain technology in software engineering. Additionally, we conduct a thorough analysis of existing obstacles and propose potential solutions. Gathering and evaluating requirements using blockchain-based requirements engineering approaches will enhance the quality and reliability of data in software development projects. This, in turn, will improve the overall quality and dependability of software, as well as increase user interest and productivity."
Mashkoor2022,Atif Mashkoor and Tim Menzies and Alexander Egyed and Rudolf Ramler,Artificial Intelligence and Software Engineering: Are We Ready?,Computer,55,3,2022,10.1109/MC.2022.3144805,15580814,Artificial intelligence and software engineering complement each other in various ways. This special issue highlights how this relationship is developing over time to address the challenges faced in modern-day computing.
Naseer2020,Mehwish Naseer and Wu Zhang and Wenhao Zhu,Prediction of coding intricacy in a software engineering team through machine learning to ensure cooperative learning and sustainable education,Sustainability (Switzerland),12,21,2020,10.3390/su12218986,20711050,"Coding deliverables are vital part of the software project. Teams are formed to develop a software project in a term. The performance of the team for each milestone results in the success or failure of the project. Coding intricacy is a major issue faced by students as coding is believed to be a complex field demanding skill and practice. Future education demands a smart environment for understanding students. Prediction of the coding intricacy level in teams can assist in cultivating a cooperative educational environment for sustainable education. This study proposed a boosting-based approach of a random forest (RF) algorithm of machine learning (ML) for predicting the coding intricacy level among software engineering teams. The performance of the proposed approach is compared with viable ML algorithms to evaluate its excellence. Results revealed promising results for the prediction of coding intricacy by boosting the RF algorithm as compared to bagging, J48, sequential minimal optimization (SMO), multilayer perceptron (MLP), and Naïve Bayes (NB). Logistic regression-based boosting (LogitBoost) and adaptive boosting (AdaBoost) are outperforming with 85.14% accuracy of prediction. The concerns leading towards high coding intricacy level can be resolved by discussing with peers and instructors. The proposed approach can ensure a responsible attitude among software engineering teams and drive towards fulfilling the goals of education for sustainable development by optimizing the learning environment."
Al-Taharwa2020,Ismail Al-Taharwa,Teamwork Distribution: Local vs. Global Software Engineering Project Development Teamwork,International Journal of Emerging Technologies in Learning,15,18,2020,10.3991/ijet.v15i18.15489,18630383,"Deliverable and course project become the preferred mean to measure learner competency and attainment of intended learning outcomes in IT-fields. Proper setup and evaluation of teamwork projects remains a crucial challenge for e-learning systems. This study investigates the possibility to improve the early prediction of academic software engineering project failure by treating teamwork differently according to the distribution of teamwork participants. Two configurations of teamwork distribution are considered. In the first configuration, a teamwork may include international participants, but all team participants are affiliated to the same institution, namely local teamwork. In the second configuration, a teamwork may include participants from different institutions, namely global teamwork. Software engineering projects are approached from two distinct perspectives. First, obeying the best practices during the system development life cycle (SDLC), namely, process perspective. Second, characteristics of the final deliverable deployed at each milestone of the SDLC, namely, product perspective. A publicly released dataset collected by a designated e-learning environment is leveraged to validate the proposed approach. Results indicate a noticeable variance among local and global distributions. These results put evidence that the reasons behind software engineering teamwork project failure may vary depending on the distribution of the teamwork, local vs. global. Consequently, it advises to customize e-learning systems according to the teamwork distribution differently."
Ahmad2024,Muhammad Ovais Ahmad and Tomas Gustavsson,"The Pandora's box of social, process, and people debts in software engineering",Journal of Software: Evolution and Process,36,2,2024,10.1002/smr.2516,20477481,"In software engineering, technical debt (TD) has been widely investigated, but debt regarding social issues, people, and processes has not been explored as much. It should be noted here that we use nontechnical debt (NTD) as an umbrella term to cover social, process, and people debts. Although the number of studies on NTD in software is increasing, the majority of them are descriptive rather than rigorous, and there is no systematic development of cumulative knowledge. As a result, identifying the fundamental causes of NTD and the associated mitigation techniques in software engineering is challenging. Therefore, this study investigates the scientific evidence regarding NTD till date by conducting a systematic mapping review of software engineering research between January 2000 and October 2021. The search strategy resulted in 175 studies, 17 of which were identified as unique and relevant primary papers. The primary studies show that NTD and TD are inextricably linked. In addition, this study also captured a plethora of causes and mitigation strategies for managing NTD and thus makes four important contributions: (i) highlighting state-of-the-art NTD research; (ii) identification of the reported causes and mitigation strategies in the primary papers; and (iii) determination of opportunities for future NTD research."
Garca2020,Sergio García and Daniel Strüber and Davide Brugali and Thorsten Berger and Patrizio Pelliccione,Robotics software engineering: A perspective from the service robotics domain,,,,2020,10.1145/3368089.3409743,,"Robots that support humans by performing useful tasks (a.k.a., service robots) are booming worldwide. In contrast to industrial robots, the development of service robots comes with severe software engineering challenges, since they require high levels of robustness and autonomy to operate in highly heterogeneous environments. As a domain with critical safety implications, service robotics faces a need for sound software development practices. In this paper, we present the first large-scale empirical study to assess the state of the art and practice of robotics software engineering. We conducted 18 semi-structured interviews with industrial practitioners working in 15 companies from 9 different countries and a survey with 156 respondents from 26 countries from the robotics domain. Our results provide a comprehensive picture of (i) the practices applied by robotics industrial and academic practitioners, including processes, paradigms, languages, tools, frameworks, and reuse practices, (ii) the distinguishing characteristics of robotics software engineering, and (iii) recurrent challenges usually faced, together with adopted solutions. The paper concludes by discussing observations, derived hypotheses, and proposed actions for researchers and practitioners."
Natali2023,Ana Candida Cruz Natali and Ricardo de Almeida Falbo,Knowledge Management in Software Engineering Environments,,,,2023,10.5753/sbes.2002.23950,,"Knowledge is one of the organization’s most important value, influencing its competitiveness. One way to capture organization’s knowledge and make it available to all their members is through the use of knowledge management systems. In this paper we discuss the importance of knowledge management in software development and we present an infrastructure to deal with knowledge management in software engineering environments (SEEs). This infrastructure is applied to manage product software quality knowledge in ODE, an ontology-based SEE."
Perez2020,Beatriz Perez and Angel L. Rubio,A project-based learning approach for enhancing learning skills and motivation in software engineering,,,,2020,10.1145/3328778.3366891,,"Software engineers must be able to manage complex projects, so that skills such as teamwork, leadership or initiative are critical to their successful development. Because of this, it is fundamental that the learning of software engineering as an academic discipline provides solid links between theory and practice. Educational frameworks such as those derived from the European Higher Education Area state that student-centered approaches are a useful tool for achieving these objectives. In this context, we present a project-based learning (PBL) experience report in a software engineering program of a Spanish university. The experience is based on the formation of small heterogeneous teams, which face the initial phases of a software methodology during the development of a project close to a real one. Through a strategy of role rotation and documentation transfer, all students perform different tasks and face different challenges throughout the project. Summative assessment is also adopted, considering not only teacher ratings but also students' peer assessment. The results prove the positive effect of using PBL to improve the training of students in acquiring different skills as future software engineers."
Giray2021,Görkem Giray,An assessment of student satisfaction with e-learning: An empirical study with computer and software engineering undergraduate students in Turkey under pandemic conditions,Education and Information Technologies,26,6,2021,10.1007/s10639-021-10454-x,15737608,"As COVID-19 reached Turkey in March 2020, all universities switched to e-learning in a very short period. Computer and software engineering (CE/SE) undergraduate students studying at university campuses have switched to e-learning. This paper seeks to understand the e-learning experience of CE/SE undergraduate students. A questionnaire was created and applied to CE/SE undergraduate students in Turkish universities. The data were analyzed using quantitative and qualitative techniques. The questionnaire received 290 usable responses. The highlights from the findings include: the participants (1) used video recordings intensively for e-learning and found them useful; (2) found face-to-face lectures more beneficial compared to digital live lectures; (3) used external online resources to improve their learning performance in courses; (4) thought that the materials and methods utilized for assessment should be adapted to e-learning for a better and fair evaluation; (5) perceived significantly less instructor support and classmate interaction and collaboration in e-learning compared to on-campus education settings; (6) rated their perceived satisfaction from e-learning as 2.85, slightly under the mid-level of the 5-point Likert scale; (7) perceived instructor support, student interaction and collaboration, and student autonomy as noteworthy factors in high-quality e-learning."
Melnikov2023,Alexey Melnikov and Mohammad Kordzanganeh and Alexander Alodjants and Ray Kuang Lee,Quantum machine learning: from physics to software engineering,Advances in Physics: X,8,1,2023,10.1080/23746149.2023.2165452,23746149,"Quantum machine learning is a rapidly growing field at the intersection of quantum technology and artificial intelligence. This review provides a two-fold overview of several key approaches that can offer advancements in both the development of quantum technologies and the power of artificial intelligence. Among these approaches are quantum-enhanced algorithms, which apply quantum software engineering to classical information processing to improve keystone machine learning solutions. In this context, we explore the capability of hybrid quantum-classical neural networks to improve model generalization and increase accuracy while reducing computational resources. We also illustrate how machine learning can be used both to mitigate the effects of errors on presently available noisy intermediate-scale quantum devices, and to understand quantum advantage via an automatic study of quantum walk processes on graphs. In addition, we review how quantum hardware can be enhanced by applying machine learning to fundamental and applied physics problems as well as quantum tomography and photonics. We aim to demonstrate how concepts in physics can be translated into practical engineering of machine learning solutions using quantum software."
Huijgens2020,Hennie Huijgens and Ayushi Rastogi and Ernst Mulders and Georgios Gousios and Arie Van Deursen,Questions for data scientists in software engineering: A replication,,,,2020,10.1145/3368089.3409717,,"In 2014, a Microsoft study investigated the sort of questions that data science applied to software engineering should answer. This resulted in 145 questions that developers considered relevant for data scientists to answer, thus providing a research agenda to the community. Fast forward to five years, no further studies investigated whether the questions from the software engineers at Microsoft hold for other software companies, including software-intensive companies with different primary focus (to which we refer as software-defined enterprises). Furthermore, it is not evident that the problems identified five years ago are still applicable, given the technological advances in software engineering. This paper presents a study at ING, a software-defined enterprise in banking in which over 15,000 IT staff provides in-house software solutions. This paper presents a comprehensive guide of questions for data scientists selected from the previous study at Microsoft along with our current work at ING. We replicated the original Microsoft study at ING, looking for questions that impact both software companies and software-defined enterprises and continue to impact software engineering. We also add new questions that emerged from differences in the context of the two companies and the five years gap in between. Our results show that software engineering questions for data scientists in the software-defined enterprise are largely similar to the software company, albeit with exceptions. We hope that the software engineering research community builds on the new list of questions to create a useful body of knowledge."
Sungkaew2022,Kornchulee Sungkaew and Piyamas Lungban and Sirinya Lamhya,Game development software engineering: digital educational game promoting algorithmic thinking,International Journal of Electrical and Computer Engineering,12,5,2022,10.11591/ijece.v12i5.pp5393-5404,20888708,"The purpose of this study is to create a digital educational game that promotes algorithmic thinking for elementary school students. However, the processes of game development differ from traditional software development which cannot guarantee its effectiveness in terms of human-machine interfaces. In this article, we propose a new game development software engineering (GDSE) as a model for game development. This new model aims to complement and mitigate the shortcomings of traditional software development. The principles of human-computer interaction are now incorporated in the new model. The GDSE includes design, development, usability inspection, game experience evaluation, educational value evaluation and release. It was used as a research method to develop a game that promotes algorithmic thinking for children. The results of this study are not only a digital educational game that promotes algorithmic thinking for children but also a new game development life cycle that guarantees the performance of digital games in terms of usability enhancement, game experience and educational value."
Razali2020,Rozilawati Razali and Mashal Kasem Alqudah and Dzulaiha Aryanee Putri Zainal,The Discovery of Grounded Theory Practices for Software Engineering Research,Electronic Journal of Business Research Methods,18,2,2020,10.34190/JBRM.18.2.007,14777029,"Software engineering (SE) research addresses not only technical issues but also human behaviour. SE is considered as an immature discipline because many technical and social issues concerning software development and management have yet to be specified. SE in general is inclined towards quantitative approaches. Nevertheless, qualitative methods are still appropriate for SE research as the methods encourage deep understanding of subject matter. Grounded Theory (GT) is regarded as one of the potential qualitative methods that is applicable to SE research. The method is able to transform less and unknown SE phenomena into cohesive theories through systematic discovery of empirical data from the ground. This paper shares some encounters of using GT in SE research based on the reflection made on several SE research projects covering various phases of software development life cycle. The encounters are then transformed into adaptations and classified as GT practices for SE research, as an effort to inspire the spirit of using GT in SE particularly among novices. The practices embrace aspects concerning formulating research questions, handling preconceptions, utilising software tools, getting access to data and presenting theory and its development process. To illustrate on how the practices were derived, a case study is presented. The proposed GT practices could act as the starting point of adopting GT in SE research. They shall be refined and improved in future to possibly become best practices when more and more experience of using GT in SE are obtained."
Cico2022,Orges Cico,Lean Software Startup Practices and Software Engineering Education,,,,2022,10.1109/ICSE-Companion55297.2022.9793731,02705257,"In the modern economy, software drives innovation and economic growth. Studies show how software increasingly influences all industry sectors. Over the past 5 decades, software engineering has also changed significantly to advance the development of various types and scales of software products. Software engineering education plays an essential role in apprising students of software technologies, processes, and practices popular in industries. Furthermore, approaches to teaching software engineering are becoming more interdisciplinary and team-centered, comparable to startup contexts. In this PhD work, I want to answer the following research questions: (1) To what extent are software engineering trends present in software engineering education research? (2) What set of common software engineering practices employed in lean software startups is transferable to the software engineering education context? (3) What is the impact of lean startup practices on software engineering students and curricula? I utilize (1) a literature review, (2) mixed-methods approaches in gathering empirical evidence, and (3) design-based research. In the first phase of the research, I pinpoint the relevance of the lean startup in software engineering education through an extensive literature review. I gather empirical evidence on lean startup practices and assess their potential transferability to software engineering education during the second research phase. I demonstrate that the lean startup is an emerging trend in software engineering education research. I demonstrate that students can acquire soft, hard, and project management skills in a more realistic context in the introduction of the growth phase of lean startup practices throughout external course activities. I expect software engineering curricula to benefit from the model and framework that I propose and validate, thus facilitating lean startup practice transfer to software engineering curricula."
Zabardast2022,Ehsan Zabardast and Julian Frattini and Javier Gonzalez-Huerta and Daniel Mendez and Tony Gorschek and Krzysztof Wnuk,Assets in Software Engineering: What are they after all?,Journal of Systems and Software,193,,2022,10.1016/j.jss.2022.111485,01641212,"During the development and maintenance of software-intensive products or services, we depend on various artefacts. Some of those artefacts, we deem central to the feasibility of a project and the product's final quality. Typically, these central artefacts are referred to as assets. However, despite their central role in the software development process, little thought is yet invested into what eventually characterises as an asset, often resulting in many terms and underlying concepts being mixed and used inconsistently. A precise terminology of assets and related concepts, such as asset degradation, are crucial for setting up a new generation of cost-effective software engineering practices. In this position paper, we critically reflect upon the notion of assets in software engineering. As a starting point, we define the terminology and concepts of assets and extend the reasoning behind them. We explore assets’ characteristics and discuss what asset degradation is as well as its various types and the implications that asset degradation might bring for the planning, realisation, and evolution of software-intensive products and services over time. We aspire to contribute to a more standardised definition of assets in software engineering and foster research endeavours and their practical dissemination in a common, more unified direction."
Hidellaarachchi2023,Dulaji Hidellaarachchi and John Grundy and Rashina Hoda and Ingo Mueller,The Influence of Human Aspects on Requirements Engineering-related Activities: Software Practitioners' Perspective,ACM Transactions on Software Engineering and Methodology,32,5,2023,10.1145/3546943,15577392,"Requirements Engineering (RE)-related activities require high collaboration between various roles in software engineering (SE), such as requirements engineers, stakeholders, developers, and so on. Their demographics, views, understanding of technologies, working styles, communication and collaboration capabilities make RE highly human-dependent. Identifying how ""human aspects""- such as motivation, domain knowledge, communication skills, personality, emotions, culture, and so on - might impact RE-related activities would help us improve RE and SE in general. This study aims at better understanding current industry perspectives on the influence of human aspects on RE-related activities, specifically focusing on motivation and personality, by targeting software practitioners involved in RE-related activities. Our findings indicate that software practitioners consider motivation, domain knowledge, attitude, communication skills and personality as highly important human aspects when involved in RE-related activities. A set of factors were identified as software practitioners' key motivational factors when involved in RE-related activities, along with important personality characteristics to have when involved in RE. We also identified factors that made individuals less effective when involved in RE-related activities and obtained some feedback on measuring individuals' performance when involved in RE. The findings from our study suggest various areas needing more investigation, and we summarise a set of key recommendations for further research."
Hans2021,Robert T. Hans and Senyeki M. Marebane and Jacqui Coosner,Computing Academics’ Perceived Level of Awareness and Exposure to Software Engineering Code of Ethics: A Case Study of a South African University of Technology,International Journal of Advanced Computer Science and Applications,12,5,2021,10.14569/IJACSA.2021.0120570,21565570,"The need for awareness on ethical computing is increasingly becoming important. As a result this challenges all stakeholders in the software engineering profession, including educators, to improve their efforts on the awareness of professional codes of ethics which provide framework for ethical reference. However, the several compromises in the software engineering practice suggest that there are some in the profession, who are not familiar with the profession’s codes of ethics and subsequently not able to practice and teach students about them. This research work investigates the extent of codes of ethics awareness by practitioners who are teaching software development courses in an academic environment. An online questionnaire with indicators for measuring awareness on software engineering code of ethics was deployed and responded to by 44 educators. Graphical, univariate and bivariate analyses were conducted on the data to determine the profile of the respondents and the extent of their level of awareness on the codes of ethics. The results indicate that majority of the lecturers (54.5 %) are not aware of software engineering codes of ethics, and those who are aware, majority of them were exposed to through self-study or personal development. Furthermore, the inclusion of codes of ethics in the learning activities is minimal as inhibited by lack of awareness and failure to apply the codes practically. This study recommends that lecturing staff as part of the professional software engineers serving as academic corps, should be placed on programmes for exposing them to professional software engineering codes of ethics. Moreover, the study calls for accreditation of software engineering courses, as it is the case with other professional engineering disciplines, to improve awareness and subsequent practical application of the codes of ethics."
Lorey2022,Tobias Lorey and Paul Ralph and Michael Felderer,Social Science Theories in Software Engineering Research,,2022-May,,2022,10.1145/3510003.3510076,02705257,"As software engineering research becomes more concerned with the psychological, sociological and managerial aspects of software development, relevant theories from reference disciplines are in-creasingly important for understanding the field's core phenomena of interest. However, the degree to which software engineering research draws on relevant social sciences remains unclear. This study therefore investigates the use of social science theories in five influential software engineering journals over 13 years. It analyzes not only the extent of theory use but also what, how and where these theories are used. While 87 different theories are used, less than two percent of papers use a social science theory, most theories are used in only one paper, most social sciences are ignored, and the theories are rarely tested for applicability to software engineering contexts. Ignoring relevant social science theories may (1) under-mine the community's ability to generate, elaborate and maintain a cumulative body of knowledge; and (2) lead to oversimplified mod-els of software engineering phenomena. More attention to theory is needed for software engineering to mature as a scientific discipline."
Mumtaz2022,Mamoona Mumtaz and Naveed Ahmad and M. Usman Ashraf and Ahmed Alshaflut and Abdullah Alourani and Hafiz Junaid Anjum,Modeling Iteration's Perspectives in Software Engineering,IEEE Access,10,,2022,10.1109/ACCESS.2022.3150878,21693536,"Iteration is ubiquitous during software development and particularly notable in complex system development. It has both positive and negative effects; the positives of iteration include improving quality and understandability, reducing complexity and maintenance, leading to innovation, and being cost-effective in the long run; Negatives of iteration include; time, cost, and effort overrun. Its management is a challenging task and becomes more complex due to the non-uniformity of the terminology used at various places. Although Software Development Life Cycles (SDLC) are highly iterative, not much work related to them has been reported in the literature. Insights into iteration are explained in this paper by defining different perspectives (Exploration, Refinement, Rework, and Negotiation) on iteration through literature review, modeling each perspective, and simulating the effect of each iterative perspective on project completion time. An attempt has been made to create awareness about efficient use of iteration during software development by informing which perspective of iteration has what kind of impact on project completion time to avoid delays."
Marutschke2022,Daniel Moritz Marutschke and Victor V. Kryssanov and Patricia Brockmann,"Balanced, Unbalances, and One-Sided Distributed Teams - An Empirical View on Global Software Engineering Education",IEICE Transactions on Information and Systems,E105D,1,2022,10.1587/transinf.2021MPP0002,17451361,"Global software engineering education faces unique challenges to reflect as close as possible real-world distributed team development in various forms. The complex nature of planning, collaborating, and upholding partnerships present administrative difficulties on top of budgetary constrains. These lead to limited opportunities for students to gain international experiences and for researchers to propagate educational and practical insights. This paper presents an empirical view on three different course structures conducted by the same research and educational team over a four-year time span. The courses were managed in Japan and Germany, facing cultural challenges, time-zone differences, language barriers, heterogeneous and homogeneous team structures, amongst others. Three semesters were carried out before and one during the Covid-19 pandemic. Implications for a recent focus on online education for software engineering education and future directions are discussed. As administrational and institutional differences typically do not guarantee the same number of students on all sides, distributed teams can be 1. balanced, where the number of students on one side is less than double the other, 2. unbalanced, where the number of students on one side is significantly larger than double the other, or 3. one-sided, where one side lacks students altogether. An approach for each of these three course structures is presented and discussed. Empirical analyses and reoccurring patterns in global software engineering education are reported. In the most recent three global software engineering classes, students were surveyed at the beginning and the end of the semester. The questionnaires ask students to rank how impactful they perceive factors related to global software development such as cultural aspects, team structure, language, and interaction. Results of the shift in mean perception are compared and discussed for each of the three team structures."
Erdogmus2022,Hakan Erdogmus,Bayesian Hypothesis Testing Illustrated: An Introduction for Software Engineering Researchers,ACM Computing Surveys,55,6,2022,10.1145/3533383,15577341,"Bayesian data analysis is gaining traction in many fields, including empirical studies in software engineering. Bayesian approaches provide many advantages over traditional, or frequentist, data analysis, but the mechanics often remain opaque to beginners due to the underlying computational complexity. Introductory articles, while successful in explaining the theory and principles, fail to provide a totally transparent operationalization. To address this gap, this tutorial provides a step-by-step illustration of Bayesian hypothesis testing in the context of software engineering research using a fully developed example and in comparison to the frequentist hypothesis testing approach. It shows how Bayesian analysis can help build evidence over time incrementally through a family of experiments. It also discusses chief advantages and disadvantages in an applied manner. A figshare package is provided for reproducing all calculations."
Nuh2021,Jamal Abdullahi Nuh and Tieng Wei Koh and Salmi Baharom and Mohd Hafeez Osman and Si Na Kew,Performance evaluation metrics for multi-objective evolutionary algorithms in search-based software engineering: Systematic literature review,Applied Sciences (Switzerland),11,7,2021,10.3390/app11073117,20763417,"Many recent studies have shown that various multi-objective evolutionary algorithms have been widely applied in the field of search-based software engineering (SBSE) for optimal solutions. Most of them either focused on solving newly re-formulated problems or on proposing new approaches, while a number of studies performed reviews and comparative studies on the performance of proposed algorithms. To evaluate such performance, it is necessary to consider a number of performance metrics that play important roles during the evaluation and comparison of investigated algorithms based on their best-simulated results. While there are hundreds of performance metrics in the literature that can quantify in performing such tasks, there is a lack of systematic review conducted to provide evidence of using these performance metrics, particularly in the software engineering problem domain. In this paper, we aimed to review and quantify the type of performance metrics, number of objectives, and applied areas in software engineering that reported in primary studies-this will eventually lead to inspiring the SBSE community to further explore such approaches in depth. To perform this task, a formal systematic review protocol was applied for planning, searching, and extracting the desired elements from the studies. After considering all the relevant inclusion and exclusion criteria for the searching process, 105 relevant articles were identified from the targeted online databases as scientific evidence to answer the eight research questions. The preliminary results show that remarkable studies were reported without considering performance metrics for the purpose of algorithm evaluation. Based on the 27 performance metrics that were identified, hypervolume, inverted generational distance, generational distance, and hypercube-based diversity metrics appear to be widely adopted in most of the studies in software requirements engineering, software design, software project management, software testing, and software verification. Additionally, there are increasing interest in the community in re-formulating many objective problems with more than three objectives, yet, currently are dominated in re-formulating two to three objectives."
Csendes2023,Viktória Flóra Csendes and Attila Egedy and Sébastien Leveneur and Alex Kummer,Application of Multi-Software Engineering: A Review and a Kinetic Parameter Identification Case Study,Processes,11,5,2023,10.3390/pr11051503,22279717,"Limitations regarding process design, optimization, and control often occur when using particular process simulators. With the implementation of connection methodologies, integrated tools could be made by coupling popular process simulation software with each other or with programming environments. In the current paper, we summarized and categorized the existing research regarding the application of multi-software engineering in the chemical industry, with an emphasis on software connections. CAPE-OPEN, COM, OPC, and native integration were discussed in detail, with the intention to serve as a guide for choosing the most suitable software combination and connection. These hybrid systems can handle complex user-defined problems and can be used for decision support, performing custom unit operations, operator training, process optimization, building control systems, and developing digital twins. In this work, we proposed the use of process simulator Aspen HYSYS linked together with the numeric computing platform MATLAB to solve a reaction kinetic parameter identification problem regarding the production of  (Formula presented.) -valerolactone."
Petersen2021,Kai Petersen and Nauman Bin Ali,An analysis of top author citations in software engineering and a comparison with other fields,Scientometrics,126,11,2021,10.1007/s11192-021-04144-1,15882861,"Ioannidis et al. provided a science-wide database of author citations. The data offers an opportunity to researchers in a field to compare the citation behavior of their field with others. In this paper, we conduct a systematic analysis of citations describing the situation in software engineering and compare it with the fields included in the data provided by Ioannidis et al. For comparison, we take the measures used by Ioannidis into consideration. We also report the top-scientists and investigate software engineering researchers’ activities in other fields. The data was obtained and provided by Ioannidis et al. based on the Scopus database. Our method for analysis focuses on descriptive statistics. We compared software engineering with other fields and reported demographic information for the top authors. The analysis was done without any modifications to the ranking. In the later analysis, we observed that 37% of researchers listed as software engineers were not in the software engineering field. On the other hand, the database included a large portion of top authors (ca. 60% to 80%) identified in other software engineering rankings. Other fields using the database are advised to review the author lists for their fields. Our research’s main risk was that researchers are listed that do not belong to our studied field."
Wrobel2020,Michal R. Wrobel,The Impact of Lexicon Adaptation on the Emotion Mining from Software Engineering Artifacts,IEEE Access,8,,2020,10.1109/ACCESS.2020.2979148,21693536,"Sentiment analysis and emotion mining techniques are increasingly being used in the field of software engineering. However, the experiments conducted so far have not yielded high accuracy results. Researchers indicate a lack of adaptation of the methods of emotion mining to the specific context of the domain as the main cause of this situation. The article describes research aimed at examining whether the adaptation of the lexicon with emotional intensity of words in the context of software engineering improves the reliability of sentiment analysis. For this purpose, a new lexicon is developed in which words are evaluated as if they were used in the field of software engineering. A comparative experiment of emotion mining based on a generic and a software engineering specific lexicon does not reveal any significant differences in the results."
Tamburri2023,Damian A. Tamburri and Rick Kazman and Hamed Fahimi,On the Relationship between Organizational Structure Patterns and Architecture in Agile Teams,IEEE Transactions on Software Engineering,49,1,2023,10.1109/TSE.2022.3150415,19393520,"Forming members of an organisation into coherent groups or teams is an important issue in any large-scale software engineering endeavour, especially so in agile software development where teams rely heavily on self-organisation and organisational flexibility. But is there a recurrent organisational structure pattern in agile software engineering teams? and if so what does that pattern imply, in terms of software architecture quality? We address these questions using mixed-methods research in industry featuring interviews, surveys, and Delphi studies of real agile teams. In our study of 30 agile software teams we found that, out of seven organisational structure patterns that recur across our dataset, a single organisational pattern occurs over 37% of the time. This pattern: (a) reflects young communities (1-12 months old); (b) disappears in established ones (13+ months); and (c) reflects the highest number of architecture smells reported. Finally, we observe a negative correlation between a proposed organisational measure and architecture smells. On the one hand, these insights may serve to aid architects in designing not only their architectures but also their communities to best support their co-evolution. On the other hand, we observe that organisational structures in software engineering influence much more than simply software architectures, and we expect our results to lay the foundations of more structured and rigorous approaches to organisational structure studies and use in software engineering research and practice."
Kovaleva2024,Yekaterina Kovaleva and Jussi Kasurinen and Eneli Kindsiko and Ari Happonen,State-of-the-Art Review on Current Approaches to Female Inclusiveness in Software Engineering and Computer Science in Higher Education,IEEE Access,12,,2024,10.1109/ACCESS.2023.3346767,21693536,"Software engineering (SE) and computer science (CS) programs in universities worldwide are marked by a gender gap, which subsequently translates into a gender gap at the industry level. However, there are positive activities that can help attract more women to these male-dominant professions. This study maps the literature related to the achievement of gender balance in SE and CS university-level education and identifies future research directions. More specifically, this article reports on a systematic mapping study of female-inclusive SE and CS tertiary education programs. The authors collected 882 publications between 2015 and 2022 from five databases (ACM, IEEE, Scopus, Web of Science, and Science Direct), selecting 143 peer-reviewed papers for further analysis. The results showed that the main academic contributors were researchers from the USA. The majority of the publications contained observations and explanations regarding the gender gap in computing education. However, an important part of the literature considered proposals and practical activities for achieving gender balance in SE and CS programs. Finally, the authors classified the literature related to female-inclusive SE and CS tertiary education programs, identified the main research focuses and regional distribution, and considered ideas for future research."
Russo2022,Daniel Russo and Klaas-Jan Stol,PLS-SEM for Software Engineering Research,ACM Computing Surveys,54,4,2022,10.1145/3447580,0360-0300,"Software Engineering (SE) researchers are increasingly paying attention to organizational and human factors. Rather than focusing only on variables that can be directly measured, such as lines of code, SE research studies now also consider unobservable variables, such as organizational culture and trust. To measure such latent variables, SE scholars have adopted Partial Least Squares Structural Equation Modeling (PLS-SEM), which is one member of the larger SEM family of statistical analysis techniques. As the SE field is facing the introduction of new methods such as PLS-SEM, a key issue is that not much is known about how to evaluate such studies. To help SE researchers learn about PLS-SEM, we draw on the latest methodological literature on PLS-SEM to synthesize an introduction. Further, we conducted a survey of PLS-SEM studies in the SE literature and evaluated those based on recommended guidelines."
Gonzalez-Barahona2023,Jesus M. Gonzalez-Barahona and Gregorio Robles,Revisiting the reproducibility of empirical software engineering studies based on data retrieved from development repositories,Information and Software Technology,164,,2023,10.1016/j.infsof.2023.107318,09505849,"Context: In 2012, our paper “On the reproducibility of empirical software engineering studies based on data retrieved from development repositories” was published. It proposed a method for assessing the reproducibility of studies based on mining software repositories (MSR studies). Since then, several approaches have happened with respect to the study of the reproducibility of this kind of studies. Objective: To revisit the proposals of that paper, analyzing to which extent they remain valid, and how they relate to current initiatives and studies on reproducibility and validation of research results in empirical software engineering. Method: We analyze the most relevant studies affecting assumptions or consequences of the approach of the original paper, and other initiatives related to the evaluation of replicability aspects of empirical software engineering studies. We compare the results of that analysis with the results of the original study, finding similarities and differences. We also run a reproducibility assessment study on current MSR papers. Based on the comparison, and the applicability of the method to current papers, we draw conclusions on the validity of the approach of the original paper. Main lessons learned: The method proposed in the original paper is still valid, and compares well with other more recent methods. It matches the results of relevant studies on reproducibility, and a systematic comparison with them shows that our approach is aligned with their proposals. Our method has practical use, and complements well the current major initiatives on the review of reproducibility artifacts. As a side result, we learn that the reproducibility of MSR studies has improved during the last decade. Vision: We propose to use our approach as a fundamental element of a more profound review of the reproducibility of MSR studies, and of the characterization of validation studies in this realm."
Arias2023,Danel Arias and Ignacio García Rodríguez de Guzmán and Moisés Rodríguez and Erik B. Terres and Borja Sanz and José Gaviria de la Puerta and Iker Pastor and Agustín Zubillaga and Pablo García Bringas,Let's do it right the first time: Survey on security concerns in the way to quantum software engineering,Neurocomputing,538,,2023,10.1016/j.neucom.2023.03.060,18728286,"Quantum computing is no longer a promise of the future but a rapidly evolving reality. Advances in quantum hardware are making it possible to make tangible a computational reality that until now was only theoretical. The proof of this is that development languages and platforms are appearing that bring physical principles closer to developers, making it feasible to begin to propose, in different areas of society, solutions to problems that until now were unsolvable. However, security vulnerabilities are also emerging that could hinder the progress of quantum computing, as well as its transition and development in industry. For this reason, this article proposes a review of some of the first artefacts that are emerging in the field of quantum computing. From this analysis, we begin to identify possible security issues that could become potential vulnerabilities in the quantum software of tomorrow. Likewise, and following the experience in classical software development, the testing technique is analysed as a possible candidate for improving security in quantum software development. Following the principles of Quantum Software Engineering, we are aware of the lack of tools, techniques and knowledge necessary to guarantee the development of quantum software in the immediate future. Therefore, this article aims to offer some first clues on what would be a roadmap to guarantee secure quantum software development."
Dorodchi2021,Mohsen Dorodchi and Nasrin Dehbozorgi and Mohammadali Fallahian and Seyedamin Pouriyeh,Teaching Software Engineering using Abstraction through Modeling,Informatics in Education,20,4,2021,10.15388/infedu.2021.23,16485831,"Teaching software engineering (SWE) as a core computer science course (ACM, 2013) is a challenging task. The challenge lies in the emphasis on what a large-scale software means, implementing teamwork, and teaching abstraction in software design while simultaneously engaging students into reasonable coding tasks. The abstraction of the system design is perhaps the most critical and theoretical part of the course and requires early engagement of the students with the necessary topics followed by implementation of the abstract model consistently. Normally, students would take such courses in the undergraduate curriculum sequence after data structures and/or object-oriented design/programming. Therefore, they would be able to learn about systematic modeling of software as a system. In this work, we address how to facilitate the teaching of SWE by introducing abstract modeling. Furthermore, functional decomposition is reviewed as a critical component which in turn, requires understanding of how different tasks are accomplished by enterprise software. Combining such pieces with concepts of architecture and design patterns of software provides foundational knowledge for students to be able to navigate around enterprise software in the real world."
Gordillo2020,Aldo Gordillo and Daniel Lopez-Fernandez and Sonsoles López-Pernas and Juan Quemada,Evaluating an Educational Escape Room Conducted Remotely for Teaching Software Engineering,IEEE Access,8,,2020,10.1109/ACCESS.2020.3044380,21693536,"With the rise of distance learning, new challenges have emerged for educators. Among these challenges, developing effective and motivating group activities for students in the remote classroom is one of the top priorities to be addressed. According to existing literature, educational escape rooms have proven to be engaging and effective learning activities when conducted face-to-face. However, no prior research has analyzed the instructional effectiveness of these activities when they are conducted remotely. Furthermore, none of the educational escape rooms reported in the literature has been designed for teaching software modeling. This article analyzes an educational escape room conducted remotely in a software engineering fundamentals course for teaching software modeling. A total of three evaluation instruments were used: a pre-test and a post-test to measure students' learning gains, a questionnaire to collect students' perceptions, and a web platform for automatically gathering data on students' interactions. The contribution of this article is two-fold. On the one hand, it provides, for the first time, evidence that remote educational escape rooms can be effective learning activities. On the other hand, it provides, also for the first time, proof that educational escape rooms are effective and engaging activities for teaching software modeling."
Eito-Brun2021,Ricardo Eito-Brun and Antonio Amescua-Seco,Developments in Aerospace Software Engineering practices for VSEs: An overview of the process requirements and practices of integrated Maturity models and Standards,Advances in Space Research,68,7,2021,10.1016/j.asr.2021.05.026,18791948,"As part of the evolution of the Space market in the last years – globally referred to as Space 2.0 - small companies are playing an increasingly relevant role in different aerospace projects. Business incubators established by European Space Agency (ESA) and similar entities are evidence of the need of moving initiatives to small companies characterized by greater flexibility to develop specific activities. Software is a key component in most aerospace projects, and the success of the initiatives and projects usually depends on the capability of developing reliable software following well-defined standards. But small entities face some difficulties when adopting software development standards that have been conceived thinking on larger organizations and big programs. The need of defining software development standards tailored to small companies and groups is a permanent subject of discussion not only in the aerospace field, and has led in recent years to the publication of the ISO/IEC 29110 series of systems and software engineering standards and guides, aimed to solve the issues that Very Small Entities (VSEs) () – settings having up to twenty-five people -, found with other standards like CMMI® or SPICE. This paper discusses the tailoring defined by different aerospace organizations for VSEs in the aerospace industry, and presents a conceptual arrangement of the standard based on meta-modeling languages that allow the extension and full customization with the incorporation of specific software engineering requirements and practices from ECSS (European Cooperation for Space Standardization)."
Hiranrat2023,Chamikorn Hiranrat and Atichart Harncharnchai and Chompunoot Duangjan,Students’ Communication Self-efficacy and Its Impact on the Enhancement of Communication Skills in Software Engineering Project Courses,TEM Journal,12,3,2023,10.18421/TEM123-50,22178333,Developing the communication skills of software engineering graduates to meet industry requirements is a challenge for educators. This study presents a project-based learning framework that promotes students9 communication skills in a software engineering project course. The questionnaire on self-efficacy for software development (CSESD) was designed for students9 self-assessment of their confidence in communication skills. Findings indicate that students9 CSESD increased significantly after the course ended. Educators can apply the designed framework to software development-related project courses. The CSESD questionnaire can be used to assess students' confidence in their communication skills and assist educators in preparing students9 readiness before graduation.
AncanBastias2021,Oscar Ancan Bastias and Jaime Diaz and Cristian Olivares Rodriguez,Evaluation of Critical Thinking in Online Software Engineering Teaching: A Systematic Mapping Study,IEEE Access,9,,2021,10.1109/ACCESS.2021.3135245,21693536,"Critical thinking consists in analysing and evaluating the coherence of reasoning. This ability is crucial when we talk about software quality (SQ). SQ is closely related with the engineer's ability to judge and discriminate between solutions correctly, so students are required to analyse, evaluate and draw conclusions. Critical thinking, therefore, becomes a crucial part of the training of software engineers. The problem arises from the diversity of proposals and the lack of rigour in existing experiences, making it difficult to find specific recommendations, especially in online contexts. This article reports a systematic mapping study (SMS), the purpose of which was to detect, organise and characterise specific dimensions in online teaching-learning of critical thinking for software engineering. Based on the results of the SMS, we propose a preliminary framework for the evaluation of critical thinking in the training of software engineers in a context of online higher education. It is expected that this proposal will serve as a basis for instructors of the discipline when evaluating critical thinking in a context of online teaching."
BenZayed2021,Hissah A. Ben Zayed and Mashael S. Maashi,Optimizing the software testing problem using search-based software engineering techniques,Intelligent Automation and Soft Computing,29,1,2021,10.32604/iasc.2021.017239,2326005X,"Software testing is a fundamental step in the software development life-cycle. Its purpose is to evaluate the quality of software applications. Regression testing is an important testing methodology in software testing. The purpose of regression testing is to validate the software after each change of its code. This involves adding new test cases to the test suite and running the test suite as the software changes, making the test suite larger. The cost and time of the project are affected by the test suite size. The challenge is to run regression testing with a smaller number of test cases and larger amount of software coverage. Minimi-zation of the test suite with maximization of the software coverage is an NP-com-plete problem. Search-based software engineering is an important topic in software engineering, which addresses software engineering optimization problems to find the optimal/approximate solution of the given problem. This study investigated an approach to reducing the regression testing effort and saving time. It also solved the regression testing optimization problem by achieving the max-imum test suite coverage with the minimum test suite size. Several experiments were conducted to obtain the optimal solutions for the regression testing problem. We propose an optimization methodology that combines a genetic algorithm and a greedy algorithm to optimize regression testing by respectively maximizing the software test coverage and minimizing the test suite size. The proposed methodology can conveniently deliver fault-free, fully covered, and powerful programs for mission-critical functions. It can be applied to test a real-time system that has high requirements for reliability, security, and safety."
Wohlin2021,Claes Wohlin and Per Runeson,Guiding the selection of research methodology in industry–academia collaboration in software engineering,Information and Software Technology,140,,2021,10.1016/j.infsof.2021.106678,09505849,"Background: The literature concerning research methodologies and methods has increased in software engineering in the last decade. However, there is limited guidance on selecting an appropriate research methodology for a given research study or project. Objective: Based on a selection of research methodologies suitable for software engineering research in collaboration between industry and academia, we present, discuss and compare the methodologies aiming to provide guidance on which research methodology to choose in a given situation to ensure successful industry–academia collaboration in research. Method: Three research methodologies were chosen for two main reasons. Design Science and Action Research were selected for their usage in software engineering. We also chose a model emanating from software engineering, i.e., the Technology Transfer Model. An overview of each methodology is provided. It is followed by a discussion and an illustration concerning their use in industry–academia collaborative research. The three methodologies are then compared using a set of criteria as a basis for our guidance. Results: The discussion and comparison of the three research methodologies revealed general similarities and distinct differences. All three research methodologies are easily mapped to the general research process describe–solve–practice, while the main driver behind the formulation of the research methodologies is different. Thus, we guide in selecting a research methodology given the primary research objective for a given research study or project in collaboration between industry and academia. Conclusions: We observe that the three research methodologies have different main objectives and differ in some characteristics, although still having a lot in common. We conclude that it is vital to make an informed decision concerning which research methodology to use. The presentation and comparison aim to guide selecting an appropriate research methodology when conducting research in collaboration between industry and academia."
Almaliki2021,Malik Almaliki,Software Engineering in Saudi Arabia: A Bibliometric Assessment,IEEE Access,9,,2021,10.1109/ACCESS.2021.3053611,21693536,"This paper presents a bibliometric assessment of the Software Engineering (SE) community in the Kingdom of Saudi Arabia (KSA). The assessment was based on the number of SE papers published by KSA-based SE researchers in SE-related venues and indexed in Scopus between years 1984-2019. The assessment aimed to measure the volume of research contribution produced by KSA-based researchers and institutions to the SE field. 802 SE papers were published by KSA-based SE researchers and the top active institution in the domain is King Fahd University of Petroleum and Minerals (KFUPM) and the top active researcher is Mohammad Alshayeb who is affiliated to KFUPM. The results estimated that KSA produced around %0.62 of the world-wide SE knowledge which indicates that KSA SE community needs to increase the volume of its publications to be more active in the worldwide SE community. The results also show that from 2007 onwards, the annual publication trend of KSA SE community has been growing in a healthy rate reaching 113 published papers in 2019. Additionally, 56% of the papers were internationally-Authored and the highest international collaborations were with researchers from the USA, Tunisia and the UK respectively. In general, KSA is performing well comparing to three of its neighboring countries (UAE, Jordan and Egypt) in terms of the quantity of the published SE papers and received citations. However, in comparison to developed countries, the results suggest that more work on the quantity, quality and visibility of the SE papers authored by KSA-based researchers is needed."
Alabduljabbar2022,Areej Alabduljabbar and Sultan Alyahya,Leveraging Social Network Analysis for Crowdsourced Software Engineering Research,Applied Sciences (Switzerland),12,3,2022,10.3390/app12031715,20763417,"Crowdsourced software engineering (CSE) is an emerging area that has been gaining much attention in the last few years. It refers to the use of crowdsourcing techniques in software engineering activities, including requirements engineering, implementation, design, testing, and verification. CSE is an alternative to traditional software engineering and uses an open call to which online developers can respond to and obtain work on various tasks, as opposed to the assigning of tasks to in‐house developers. The great benefits of CSE have attracted the attention of many re-searchers, and many studies have recently been carried out in the field. This research aims to analyze publications on CSE using social network analysis (SNA). A total of 509 CSE publications from six popular databases were analyzed to determine the characteristics of the collaborative networks of co‐authorship of the research (i.e., the co‐authors, institutions involved in co‐authorship, and coun-tries involved in co‐authorship) and of the citation networks on which the publications of the studies are listed. The findings help identify CSE research productivity, trends, performances, community structures, and relationships between various collaborative patterns to provide a more complete picture of CSE research."
Akbar2023,Muhammad Azeem Akbar and Arif Ali Khan and Saima Rafi,A systematic decision-making framework for tackling quantum software engineering challenges,Automated Software Engineering,30,2,2023,10.1007/s10515-023-00389-7,15737535,"Quantum computing systems harness the power of quantum mechanics to execute computationally demanding tasks more effectively than their classical counterparts. This has led to the emergence of Quantum Software Engineering (QSE), which focuses on unlocking the full potential of quantum computing systems. As QSE gains prominence, it seeks to address the evolving challenges of quantum software development by offering comprehensive concepts, principles, and guidelines. This paper aims to identify, prioritize, and develop a systematic decision-making framework of the challenging factors associated with QSE process execution. We conducted a literature survey to identify the challenging factors associated with QSE process and mapped them into 7 core categories. Additionally, we used a questionnaire survey to collect insights from practitioners regarding these challenges. To examine the relationships between core categories of challenging factors, we applied Interpretive Structure Modeling (ISM). Lastly, we applied fuzzy TOPSIS to rank the identified challenging factors concerning to their criticality for QSE process. We have identified 22 challenging factors of QSE process and mapped them to 7 core categories. The ISM results indicate that the ‘resources’ category has the most decisive influence on the other six core categories of the identified challenging factors. Moreover, the fuzzy TOPSIS indicates that ‘complex programming’, ‘limited software libraries’, ‘maintenance complexity’, ‘lack of training and workshops’, and ‘data encoding issues’ are the highest priority challenging factor for QSE process execution. Organizations using QSE could consider the identified challenging factors and their prioritization to improve their QSE process."
Silva2021,Camila Costa Silva and Matthias Galster and Fabian Gilson,Topic modeling in software engineering research,Empirical Software Engineering,26,6,2021,10.1007/s10664-021-10026-0,15737616,"Topic modeling using models such as Latent Dirichlet Allocation (LDA) is a text mining technique to extract human-readable semantic “topics” (i.e., word clusters) from a corpus of textual documents. In software engineering, topic modeling has been used to analyze textual data in empirical studies (e.g., to find out what developers talk about online), but also to build new techniques to support software engineering tasks (e.g., to support source code comprehension). Topic modeling needs to be applied carefully (e.g., depending on the type of textual data analyzed and modeling parameters). Our study aims at describing how topic modeling has been applied in software engineering research with a focus on four aspects: (1) which topic models and modeling techniques have been applied, (2) which textual inputs have been used for topic modeling, (3) how textual data was “prepared” (i.e., pre-processed) for topic modeling, and (4) how generated topics (i.e., word clusters) were named to give them a human-understandable meaning. We analyzed topic modeling as applied in 111 papers from ten highly-ranked software engineering venues (five journals and five conferences) published between 2009 and 2020. We found that (1) LDA and LDA-based techniques are the most frequent topic modeling techniques, (2) developer communication and bug reports have been modelled most, (3) data pre-processing and modeling parameters vary quite a bit and are often vaguely reported, and (4) manual topic naming (such as deducting names based on frequent words in a topic) is common."
Urrea-Contreras2022,Silvia Jaqueline Urrea-Contreras and Brenda Leticia Flores-Rios and María Angélica Astorga-Vargas and Jorge Eduardo Ibarra-Esquer,Process Mining in Software Engineering: A Patent Review,Revista Colombiana de Computacion,23,1,2022,10.29375/25392115.4180,25392115,"Process mining has been consolidated as a discipline-oriented to discover, monitor, and improve organizational processes. This research analyzes, from the intellectual property approach, patents related to the application of process mining in software engineering based on a review of patents extracted from the main patent databases from 2009-2020. Eighteen patents were selected that mainly belong to the countries of China and the United States, covering topics such as event log management, data handling, process models, and process conformance. The purpose of the review is to provide specific information (duration, scope, and originality) on patents in the context of Process Mining and to be a reference point for disseminating new knowledge and technological progress identified in Software Engineering."
Amin2023,Aamir Amin and Mobashar Rehman and Shuib Basri and Luiz Fernando Capretz and Muhammad Awais Shakir Goraya and Rehan Akbar,"The impact of stressors on the relationship between personality traits, knowledge collection behaviour and programmer creativity intention in software engineering",Information and Software Technology,163,,2023,10.1016/j.infsof.2023.107288,09505849,"Context: Individual and contextual factors have a profound impact on an individual's creativity. In the first part of this research, we concluded that, for a programmer's creativity intention, individual factors including big 5 personality traits and knowledge collection behaviour play a key role. However, it is important to bring contextual factors into the model to provide a holistic understanding. Objectives: Hence, the objective of the present research is to expand the earlier work by (i) identifying the software engineering occupational stressors relevant to programmers, and (ii) examining their impact as moderators for the relationship between individual factors (i.e., big five personality traits and knowledge collection behaviour) and the creativity intention of the programmer. Methods: To analyse the moderating impact of 6 stressors, the survey questionnaire was used to collect data from 294 programmers working in software companies in Pakistan. The data were analysed using the Structural Equation Modelling (SEM) – Partial Least Square (PLS) technique. Results: The findings revealed that in the presence of a moderate level of stress, the relationship between knowledge collection behaviour and creativity intention was strengthened. Furthermore, stressors interacted differently with different personality traits. An overarching statement could be that most of the stressors positively moderated the relationships between different personality traits and creativity intentions. However, contrary to the prior research, the majority of the stressors negatively affected the impact of the openness to experience trait on creativity intention. Conclusion: The research significantly contributes to the body of knowledge of behavioural software engineering. The findings of this research are novel and intriguing in many aspects and will benefit software organizations to increase innovation, by increasing programmers’ creativity through mitigating stress. The study is also one of the few studies which have attempted to understand the interaction between individual and contextual factors with a programmer's creativity."
Fauzan2023,Reza Fauzan and Daniel Siahaan and Mirotus Solekhah and Vriza Wahyu Saputra and Aditya Eka Bagaskara and Muhammad Ihsan Karimi,A Systematic Literature Review of Student Assessment Framework in Software Engineering Courses,Journal of Information Systems Engineering and Business Intelligence,9,2,2023,10.20473/jisebi.9.2.264-275,24432555,"Background: Software engineering are courses comprising various project types, including simple assignments completed in supervised settings and more complex tasks undertaken independently by students, without the oversight of a constant teacher or lab assistant. The imperative need arises for a comprehensive assessment framework to validate the fulfillment of learning objectives and facilitate the measurement of student outcomes, particularly in computer science and software engineering. This leads to the delineation of an appropriate assessment structure and pattern. Objective: This study aimed to acquire the expertise required for assessing student performance in computer science and software engineering courses. Methods: A comprehensive literature review spanning from 2012 to October 2021 was conducted, resulting in the identification of 20 papers addressing the assessment framework in software engineering and computer science courses. Specific inclusion and exclusion criteria were meticulously applied in two rounds of assessment to identify the most pertinent studies for this investigation. Results: The results showed multiple methods for assessing software engineering and computer science courses, including the Assessment Matrix, Automatic Assessment, CDIO, Cooperative Thinking, formative and summative assessment, Game, Generative Learning Robot, NIMSAD, SECAT, Self-assessment and Peer-assessment, SonarQube Tools, WRENCH, and SEP-CyLE. Conclusion: The evaluation framework for software engineering and computer science courses required further refinement, ultimately leading to the selection of the most suitable technique, known as learning framework."
Lu2022,Qinghua Lu and Liming Zhu and Xiwei Xu and Jon Whittle and Zhenchang Xing,Towards a Roadmap on Software Engineering for Responsible AI,,,,2022,10.1145/3522664.3528607,,"Although AI is transforming the world, there are serious concerns about its ability to behave and make decisions responsibly. Many ethical regulations, principles, and frameworks for responsible AI have been issued recently. However, they are high level and difficult to put into practice. On the other hand, most AI researchers focus on algorithmic solutions, while the responsible AI challenges actually crosscut the entire engineering lifecycle and components of AI systems. To close the gap in operationalizing responsible AI, this paper aims to develop a roadmap on software engineering for responsible AI. The roadmap focuses on (i) establishing multi-level governance for responsible AI systems, (ii) setting up the development processes incorporating process-oriented practices for responsible AI systems, and (iii) building responsible-AI-by-design into AI systems through system-level architectural style, patterns and techniques. CCS CONCEPTS • Software and its engineering;"
Gurcan2022,Fatih Gurcan and Gonca Gokce Menekse Dalveren and Nergiz Ercil Cagiltay and Ahmet Soylu,Detecting Latent Topics and Trends in Software Engineering Research Since 1980 Using Probabilistic Topic Modeling,IEEE Access,10,,2022,10.1109/ACCESS.2022.3190632,21693536,"The landscape of software engineering research has changed significantly from one year to the next in line with industrial needs and trends. Therefore, today's research literature on software engineering has a rich and multidisciplinary content that includes a large number of studies; however, not many of them demonstrate a holistic view of the field. From this perspective, this study aimed to reveal a holistic view that reflects topics, trends, and trajectories in software engineering research by analyzing the majority of domain-specific articles published over the last 40 years. This study first presents an objective and systematic method for corpus creation through major publication sources in the field. A corpus was then created using this method, which includes 44 domain-specific conferences and journals and 57,174 articles published between 1980 and 2019. Next, this corpus was analyzed using an automated text-mining methodology based on a probabilistic topic-modeling approach. As a result of this analysis, 24 main topics were found. In addition, topical trends in the field were revealed. Finally, three main developmental stages of the field were identified as: the programming age, the software development age, and the software optimization age."
Lu2023,Qinghua Lu and Liming Zhu and Jon Whittle and James Bret Michael,Software Engineering for Responsible AI,Computer,56,4,2023,10.1109/MC.2023.3242055,15580814,"The unique characteristics of artificial intelligence (AI) systems pose new challenges to traditional software engineering approaches. Thus, new software engineering approaches are required to develop AI systems in a responsible manner."
Jolak2020,Rodi Jolak and Maxime Savary-Leblanc and Manuela Dalibor and Andreas Wortmann and Regina Hebig and Juraj Vincur and Ivan Polasek and Xavier Le Pallec and Sébastien Gérard and Michel R.V. Chaudron,Software engineering whispers: The effect of textual vs. graphical software design descriptions on software design communication,Empirical Software Engineering,25,6,2020,10.1007/s10664-020-09835-6,15737616,"Context: Software engineering is a social and collaborative activity. Communicating and sharing knowledge between software developers requires much effort. Hence, the quality of communication plays an important role in influencing project success. To better understand the effect of communication on project success, more in-depth empirical studies investigating this phenomenon are needed. Objective: We investigate the effect of using a graphical versus textual design description on co-located software design communication. Method: Therefore, we conducted a family of experiments involving a mix of 240 software engineering students from four universities. We examined how different design representations (i.e., graphical vs. textual) affect the ability to Explain, Understand, Recall, and Actively Communicate knowledge. Results: We found that the graphical design description is better than the textual in promoting Active Discussion between developers and improving the Recall of design details. Furthermore, compared to its unaltered version, a well-organized and motivated textual design description–that is used for the same amount of time–enhances the recall of design details and increases the amount of active discussions at the cost of reducing the perceived quality of explaining."
Sedelmaier2020,Yvonne Sedelmaier and Dieter Landes,A Research Agenda for Identifying and Developing Required Competencies in Software Engineering,International Journal of Emerging Technologies in Learning,3,2,2020,10.3991/ijep.v3i2.2448,18630383,"Various issues make learning and teaching software engineering a challenge for both students and instructors. Since there are no standard curricula and no cookbook recipes for successful software engineering, it is fairly hard to figure out which specific topics and competencies should be learned or acquired by a particular group of students. Furthermore, it is not clear which particular didactic approaches might work well for a specific topic and a particular group of students. This contribution presents a research agenda that aims at identifying relevant competencies and environmental constraints as well as their effect on learning and teaching software engineering. To that end, an experimental approach will be taken. As a distinctive feature, this approach iteratively introduces additional or modified didactical methods into existing courses and carefully evaluates their appropriateness. Thus, it continuously improves these methods."
Kitchenham2020,Barbara Kitchenham and Lech Madeyski and Pearl Brereton,Meta-analysis for families of experiments in software engineering: a systematic review and reproducibility and validity assessment,Empirical Software Engineering,25,1,2020,10.1007/s10664-019-09747-0,15737616,"Context: Previous studies have raised concerns about the analysis and meta-analysis of crossover experiments and we were aware of several families of experiments that used crossover designs and meta-analysis. Objective: To identify families of experiments that used meta-analysis, to investigate their methods for effect size construction and aggregation, and to assess the reproducibility and validity of their results. Method: We performed a systematic review (SR) of papers reporting families of experiments in high quality software engineering journals, that attempted to apply meta-analysis. We attempted to reproduce the reported meta-analysis results using the descriptive statistics and also investigated the validity of the meta-analysis process. Results: Out of 13 identified primary studies, we reproduced only five. Seven studies could not be reproduced. One study which was correctly analyzed could not be reproduced due to rounding errors. When we were unable to reproduce results, we provide revised meta-analysis results. To support reproducibility of analyses presented in our paper, it is complemented by the reproducer R package. Conclusions: Meta-analysis is not well understood by software engineering researchers. To support novice researchers, we present recommendations for reporting and meta-analyzing families of experiments and a detailed example of how to analyze a family of 4-group crossover experiments."
Bjarnason2022,Elizabeth Bjarnason and Baldvin Gislason Bern and Linda Svedberg,Inter-team communication in large-scale co-located software engineering: a case study,Empirical Software Engineering,27,2,2022,10.1007/s10664-021-10027-z,15737616,"Large-scale software engineering is a collaborative effort where teams need to communicate to develop software products. Managers face the challenge of how to organise work to facilitate necessary communication between teams and individuals. This includes a range of decisions from distributing work over teams located in multiple buildings and sites, through work processes and tools for coordinating work, to softer issues including ensuring well-functioning teams. In this case study, we focus on inter-team communication by considering geographical, cognitive and psychological distances between teams, and factors and strategies that can affect this communication. Data was collected for ten test teams within a large development organisation, in two main phases: (1) measuring cognitive and psychological distance between teams using interactive posters, and (2) five focus group sessions where the obtained distance measurements were discussed. We present ten factors and five strategies, and how these relate to inter-team communication. We see three types of arenas that facilitate inter-team communication, namely physical, virtual and organisational arenas. Our findings can support managers in assessing and improving communication within large development organisations. In addition, the findings can provide insights into factors that may explain the challenges of scaling development organisations, in particular agile organisations that place a large emphasis on direct communication over written documentation."
Marquez2020,Gaston Marquez and Hernan Astudillo and Carla Taramasco,Security in Telehealth Systems from a Software Engineering Viewpoint: A Systematic Mapping Study,IEEE Access,8,,2020,10.1109/ACCESS.2020.2964988,21693536,"Telehealth systems deliver remote care of elderly and physically less able patients as well as remote surgeries, treatments, and diagnoses. In this regard, several systemic properties must be satisfied (such as security) in order to ensure the functionality of Telehealth systems. Although existing studies discuss different security episodes that involve Telehealth systems, it is difficult to have a clear standpoint about which are the most reported security issues and which solutions have been proposed. Furthermore, since Telehealth systems are composed of several software systems, it is not clear which critical areas of Software Engineering are relevant to develop secure Telehealth systems. This article reports a systematic mapping study (SMS) whose purpose is to detect, organize, and characterize security issues in Telehealth systems. Based on the SMS results, we examine how Software Engineering may help to develop secure Telehealth systems. From over a thousand studies, we distinguished and classified 41 primary studies. Results show that (i) four security classifications (attacks, vulnerabilities, weaknesses, and threats) concentrate the most reported security issues; (ii) three security strategies (detect attacks, stop or mitigate attacks and react to attacks) characterize security issues, and (iii) the most relevant research themes are related to insecure data transmission and privacy. The SMS's findings suggest that software design, requirements, and models are key areas to develop secure Telehealth systems."
Brstler2023,Jürgen Börstler and Nauman bin Ali and Martin Svensson and Kai Petersen,Investigating acceptance behavior in software engineering—Theoretical perspectives,Journal of Systems and Software,198,,2023,10.1016/j.jss.2022.111592,01641212,"Background: Software engineering research aims to establish software development practice on a scientific basis. However, the evidence of the efficacy of technology is insufficient to ensure its uptake in industry. In the absence of a theoretical frame of reference, we mainly rely on best practices and expert judgment from industry-academia collaboration and software process improvement research to improve the acceptance of the proposed technology. Objective: To identify acceptance models and theories and discuss their applicability in the research of acceptance behavior related to software development. Method: We analyzed literature reviews within an interdisciplinary team to identify models and theories relevant to software engineering research. We further discuss acceptance behavior from the human information processing perspective of automatic and affect-driven processes (“fast” system 1 thinking) and rational and rule-governed processes (“slow” system 2 thinking). Results: We identified 30 potentially relevant models and theories. Several of them have been used in researching acceptance behavior in contexts related to software development, but few have been validated in such contexts. They use constructs that capture aspects of (automatic) system 1 and (rational) system 2 oriented processes. However, their operationalizations focus on system 2 oriented processes indicating a rational view of behavior, thus overlooking important psychological processes underpinning behavior. Conclusions: Software engineering research may use acceptance behavior models and theories more extensively to understand and predict practice adoption in the industry. Such theoretical foundations will help improve the impact of software engineering research. However, more consideration should be given to their validation, overlap, construct operationalization, and employed data collection mechanisms when using these models and theories."
Song2023,Qunying Song and Per Runeson,Industry-academia collaboration for realism in software engineering research: Insights and recommendations,Information and Software Technology,156,,2023,10.1016/j.infsof.2022.107135,09505849,"Context: Effective industry-academia collaboration may increase software engineering research relevance by increased realism, yet very challenging for reasons like confidentiality concerns, different objectives and priorities. Objective: We analyse industry-academia collaboration scenarios based on our own experiences as Ph.D. student and supervisor, and provide insights and recommendations to facilitate future collaborations with industry. Method: We first present our industry-academia collaboration experiences that span over two and a half years with different companies. Then, we analyse both facilitators and problems from those scenarios and synthesize recommendations based on that. Results: Five different scenarios are analysed, including both success and failure scenarios. Reflections and insights into these experiences as well as some general recommendations are presented. Conclusion: We believe such experiences and insights are helpful for academic researchers to pursue industry-academia collaboration. We plan to continuously report our experience and provide our suggestions for effective collaboration with industry."
Zainol2020,Azida Zainol and Wafa Sulaiman Almukadi,Implementing problem-based learning in the software engineering course,International Journal of Advanced and Applied Sciences,7,12,2020,10.21833/ijaas.2020.12.002,23133724,"The student learning process using real-life contextualization and soft skills are the factors that every student should acquire after completing a Software Engineering course. Problem-based learning (PBL) that promotes learning by doing is used to be implemented in the Software Engineering course in order to solve these issues. A study is conducted with the intention to investigate the introduction of implementing PBL in the Software Engineering course at the Faculty of Computer Science and Engineering, University of Jeddah. Thus, this paper aims to report the findings of introducing PBL to stimulate students' learning and skills. The PBL was implemented in the Software Engineering course as an ongoing teaching method for Fall Semester 2019/2020, and after completing this course, a set of the questionnaire is distributed to all students to get their perceptions of using PBL. The results indicated that PBL stimulates the students' learning by allowing them to be responsible for their learning, become self-directed learners and given the students the ability to be responsible for their own learning in a simulated environment, working with fellow students to achieve an outcome. These results are comparable with previous works on PBL in the Software Engineering course, and PBL has been emphasized as a new teaching paradigm in an ongoing review of software engineering undergraduate program by IEEE/ACM. Therefore, the result of this study is significant as it highlights the positive conclusion of PBL, and this led to the theoretical of Software Engineering education in reducing the gap between theory and practice in curriculum development."
Marebane2021,Senyeki M. Marebane and Robert T. Hans,Software Engineering Ethics Competency Gap in Undergraduate Computing Qualifications within South African Universities of Technology,International Journal of Advanced Computer Science and Applications,12,4,2021,10.14569/IJACSA.2021.0120474,21565570,"Computing graduates working as software engineers are expected to demonstrate competencies in various categories of software engineering ethics as a component of nontechnical skills that complement technical skills. Therefore, university programme offerings should provide opportunities for students to develop software engineering ethical competence. This study analyses curriculum documents to determine the extent to which entry-level undergraduate computing qualifications of Universities of Technology (UoTs) in South Africa provide opportunities to empower students with software engineering ethical competence. We used summative content analysis to analyze texts within the UoT computing undergraduate qualifications related to software development as retrieved from the South African Qualifications Authority database. ATLAS.ti text analysis tool was used to classify texts according to predetermined software engineering ethics categories to determine the extent to which the qualifications under study expose students to software engineering ethics. The results show that the coverage of the various categories of software engineering ethics by UoT computing qualifications for software development is insufficient, incomplete and superficial, providing only limited opportunities for prospective software engineers to develop software engineering ethical competence. Lack of adequate inclusion of software engineering ethics by UoT qualifications in South Africa deprives prospective software engineers an opportunity to develop ethical competence required to become ethically successful software engineers. Such limited exposure by software development graduates risks the development of potentially unethical software products in the software industry."
Serban2024,Alex Serban and Koen van der Blom and Holger Hoos and Joost Visser,"Software engineering practices for machine learning — Adoption, effects, and team assessment",Journal of Systems and Software,209,,2024,10.1016/j.jss.2023.111907,01641212,"Machine learning (ML) is extensively used in production-ready applications, calling for mature engineering techniques to ensure robust development, deployment and maintenance. Given the potential negative impact machine learning (ML) can have on people, society or the environment, engineering techniques that can ensure robustness against technical errors and adversarial attacks are of considerable importance. In this work, we investigate how teams of experts develop, deploy and maintain software with ML components. Moreover, we link what teams do to the effects they aim to achieve and provide means for improvement. Towards this goal, we performed a mixed-methods study with a sequential exploratory strategy. First, we performed a systematic literature review through which we mined both academic and grey literature, and compiled a catalogue of engineering practices for ML. Second, we validated this catalogue using a large-scale survey, which measured the degree of adoption of the practices and their perceived effects. Third, we ran validation interviews with practitioners to add depth to the survey results. The catalogue covers a broad range of practices for engineering software systems with ML components and for ensuring non-functional properties that fall under the umbrella of trustworthy ML, such as fairness, security or accountability. Here, we present the results of our study, which indicate, for example, that larger and more experienced teams tend to adopt more practices, but that trustworthiness practices tend to be neglected. Moreover, we show that the effects measured in our survey, such as team agility or accountability, can be predicted quite accurately from groups of practices. This allowed us to contrast the importance of the practices for these effects as well as adoption rates, revealing, for example, that widely adopted practices are, in reality, less important with respect to some effects. For instance, writing reusable scripts for data cleaning and merging is highly adopted, but has a limited impact on reproducibility. Overall, our study provides a quantitative assessment of ML engineering practices and their impact on desirable properties of software with ML components, by which we open multiple avenues for improving the adoption of useful practices. Editor's note: Open Science material was validated by the Journal of Systems and Software Open Science Board."
Lawlor2020,Brendan Lawlor and Roy D. Sleator,The democratization of bioinformatics: A software engineering perspective,GigaScience,9,6,2020,10.1093/GIGASCIENCE/GIAA063,2047217X,"Today, thanks to advances in cloud computing, it is possible for small teams of software developers to produce internet-scale products, a feat that was previously the preserve of large organizations. Herein, we describe how these advances in software engineering can be made more readily available to bioinformaticians. In the same way that cloud computing has democratized access to distributed systems engineering for generalist software engineers, access to scalable and reproducible bioinformatic engineering can be democratized for generalist bioinformaticians and biologists. We present solutions, based on our own efforts, to achieve this goal."
Escalona2022,María José Escalona and Nora Parcus de Koch and Gustavo Rossi,A Quantitative SWOT-TOWS Analysis for the Adoption of Model-Based Software Engineering,Journal of Object Technology,21,4,2022,10.5381/jot.2022.21.4.a9,16601769,"Enterprises’ trend to low-code development revives model-based software engineering (MBSE) since several low-code platforms are based on the principles of model-based design, automatic code generation, and visual programming. Changes in an enterprise’s software development process, however, always require strategic planning. To find an appropriate strategy, we present an analytical tool for identifying and evaluating strengths, weaknesses, opportunities and threats factors for the adoption of MBSE. This tool provides a SWOT-TOWS analysis supplemented by a quantitative evaluation of strategies based on a multiple-criteria decision technique drawing on the knowledge of industry experts. Our analytical tool is general so it can be used in the industrial context for making other strategic decisions."
Rababaah2021,Aaron R. Rababaah,Enhancing software engineering learning environment with computer games: A case study,Journal of Engineering Education Transformations,35,1,2021,10.16920/jeet/2021/v35i1/22065,23941707,"Education gamification has been spreading in various disciplines such as languages, computer programming, medicine, natural languages, engineering, etc. Software Engineering is our interest in this work as we saw an opportunity of contribution to enrich literature and empirical studies in this area. Traditional methods of teaching Software Engineering could significantly benefit from gamification as a complementary component in student learning outcomes. We believe we can provide our students with more effective learning environment in number of aspects including: providing enjoyable practice, immediate feedback, enhancing the sense of responsibility, enhanced engagement and performance real time tracking. In this paper, we will present our case study in adopting a computer game in software engineering course. Further, we will present the results of a course exist survey that shows the responses of 114 participating students. The analysis of the survey showed significant positive impact on number of aspects including: student engagement, learning concepts and critical thinking. The overall mean of positive responses was 81.2%."
Kitchenham2023,Barbara Kitchenham and Lech Madeyski and David Budgen,How Should Software Engineering Secondary Studies Include Grey Material?,IEEE Transactions on Software Engineering,49,2,2023,10.1109/TSE.2022.3165938,19393520,"Context: Recent papers have proposed the use of grey literature (GL) and multivocal reviews. These papers have raised issues about the practices used for systematic reviews (SRs) in software engineering (SE) and suggested that there should be changes to the current SR guidelines. Objective: To investigate whether current SR guidelines need to be changed to support GL and multivocal reviews. Method: We discuss the definitions of GL and the importance of GL and of industry-based field studies in SE SRs. We identify properties of SRs that constrain the material used in SRs: a) the nature of primary studies; b) the requirements of SRs to be auditable, traceable, and reproducible; and explain why these requirements restrict the use of blogs in SRs. Results: SR guidelines have always considered GL as a possible source of primary studies and have never supported exclusion of field studies that incorporate the practitioners' viewpoint. However, the concept of GL, which was meant to refer to documents that were not formally published, is now being extended to information from sources such as blogs/tweets/Q&A posts. Thus, it might seem that SRs do not make full use of GL because they do not include such information. However, the unit of analysis for an SR is the primary study. Thus, it is not the source but the type of information that is important. Any report describing a rigorous empirical evaluation is a candidate primary study. Whether it is actually included in an SR depends on the SR eligibility criteria. However, any study that cannot be guaranteed to be publicly available in the long term should not be used as a primary study in an SR. This does not prevent such information from being aggregated in surveys of social media and used in the context of evidence-based software engineering (EBSE). Conclusions: Current guidelines for SRs do not require extensions, but their scope needs to be better defined. SE researchers require guidelines for analysing social media posts (e.g., blogs, tweets, vlogs), but these should be based on qualitative primary (not secondary) study guidelines. SE researchers can use mixed-methods SRs and/or the fourth step of EBSE to incorporate findings from social media surveys with those from SRs and to develop industry-relevant recommendations."
Grande2024,Rubén Grande and Aurora Vizcaíno and Félix O. García,Is it worth adopting DevOps practices in Global Software Engineering? Possible challenges and benefits,Computer Standards and Interfaces,87,,2024,10.1016/j.csi.2023.103767,09205489,"The software industry is adopting the DevOps paradigm to an increasingly frequent extent. In addition, the trend of developing software in a distributed manner greatly increased as a result of the COVID-19 pandemic, which forced team members from software companies to work remotely. We present the results of a Systematic Mapping Study (SMS) of how DevOps has been applied in distributed and global settings that adopts Global Software Development (GSD). The results were obtained from analysing 27 papers. The main conclusions obtained after carrying our SMS show that adopting DevOps in such settings by implementing certain practices brings several advantages to software companies, even though there are difficulties to be confronted when adopting DevOps in global and distributed contexts. Moreover, we (1) proposed definition of DevOps in distributed and global settings, (2) mapped the challenges detected with a list of well-known GSD risks and (3) mapped the benefits that can be obtained from applying certain practices identified and the challenges that should be overcame to obtain such benefits."
Haindl2022,Philipp Haindl and Reinhold Plösch,Value-oriented quality metrics in software development: Practical relevance from a software engineering perspective,IET Software,16,2,2022,10.1049/sfw2.12051,17518814,"When following the principles of value-based software engineering, business, customer satisfaction, and engineering considerations need to be balanced to develop and operate the software so that it satisfies the different stakeholders' expectations. This, however, requires knowing the relevant quality metrics covering these value-oriented expectations and potential sources for their measurement. In this work, a categorisation of value-oriented quality metrics that are practically relevant is presented. Therefore, the authors conducted an online survey with practitioners who assessed the relevance of 61 value-oriented metrics, gathered from a preceding systematic mapping study. The authors grouped these metrics into 10 categories, based on financial, customer satisfaction, value proposition, and creation perspectives. Also, the authors examined the frequency of particular steps at which these measures accrue and identified their most relevant data sources. The participants rated metrics for feature reliability, performance, as well as test and development efficiency as most relevant for value orientation. According to the participants, the authors' collection covers all relevant metrics for addressing financial and market and feature usability aspects. The authors' categorisation and the metrics' relevance assessments shall support software engineers in selecting relevant metrics and their sources for software product development."
Garousi2020,Vahid Garousi and Markus Borg and Markku Oivo,Practical relevance of software engineering research: synthesizing the community’s voice,Empirical Software Engineering,25,3,2020,10.1007/s10664-020-09803-0,15737616,"Software engineering (SE) research should be relevant to industrial practice. There have been regular discussions in the SE community on this issue since the 1980’s, led by pioneers such as Robert Glass. As we recently passed the milestone of “50 years of software engineering”, some recent positive efforts have been made in this direction, e.g., establishing “industrial” tracks in several SE conferences. However, many researchers and practitioners believe that we, as a community, are still struggling with research relevance and utility. The goal of this paper is to synthesize the evidence and experience-based opinions shared on this topic so far in the SE community, and to encourage the community to further reflect and act on the research relevance. For this purpose, we have conducted a Multi-vocal Literature Review (MLR) of 54 systematically-selected sources (papers and non peer-reviewed articles). Instead of relying on and considering the individual opinions on research relevance, mentioned in each of the sources, the MLR aims to synthesize and provide the “holistic” view on the topic. The highlights of our MLR findings are as follows. The top three root causes of low relevance, discussed in the community, are: (1) Researchers having simplistic views (or wrong assumptions) about SE in practice; (2) Lack of connection with industry; and (3) Wrong identification of research problems. The top three suggestions for improving research relevance are: (1) Using appropriate research approaches such as action-research; (2) Choosing relevant (practical) research problems; and (3) Collaborating with industry. By synthesizing all the discussions on this important topic so far, this paper aims to encourage further discussions and actions in the community to increase our collective efforts to improve the research relevance. Furthermore, we raise the need for empirically-grounded and rigorous studies on the relevance problem in SE research, as carried out in other fields such as management science."
Croock2021,Muayad Sadik Croock and Rawan Ali Taaban,Software engineering based secured E-payment system,International Journal of Electrical and Computer Engineering,11,5,2021,10.11591/ijece.v11i5.pp4413-4422,20888708,"Nowadays, the E-payment systems have been considered to be the safe way of money transfer in most of modern institutes and companies. Moreover, the security is important side of these systems to ensure that the money transfer is done safely. Software engineering techniques are used for guaranteeing the applying of security and privacy of such systems. In this paper, a secure E-payment system is proposed based on software engineering model and neural network technology. This system uses different proposed algorithms for applying authentication to the devices of users as mobile application. They are used to control the key management in the system. It uses the neural network back-propagation method for ensuring the security of generated keys that have sufficient random levels. The proposed system is tested over numerous cases and the obtained results show an efficient performance in terms of security and money transfer. Moreover, the generated keys are tested according to NIST standards."
Moreb2020,Mohammed Moreb and Tareq Abed Mohammed and Oguz Bayat,A novel software engineering approach toward using machine learning for improving the efficiency of health systems,IEEE Access,8,,2020,10.1109/ACCESS.2020.2970178,21693536,"Recently, machine learning has become a hot research topic. Therefore, this study investigates the interaction between software engineering and machine learning within the context of health systems. We proposed a novel framework for health informatics: the framework and methodology of software engineering for machine learning in health informatics (SEMLHI). The SEMLHI framework includes four modules (software, machine learning, machine learning algorithms, and health informatics data) that organize the tasks in the framework using a SEMLHI methodology, thereby enabling researchers and developers to analyze health informatics software from an engineering perspective and providing developers with a new road map for designing health applications with system functions and software implementations. Our novel approach sheds light on its features and allows users to study and analyze the user requirements and determine both the function of objects related to the system and the machine learning algorithms that must be applied to the dataset. Our dataset used in this research consists of real data and was originally collected from a hospital run by the Palestine government covering the last three years. The SEMLHI methodology includes seven phases: designing, implementing, maintaining and defining workflows; structuring information; ensuring security and privacy; performance testing and evaluation; and releasing the software applications."
Metrlho2022,José Metrôlho and Fernando Ribeiro and Paula Graça and Ana Mourato and David Figueiredo and Hugo Vilarinho,Aligning Software Engineering Teaching Strategies and Practices with Industrial Needs,Computation,10,8,2022,10.3390/computation10080129,20793197,"Several approaches have been proposed to reduce the gap between software engineering education and the needs and practices of the software industry. Many of them aim to promote a more active learning attitude in students and provide them with more realistic experiences, thus recreating industry software development environments and collaborative development and, in some cases, with the involvement of companies mainly acting as potential customers. Since many degree courses typically offer separate subjects to teach requirements engineering, analysis and design, coding, or validation, the integration of all these phases normally necessitates experience in a project context and is usually carried out in a final year project. The approach described in this article benefits from the close involvement of a software house company which goes beyond the common involvement of a potential customer. Students are integrated into distributed teams comprising students, teachers and IT professionals. Teams follow the agile Scrum methodology and use the OutSystems low-code development platform providing students with the experience of an almost real scenario. The results show that this approach complements the knowledge and practice acquired in course subjects, develops the students’ technical and non-technical skills, such as commitment, teamwork, and communication, and initiates them in the methodologies and development strategies used in these companies. The feedback from the teachers involved, software companies and students was very positive."
Matturro2023,Gerardo Matturro,Undergraduate research in software engineering. An experience and evaluation report,Journal of Universal Computer Science,29,3,2023,10.3897/jucs.95718,09486968,"The purpose of this paper is to present an undergraduate research experience process model and the evaluation of seven years of its application in an undergraduate research program in software engineering. Undergraduate students who participated in research projects between 2015 and 2022 were surveyed to find out a) their motivations for participating in research projects in software engineering, b) the skills they consider they have acquired or improved by participating in those projects, and c) their perception of benefits and utility for their future work and professional activities. Results reveal that participation in real research projects in software engineering is highly valued by undergraduate students, who perceive benefits in the development of research and soft skills, and for their future professional activity. In addition, these undergraduate research projects and the process followed show that it is feasible to make original contributions to the body of knowledge of software engineering."
Ciancarini2021,Paolo Ciancarini and Mirko Farina and Sergey Masyagin and Giancarlo Succi and Sofiia Yermolaieva and Nadezhda Zagvozkina,Non Verbal Communication in Software Engineering-An Empirical Study,IEEE Access,9,,2021,10.1109/ACCESS.2021.3075983,21693536,"Communication among humans consists of both verbal and non verbal components. The latter may sometimes express concepts or ideas not conveyable by the former. This is also true in Software Engineering. This paper first analyses theoretically the role of non verbal communication in software development teams, using the framework provided by distributed cognition as a conceptual palette and as a point of reference. Then, it presents an empirical investigation involving 38 IT professionals from Russia, sharing their experiences in communicating and interacting when developing software artifacts. The results of this empirical investigation are consistent with many of the ideas underlying a distributed approach to cognition. In addition, our findings provide valuable insights to make communication more effective in software development teams, while defining a new framework for follow-up studies."
CeciliaBastarrica2023,Maria Cecilia Bastarrica and Francisco J. Gutierrez and Maria Marques and Daniel Perovich,On the Impact of Grading on Teamwork Quality in a Software Engineering Capstone Course,IEEE Access,11,,2023,10.1109/ACCESS.2023.3265302,21693536,"Every semester, we deliver a capstone course on software engineering where students undertake a real-world project in three iterations. By the end of each iteration, students are graded in several dimensions: software quality, project management, and peer assessment. The latter is the only grade assigned individually; therefore, students who are penalized by their teams (e.g., for being perceived as low contributors to the team effort) are not severely affected. This results in little incentive for improvement, which potentially jeopardizes the overall quality of the project outcome. Envisaging to promote team cohesion, we devised a new grading rule: if the peer assessment of a student is lower than a threshold, that would be their final grade in the iteration. This paper reports the results of studying the effectiveness of the proposed rule. We recorded peer assessments over six consecutive semesters: (1) the first three as the baseline measure; (2) the semester where we introduced the new grading rule; and (3) the following two semesters, as a contrast. When the rule was first introduced, peer assessments resulted low and heavily spread at the beginning, but they consistently improved toward the end of the semester. When the instructional team already trusted the rule and explicitly emphasized its application, peer assessments consistently grew along the semester but resulted in fewer outliers. The study results show that exposing peer assessments earlier on helps promote team reflection. They also made evident the positive impact of teamwork for producing quality products in a software engineering capstone course."
Barenkamp2020,Marco Barenkamp and Jonas Rebstadt and Oliver Thomas,Applications of AI in classical software engineering,AI Perspectives,2,1,2020,10.1186/s42467-020-00005-4,,"Although Artificial Intelligence (AI) has become a buzzword for self-organizing IT applications, its relevance to software engineering has hardly been analyzed systematically. This study combines a systematic review of previous research in the field and five qualitative interviews with software developers who use or want to use AI tools in their daily work routines, to assess the status of development, future development potentials and equally the risks of AI application to software engineering. The study classifies the insights in the software development life cycle.The analysis results that major achievements and future potentials of AI are a) the automation of lengthy routine jobs in software development and testing using algorithms, e.g. for debugging and documentation, b) the structured analysis of big data pools to discover patterns and novel information clusters and c) the systematic evaluation of these data in neural networks. AI thus contributes to speed up development processes, realize development cost reductions and efficiency gains. AI to date depends on man-made structures and is mainly reproductive, but the automation of software engineering routines entails a major advantage: Human developers multiply their creative potential when using AI tools effectively."
Hasselbring2020,Wilhelm Hasselbring and André van Hoorn,Kieker: A monitoring framework for software engineering research,Software Impacts,5,,2020,10.1016/j.simpa.2020.100019,26659638,"Application-level monitoring and dynamic analysis of software systems are a basis for various tasks in software engineering research, such as performance evaluation and reverse engineering. The Kieker framework provides monitoring, analysis, and visualization support for these purposes. It commenced in 2006, and grew toward a high-quality open-source software that has been employed in a variety of software engineering research projects over the last decade. Several research groups constitute the open-source community to advance the Kieker framework. In this paper, we review Kieker's history, development, and impact both in research and technology transfer with industry."
Szydowska2023,Justyna Szydłowska and Jakub Swacha,Polish Contribution to Software Engineering Research 1992-2021,Foundations of Computing and Decision Sciences,48,1,2023,10.2478/fcds-2023-0005,23003405,"Since the collapse of the Soviet Bloc, Polish researchers in the software engineering area made a large e ort to close the gap developed during the times of the Iron Curtain and the ensuing limited access to Western technology. The question arises as to whether they eventually managed to attain - or maybe even to surpass the scientific output in this area of the developed Western countries. In this paper, we perform analysis on papers published in 16 high-quality software engineering journals within the last 30 years to measure the Polish contribution to the global scientific output in the area of software engineering. We observe how the Polish contribution changed in quantity in relation to the global scientific output in subsequent years. We also identify the journals that attracted the Polish authors the most, the institutions located in Poland that contributed the most, as well as the most productive Polish authors and the most acknowledged works (in terms of citation number) written by Polish authors. Moreover, we investigate the topics most often chosen by them."
Li2020,Tun Li and Wanwei Liu and Juan Chen and Xiaoguang Mao and Xinjun Mao,Towards connecting discrete mathematics and software engineering,Tsinghua Science and Technology,25,3,2020,10.26599/TST.2019.9010012,18787606,"To enhance training in software development, we argue that students of software engineering should be exposed to software development activities early in the curriculum. This entails meeting the challenge of engaging students in software development before they take the software engineering course. In this paper, we propose a method to connect courses in the software engineering curriculum by setting comprehensive development projects to students in prerequisite courses for software development. Using the Discrete Mathematics (DM) course as an example, we describe the implementation of the proposed method and teaching practices using several practical and comprehensive projects derived from topics in discrete mathematics. Detailed descriptions of the sample projects, their application, and training results are given. Results and lessons learned from applying these practices show that it is a promising way to connect courses in the software engineering curriculum."
Flores2020,Nuno Flores and Ana C.R. Paiva and Nuno Cruz,Teaching software engineering topics through pedagogical game design patterns: An empirical study,Information (Switzerland),11,3,2020,10.3390/info11030153,20782489,"Teaching software engineering in its many different forms using traditional teaching methods is difficult. Serious games can help overcome these challenges because they allow real situations to be simulated. However, the development of serious games is not easy and, although there are good practices for relating game design patterns to teaching techniques, there is no methodology to support its use in a specific context such as software engineering. This article presents a case study to validate a methodology that links the Learning and Teaching Functions (LTF) to the Game Design Patterns (PIB) in the context of Software Engineering Education. A serious game was developed from scratch using this methodology to teach software estimation (a specific topic of software engineering). An experiment was carried out to validate the effectiveness of the game by comparing the results of two different groups of students. The results indicate that the methodology can help to develop effective educational games on specific learning topics."
Wohlin2020,Claes Wohlin and Emilia Mendes and Katia Romero Felizardo and Marcos Kalinowski,Guidelines for the search strategy to update systematic literature reviews in software engineering,Information and Software Technology,127,,2020,10.1016/j.infsof.2020.106366,09505849,"Context: Systematic Literature Reviews (SLRs) have been adopted within Software Engineering (SE) for more than a decade to provide meaningful summaries of evidence on several topics. Many of these SLRs are now potentially not fully up-to-date, and there are no standard proposals on how to update SLRs in SE. Objective: The objective of this paper is to propose guidelines on how to best search for evidence when updating SLRs in SE, and to evaluate these guidelines using an SLR that was not employed during the formulation of the guidelines. Method: To propose our guidelines, we compare and discuss outcomes from applying different search strategies to identify primary studies in a published SLR, an SLR update, and two replications in the area of effort estimation. These guidelines are then evaluated using an SLR in the area of software ecosystems, its update and a replication. Results: The use of a single iteration forward snowballing with Google Scholar, and employing as a seed set the original SLR and its primary studies is the most cost-effective way to search for new evidence when updating SLRs. Furthermore, the importance of having more than one researcher involved in the selection of papers when applying the inclusion and exclusion criteria is highlighted through the results. Conclusions: Our proposed guidelines formulated based upon an effort estimation SLR, its update and two replications, were supported when using an SLR in the area of software ecosystems, its update and a replication. Therefore, we put forward that our guidelines ought to be adopted for updating SLRs in SE."
Naseer2020,Mehwish Naseer and Wu Zhang and Wenhao Zhu,Early prediction of a team performance in the initial assessment phases of a software project for sustainable software engineering education,Sustainability (Switzerland),12,11,2020,10.3390/su12114663,20711050,"Software engineering is a competitive field in education and practice. Software projects are key elements of software engineering courses. Software projects feature a fusion of process and product. The process reflects the methodology of performing the overall software engineering practice. The software product is the final product produced by applying the process. Like any other academic domain, an early evaluation of the software product being developed is vital to identify the at-risk teams for sustainable education in software engineering. Guidance and instructor attention can help overcome the confusion and difficulties of low performing teams. This study proposed a hybrid approach of information gain feature selection with a J48 decision tree to predict the earliest possible phase for final performance prediction. The proposed technique was compared with the state-of-the-art machine learning (ML) classifiers, naive Bayes (NB), artificial neural network (ANN), logistic regression (LR), simple logistic regression (SLR), repeated incremental pruning to produce error reduction (RIPPER), and sequential minimal optimization (SMO). The goal of this process is to predict the teams expected to obtain a below-average grade in software product development. The proposed technique outperforms others in the prediction of low performing teams at an early assessment stage. The proposed J48-based technique outperforms others by making 89% correct predictions."
Daun2023,Marian Daun and Jennifer Brings and Patricia Aluko Obe and Bastian Tenbergen,"An industry survey on approaches, success factors, and barriers for technology transfer in software engineering",Software - Practice and Experience,53,7,2023,10.1002/spe.3200,1097024X,"One central aspect of software engineering research is the transfer of the proposed approaches into industrial practice. In the past, a number of technology transfer approaches and experiences from technology transfer projects in software engineering have already been reported. However, many researchers still struggle to get their research results noticed by practitioners. To investigate what is important to practitioners, we conducted a mixed-methods study that provides us with reliable quantitative data as well as deeper insights from qualitative data. Our results show that there is a mismatch between industry professionals' needs and commonly proposed technology transfer approaches in the software engineering field. For instance, collaboration between industry and academia as well as participation in empirical evaluations is not deemed important from an industry point of view. In contrast, industry professionals emphasize the use of company-specific pilot projects conducted by industry and the need for experts to be available in every phase of technology transfer."
Fadhil2020,Julanar Ahmed Fadhil and Koh Tieng Wei and Kew Si Na,Artificial Intelligence for Software Engineering: An Initial Review on Software Bug Detection and Prediction,Journal of Computer Science,16,12,2020,10.3844/jcssp.2020.1709.1717,15526607,"The need for speed and quality in delivering all software engineering artifacts has inevitably remained the biggest challenge in today’s software development environment. While everyone caters to complex software engineering processes, new releases are expected by the market on almost a daily basis. Thus, several Artificial Intelligence (AI) techniques have been introduced that are intensively used in the modern software engineering industry to fulfill market needs. This paper presents the initial results of our review work on software bug detection and prediction studies using AI techniques. Our focus is to (i) identify factors affecting the effectiveness of current software bug detection and prediction techniques and (ii) identify the effectiveness of AI techniques in improving current software bug detection and prediction techniques. The evidence showed that the software engineering domain has utilized artificial intelligence approaches and techniques to facilitate the complex tasks of software bug detection and bug prediction. It mainly demonstrates the significance of merging artificial intelligence with the software engineering domain in terms of reduced overhead and efficient results to enhance the quality of software products."
Al-Ahmad2022,Bilal I. Al-Ahmad and Ala’ A. Al-Zoubi and Md Faisal Kabir and Marwan Al-Tawil and Ibrahim Aljarah,Swarm intelligence-based model for improving prediction performance of low-expectation teams in educational software engineering projects,PeerJ Computer Science,8,,2022,10.7717/PEERJ-CS.857,23765992,"Software engineering is one of the most significant areas, which extensively used in educational and industrial fields. Software engineering education plays an essential role in keeping students up to date with software technologies, products, and processes that are commonly applied in the software industry. The software development project is one of the most important parts of the software engineering course, because it covers the practical side of the course. This type of project helps strengthening students’ skills to collaborate in a team spirit to work on software projects. Software project involves the composition of software product and process parts. Software product part represents software deliverables at each phase of Software Development Life Cycle (SDLC) while software process part captures team activities and behaviors during SDLC. The low-expectation teams face challenges during different stages of software project. Consequently, predicting performance of such teams is one of the most important tasks for learning process in software engineering education. The early prediction of performance for low-expectation teams would help instructors to address difficulties and challenges related to such teams at earliest possible phases of software project to avoid project failure. Several studies attempted to early predict the performance for low-expectation teams at different phases of SDLC. This study introduces swarm intelligence -based model which essentially aims to improve the prediction performance for low-expectation teams at earliest possible phases of SDLC by implementing Particle Swarm Optimization-K Nearest Neighbours (PSO-KNN), and it attempts to reduce the number of selected software product and process features to reach higher accuracy with identifying less than 40 relevant features. Experiments were conducted on the Software Engineering Team Assessment and Prediction (SETAP) project dataset. The proposed model was compared with the related studies and the state-of-the-art Machine Learning (ML) classifiers: Sequential Minimal Optimization (SMO), Simple Linear Regression (SLR), Naïve Bayes (NB), Multilayer Perceptron (MLP), standard KNN, and J48. The proposed model provides superior results compared to the traditional ML classifiers and state-of-the-art studies in the investigated phases of software product and process development."
Ferrario2023,Maria Angela Ferrario and Emily Winter,Applying Human Values Theory to Software Engineering Practice: Lessons and Implications,IEEE Transactions on Software Engineering,49,3,2023,10.1109/TSE.2022.3170087,19393520,"The study of human values in software engineering (SE) is increasingly recognised as a fundamental human-centric issue of SE decision making. However, values studies in SE still face a number of issues, including the difficulty of eliciting values in a systematic and structured way, the challenges of measuring and tracking values over time, and the lack of practice-based understanding of values among software practitioners. This paper aims to help address these issues by: 1) outlining a research framework that supports a systematic approach to values elicitation, analysis, and understanding; 2) introducing tools and techniques that help elicit and measure values during SE decision making processes in a systematic way; and 3) applying such tools to a month-long research sprint co-designed with an industry partner and conducted with 27 software practitioners. The case study builds on lessons from an earlier pilot (12 participants) and combines in-situ observations with the use of two values-informed tools: the Values Q-Sort (V-QS), and the Values-Retro. The V-QS adapts instruments from values research to the SE context, the Values-Retro adapts existing SE techniques to values theory. We distil implications for research and practice in ten lessons learned."
Gonzlez-Prieto2023,Ángel González-Prieto and Jorge Perez and Jessica Diaz and Daniel López-Fernández,Reliability in software engineering qualitative research through Inter-Coder Agreement,Journal of Systems and Software,202,,2023,10.1016/j.jss.2023.111707,01641212,"The research on empirical software engineering that uses qualitative data analysis is increasing. However, most of them do not deepen into the validity of the findings, specifically in the reliability of coding in which these methodologies rely on. This paper aims to establish a novel theoretical framework that enables a methodological approach for conducting this validity analysis through Inter-Coder Agreement (ICA), based on the use of coefficients to measure the degree of agreement in collaborative coding. We systematically review several existing variants of Krippendorff's α coefficients and provide a novel common mathematical framework to unify them. Finally, this paper illustrates the use of this theoretical framework in a large case study on DevOps culture. We expect that this work will help researchers who are committed to measuring consensus with quantitative techniques in collaborative coding, conducted as part of a qualitative research, to improve the rigor of their findings."
Cruz2020,Margarita Cruz and Beatriz Bernardez and Amador Duran and Jose A. Galindo and Antonio Ruiz-Cortes,"Replication of Studies in Empirical Software Engineering: A Systematic Mapping Study, from 2013 to 2018",IEEE Access,8,,2020,10.1109/ACCESS.2019.2952191,21693536,"Context: In any discipline, replications of empirical studies are necessary to consolidate the acquired knowledge. In Software Engineering, replications have been reported since the 1990s, although their number is still small. The difficulty in publishing, the lack of guidelines, and the unavailability of replication packages are pointed out by the community as some of the main causes. Objective: Understanding the current state of replications in Software Engineering studies by evaluating current trends and evolution during the last 6 years. Method: A Systematic Mapping Study including articles published in the 2013-2018 period that report at least one replication of an empirical study in Software Engineering. Results: 137 studies were selected and analysed, identifying: \{i\}) forums; ii) authors, co-authorships and institutions; iii) most cited studies; iv) research topics addressed; \{v\}) empirical methods used; vi) temporal distribution of publications; and vii) distribution of studies according to research topics and empirical methods. Conclusions: According to our results, the most relevant forums are the Empirical Software Engineering and Information and Software Technology journals, and the Empirical Software Engineering and Measurement conference. We observed that, as in previous reviews by other researchers, most of the studies were carried out by European institutions, especially Italian, Spanish, and German researchers and institutions. The studies attracting more citations were published mainly in journals and in the International Conference on Software Engineering. Testing, requirements, and software construction were the most frequent topics of replication studies, whereas the usual empirical method was the controlled experiment. On the other hand, we identified research gaps in areas such as software engineering process, software configuration management, and software engineering economics. When analysed together with previous reviews, there is a clear increasing trend in the number of published replications in the 2013-2018 period."
Tamburri2021,Damian A. Tamburri and Fabio Palomba and Rick Kazman,Success and Failure in Software Engineering: A Followup Systematic Literature Review,IEEE Transactions on Engineering Management,68,2,2021,10.1109/TEM.2020.2976642,15580040,"Success and failure in software engineering are still among the least understood phenomena in the discipline. In a recent special journal issue on the topic, Mäntylä et al. started discussing these topics from different angles; the authors focused their contributions on offering a general overview of both topics without deeper detail. Recognizing the importance and impact of the topic, in this article we have executed a followup, more in-depth systematic literature review with additional analyses beyond what was previously provided. These new analyses offer: a grounded-theory of success and failure factors, harvesting over 500+ factors from the literature; 14 manually validated clusters of factors that provide relevant areas for success- and failure-specific measurement and risk-analysis; a quality model composed of previously unmeasured organizational structure quantities which are germane to software product, process, and community quality. We show that the topics of success and failure deserve further study as well as further automated tool support, e.g., monitoring tools and metrics able to track the factors and patterns emerging from this article. This article provides managers with risks as well as a more fine-grained analysis of the parameters that can be appraised to anticipate the risks."
Yahya2023,Norzariyah Yahya and Siti Sarah Maidin,Hybrid agile development phases: The practice in software projects as performed by software engineering team,Indonesian Journal of Electrical Engineering and Computer Science,29,3,2023,10.11591/ijeecs.v29.i3.pp1738-1749,25024760,"The combination of scrum and waterfall is one of the software engineering teams that preferred hybrid agile models. The purpose of combining the two models is to leverage the advantages of each also to tailor the hybrid agile model to the needs of the project. However, to what extent are the phases, stages, and features of scrum and waterfall implemented in a software project remains unclear. Additionally, which phase will employ scrum, and when will waterfall be deemed optimal is also the arising question. This research adopted a qualitative study, and interviews are used as a data collection instrument. The interview is conducted based on an interview protocol, and thematic analysis is utilized to extract the themes from the interviews. This study investigates how the scrum and waterfall models are utilized in a software project, and three themes were identified in answering the research question. The findings indicate five development phases in a hybrid agile project and that waterfall is the preferable model in planning, while development is on scrum, and project testing and deployment could be either waterfall or scrum."
Alanazi2023,Fayez Alanazi and Mamdouh Alenezi,Software Engineering Techniques for Building Sustainable Cities with Electric Vehicles,Applied Sciences (Switzerland),13,15,2023,10.3390/app13158741,20763417,"As the process of urbanization continues to accelerate, the demand for sustainable cities has become more critical than ever before. The incorporation of electric vehicles (EVs) is a key component in creating sustainable cities. However, the development of smart cities for EVs entails more than just the installation of charging stations. Software engineering plays a crucial role in realizing smart cities for electric vehicles. This paper examines the role of software engineering in the creation of smart cities for electric vehicles, the techniques utilized in electric vehicle charging infrastructure, the obstacles faced by software engineers, and the future of software engineering in sustainable cities. Specifically, the paper explores the significance of software engineering in integrating EVs into the transportation system, including the design of smart charging and energy management systems, and the establishment of intelligent transportation systems. Additionally, the paper offers case studies to demonstrate successful software engineering implementations for smart cities. Finally, the paper concludes with a discussion of the challenges that software engineers encounter in implementing intelligent transportation systems for EVs and provides future directions for software engineering in sustainable cities."
Asikainen2022,Timo Asikainen and Tomi Männistö,Undulate: A framework for data-driven software engineering enabling soft computing,Information and Software Technology,152,,2022,10.1016/j.infsof.2022.107039,09505849,"Context.: Especially web-facing software systems enable the collection of usage data at a massive scale. At the same time, the scale and scope of software processes have grown substantively. Automated tools are needed to increase the speed and quality of controlling software processes. The usage data has great potential as a driver for software processes. However, research still lacks constructs for collecting, refining and utilising usage data in controlling software processes. Objective.: The objective of this paper is to introduce a framework for data-driven software engineering. The UNDULATE framework covers generating, collecting and utilising usage data from software processes and business processes supported by the software produced. In addition, we define the concepts and process of extreme continuous experimentation as an exemplar of a software engineering process. Method.: We derive requirements for the framework from the research literature, with a focus on papers inspired by practical problems. In addition, we apply a multilevel modelling language to describe the concepts related to extreme continuous experimentation. Results.: We introduce the UNDULATE framework and give requirements and provide an overview of the processes of collecting usage data, augmenting it with additional dimensional data, aggregating the data along the dimensions and computing different metrics based on the data and other metrics. Conclusions.: The paper represents significant steps inspired by previous research and practical insight towards standardised processes for data-driven software engineering, enabling the application of soft computing and other methods based on artificial intelligence."
Sarasa-Cabezuelo2021,Antonio Sarasa-Cabezuelo and Covadonga Rodrigo,Development of an educational application for software engineering learning,Computers,10,9,2021,10.3390/computers10090106,2073431X,"Software engineering is a complicated subject for computer engineering students since the explained knowledge and necessary competencies are more related to engineering as a general knowledge area than to computer science. This article describes a software engineering learning application that aims to provide a solution to this problem. Two ideas are used for this. On the one hand, to facilitate its use it has been implemented as an Android app (in this way it can be used anywhere and at any time). In addition, and on the other hand, a gamification system has been implemented with different learning paths that adapt to the learning styles of each student. In this way, the student is motivated by competing with other classmates, and on the other hand, the application adapts to the way of learning that each one has."
Brennan2023,Attracta Brennan and Mary Dempsey and John McAvoy and Majella O’Dea and Sharon O’Leary and Margaret Prendergast,How COVID-19 impacted soft skills development: The views of software engineering students,Cogent Education,10,1,2023,10.1080/2331186X.2023.2171621,2331186X,"Since a pandemic was declared in 2020, Irish higher education institutions transitioned from on-campus to online delivery. This disruption created challenges to students’ acquisition of hard and soft skills. With greater employee mobility, there is an increased emphasis on soft skills development, especially those skills that enhance employability, i.e., creativity, leadership, communication, innovation, teamwork, adaptability, resilience, time management, organization, self motivation, ability to work under pressure, critical thinking and problem solving and organizational ability. The purpose of this study was to understand the effects of COVID-19 on fears for the future and on soft skills development. In this study, 111 Software Engineering university students were surveyed. The results show heightened fears for the future with regard to job opportunities, the loss of time and the lack of control. While females reported to being more fearful, they also reported enhanced empathy and strengthened resilience. Postgraduate students were less fearful about the future compared to undergraduate students whilst also reporting better time management and organization skills. This study showed that despite disruptions to education, the Software Engineering students self-reported enhancements to resilience, empathy, time management and organizational skills, with the greatest impact on resilience and time management."
Sasmito2021,Agung Panji Sasmito and Djoko Kustono and Purnomo and Hakkun Elmunsyah and Didik Nurhadi and Putri Sekarsari,Development of android-based teaching material in software engineering subjects for informatics engineering students,International Journal of Engineering Pedagogy,11,2,2021,10.3991/IJEP.V11I2.16299,21924880,"The research aimed to develop Android-based teaching material to improve the understanding of Software Engineering of Informatics Engineering students whose validity and effectiveness are measured. The method used in this paper is Research and Development (R&D) with the Analysis, Design, Development, Implementation, and Evaluation (ADDIE) model which is used to develop Android-based teaching material. The validity of teaching material product has confirmed by media experts and material experts in their fields. The effectiveness of the product was measured through the post-test only control group design with two groups of Informatics Engineering students in Higher Education involving 57 people. Data collection was carried out through questionnaires and tests, with descriptive analysis for the results of the validation of teaching material and pilot tests and t-tests for testing the effectiveness of teaching material. The results showed that Android-based teaching material is easy to use and has video tutorial features and quiz menus that provide direct feedback to users, so teaching material can be applied as a valid and effective learning media in increasing student understanding of Software Engineering material which has never been applied in the research location so far. Based on the results obtained, further research can be carried out on a broader subject and pay attention to other aspects such as increasing motivation and metacognitive students of Informatics Engineering."
Lin2021,Yen Ting Lin,Effects of flipped learning approaches on students’ learning performance in software engineering education,Sustainability (Switzerland),13,17,2021,10.3390/su13179849,20711050,"Software engineering education plays an important role in keeping students educated with software technologies, processes, and practices that are needed by industries. Nevertheless, the nature of software engineering learning activities in traditional classrooms is limited in scope and time, making it more difficult to achieve a proper balance between theory and practice and address industrial demands. This makes scant provision for assisting students in keeping their software engineering knowledge current. To support software engineering education, flipped learning is a suitable strategy. Prior studies have shown that students’ perceptions in flipped learning environments are better than those in traditional learning environments. Nevertheless, in flipped learning, students may not have sufficient ability to conduct learning out of class. Therefore, the flipped learning strategy should aim to meet the needs of students to ensure that they get the appropriate support or feedback during the learning process before the class. The aim of this study was to propose a flipped learning diagnosis approach to promote students’ learning out of class in the flipped classroom. To explore students’ learning performance in software engineering courses, three classes of students were invited to learn with three different learning approaches (traditional learning approach, flipped learning approach, and flipped learning diagnosis approach). The results showed that the students who learned with the flipped learning diagnosis approach outperformed those students who learned with the flipped learning approach or the traditional learning approach."
Kozov2024,Vasil Kozov and Galina Ivanova and Desislava Atanasova,Practical Application of AI and Large Language Models in Software Engineering Education,International Journal of Advanced Computer Science and Applications,15,1,2024,10.14569/IJACSA.2024.0150168,21565570,"Subjects with limited application in the software industry like AI have recently received tremendous boon due to the development and raise of publicity of LLMs. LLM-powered software has a wide array of practical applications that must be taught to Software Engineering students, so that they can be relevant in the field. The speed of technological change is extremely fast, and university curriculums must include those changes. Renewing and creating new methodologies and workshops is a difficult task to complete successfully in such a dynamic environment full of cutting-edge technologies. This paper aims to showcase our approach to using LLM-powered software for AI generated images, like Stable diffusion and code generation tools like ChatGPT in workshops for two relevant subjects - Analysis of Software Requirements and Specifications, as well as Artificial Intelligence. A comparison between the different available LLMs that generate images is made, and the choice between them is explained. Student feedback is shown and a general positive and motivational impact is noted during and after the workshop. A brief introduction that covers the subjects where AI is applied is made. The proposed solutions for several uses of AI in the field of higher education, more specifically software engineering, are presented. Several workshops have been made and included in the curriculum. The results of their application have been noted and an analysis is made. More propositions on further development based on the gained experience, feedback and retrieved data are made. Conclusions are made on the application of AI in higher education and different ways to utilize such tools are presented."
Dorner2024,Michael Dorner and Maximilian Capraro and Oliver Treidler and Tom Eric Kunz and Darja Smite and Ehsan Zabardast and Daniel Mendez and Krzysztof Wnuk,Taxing Collaborative Software Engineering: The Challenges for Tax Compliance in Software Engineering,IEEE Software,41,4,2024,10.1109/MS.2023.3346646,19374194,"The engineering of complex software systems is often the result of a highly collaborative effort. However, collaboration within a multinational enterprise has an overlooked legal implication when developers collaborate across national borders: It is taxable. In this article, we discuss the unsolved problem of taxing collaborative software engineering across borders."
Brsting2022,Ingo Börsting and Markus Heikamp and Marc Hesenius and Wilhelm Koop and Volker Gruhn,Software Engineering for Augmented Reality-A Research Agenda,Proceedings of the ACM on Human-Computer Interaction,6,EICS,2022,10.1145/3532205,25730142,"Augmented reality changes the way we perceive reality and how we interact with computers. However, we argue that to create augmented reality solutions, we need to rethink the way we develop software. In this paper, we review the state of the art in software engineering for augmented reality applications, derive open questions, and define a research agenda. For this purpose, we consider different engineering phases and evaluate conventional techniques regarding their applicability for AR development. In requirements engineering, we found the integration of AR experts and the associated collaboration between actors to be of key aspect in the development process. Additionally, requirements about the physical world must be considered, which in turn has a huge impact on UI design. The relevance of the physical environment is not yet sufficiently addressed in applicable techniques, which also applies to current implementation frameworks and tools, complicating the AR development process. When evaluating AR software iterations, we found interaction testing and test automation to have great potential, although they have not yet been sufficiently researched. Our paper contributes to AR research by revealing current core challenges within the AR development process and formulating explicit research questions that should be considered by future research."
Wang2022,Yingxu Wang,On the Frontiers of Software Science and Software Engineering,Frontiers in Computer Science,3,,2022,10.3389/fcomp.2021.766053,26249898,"Advances in software engineering, software science, computational intelligence, and intelligent mathematics have led to the establishment of Frontiers in Computer Science—Software (FCSS). FCSS aims to promote transdisciplinary research on software science and engineering (SSE), autonomous systems, and computational intelligence. FCSS covers not only classical empirical software engineering and industrial processes, but also contemporary topics of software science, intelligent programming languages, autonomous code generation, mathematical foundations of software, and programming knowledge bases. FCSS reports empirical studies and emerging topics in software engineering including tools, development platforms, industrial processes, management infrastructures, quality assurance schemes, big data systems, and software migrations across languages and platforms."
Anbarkhan2023,Samar Hussni Anbarkhan,A Fuzzy-TOPSIS-Based Approach to Assessing Sustainability in Software Engineering: An Industry 5.0 Perspective,Sustainability (Switzerland),15,18,2023,10.3390/su151813844,20711050,"New possibilities and challenges have evolved in the setting of the software engineering sector’s rapid transition to Industry 5.0, wherein sustainability takes centre stage. Appropriate evaluation approaches are required for analysing the long-term viability of software engineering practices within this paradigm. This study proposes an innovative approach to evaluating sustainability in software engineering within Industry 5.0 by utilising the fuzzy technique for order of preference by similarity to ideal solution (fuzzy TOPSIS) methodology. The fuzzy TOPSIS approach is effective at accounting for the inherent uncertainties as well as imprecisions related to sustainability assessments, allowing for informed decision-making. This approach helps in the recognition of the most sustainable software engineering practices in Industry 5.0 by taking into account a defined set of sustainability parameters. We rigorously analyse the current literature and expert views to provide an extensive set of relevant sustainability standards for the area of software engineering. Following that, we develop an evaluation methodology based on fuzzy TOPSIS that can handle the subjectivity as well as fuzziness inherent in sustainability evaluations. A case study with a software development company functioning in Industry 5.0 demonstrates the utility and efficacy of our suggested framework. The case study outcomes reveal the benefits and drawbacks of various software engineering methodologies in terms of sustainability. The study’s findings provide substantial information for decision-makers in the software engineering field, assisting them in making educated decisions about sustainable. Finally, this study helps to establish environmentally and socially appropriate techniques within the context of Industry 5.0."
Revoredo2021,Kate Revoredo and Djordje Djurica and Jan Mendling,A study into the practice of reporting software engineering experiments,Empirical Software Engineering,26,6,2021,10.1007/s10664-021-10007-3,15737616,"It has been argued that reporting software engineering experiments in a standardized way helps researchers find relevant information, understand how experiments were conducted and assess the validity of their results. Various guidelines have been proposed specifically for software engineering experiments. The benefits of such guidelines have often been emphasized, but the actual uptake and practice of reporting have not yet been investigated since the introduction of many of the more recent guidelines. In this research, we utilize a mixed-method study design including sequence analysis techniques for evaluating to which extent papers follow such guidelines. Our study focuses on the four most prominent software engineering journals and the time period from 2000 to 2020. Our results show that many experimental papers miss information suggested by guidelines, that no de facto standard sequence for reporting exists, and that many papers do not cite any guidelines. We discuss these findings and implications for the discipline of experimental software engineering focusing on the review process and the potential to refine and extend guidelines, among others, to account for theory explicitly."
Dbrowski2022,Jacek Dąbrowski and Emmanuel Letier and Anna Perini and Angelo Susi,Analysing app reviews for software engineering: a systematic literature review,Empirical Software Engineering,27,2,2022,10.1007/s10664-021-10065-7,15737616,"App reviews found in app stores can provide critically valuable information to help software engineers understand user requirements and to design, debug, and evolve software products. Over the last ten years, a vast amount of research has been produced to study what useful information might be found in app reviews, and how to mine and organise such information as efficiently as possible. This paper presents a comprehensive survey of this research, covering 182 papers published between 2012 and 2020. This survey classifies app review analysis not only in terms of mined information and applied data mining techniques but also, and most importantly, in terms of supported software engineering activities. The survey also reports on the quality and results of empirical evaluation of existing techniques and identifies important avenues for further research. This survey can be of interest to researchers and commercial organisations developing app review analysis techniques and to software engineers considering to use app review analysis."
Klotins2022,Eriks Klotins and Tony Gorschek and Katarina Sundelin and Erik Falk,Towards cost-benefit evaluation for continuous software engineering activities,Empirical Software Engineering,27,6,2022,10.1007/s10664-022-10191-w,15737616,"Context:: Software companies must become better at delivering software to remain relevant in the market. Continuous integration and delivery practices promise to streamline software deliveries to end-users by implementing an automated software development and delivery pipeline. However, implementing or retrofitting an organization with such a pipeline is a substantial investment, while the reporting on benefits and their relevance in specific contexts/domains are vague. Aim:: In this study, we explore continuous software engineering practices from an investment-benefit perspective. We identify what benefits can be attained by adopting continuous practices, what the associated investments and risks are, and analyze what parameters determine their relevance. Method:: We perform a multiple case study to understand state-of-practice, organizational aims, and challenges in adopting continuous software engineering practices. We compare state-of-practice with state-of-the-art to validate the best practices and identify relevant gaps for further investigation. Results:: We found that companies start the CI/CD adoption by automating and streamlining the internal development process with clear and immediate benefits. However, upgrading customers to continuous deliveries is a major obstacle due to existing agreements and customer push-back. Renegotiating existing agreements comes with a risk of losing customers and disrupting the whole organization. Conclusions:: We conclude that the benefits of CI/CD are overstated in literature without considering the contextual and domain complexities rendering some benefits infeasible. We identify the need to understand the customer and organizational perspectives further and understand the contextual requirements towards the CI/CD."
Marijan2021,Dusica Marijan and Arnaud Gotlieb,Industry-Academia research collaboration in software engineering: The Certus model,Information and Software Technology,132,,2021,10.1016/j.infsof.2020.106473,09505849,"Context: Research collaborations between software engineering industry and academia can provide significant benefits to both sides, including improved innovation capacity for industry, and real-world environment for motivating and validating research ideas. However, building scalable and effective research collaborations in software engineering is known to be challenging. While such challenges can be varied and many, in this paper we focus on the challenges of achieving participative knowledge creation supported by active dialog between industry and academia and continuous commitment to joint problem solving. Objective: This paper aims to understand what are the elements of a successful industry-academia collaboration that enable the culture of participative knowledge creation. Method: We conducted participant observation collecting qualitative data spanning 8 years of collaborative research between a software engineering research group on software V&V and the Norwegian IT sector. The collected data was analyzed and synthesized into a practical collaboration model, named the Certus Model. Results: The model is structured in seven phases, describing activities from setting up research projects to the exploitation of research results. As such, the Certus model advances other collaborations models from literature by delineating different phases covering the complete life cycle of participative research knowledge creation. Conclusion: The Certus model describes the elements of a research collaboration process between researchers and practitioners in software engineering, grounded on the principles of research knowledge co-creation and continuous commitment to joint problem solving. The model can be applied and tested in other contexts where it may be adapted to the local context through experimentation."
Suzanna2023,Suzanna and Sasmoko and Ford Lumban Gaol and Tanty Oktavia,Continuous Software Engineering for Augmented Reality,International Journal of Advanced Computer Science and Applications,14,7,2023,10.14569/IJACSA.2023.0140719,21565570,"Continuous software engineering is a new trend that has attracted increasing attention from the research community in recent years. In software engineering there are “continuous” stages that are used depending on the number of artifact repositories such as databases, meta data, virtual machines, networks and servers, various logs, and reports. Augmented Reality (AR) technology is currently growing rapidly. We can find this technology in various fields of life, but unfortunately sustainable software engineering for Augmented Reality is not found. The method shown in previous research is a general method in software engineering so that a theory is needed for sustainable software engineering for AR considering that AR is not just an ordinary application but there are 3D elements and specific components that must be met so that it can be called AR. The main idea behind this research is to find a continuous pattern from the stages of the existing method so far. For example, in general the stages of system development are planning, analysis, design, implementation and maintenance. Then after the application has been built, does it finish there? As we know software always grows and develops according to human needs. Therefore, there are continuous stages that must be patterned so that the life cycle process can be maintained. In this paper we present our initial findings about the continuous stages of continuous software engineering namely continuous planning, continuous analysis, continuous design, continuous programming, continuous integration, and continuous maintenance."
Martnez-Lpez2024,José Antonio Martínez-López and Félix García and Francisco Ruiz and Aurora Vizcaíno,Contributions of enterprise architecture to software engineering: A systematic literature review,Journal of Software: Evolution and Process,36,4,2024,10.1002/smr.2572,20477481,"Enterprise architecture is a growing trend that aims to help deal with the complexity of socio-technical systems such as human organizations, as well as their information technology and systems areas. Nevertheless, the contribution of enterprise architecture to the field of software engineering remains unclear. The purpose of this systematic literature review is to see how enterprise architecture is used in software development and maintenance practice. To this end, we first carried out a search in the SCOPUS database and then organized the papers according to the Software Engineering Body of Knowledge to determine what areas of software engineering are covered by each research study. To understand how enterprise architecture is used, we established a classification based on ISO 42010 and TOGAF. From the systematic literature review, we noticed that the early stages of development are the most impacted by the enterprise architecture. On the other hand, we observed that enterprise architecture is of assistance in the areas of engineering management, engineering processes, and engineering models and methods; these tasks are carried out by teams or managers using different, often agile, development methods or standards. In turn, we found that the most common categories are architecture descriptions; these are often used to facilitate communication and information-sharing between different stakeholders, in addition to frameworks, which will help to establish common practices in the organization related to the joint use of enterprise architecture and software development."
Raharjo2023,Teguh Raharjo and Betty Purwandari and Eko K. Budiardjo and Rina Yuniarti,The Essence of Software Engineering Framework-based Model for an Agile Software Development Method,International Journal of Advanced Computer Science and Applications,14,7,2023,10.14569/IJACSA.2023.0140788,21565570,"Agile development's rapid growth is due to its ability to address complex problems and facilitate a smooth transition from traditional methods. However, no single Agile method can fit every organization, which leads to a lack of adoption guidelines. It triggers this investigation by proposing an Agile development method model based on the Essence of software engineering framework and incorporating the common ground of popular methods such as Scrum, Kanban, Extreme programming, SAFe, Less, Nexus, Spotify Agile, Scrum of Scrums, and Disciplined Agile. The Essence of software engineering framework provides an approach for organizations to develop software development methods based on common ground or shared understanding among methods. We enhance this approach for Agile methods, resulting in a model to support organizations in developing their Agile methods and practices. Moreover, Design Science Research (DSR) was employed as a methodology to construct the artifact, demonstration, and evaluation. We demonstrated the model in an Agile product development at a national-wide bank in Indonesia. This investigation enhances Agile methods in SWEBOK's Software Engineering Models and Methods knowledge area, benefiting academics and practitioners. Practitioners can use the model as a reference to implement their Agile projects."
Ernst2023,Neil A. Ernst and Maria Teresa Baldassarre,Registered reports in software engineering,Empirical Software Engineering,28,2,2023,10.1007/s10664-022-10277-5,15737616,"Registered reports are scientific publications which begin the publication process by first having the detailed research protocol, including key research questions, reviewed and approved by peers. Subsequent analysis and results are published with minimal additional review, even if there was no clear support for the underlying hypothesis, as long as the approved protocol is followed. Registered reports can prevent several questionable research practices and give early feedback on research designs. In software engineering research, registered reports were first introduced in the International Conference on Mining Software Repositories (MSR) in 2020. They are now established in three conferences and two pre-eminent journals, including this one (EMSE). We explain the motivation for registered reports, outline the way they have been implemented in software engineering, and outline some ongoing challenges for addressing high quality software engineering research."
Sjberg2023,Dag I.K. Sjøberg and Gunnar Rye Bergersen,Construct Validity in Software Engineering,IEEE Transactions on Software Engineering,49,3,2023,10.1109/TSE.2022.3176725,19393520,"Empirical research aims to establish generalizable claims from data. Such claims may involve concepts that must be measured indirectly by using indicators. Construct validity is concerned with whether one can justifiably make claims at the conceptual level that are supported by results at the operational level. We report a quantitative analysis of the awareness of construct validity in the software engineering literature between 2000 and 2019 and a qualitative review of 83 articles about human-centric experiments published in five high-quality journals between 2015 and 2019. Over the two decades, the appearance in the literature of the term construct validity increased sevenfold. Some of the reviewed articles we reviewed employed various ways to ensure that the indicators span the concept in an unbiased manner. We also found articles that reuse formerly validated constructs. However, the articles disagree about how to define construct validity. Several interpret construct validity excessively by including threats to internal, external, or statistical conclusion validity. A few articles also include fundamental challenges of a study, such as cheating and misunderstanding of experiment material. The diversity of topics included as threats to construct validity calls for a more minimalist approach. Based on the review, we propose seven guidelines to improve how construct validity is handled and reported in software engineering."
Torkar2022,Richard Torkar and Carlo A. Furia and Robert Feldt and Francisco Gomes De Oliveira Neto and Lucas Gren and Per Lenberg and Neil A. Ernst,A Method to Assess and Argue for Practical Significance in Software Engineering,IEEE Transactions on Software Engineering,48,6,2022,10.1109/TSE.2020.3048991,19393520,"A key goal of empirical research in software engineering is to assess practical significance, which answers the question whether the observed effects of some compared treatments show a relevant difference in practice in realistic scenarios. Even though plenty of standard techniques exist to assess statistical significance, connecting it to practical significance is not straightforward or routinely done; indeed, only a few empirical studies in software engineering assess practical significance in a principled and systematic way. In this paper, we argue that Bayesian data analysis provides suitable tools to assess practical significance rigorously. We demonstrate our claims in a case study comparing different test techniques. The case study's data was previously analyzed (Afzal et al., 2015) using standard techniques focusing on statistical significance. Here, we build a multilevel model of the same data, which we fit and validate using Bayesian techniques. Our method is to apply cumulative prospect theory on top of the statistical model to quantitatively connect our statistical analysis output to a practically meaningful context. This is then the basis both for assessing and arguing for practical significance. Our study demonstrates that Bayesian analysis provides a technically rigorous yet practical framework for empirical software engineering. A substantial side effect is that any uncertainty in the underlying data will be propagated through the statistical model, and its effects on practical significance are made clear. Thus, in combination with cumulative prospect theory, Bayesian analysis supports seamlessly assessing practical significance in an empirical software engineering context, thus potentially clarifying and extending the relevance of research for practitioners."
Nazir2020,Sumaira Nazir and Nargis Fatima and Suriayati Chuprat and Haslina Sarkan and F. Nurulhuda and Nilam N.A. Sjarif,Sustainable software engineering: A perspective of individual sustainability,"International Journal on Advanced Science, Engineering and Information Technology",10,2,2020,10.18517/ijaseit.10.2.10190,24606952,"Sustainable software engineering is a mean of developing sustainable software with sustainable software engineering process activities while balancing its various dimensions for instance economic, environmental, social, technical and individual. It is conveyed that the economic, technical, environmental and social dimensions are explored to satisfactory degree however the individual dimension of sustainable software engineering which is concerned with wellbeing of software engineers is not explored to satisfactory degree with respect to its understanding and challenges. Therefore, the aim of the study is to highlight and prioritize the challenges regarding individual sustainability dimension. The study also provides the mitigation strategies for the top five individual sustainability challenges. The systematic literature review has been performed to report the challenges and mitigation strategies. The study finding shows that lack of domain knowledge, lack of methodologies and tool support, lack of education, varying and unidentified situations and lack of sustainable software engineering practices are top most challenges regarding individual sustainability. These challenges need an urgent attention to achieve the goal of sustainable software engineering. The study also reports various mitigation strategies to overcome the risk of identified top most individual sustainability challenges such as to introduce sustainable software engineering education and knowledge in software engineering curricula, development of knowledge sharing frameworks and awareness regarding unclear and varying situations for each software engineering activity etc. The study will be beneficial for sustainable software engineering body of knowledge, sustainable software engineering practitioners and researchers by providing classified list of individual sustainability challenges and their mitigation strategies."
Demi2021,Selina Demi and Ricardo Colomo-Palacios and Mary Sánchez-Gordón,Software engineering applications enabled by blockchain technology: A systematic mapping study,Applied Sciences (Switzerland),11,7,2021,10.3390/app11072960,20763417,"The novel, yet disruptive blockchain technology has witnessed growing attention, due to its intrinsic potential. Besides the conventional domains that benefit from such potential, such as finance, supply chain and healthcare, blockchain use cases in software engineering have emerged recently. In this study, we aim to contribute to the body of knowledge of blockchain-oriented software engineering by providing an adequate overview of the software engineering applications enabled by blockchain technology. To do so, we carried out a systematic mapping study and iden-tified 22 primary studies. Then, we extracted data within the research type, research topic and contribution type facets. Findings suggest an increasing trend of studies since 2018. Additionally, findings reveal the potential of using blockchain technologies as an alternative to centralized sys-tems, such as GitHub, Travis CI, and cloud-based package managers, and also to establish trust between parties in collaborative software development. We also found out that smart contracts can enable the automation of a variety of software engineering activities that usually require human reasoning, such as the acceptance phase, payments to software engineers, and compliance adher-ence. In spite of the fact that the field is not yet mature, we believe that this systematic mapping study provides a holistic overview that may benefit researchers interested in bringing blockchain to the software industry, and practitioners willing to understand how blockchain can transform the software development industry."
Davoudian2020,Ali Davoudian and Mengchi Liu,Big Data Systems: A Software Engineering Perspective,ACM Computing Surveys,53,5,2020,10.1145/3408314,15577341,"Big Data Systems (BDSs) are an emerging class of scalable software technologies whereby massive amounts of heterogeneous data are gathered from multiple sources, managed, analyzed (in batch, stream or hybrid fashion), and served to end-users and external applications. Such systems pose specific challenges in all phases of software development lifecycle and might become very complex by evolving data, technologies, and target value over time. Consequently, many organizations and enterprises have found it difficult to adopt BDSs. In this article, we provide insight into three major activities of software engineering in the context of BDSs as well as the choices made to tackle them regarding state-of-the-art research and industry efforts. These activities include the engineering of requirements, designing and constructing software to meet the specified requirements, and software/data quality assurance. We also disclose some open challenges of developing effective BDSs, which need attention from both researchers and practitioners."
Heldal2024,Rogardt Heldal and Ngoc Thanh Nguyen and Ana Moreira and Patricia Lago and Leticia Duboc and Stefanie Betz and Vlad C. Coroamă and Birgit Penzenstadler and Jari Porras and Rafael Capilla and Ian Brooks and Shola Oyedeji and Colin C. Venters,Sustainability competencies and skills in software engineering: An industry perspective,Journal of Systems and Software,211,,2024,10.1016/j.jss.2024.111978,01641212,"Context: Achieving the UN Sustainable Development Goals (SDGs) demands a shift by industry, governments, society, and individuals to reach adequate levels of awareness and actions to address sustainability challenges. Software systems will play an important role in moving towards these targets. Sustainability skills are necessary to support the development of software systems and to provide sustainable IT-supported services for citizens. Gap: While there is a growing number of academic bodies including sustainability education in engineering and computer science curricula, there is not yet comprehensive research on the competencies and skills required by IT professionals to develop such systems. Research goal: This study aims to identify the industrial sustainability needs for education and training from software engineers’ perspective. For this, we answer the following questions: (1) what are the interests of organisations with an IT division with respect to sustainability? (2) what do organisations want to achieve with respect to sustainability, and how? and (3) what are the sustainability-related competencies and skills that organisations need to achieve their sustainability goals? Methodology: We conducted a qualitative study with interviews and focus groups with experts from twenty-eight organisations with an IT division from nine countries to understand their interests, goals, and achievements related to sustainability, and the skills and competencies needed to achieve their goals. Results: Our findings show that organisations are interested in sustainability, both idealistically and increasingly for core business reasons. They seek to improve the sustainability of software processes and products but encounter difficulties, like the trade-off between short-term financial profitability and long-term sustainability goals or an unclear understanding of sustainability concepts from a software engineering perspective. To fill these gaps, they have promoted in-house training courses, collaborated with universities, and sent employees to external training. The acquired competencies should support translating environmental and social benefits into economic ones and make sustainability an integral part of software development."
Lopez-Fernandez2021,Daniel Lopez-Fernandez and Aldo Gordillo and Fernando Ortega and Agustin Yague and Edmundo Tovar,LEGO® Serious Play in Software Engineering Education,IEEE Access,9,,2021,10.1109/ACCESS.2021.3095552,21693536,"Nowadays, it is mandatory to complement the traditional learning methods with active ones that enhance the student's motivation and facilitate the development of technical and soft competences. LEGO®Serious Play is an experiential methodology designed to enhance innovation and performance in the business world and it is fully aligned with these needs. Previous researches show that this methodology has great potential in engineering education, and specifically in software engineering education. This paper presents an original LEGO®Serious Play activity to teach in a playful and active way software engineering concepts such as life cycle models and software development activities, which can be extrapolated to other engineering fields. This activity was validated through a case study involving 242 computer science students and it was supported by quantitative and qualitative data gathered from a survey and a post-test. The results indicate that the students found this activity highly fun and motivating as well as very useful to learn about the addressed topics and develop soft skills."
Kumeno2020,Fumihiro Kumeno,Software engineering challenges for machine learning applications: A literature review,Intelligent Decision Technologies,13,4,2020,10.3233/IDT-190160,18758843,"Machine learning techniques, especially deep learning, have achieved remarkable breakthroughs over the past decade. At present, machine learning applications are deployed in many fields. However, the outcomes of software engineering researches are not always easily utilized in the development and deployment of machine learning applications. The main reason for this difficulty is the many differences between machine learning applications and traditional information systems. Machine learning techniques are evolving rapidly, but face inherent technical and non-technical challenges that complicate their lifecycle activities. This review paper attempts to clarify the software engineering challenges for machine learning applications that either exist or potentially exist by conducting a systematic literature collection and by mapping the identified challenge topics to knowledge areas defined by the Software Engineering Body of Knowledge (Swebok)."
Shaikh2021,Muhammad Khalid Shaikh,How to form a software engineering capstone team?,Heliyon,7,4,2021,10.1016/j.heliyon.2021.e06629,24058440,"This research paper answers the question that how shall the students of software engineering undergraduate courses form teams for the capstone projects that can be cohesive too. In this research, 128 criteria for team formation are proposed for building teams for self-managing software engineering capstone projects. A comparison is also conducted to ascertain the level of cohesion among those teams that were formed using the proposed criteria and those that were not formed using the proposed criteria. The criteria were identified through a combination of qualitative questionnaire survey targeted at the graduated students of the past batches of Computer Science degree program and through synthesizing the literature on engineering capstone project teams identified under the guidance of KSAO framework for software engineering students. To check the effectiveness of the criteria, 100 students were asked to form the teams using the proposed criteria and other 100 students formed the teams without the proposed criteria. Those students that had used the proposed criteria for building teams and those that had formed teams without using the proposed criteria were asked to fill the modified Group Environment Questionnaire to ascertain the level of cohesion among the team members. The results were analyzed qualitatively and through descriptive quantification. The results show that the level of cohesion in teams that were formed using the proposed team building criteria was higher. There was a need for team building criteria in the literature on software engineering capstone project teams that conforms to a conceptual, theoretical framework; this gap is now filled through this research. This paper may also serve as a literature review paper for some readers."
Yang2023,Xinli Yang and Jingjing Liu and Denghui Zhang,A Comprehensive Taxonomy for Prediction Models in Software Engineering,Information (Switzerland),14,2,2023,10.3390/info14020111,20782489,"Applying prediction models to software engineering is an interesting research area. There have been many related studies which leverage prediction models to achieve good performance in various software engineering tasks. With more and more researches in software engineering leverage prediction models, there is a need to sort out related studies, aiming to summarize which software engineering tasks prediction models can apply to and how to better leverage prediction models in these tasks. This article conducts a comprehensive taxonomy on prediction models applied to software engineering. We review 136 papers from top conference proceedings and journals in the last decade and summarize 11 research topics prediction models can apply to. Based on the papers, we conclude several big challenges and directions. We believe that the comprehensive taxonomy will help us understand the research area deeper and infer several useful and practical implications."
Kitchenham2023,Barbara Kitchenham and Lech Madeyski and David Budgen,SEGRESS: Software Engineering Guidelines for REporting Secondary Studies,IEEE Transactions on Software Engineering,49,3,2023,10.1109/TSE.2022.3174092,19393520,"Context: Several tertiary studies have criticized the reporting of software engineering secondary studies. Objective: Our objective is to identify guidelines for reporting software engineering (SE) secondary studies which would address problems observed in the reporting of software engineering systematic reviews (SRs). Method: We review the criticisms of SE secondary studies and identify the major areas of concern. We assess the PRISMA 2020 (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) statement as a possible solution to the need for SR reporting guidelines, based on its status as the reporting guideline recommended by the Cochrane Collaboration whose SR guidelines were a major input to the guidelines developed for SE. We report its advantages and limitations in the context of SE secondary studies. We also assess reporting guidelines for mapping studies and qualitative reviews, and compare their structure and content with that of PRISMA 2020. Results: Previous tertiary studies confirm that reports of secondary studies are of variable quality. However, ad hoc recommendations that amend reporting standards may result in unnecessary duplication of text. We confirm that the PRISMA 2020 statement addresses SE reporting problems, but is mainly oriented to quantitative reviews, mixed-methods reviews and meta-analyses. However, we show that the PRISMA 2020 item definitions can be extended to cover the information needed to report mapping studies and qualitative reviews. Conclusions: In this paper and its Supplementary Material, we present and illustrate an integrated set of guidelines called SEGRESS (Software Engineering Guidelines for REporting Secondary Studies), suitable for quantitative systematic reviews (building upon PRISMA 2020), mapping studies (PRISMA-ScR), and qualitative reviews (ENTREQ and RAMESES), that addresses reporting problems found in current SE SRs."
Ahmad2021,Aakash Ahmad and Mahdi Fahmideh and Ahmed B. Altamimi and Iyad Katib and Aiiad Albeshri and Abdulrahman Alreshidi and Adwan Alownie Alanazi and Rashid Mehmood,Software Engineering for IoT-Driven Data Analytics Applications,IEEE Access,9,,2021,10.1109/ACCESS.2021.3065528,21693536,"Internet of Things Driven Data Analytics (IoT-DA) has the potential to excel data-driven operationalisation of smart environments. However, limited research exists on how IoT-DA applications are designed, implemented, operationalised, and evolved in the context of software and system engineering life-cycle. This article empirically derives a framework that could be used to systematically investigate the role of software engineering (SE) processes and their underlying practices to engineer IoT-DA applications. First, using existing frameworks and taxonomies, we develop an evaluation framework to evaluate software processes, methods, and other artefacts of SE for IoT-DA. Secondly, we perform a systematic mapping study to qualitatively select 16 processes (from academic research and industrial solutions) of SE for IoT-DA. Thirdly, we apply our developed evaluation framework based on 17 distinct criterion (a.k.a. process activities) for fine-grained investigation of each of the 16 SE processes. Fourthly, we apply our proposed framework on a case study to demonstrate development of an IoT-DA healthcare application. Finally, we highlight key challenges, recommended practices, and the lessons learnt based on framework's support for process-centric software engineering of IoT-DA. The results of this research can facilitate researchers and practitioners to engineer emerging and next-generation of IoT-DA software applications."
Daun2023,Marian Daun and Jennifer Brings,How ChatGPT Will Change Software Engineering Education,,1,,2023,10.1145/3587102.3588815,1942647X,"This position paper discusses the potential for using generative AIs like ChatGPT in software engineering education. Currently, discussions center around potential threats emerging from student's use of ChatGPT. For instance, generative AI will limit the usefulness of graded homework dramatically. However, there exist potential opportunities as well. For example, ChatGPT's ability to understand and generate human language allows providing personalized feedback to students, and can thus accompany current software engineering education approaches. This paper highlights the potential for enhancing software engineering education. The availability of generative AI will improve the individualization of education approaches. In addition, we discuss the need to adapt software engineering curricula to the changed profiles of software engineers. Moreover, we point out why it is important to provide guidance for using generative AI and, thus, integrate it in courses rather than accepting the unsupervised use by students, which can negatively impact the students' learning."
Al-Sarayreh2021,Khalid T. Al-Sarayreh and Kenza Meridji and Alain Abran,Software engineering principles: A systematic mapping study and a quantitative literature review,"Engineering Science and Technology, an International Journal",24,3,2021,10.1016/j.jestch.2020.11.005,22150986,"Software engineering, a fairly recent engineering discipline, is still evolving without a wide consensus on a body of fundamental principles as in traditional engineering fields with their own long-established principles originating from physics, chemistry and mathematics. This paper reports on a systematic mapping study (SMS) that identified 30 papers and books from 1969 to January 2020, each proposing their own sets of software engineering principles (SEP). Within these studies a total of 592 SEP were proposed, these studies were reviewed and classified on the basis of four mapping questions examining publication trends, use of explicit criteria for the proposed SEP, whether authors clearly described a methodology to come up with the proposed SEP, and the applicability of SEP throughout the development process. The key finding in this study are: a) the majority of the studies were published over two decades from 1989 to 2009, and then the publication rate slowed in the following decade; b) the largest number of SEP, by far, was proposed by Davis; c) only six studies used explicit criteria to identify their proposed SEP, and the other 24 studies identified their principles based on their own analysis without traceability to a methodology or selection criteria; d) most authors did not identify which principles to use in specific contexts of the software engineering domain; e) only two studies used some of the proposed SEP throughout the software development process."
Pedreira2020,Oscar Pedreira and Felix Garcia and Mario Piattini and Alejandro Cortinas and Ana Cerdeira-Pena,An Architecture for Software Engineering Gamification,Tsinghua Science and Technology,25,6,2020,10.26599/TST.2020.9010004,18787606,"Gamification has been applied in software engineering to improve quality and results by increasing people's motivation and engagement. A systematic mapping has identified research gaps in the field, one of them being the difficulty of creating an integrated gamified environment comprising all the tools of an organization, since most existing gamified tools are custom developments or prototypes. In this paper, we propose a gamification software architecture that allows us to transform the work environment of a software organization into an integrated gamified environment, i.e., the organization can maintain its tools, and the rewards obtained by the users for their actions in different tools will mount up. We developed a gamification engine based on our proposal, and we carried out a case study in which we applied it in a real software development company. The case study shows that the gamification engine has allowed the company to create a gamified workplace by integrating custom-developed tools and off-The-shelf tools such as Redmine, TestLink, or JUnit, with the gamification engine. Two main advantages can be highlighted: (i) our solution allows the organization to maintain its current tools, and (ii) the rewards for actions in any tool accumulate in a centralized gamified environment."
Noorar2022,Adeeb Noorar,Improving bioinformatics software quality through incorporation of software engineering practices,PeerJ Computer Science,8,,2022,10.7717/PEERJ-CS.839,23765992,"Background: Bioinformatics software is developed for collecting, analyzing, integrating, and interpreting life science datasets that are often enormous. Bioinformatics engineers often lack the software engineering skills necessary for developing robust, maintainable, reusable software. This study presents review and discussion of the findings and efforts made to improve the quality of bioinformatics software. Methodology: A systematic review was conducted of related literature that identifies core software engineering concepts for improving bioinformatics software development: requirements gathering, documentation, testing, and integration. The findings are presented with the aim of illuminating trends within the research that could lead to viable solutions to the struggles faced by bioinformatics engineers when developing scientific software. Results: The findings suggest that bioinformatics engineers could significantly benefit from the incorporation of software engineering principles into their development efforts. This leads to suggestion of both cultural changes within bioinformatics research communities as well as adoption of software engineering disciplines into the formal education of bioinformatics engineers. Open management of scientific bioinformatics development projects can result in improved software quality through collaboration amongst both bioinformatics engineers and software engineers. Conclusions: While strides have been made both in identification and solution of issues of particular import to bioinformatics software development, there is still room for improvement in terms of shifts in both the formal education of bioinformatics engineers as well as the culture and approaches of managing scientific bioinformatics research and development efforts."
Wohlin2021,Claes Wohlin,"Case Study Research in Software Engineering—It is a Case, and it is a Study, but is it a Case Study?",Information and Software Technology,133,,2021,10.1016/j.infsof.2021.106514,09505849,"Background: Case studies are regularly published in the software engineering literature, and guidelines for conducting case studies are available. Based on a perception that the label “case study” is assigned to studies that are not case studies, an investigation has been conducted. Objective: The aim was to investigate whether or not the label “case study” is correctly used in software engineering research. Method: To address the objective, 100 recent articles found through Scopus when searching for case studies in software engineering have been investigated and classified. Results: Unfortunately, the perception of misuse of the label “case study” is correct. Close to 50% of the articles investigated were judged as not being case studies according to the definition of a case study. Conclusions: We either need to ensure correct use of the label “case study”, or we need another label for its definition. Given that “case study” is a well-established label, it is probably impossible to change the label. Thus, we introduce an alternative definition of case study emphasising its real-life context, and urge researchers to carefully follow the definition of different research methods when presenting their research."
Yang2022,Yanming Yang and Xin Xia and David Lo and Tingting Bi and John Grundy and Xiaohu Yang,Predictive Models in Software Engineering: Challenges and Opportunities,ACM Transactions on Software Engineering and Methodology,31,3,2022,10.1145/3503509,15577392,"Predictive models are one of the most important techniques that are widely applied in many areas of software engineering. There have been a large number of primary studies that apply predictive models and that present well-performed studies in various research domains, including software requirements, software design and development, testing and debugging, and software maintenance. This article is a first attempt to systematically organize knowledge in this area by surveying a body of 421 papers on predictive models published between 2009 and 2020. We describe the key models and approaches used, classify the different models, summarize the range of key application areas, and analyze research results. Based on our findings, we also propose a set of current challenges that still need to be addressed in future work and provide a proposed research road map for these opportunities."
Sofian2022,Hazrina Sofian and Nur Arzilawati Md Yunus and Rodina Ahmad,Systematic Mapping: Artificial Intelligence Techniques in Software Engineering,IEEE Access,10,,2022,10.1109/ACCESS.2022.3174115,21693536,"Artificial Intelligence (AI) has become a core feature of today's real-world applications, making it a trending topic within the software engineering (SE) community. The rise in the availability of AI techniques encompasses the capability to make rapid, automated, impactful decisions and predictions, leading to the adoption of AI techniques in SE. With industry revolution 4.0, the role of software engineering has become critical for developing productive, efficient, and quality software. Thus, there is a major need for AI techniques to be applied to enhance and improve the critical activities within the software engineering phases. Software is developed through intelligent software engineering phases. This paper concerns a systematic mapping study that aimed to characterize the publication landscape of AI techniques in software engineering. Gaps are identified and discussed by mapping these AI techniques against the SE phases to which they contributed. Many systematic mapping review papers have been produced only for a specific AI technique or a specific SE phase or activity. Hence, to our best of knowledge within the last decade, there is no systematic mapping review that has fully explored the overall trends in AI techniques and their application to all SE phases."
Ahonen2023,Andrei Ahonen and Marea de Koning and Tyrone Machado and Reza Ghabcheloo and Outi Sievi-Korte,An exploratory study of software engineering in heavy-duty mobile machine automation,Robotics and Autonomous Systems,165,,2023,10.1016/j.robot.2023.104424,09218890,"As the amount and complexity of software for automating heavy-duty mobile machinery is increasing, software engineering in this domain is becoming more important. To characterize the industry's current state of software engineering and its issues to guide future research, we performed an empirical exploratory study. We interviewed 16 software engineering professionals from 13 different companies conducting business in heavy-duty mobile machines and their automation. The interviews were analyzed qualitatively, and quantification of the analysis results is presented. We first create an overview of software engineering in the heavy-duty mobile machinery industry. We then identify problem areas affecting software development and discuss some of the possible solutions found in literature. Our findings indicate that the major problem areas faced in the industry that require more research are its digital transformation, autonomous machine functional safety, low availability of workforce for developing software for robotic mobile machines and the lack of established software standards."
Assyne2022,Nana Assyne and Hadi Ghanbari and Mirja Pulkkinen,The state of research on software engineering competencies: A systematic mapping study,Journal of Systems and Software,185,,2022,10.1016/j.jss.2021.111183,01641212,"Considering the critical role of software in modern societies, we face an urgent need to educate more competent software professionals. Software engineering competencies (SEC) are considered the backbone of successfully developing software products. Consequently, SEC has become a hotspot for software engineering research and practice. Although scientific literature on SEC is not lacking, to our knowledge, a comprehensive overview of the current state of SEC research is missing. To that end, we conducted an extensive and systematic review of the SEC literature. We provide an overview of the current state of research on SEC, with a particular focus on common SEC research areas. In addition to reporting the available SEC models and frameworks, we compile a list of 49 unique essential competencies of software professionals. Finally, we highlight several gaps in the literature that deserve further research. In particular, we call for a better understanding of how the essential competencies of software professionals change over time, as well as fresh accounts of the essential competencies of software professionals. Additionally, considering recent shifts toward Agile and DevOps methods, future research must explore the competencies required for developing software products in modern development environments."
Engstrm2020,Emelie Engström and Margaret Anne Storey and Per Runeson and Martin Höst and Maria Teresa Baldassarre,How software engineering research aligns with design science: a review,Empirical Software Engineering,25,4,2020,10.1007/s10664-020-09818-7,15737616,"Background: Assessing and communicating software engineering research can be challenging. Design science is recognized as an appropriate research paradigm for applied research, but is rarely explicitly used as a way to present planned or achieved research contributions in software engineering. Applying the design science lens to software engineering research may improve the assessment and communication of research contributions. Aim: The aim of this study is 1) to understand whether the design science lens helps summarize and assess software engineering research contributions, and 2) to characterize different types of design science contributions in the software engineering literature. Method: In previous research, we developed a visual abstract template, summarizing the core constructs of the design science paradigm. In this study, we use this template in a review of a set of 38 award winning software engineering publications to extract, analyze and characterize their design science contributions. Results: We identified five clusters of papers, classifying them according to their different types of design science contributions. Conclusions: The design science lens helps emphasize the theoretical contribution of research output—in terms of technological rules—and reflect on the practical relevance, novelty and rigor of the rules proposed by the research."
Tulili2023,Tien Rahayu Tulili and Andrea Capiluppi and Ayushi Rastogi,Burnout in software engineering: A systematic mapping study,Information and Software Technology,155,,2023,10.1016/j.infsof.2022.107116,09505849,"Context: Burnout is a work-related syndrome that, similar to many occupations, influences most software developers. For decades, studies in software engineering(SE) have explored the causes of burnout and its consequences among IT professionals. Objective: This paper is a systematic mapping study (SMS) of the studies on burnout in SE, exploring its causes and consequences, and how it is studied (e.g., choice of data). Method: We conducted a systematic mapping study and identified 92 relevant research articles dating as early as the early 1990s, focusing on various aspects and approaches to detect burnout in software developers and IT professionals. Results: Our study shows that early research on burnout was primarily qualitative, which has steadily moved to more quantitative, data-driven in the last decade. The emergence of machine learning (ML) approaches to detect burnout in developers has become a de-facto standard. Conclusion: Our study summarises what we now know about burnout, how software artifacts indicate burnout, and how machine learning can help its early detection. As a comprehensive analysis of past and present research works in the field, we believe this paper can help future research and practice focus on the grand challenges ahead and offer necessary tools."
Rodrguez-Prez2021,Gema Rodríguez-Pérez and Reza Nadri and Meiyappan Nagappan,Perceived diversity in software engineering: a systematic literature review,Empirical Software Engineering,26,5,2021,10.1007/s10664-021-09992-2,15737616,"We define perceived diversity as the diversity factors that individuals are born with. Perceived diversity in Software Engineering has been recognized as a high-value team property and companies are willing to increase their efforts to create more diverse work teams. The current diversity state-of-the-art shows that gender diversity studies have been growing during the past decade, and they have shown the benefits of including women in software teams. However, less is known about how other perceived diversity factors such as race, nationality, disability, and age of developers are related to Software Engineering. Through a systematic literature review, we aim to clarify the research area concerned with perceived diversity in Software Engineering. Our goal is to identify (1) what issues have been studied and what results have been reported; (2) what methods, tools, models, and processes have been proposed to help perceived diversity issues; and (3) what limitations have been reported when studying perceived diversity in Software Engineering. Furthermore, our ultimate goal is to identify gaps in the current literature and create a call for future action in perceived diversity in Software Engineering. Our results indicate that the individual studies have typically had a gender diversity perspective focusing on showing gender bias or gender differences instead of developing methods and tools to mitigate the gender diversity issues faced in SE. Moreover, perceived diversity aspects related to SE participants’ race, age, and disability need to be further analyzed in Software Engineering research. From our systematic literature review, we conclude that researchers need to consider a wider set of perceived diversity aspects for future research."
Eramo2024,Romina Eramo and Michele Tucci and Daniele Di Pompeo and Vittorio Cortellessa and Antinisca Di Marco and Davide Taibi,Architectural support for software performance in continuous software engineering: A systematic mapping study,Journal of Systems and Software,207,,2024,10.1016/j.jss.2023.111833,01641212,"The continuous software engineering paradigm is gaining popularity in modern development practices, where the interleaving of design and runtime activities is induced by the continuous evolution of software systems. In this context, performance assessment is not easy, but recent studies have shown that architectural models evolving with the software can support this goal. In this paper, we present a mapping study aimed at classifying existing scientific contributions that deal with the architectural support for performance-targeted continuous software engineering. We have applied the systematic mapping methodology to an initial set of 215 potentially relevant papers and selected 66 primary studies that we have analyzed to characterize and classify the current state of research. This classification helps to focus on the main aspects that are being considered in this domain and, mostly, on the emerging findings and implications for future research. Editor's note: Open Science material was validated by the Journal of Systems and Software Open Science Board. (see [https://www.sciencedirect.com/science/article/pii/S0164121221002168] for an example for where to place the statement and how to format it)."
Khan2021,Firoz Khan and R. Lakshmana Kumar and Seifedine Kadry and Yunyoung Nam,The future of software engineering: Visions of 2025 and beyond,International Journal of Electrical and Computer Engineering,11,4,2021,10.11591/ijece.v11i4.pp3443-3450,20888708,"In the current technological scenario of the industry and businesses, there has been increasing need of software within systems and also an increasing demand being put onto software-intensive systems. This in effect will lead to a significant evolution of software engineering processes over the next twenty years. This is due to the fact of emerging technological advancements like Industry 4.0 and Internet of Things in the IT field, among other new developments. This paper addresses and tries to analyses the key research challenges being faced by the software engineering field and articulates information that is derived from the key research specializations within software engineering. The paper analyses the past and current trends in software engineering. The future of software engineering is also looked with respect to Industry 4.0 which including emerging technological platforms like Internet of Things. The societal impact aspect of future trends in software engineering is also addressed in this paper."
Santhanam2022,Sivasurya Santhanam and Tobias Hecking and Andreas Schreiber and Stefan Wagner,Bots in software engineering: a systematic mapping study,PeerJ Computer Science,8,,2022,10.7717/peerj-cs.866,23765992,"Bots have emerged from research prototypes to deployable systems due to the recent developments in machine learning, natural language processing and understanding techniques. In software engineering, bots range from simple automated scripts to decision-making autonomous systems. The spectrum of applications of bots in software engineering is so wide and diverse, that a comprehensive overview and categorization of such bots is needed. Existing works considered selective bots to be analyzed and failed to provide the overall picture. Hence it is significant to categorize bots in software engineering through analyzing why, what and how the bots are applied in software engineering. We approach the problem with a systematic mapping study based on the research articles published in this topic. This study focuses on classification of bots used in software engineering, the various dimensions of the characteristics, the more frequently researched area, potential research spaces to be explored and the perception of bots in the developer community. This study aims to provide an introduction and a broad overview of bots used in software engineering. Discussions of the feedback and results from several studies provide interesting insights and prospective future directions."
Schneider2022,Christoph Schneider and Stefanie Betz,Transformation²: Making software engineering accountable for sustainability,Journal of Responsible Technology,10,,2022,10.1016/j.jrt.2022.100027,26666596,"Software engineering, as a central practice of digitalization, needs to become accountable for sustainability. In light of the ecological crises and the tremendous impact of digital systems on reshaping economic and social arrangements - often with negative side-effects - we need a sustainability transformation of the digital transformation. However, this is a complex and long-term task. In this article we combine an analysis of accountability arrangements in software engineering and a model of sustainability transformations to trace how certain dynamics are starting to make software engineering accountable for sustainability in the technological, cultural, economic and governance domains. The article discusses existing approaches for sustainable software engineering and software engineering for sustainability, traces emerging discourses that connect digitalization and sustainability, highlights new digital business models that may support sustainability and shows governance efforts to highlight “green and digital” policy problems. Yet, we argue that these are so far niche dynamics and that a sustainability transformation requires a collective and long-lasting effort to engender systemic changes. The goal should be to create varied accountability arrangements for sustainability in software engineering which is embedded in complex ways in society and economy."
Tenhunen2023,Saara Tenhunen and Tomi Männistö and Matti Luukkainen and Petri Ihantola,A systematic literature review of capstone courses in software engineering,Information and Software Technology,159,,2023,10.1016/j.infsof.2023.107191,09505849,"Context: Tertiary education institutions aim to prepare their computer science and software engineering students for working life. While much of the technical principles are covered in lower-level courses, team-based capstone courses are a common way to provide students with hands-on experience and teach soft skills. Objective: This paper explores the characteristics of project-based software engineering capstone courses presented in the literature. The goal of this work is to understand the pros and cons of different approaches by synthesising the various aspects of software engineering capstone courses and related experiences. Method: In a systematic literature review for 2007–2022, we identified 127 articles describing real-world capstone courses. These articles were analysed based on their presented course characteristics and the reported course outcomes. Results: The characteristics were synthesised into a taxonomy consisting of duration, team sizes, client and project sources, project implementation, and student assessment. We found out that capstone courses generally last one semester and divide students into groups of 4–5 where they work on a project for a client. For a slight majority of courses, the clients are external to the course staff and students are often expected to produce a proof-of-concept level software product as the main end deliverable. The courses generally include various forms of student assessment both during and at the end of the course. Conclusions: This paper provides researchers and educators with a classification of characteristics of software engineering capstone courses based on previous research. We also further synthesise insights on the reported course outcomes. Our review study aims to help educators to identify various ways of organising capstones and effectively plan and deliver their own capstone courses. The characterisation also helps researchers to conduct further studies on software engineering capstones."
Albonico2023,Michel Albonico and Milica Đorđević and Engel Hamer and Ivano Malavolta,Software engineering research on the Robot Operating System: A systematic mapping study,Journal of Systems and Software,197,,2023,10.1016/j.jss.2022.111574,01641212,"The Robot Operating System (ROS) has become the de-facto standard framework for robotics software, and a great part of commercial robots is expected to have at least one ROS package on board in the coming years. For good quality, robotics software should rely on strong software engineering principles. In this paper, we perform a systematic mapping study on several works in software engineering on ROS, published at the top software engineering and robotics venues. Our goal is to analyze and evaluate such state-of-the-art regarding its relevance to the robotics software industry. The potentially-relevant studies are subject to a rigorously defined selection process. This results in a set of 63 primary studies on software engineering research on ROS. Those primary studies are then qualitatively analyzed according to a rigorously-defined classification framework. The results are of interest to both researchers and practitioners: (i) we provide an up-to-date overview of the state of the art on software engineering research on ROS and its potential for industrial adoption, (ii) a broad discussion of the research area as a whole, and (iii) point out routes of action for a better alignment between research and industry."
Dada2022,Oluwaseun Alexander Dada and Ismaila Temitayo Sanusi,The adoption of Software Engineering practices in a Scrum environment,"African Journal of Science, Technology, Innovation and Development",14,6,2022,10.1080/20421338.2021.1955431,20421346,"The competition in the software market demands that the time required for any software product to reach the market be reduced if the product is to survive competition from other developers. The pursuit of this goal has led to the adoption of agile software development methodologies. While other agile methodologies provide guidelines as to the software engineering (SE) practices to be used during the development lifecycle, Scrum does not. The purpose of this study is twofold: first, to identify the usage and level of importance of software engineering practices in the Scrum development environment; and second, to investigate how Scrum teams adopt an appropriate set of SE techniques and whether a hybrid Scrum/Extreme Programming (XP) methodology is an appropriate approach to take. This research was conducted by examining sample data from five organizations using the Scrum methodology. The sample included a range of industries including communications and embedded systems, financial asset management, software development houses and consulting firms in South Africa. The study employed a mixed method approach. A key finding was that, regardless of the fact that Scrum does not explicitly recommend engineering practices, there was extensive use of these practices by all of the participating organizations. The study also found that the lack of software engineering practices in Scrum does not constitute a barrier to a successful adoption of Scrum, provided the ‘inspect and adapt’ principle inherent in Scrum is properly followed. The study discusses the findings, explains the implications and suggests future research."
Cico2021,Orges Cico and Letizia Jaccheri and Anh Nguyen-Duc and He Zhang,Exploring the intersection between software industry and Software Engineering education - A systematic mapping of Software Engineering Trends,Journal of Systems and Software,172,,2021,10.1016/j.jss.2020.110736,01641212,"Context: Software has become ubiquitous in every corner of modern societies. During the last five decades, software engineering has also changed significantly to advance the development of various types and scales of software products. In this context, Software Engineering Education plays an important role in keeping students updated with software technologies, processes, and practices that are popular in industries. Objective: We investigate from literature the extent Software Engineering Education addresses major Software Engineering Trends in the academic setting. Method: We conducted a systematic mapping study about teaching major Software Engineering Trends in project courses. We classified 126 papers based on their investigated Software Engineering Trends, specifically Software Engineering processes and practices, teaching approaches, and the evolution of Software Engineering Trends over time. Results: We reveal that Agile Software Development is the major trend. The other Trends, i.e., Software Implementation, Usability and Value, Global Software Engineering, and Lean Software Startup, are relatively small in the academic setting, but continuously growing in the last five years. System of Systems is the least investigated among all Trends. Conclusions: The study points out the possible gaps between Software Industry and Education, which implies actionable insights for researchers, educators, and practitioners."
Adamczyk2021,Jakub Adamczyk and Filip Malawski,COMPARISON OF MANUAL AND AUTOMATED FEATURE ENGINEERING FOR DAILY ACTIVITY CLASSIFICATION IN MENTAL DISORDER DIAGNOSIS,Computing and Informatics,40,4,2021,10.31577/CAI_2021_4_850,25858807,"Motor activity data allows for analysis of complex behavioral patterns, including the diagnosis of mental disorders, such as depression or schizophrenia. However, the classification of actigraphy signals remains a challenge. The main reasons are small datasets and the need for sophisticated feature engineering. The recent development of AutoML approaches allows for automating feature extraction and selection. In this work, we compare automatic and manual feature engineering for applications in mental health. We also analyze classifier evaluation methods for small datasets. The automated approach results in better classification, as measured with several metrics, and in a shorter, cleaner code, providing software engineering advantages."
Abdelazim2020,Khaled Abdelazim and Ramadan Moawad and Essam Elfakharany,A Framework for Requirements Prioritization Process in Agile Software Development,,1454,1,2020,10.1088/1742-6596/1454/1/012001,17426596,"Requirements engineering is a crucial phase of software engineering, and requirements prioritization is an essential stage of requirements engineering particularly in agile software development. Requirements prioritization goals at eliciting which requirements of software need to be covered in a particular release. The key point is which requirement will be selected in the next iteration and which one will be delayed to other iterations for minimizing risk during development and meeting stakeholders' needs. There are many existing techniques for requirement prioritization, but most of these techniques do not cover continuous growth and change of requirements or cover requirements dependencies. So, most of these prioritization techniques need to be more continuous, scalable, implemented merely and integrated with software development life cycle and not work separately. This paper introduces a framework to prioritize requirements in agile software development. This framework tries to find solutions for the challenges facing this prioritization process such as how to make this prioritization continuous and scalable and how to deal with rapidly requirement changes and its dependencies."
Shailesh2020,Tanuja Shailesh and Ashalatha Nayak and Devi Prasad,An UML based performance evaluation of real-time systems using timed petri net,Computers,9,4,2020,10.3390/computers9040094,2073431X,"Performance is a critical non-functional parameter for real-time systems and performance analysis is an important task making it more challenging for complex real-time systems. Mostly performance analysis is performed after the system development but an early stage analysis and validation of performance using system models can improve the system quality. In this paper, we present an early stage automated performance evaluation methodology to analyse system performance using the UML sequence diagram model annotated with modeling and analysis of real-time and embedded systems (MARTE) profile. MARTE offers a performance domain sub-profile that is used for representing real-time system properties essential for performance evaluation. In this paper, a transformation technique and transformation rules are proposed to map the UML sequence diagram model into a Generalized Stochastic Timed Petri net model. All the transformation rules are implemented using a metamodel based approach and Atlas Transformation Language (ATL). A case study from the manufacturing domain a Kanban system is used for validating the proposed technique."
Berardi2022,Davide Berardi and Saverio Giallorenzo and Andrea Melis and Marco Prandini and Jacopo Mauro and Fabrizio Montesi,Microservice security: a systematic literature review,PeerJ Computer Science,7,,2022,10.7717/PEERJ-CS.779,23765992,"Microservices is an emerging paradigm for developing distributed systems. With their widespread adoption, more and more work investigated the relation between microservices and security. Alas, the literature on this subject does not form a well-defined corpus: it is spread over many venues and composed of contributions mainly addressing specific scenarios or needs. In this work, we conduct a systematic review of the field, gathering 290 relevant publications—at the time of writing, the largest curated dataset on the topic. We analyse our dataset along two lines: (a) quantitatively, through publication metadata, which allows us to chart publication outlets, communities, approaches, and tackled issues; (b) qualitatively, through 20 research questions used to provide an aggregated overview of the literature and to spot gaps left open. We summarise our analyses in the conclusion in the form of a call for action to address the main open challenges."
Eaton2020,Sarah Elaine Eaton and Katherine Crossman and Laleh Behjat and Robin Michael Yates and Elise Fear and Milana Trifkovic,An Institutional Self-Study of Text-Matching Software in a Canadian Graduate-Level Engineering Program,Journal of Academic Ethics,18,3,2020,10.1007/s10805-020-09367-0,15728544,"This institutional self-study investigated the use of text-matching software (TMS) to prevent plagiarism by students in a Canadian university that did not have an institutional license for TMS at the time of the study. Assignments from a graduate-level engineering course were analyzed using iThenticate®. During the initial phase of the study, similarity scores from the first student assignments (N = 132) were collected to determine a baseline level of textual similarity. Students were then offered an educational intervention workshop on academic integrity. Another set of similarity scores from consenting participants’ second assignments (n = 106) were then collected, and a statistically significant assignment effect (p < 0.05) was found between the similarity scores of the two assignments. The results of this study indicate that TMS, when used in conjunction with educational interventions about academic integrity, can be useful to students and educators to prevent and identify academic misconduct. This study adds to the growing body of empirical research about academic integrity in Canadian higher education and, in particular, in engineering fields."
Shahin2020,Mojtaba Shahin and M. Ali Babar,On the role of software architecture in DevOps transformation: An industrial case study,,,,2020,10.1145/3379177.3388891,,"Development and Operations (DevOps), a particular type of Continuous Software Engineering, has become a popular Software System Engineering paradigm. Software architecture is critical in succeeding with DevOps. However, there is little evidence-based knowledge of how software systems are architected in the industry to enable and support DevOps. Since architectural decisions, along with their rationales and implications, are very important in the architecting process, we performed an industrial case study that has empirically identified and synthesized the key architectural decisions considered essential to DevOps transformation by two software development teams. Our study also reveals that apart from the chosen architecture style, DevOps works best with modular architectures. In addition, we found that the performance of the studied teams can improve in DevOps if operations specialists are added to the teams to perform the operations tasks that require advanced expertise. Finally, investment in testing is inevitable for the teams if they want to release software changes faster."
Logachev2022,Maxim Logachev and Vera Chernova and Yuliya Laamarti and Tair Makhamatov and Vitaliy Ivlev and Lucio Giulodori and Irina Tutkova,Information System for Learning Control in Teaching Russian Sign Language: Process and Data Modeling,International Journal of Instruction,15,3,2022,10.29333/iji.2022.1539a,13081470,"The article describes project of the information system to control the teaching of Russian sign language. The aim of the conducted study is to consolidate existing algorithms and software tools to ensure the integrity, and objectivity in the implementation of professional training of specialists in the field of Russian sign language. The authors conducted a study of specialized sources in order to highlight the features of the subject area for the implementation of design processes. With the employment of structural analysis methods, a formal data model is obtained that describes the concept of a database that provides data storage and processing necessary for the full-scale operation of the information system. Using a combination of methods of structural analysis and object-oriented design, a model of the process of user interaction with the designed information system is obtained. The advantage of such a model is the selection of data objects necessary for the implementation of each stage of the process, as well as the objects obtained and available after their execution. All users of the system are classified according to the principle of data availability. The results obtained at the project stage in the future are the basis for the development of the corresponding software product."
Cortellessa2021,Vittorio Cortellessa and Daniele Di Pompeo,Analyzing the sensitivity of multi-objective software architecture refactoring to configuration characteristics,Information and Software Technology,135,,2021,10.1016/j.infsof.2021.106568,09505849,"Context: Software architecture refactoring can be induced by multiple reasons, such as satisfying new functional requirements or improving non-functional properties. Multi-objective optimization approaches have been widely used in the last few years to introduce automation in the refactoring process, and they have revealed their potential especially when quantifiable attributes are targeted. However, the effectiveness of such approaches can be heavily affected by configuration characteristics of the optimization algorithm, such as the composition of solutions. Objective: In this paper, we analyze the behavior of EASIER, which is an Evolutionary Approach for Software archItecturE Refactoring, while varying its configuration characteristics, with the objective of studying its potential to find near-optimal solutions under different configurations. Method: In particular, we use two different solution space inspection algorithms (i.e., NSGA−II and SPEA2) while varying the genome length and the solution composition. Results: We have conducted our experiments on a specific case study modeled in Æmilia ADL, on which we have shown the ability of EASIER to identify performance-critical elements in the software architecture where refactoring is worth to be applied. Beside this, from the comparison of multi-objective algorithms, NSGA−II has revealed to outperform SPEA2 in most of cases, although the latter one is able to induce more diversity in the proposed solutions. Conclusion: Our results show that the EASIER thoroughly automated process for software architecture refactoring allows to identify configuration contexts of the evolutionary algorithm in which multi-objective optimization more effectively finds near-optimal Pareto solutions."
Khan2021,Habib Ullah Khan and Mahmood Niazi and Mohamed El-Attar and Naveed Ikram and Siffat Ullah Khan and Asif Qumer Gill,Empirical Investigation of Critical Requirements Engineering Practices for Global Software Development,IEEE Access,9,,2021,10.1109/ACCESS.2021.3092679,21693536,"There is a need to identify requirements engineering (RE) practices that are important to global software development (GSD) project success. The objective of this paper is to report our recent empirical study results which aimed to identify the RE practices that are important to GSD projects. This study used an online survey questionnaire to elicit data from 56 RE experts of GSD projects. The survey included 66 RE practices identified by Sommerville et al. for non-GSD projects. The participants were asked to rank each RE practice on a four-point scale to determine the degree of importance of each practice in the context of GSD projects. This research identified a set of six key RE practices that mainly focuses on GSD project stakeholders, scope, standards and requirements traceability management. One common theme that is evident from the RE experts' feedback analysis is the standardization of requirements documents to reduce requirements inconsistencies and improve communication in diverse and distributed GSD project environments Our results show that not all 66 RE best practices are important for GSD projects. We believe that a good understanding of the identified RE practices is vital in developing and implementing the situation-specific RE processes for GSD projects."
Quionez2021,Yadira Quiñonez and Carmen Lizarraga and Raquel Aguayo and David Arredondo,Communication architecture based on iot technology to control and monitor pets feeding,Journal of Universal Computer Science,27,2,2021,10.3897/jucs.65094,09486968,"Technology is currently a significant benchmark in any application area; science and technology have permitted the invention of tools and devices that simplify daily activities by developing software engineering applications that provide automated solutions. In this sense, this work proposes two architectures that allow communication between the electronic device and the mobile application remotely, using the GSM/GPRS communication services and the Twitter social network. This development aims to control dogs' feeding adequately and healthily, providing the ration of food a dog needs according to the daily energy requirements. A nutritional assessment has also been performed considering different factors such as the size, breed, and weight of the dog to calculate the daily ration of healthy and balanced food according to daily energy requirements. Essentially, the electronic device consists of two parts: on the one hand, the electronic design is formed with an Arduino board, a Sim900 module to send and receive text messages, and the ESP8266 Wi-Fi serial transceiver module, which allows establishing the internet connection to receive the tweet that users post, both modules permit remote communication with the device using the Arduino board. On the other hand, the mobile application developed on Android uses a standard design according to the Google material design guidelines, allowing the owner to feed, schedule the feeding, review the dog's food history, and receive alerts when the food is going to be finished."
Chatzipetrou2020,Panagiota Chatzipetrou and Efi Papatheocharous and Krzysztof Wnuk and Markus Borg and Emil Alégroth and Tony Gorschek,Component attributes and their importance in decisions and component selection,Software Quality Journal,28,2,2020,10.1007/s11219-019-09465-2,15731367,"Component-based software engineering is a common approach in the development and evolution of contemporary software systems. Different component sourcing options are available, such as: (1) Software developed internally (in-house), (2) Software developed outsourced, (3) Commercial off-the-shelf software, and (4) Open-Source Software. However, there is little available research on what attributes of a component are the most important ones when selecting new components. The objective of this study is to investigate what matters the most to industry practitioners when they decide to select a component. We conducted a cross-domain anonymous survey with industry practitioners involved in component selection. First, the practitioners selected the most important attributes from a list. Next, they prioritized their selection using the Hundred-Dollar ($100) test. We analyzed the results using compositional data analysis. The results of this exploratory analysis showed that cost was clearly considered to be the most important attribute for component selection. Other important attributes for the practitioners were: support of the component, longevity prediction, and level of off-the-shelf fit to product. Moreover, several practitioners still consider in-house software development to be the sole option when adding or replacing a component. On the other hand, there is a trend to complement it with other component sourcing options and, apart from cost, different attributes factor into their decision. Furthermore, in our analysis, nonparametric tests and biplots were used to further investigate the practitioners’ inherent characteristics. It seems that smaller and larger organizations have different views on what attributes are the most important, and the most surprising finding is their contrasting views on the cost attribute: larger organizations with mature products are considerably more cost aware."
Moin2022,Armin Moin and Moharram Challenger and Atta Badii and Stephan Günnemann,A model-driven approach to machine learning and software modeling for the IoT: Generating full source code for smart Internet of Things (IoT) services and cyber-physical systems (CPS),Software and Systems Modeling,21,3,2022,10.1007/s10270-021-00967-x,16191374,"Models are used in both Software Engineering (SE) and Artificial Intelligence (AI). SE models may specify the architecture at different levels of abstraction and for addressing different concerns at various stages of the software development life-cycle, from early conceptualization and design, to verification, implementation, testing and evolution. However, AI models may provide smart capabilities, such as prediction and decision-making support. For instance, in Machine Learning (ML), which is currently the most popular sub-discipline of AI, mathematical models may learn useful patterns in the observed data and can become capable of making predictions. The goal of this work is to create synergy by bringing models in the said communities together and proposing a holistic approach to model-driven software development for intelligent systems that require ML. We illustrate how software models can become capable of creating and dealing with ML models in a seamless manner. The main focus is on the domain of the Internet of Things (IoT), where both ML and model-driven SE play a key role. In the context of the need to take a Cyber-Physical System-of-Systems perspective of the targeted architecture, an integrated design environment for both SE and ML sub-systems would best support the optimization and overall efficiency of the implementation of the resulting system. In particular, we implement the proposed approach, called ML-Quadrat, based on ThingML, and validate it using a case study from the IoT domain, as well as through an empirical user evaluation. It transpires that the proposed approach is not only feasible, but may also contribute to the performance leap of software development for smart Cyber-Physical Systems (CPS) which are connected to the IoT, as well as an enhanced user experience of the practitioners who use the proposed modeling solution."
Banimustafa2020,Ahmed Banimustafa and Nigel Hardy,A Scientific Knowledge Discovery and Data Mining Process Model for Metabolomics,IEEE Access,8,,2020,10.1109/ACCESS.2020.3039064,21693536,"This work presents a scientific data mining process model for metabolomics that provides a systematic and formalised framework for guiding and performing metabolomics data analysis in a justifiable and traceable manner. The process model is designed to promote the achievement of the analytical objectives of metabolomics investigations and to ensure the validity, interpretability and reproducibility of their results. It satisfies the requirements of metabolomics data mining, focuses on the contextual meaning of metabolomics knowledge, and addresses the shortcomings of existing data mining process models, while paying attention to the practical aspects of metabolomics investigations and other desirable features. The process model development involved investigating the ontologies and standards of science, data mining and metabolomics and its design was based on the principles, best practices and inspirations from Process Engineering, Software Engineering, Scientific Methodology and Machine Learning. A software environment was built to realise and automate the process model execution and was then applied to a number of metabolomics datasets to demonstrate and evaluate its applicability to different metabolomics investigations, approaches and data acquisition instruments on one hand, and to different data mining approaches, goals, tasks and techniques on the other. The process model was successful in satisfying the requirements of metabolomics data mining and can be generalised to perform data mining in other scientific disciplines."
Tubino2020,Laura Tubino and Andrew Cain and Jean Guy Schneider and Dhananjay Thiruvady and Niroshinie Fernando,Authentic individual assessment for team-based software engineering projects,,,,2020,10.1145/3377814.3381702,02705257,"In order to give students an authentic learning experience and better prepare them for the life-long learning required in contemporary workplaces, educational institutions increasingly use project-based learning in teams. However, this poses the challenge of developing authentic and equitable assessment criteria that reflect individual contributions in a team-work setting without jeopardizing project outcomes.We present a novel and innovative portfolio-based assessment framework that focuses on qualitative outcomes and ensures that the level of achievement reflects a student's competency across each of the defined learning dimensions, including professional behaviour, teamwork, process and relevant contributions towards project deliverables. In this paper, we present the main motivation behind devising and introducing the framework and also reflect on the educational outcomes and challenges of implementing the framework in the context of two final year Software Engineering project units."
Yevseiev2022,Serhii Yevseiev and Ruslan Hryshchuk and Kateryna Molodetska and Mariia Nazarkevych and Volodymyr Hrytsyk and Oleksandr Milov and Olha Korol and Stanislav Milevskyi and Roman Korolev and Serhii Pohasii and Andrii Tkachov and Yevgen Melenti and Oleksandr Lavrut and Alla Havrylova and Serhii Herasymov and Halyna Holotaistrova and Dmytro Avramenko and Roman Vozniak and Oleksandr Voitko and Kseniia Yerhidzei and Serhii Mykus and Yurii Pribyliev and Olena Akhiiezer and Mykhailo Shyshkin and Ivan Opirskyy and Oleh Harasymchuk and Olha Mykhaylova and Yuriy Nakonechnyy and Marta Stakhiv and Bogdan Tomashevsky,Modeling of security systems for critical infrastructure facilities,Modeling of security systems for critical infrastructure facilities,,,2022,10.15587/978-617-7319-57-2,,"The development of Industry 4.0 technologies is based on the rapid growth of the computing capabilities of mobile wireless technologies, which has made it possible to significantly expand the range of digital services and form a conglomeration of socio-cyber-physical systems and smart technologies. The First Section discusses the issues of building security systems based on the proposed Concept of multi-contour security systems, taking into account the hybridity and synergy of modern targeted cyber-attacks, their integration with social engineering methods. This approach not only increases the level of security, but also forms an objective approach to the use of post-quantum security mechanisms based on the proposed Lotka-Volterra models. The Second Section analyzes the features of the functioning of social Internet services and establishes their role in ensuring the information security of the state. An approach is proposed to identify signs of threats in the text content of social Internet services, which will allow to quickly respond to changing situations and effectively counteract such threats. A classifier of information security profiles of users of social Internet services has been developed to assess the level of their danger as potential participants in disinformation campaigns. A method for identifying and evaluating the information and psychological impact on user communities in services is proposed. Models of conflict interaction of user groups in social Internet services are considered on the example of civil movements. To effectively counter threats to information security of the state, it is proposed to use the concept of synergistic user interaction and self-organization processes in a virtual community. Particular attention is paid to countering the manipulation of public opinion in the decision-making process by users of social Internet services. The Third Section proposes a biometric security system that works to authenticate users based on a comparison of their fingerprints and certain templates stored in a biometric database. A method for determining the contour based on the passage of a curve and the filtering function of contour lines has been developed. The stage of skeletal identification is analyzed in detail. The Ateb-Gabor method with wave thinning has been developed. The performance of skeletal algorithms such as the Zhang-Suen thinning algorithm, the Hilditch algorithm, and the Ateb-Gabor method with wave decimation is analyzed. The presented results of experiments with biometric fingerprints based on the NIST Special Database 302 database showed the effectiveness of the proposed method. The software and firmware were developed using the Arduino Nano."
Henning2022,Sören Henning and Wilhelm Hasselbring,A configurable method for benchmarking scalability of cloud-native applications,Empirical Software Engineering,27,6,2022,10.1007/s10664-022-10162-1,15737616,"Cloud-native applications constitute a recent trend for designing large-scale software systems. However, even though several cloud-native tools and patterns have emerged to support scalability, there is no commonly accepted method to empirically benchmark their scalability. In this study, we present a benchmarking method, allowing researchers and practitioners to conduct empirical scalability evaluations of cloud-native applications, frameworks, and deployment options. Our benchmarking method consists of scalability metrics, measurement methods, and an architecture for a scalability benchmarking tool, particularly suited for cloud-native applications. Following fundamental scalability definitions and established benchmarking best practices, we propose to quantify scalability by performing isolated experiments for different load and resource combinations, which asses whether specified service level objectives (SLOs) are achieved. To balance usability and reproducibility, our benchmarking method provides configuration options, controlling the trade-off between overall execution time and statistical grounding. We perform an extensive experimental evaluation of our method’s configuration options for the special case of event-driven microservices. For this purpose, we use benchmark implementations of the two stream processing frameworks Kafka Streams and Flink and run our experiments in two public clouds and one private cloud. We find that, independent of the cloud platform, it only takes a few repetitions (≤ 5) and short execution times (≤ 5 minutes) to assess whether SLOs are achieved. Combined with our findings from evaluating different search strategies, we conclude that our method allows to benchmark scalability in reasonable time."
Palagin2022,Oleksandr V. Palagin and Kyrylo S. Malakhov and Vitalii Y.U. Velychko and Tetiana V. Semykopna,HYBRID E-REHABILITATION SERVICES: SMART-SYSTEM FOR REMOTE SUPPORT OF REHABILITATION ACTIVITIES AND SERVICES,International Journal of Telerehabilitation,2022,Special Issue,2022,10.5195/ijt.2022.6480,19452020,"One of the most effective solutions in medical rehabilitation assistance is remote patient / person-centered rehabilitation. Rehabilitation also needs effective methods for the “Physical therapist – Patient – Multidisciplinary team” system, including the statistical processing of large volumes of data. Therefore, along with the traditional means of rehabilitation, as part of the “Transdisciplinary intelligent information and analytical system for the rehabilitation processes support in a pandemic (TISP)” in this paper, we introduce and define: the basic concepts of the new hybrid e-rehabilitation notion and its fundamental foundations; the formalization concept of the new Smart-system for remote support of rehabilitation activities and services; and the methodological foundations for the use of services (UkrVectōrēs and vHealth) of the remote Patient / Person-centered Smart-system. The software implementation of the services of the Smart-system has been developed."
Escobar2020,Pilar Escobar and María del Mar Roldán-García and Jesús Peral and Gustavo Candela and José García-Nieto,An ontology-based framework for publishing and exploiting linked open data: A use case on water resources management,Applied Sciences (Switzerland),10,3,2020,10.3390/app10030779,20763417,"Nowadays, the increasing demand of water for electricity production, agricultural and industrial uses are directly affecting the reduction of available quality water for human consumption in the world. Efficient and sustainable maintenance of water reservoirs and supply networks implies a holistic strategy that takes into account, as much as possible, information from the stages of water usage. Next,-generation decision-making software tools, for supporting water management, require the integration of multiple and heterogeneous data sources of different knowledge domains. In this regard, Linked Data and SemanticWeb technologies enable harmonization of different data sources, as well as the efficient querying for feeding upper-level Business Intelligence processes. This work investigates the design, implementation and usage of a semantic approach driven by ontology to capture, store, integrate and exploit real-world data concerning water supply networks management. As a main contribution, the proposal helps with obtaining semantically enriched linked data, enhancing the analysis of water network performance. For validation purposes, in the use case, a series of data sources from different measures have been considered, in the scope of an actual water management system of the Mediterranean region of Valencia (Spain), throughout several years of activity. The obtained experience shows the benefits of using the proposed approach to identify possible correlations between the measures such as the supplied water, the water leaks or the population."
Lesov2021,Kuvandik Lesov and Mukhamedali Kenjaliyev and Akhmadjan Mavlanov and Sherzod Tadjibaev,Stability of the embankment of fine sand reinforced with geosynthetic materials,,264,,2021,10.1051/e3sconf/202126402011,22671242,"This research paper validates the significance of an assimilated attitude when choosing the design of the subgrade of railways constructed from fine sands in difficult engineering and geological conditions in the desert and steppe territories of Uzbekistan to ensure their sustainability with modern technologies and materials. The analysis of the results obtained in the calculations of the stability coefficient of the design structure of the embankment. The characteristics of the embankment soils correspond to the design data of the object Removal of the existing railway section Dunguluk-Burgutli-Misken from the flood zone of the Shurbulak reservoir The results of the calculations performed by the method of G. M. Shakhunyants, as well as the software GEO 5 and Plaxis 2D show that the structure of the embankment with a height of more than 6 meters does not provide the required standard coefficient of stability. Complete warps of the embankment and graphs of changes in the stability coefficient of the design structure of the embankment, and with the laying of geotextiles as reinforcement of the pattern of the subgrade of railways, are given. Beneficial solutions to increase the stability coefficient of the embankment from fine sands are proposed, and the efficiency of using geosynthetic materials as a reinforcement of the pattern of the subgrade is substantiated. Theoretical calculations and analysis of prevailing embankment structures confirm the feasibility of reinforcing the structure of the subgrade erected from fine sands on the railway lines of Uzbekistan."
Vasilakakis2020,Michael D. Vasilakakis and Anastasios Koulaouzidis and Wojciech Marlicz and Dimitris K. Iakovidis,The future of capsule endoscopy in clinical practice: From diagnostic to therapeutic experimental prototype capsules,Przeglad Gastroenterologiczny,15,3,2020,10.5114/pg.2019.87528,18974317,"Capsule endoscopy (CE) is indicated as a first-line clinical examination for the detection of small-bowel pathology, and there is an ever-growing drive for it to become a method for the screening of the entire gastrointestinal tract (GI). Although CE's main function is diagnosis, the research for therapeutic capabilities has intensified to make therapeutic capsule endoscopy (TCE) a target within reach. This manuscript presents the research evolution of CE and TCE through the last 5 years and describes notable problems, as well as clinical and technological challenges to overcome. This review also reports the state-of-the-art of capsule devices with a focus on CE research prototypes promising an enhanced diagnostic yield (DY) and treatment. Lastly, this article provides an overview of the research progress made in software for enhancing DY by increasing the accuracy of abnormality detection and lesion localisation."
Ali2022,Aamir Ali and Hajra Safdar Khan and Salman Saleem and Muhammad Hussan,EMHD Nanofluid Flow with Radiation and Variable Heat Flux Effects along a Slandering Stretching Sheet,Nanomaterials,12,21,2022,10.3390/nano12213872,20794991,"Nanofluids have gained prominence due to their superior thermo-physical properties. The current paper deals with MHD nanofluid flow over a non-linear stretchable surface of varying thickness in the presence of an electric field. We investigated the effects of nanometer-sized copper (Cu) particles in water (base fluid) as a nanofluid, as well as non-linear thermal radiation, variable fluid viscosity, Joule heating, viscous dissipation, and non-uniform heat flux. The current study’s aim is influenced by the immense applications in industry and machine building. It has been observed that linear stretching sheets have been extensively used in heat transfer research. Moreover, no effort has been made yet to model a non-linear stretching sheet with variable thickness. Furthermore, the effects of electromagnetohydrodynamics (EMHD) boundary-layer flow of a nanofluid with the cumulative impact of thermal radiation, variable viscosity, viscous dissipation, Joule heating, and variable heat flux have been investigated. Sheets with variable thicknesses are practically significant in real-life applications and are being used in metallurgical engineering, appliance structures and patterns, atomic reactor mechanization and paper production. To investigate the physical features of the problem, we first examined the model and identified all the physical properties of the problem. This problem has been formulated using basic laws and governing equations. The partial differential equations (PDEs) that govern the flow are converted into a system of non-dimensional ordinary differential equations (ODE’s), using appropriate transformations. The Adam–Bashforth predictor-corrector technique and Mathematica software are utilized to numerically solve the resulting non-dimensionalized system. The interaction of various developing parameters with the flow is described graphically for temperature and velocity profiles. It is concluded that the velocity of nanoparticles declines as the intensity of the magnetic field increases. However, the temperature of the nanomaterials rises, as increasing the values of the electric field also increases the velocity distribution. The radiation parameter enhances the temperature field. The temperature of the fluid increases the occurrence of space- and time-dependent parameters for heat generation and absorption and radiation parameters."
Achebe2020,C. H. Achebe and B. M.O. Ogunedo and J. L. Chukwuneke and N. B. Anosike,Analysis of diesel engine injector nozzle spray characteristics fueled with residual fuel oil,Heliyon,6,8,2020,10.1016/j.heliyon.2020.e04637,24058440,"Experimental analysis on the spray characteristics of a diesel engine injector nozzle fueled with Residual Fuel Oil (RFO) was carried out in this study. To achieve this, the fuel was characterized to determine its physicochemical properties, and an experimental set up was designed to visualize and capture the spray pattern of the fuel. The images obtained were processed and analysed using Image J software to determine the spray length, spray cone angle, spray area, spray volume, and spray velocity values of the fuel. Experimental results obtained agree with validation models and reveal that spray parameter values of RFO are higher than those of diesel fuel. The values of spray parameters of RFO such as 456mm spray length, 2.85mm Sauter Mean Diameter (SMD) and the low spray cone angle of 12.69°, led to a higher spray volume causing the engine to run on a rich mixture after initial start-up conditions. This would create such challenges as reduction in power and clogging of injector nozzle tip due to an increase in carbon deposits. Regression models generated reveal that these challenges could be eliminated when the spray parameters run on optimal values of 256mm, 6.41cm2, 16.18cm3, 0.96 mm/s and 13.59° for the spray length, spray area, spray volume, spray velocity and the spray angle respectively. These optimal values were obtained when the engine fuel injection time was set to 500μs while running on fuel of viscosity 4.305 mPa.s and temperature of 48 °C."
Suzuki2020,Kei Suzuki and Hiroyuki Nakano and Kazuya Inoue and Yoichiro Nakajima and Sho Mizobuchi and Michi Omori and Nahoko Kato-Kogoe and Katsuaki Mishima and Takaaki Ueno,Examination of new parameters for sex determination of mandible using Japanese computer tomography data,Dentomaxillofacial Radiology,49,5,2020,10.1259/dmfr.20190282,1476542X,"Objective: In the field of forensic science, sex discrimination of skeletons is an important identification item for personal identification. The individual sex discrimination method using skeletons includes a determination method using measurement values and a macroscopic form observation method. Both methods have advantage and disadvantage. In this study, we used the homologous model technique and principal component (PC) analysis to determine gender difference from morphology of the mandible. Methods and materials: 45 patients (23 males and 22 females) of CT imaging for tooth extraction from January 2018 to March 2019 at department of oral surgery, Osaka Medical College. The mean age was 43.1 ± 14.6. Patients with less than 14 remaining teeth were excluded because the number of remaining teeth may affect the shape of the mandible. 3D images were constructed, and 20 landmarks plotting on the 3D model surfaces. We generated template models of the mandible consisting of approximately 8434 polygons. The template model automatically fitted into the individually scanned point cloud of the mandible by minimising external and internal energy functions. As described above, the mandibles were constructed for each sample by using the Homologous Body Modeling software (HBM, Digital Human Technology, Inc.) and the mHBM-Rugle (Medic Engineering Corporation). The mandibles were analysed using the PCA. Results: The contribution of the most important PC was found to be 27.2%. 12 PCs explained over 75% of the total variance. That is, it was able to express 75% or more of the mandible expression with 12 PCs. A significant difference between male and female was observed in the first PCs (Wilcoxon test, p < 0.05). Visualising the result of the first PC showed that the mandibular branch of male was larger than that of female, and the mandible angle was overhanging outside. Conclusion: This method is a combination of the determination method using the previous measurement values and the determination using macroscopic observation, and is considered to be innovative method."
Gao2021,Han Gao and Shaoyin Cheng and Yinxing Xue and Weiming Zhang,A lightweight framework for function name reassignment based on large-scale stripped binaries,,,,2021,10.1145/3460319.3464804,,"Software in the wild is usually released as stripped binaries that contain no debug information (e.g., function names). This paper studies the issue of reassigning descriptive names for functions to help facilitate reverse engineering. Since the essence of this issue is a data-driven prediction task, persuasive research should be based on sufficiently large-scale and diverse data. However, prior studies can only be based on small-scale datasets because their techniques suffer from heavyweight binary analysis, making them powerless in the face of big-size and large-scale binaries. This paper presents the Neural Function Rename Engine (NFRE), a lightweight framework for function name reassignment that utilizes both sequential and structural information of assembly code. NFRE uses fine-grained and easily acquired features to model assembly code, making it more effective and efficient than existing techniques. In addition, we construct a large-scale dataset and present two data-preprocessing approaches to help improve its usability. Benefiting from the lightweight design, NFRE can be efficiently trained on the large-scale dataset, thereby having better generalization capability for unknown functions. The comparative experiments show that NFRE outperforms two existing techniques by a relative improvement of 32% and 16%, respectively, while the time cost for binary analysis is much less."
Li2020,Zenan Li and Xiaoxing Ma and Chang Xu and Jingwei Xu and Chun Cao and Jian Lü,Operational calibration: Debugging confidence errors for DNNs in the field,,,,2020,10.1145/3368089.3409696,,"Trained DNN models are increasingly adopted as integral parts of software systems, but they often perform deficiently in the field. A particularly damaging problem is that DNN models often give false predictions with high confidence, due to the unavoidable slight divergences between operation data and training data. To minimize the loss caused by inaccurate confidence, operational calibration, i.e., calibrating the confidence function of a DNN classifier against its operation domain, becomes a necessary debugging step in the engineering of the whole system. Operational calibration is difficult considering the limited budget of labeling operation data and the weak interpretability of DNN models. We propose a Bayesian approach to operational calibration that gradually corrects the confidence given by the model under calibration with a small number of labeled operation data deliberately selected from a larger set of unlabeled operation data. The approach is made effective and efficient by leveraging the locality of the learned representation of the DNN model and modeling the calibration as Gaussian Process Regression. Comprehensive experiments with various practical datasets and DNN models show that it significantly outperformed alternative methods, and in some difficult tasks it eliminated about 71% to 97% high-confidence (>0.9) errors with only about 10% of the minimal amount of labeled operation data needed for practical learning techniques to barely work"
Le2023,Quynh Hoang Le and Zakir Hussain and Nazar Khan and Sergei Zuev and Khurram Javid and Sami Ullah Khan and Zahra Abdelmalek and Iskander Tlili,Chebyshev collocation simulations for instability of Hartmann flow due to porous medium: A neutral stability and growth rate assessment,Ain Shams Engineering Journal,14,12,2023,10.1016/j.asej.2023.102215,20904479,"In the modern world, research in the field of thermal enhancement is going to increasing due to their diverse applications in the field of chemical industries and engineering domains. In the current study, the hydrodynamic or magnetohydrodynamics (MHD) instability of Hartmann flow in the porous medium is considered. Here, the special nature of magnetic field known as the transverse magnetic field is used in the current analysis. The investigation of hydrodynamic stability of electrically conductive for Hartmann flow in channel along with applied magnetize field is analyzed. The fluid layers are penetrated by a constant magnetize field and flow is considered through porous medium. Reynolds number (Re) is utilized to main system of hydrodynamic stability equations. A Chebyshev collocation technique is applied through numerical method to analyze magnetohydrodynamic instability system. The obtained flow equations represent system of ODEs. The current analysis makes use of a unique type of magnetic field known as the transverse magnetic field. The instabilities of nanofluids that contains nanoparticles with water as based fluid for with physical parameters are compared and discussed for growth rate and neutral graphs. These flow equations are solved numerically by using “Cheybeshev Collocation Method”. The mathematical technique “QZ (Qualitat and Zuverlassigkeit)” is applied to find out eigenvalues from comprehensive Orr-sommerfeld technique by using MATLAB software. Different embedded physical parameters Reynolds number (Re), Hartmann number (Ha) wave number (k) are compared and discussed for growth rate and neutral graphs. The instabilities of Hartmann flow in porous medium different embedded physical parameters are compared and discussed using growth rate and neutral graphs. It predicted that the flow become stable due to the magnetic field, Reynolds number and wave number the fluids transportation. The outcomes of current study are utilized in drug-delivery systems, photodynamic therapy and delivery of antitumor."
Anandh2020,K. S. Anandh and K. Prasanna and M. G.Soundarya Priya and S. Manna Simon,An industrial study of just in time (JIT) management in precast construction projects,,2277,,2020,10.1063/5.0025220,15517616,"Precast construction is an industrialized way to build, meaning transfer of work from factory to site. This increases the quality and productivity of construction and reduces the construction time of a building significantly. The Precast technology is extremely technically challenging, which needs lot of pre engineering and fast decision making from all the professionals involved, like client, architect, service consultants and structural design teams. The JIT philosophy is a vital activity in supervising the transportation of precast concrete elements and the handling of it within the site. This project work is carried out to know about the adaption rate of JIT management in precast construction sites confined to Indian scenario. The different methods used to find the significance of JIT in precast construction are Conversational interviewing, Survey and Journal study. Statistical Package for Social Sciences (SPSS) Software Version 21.0 is used to analyzing the data. From this project work, we found that there is significant variations among the professionals with different educational qualifications on a few factors like attitude, designing and workplace management."
BambadeAntoineBambadeInriaFr2022,Antoine Bambade Antoine.Bambade@Inria.Fr and Sarah El-Kazdadi and Adrien Taylor and Justin Carpentier,ProxQP: Yet another Quadratic Programming Solver for Robotics and beyond,,,,2022,10.15607/RSS.2022.XVIII.040,2330765X,"Quadratic programming (QP) has become a core modelling component in the modern engineering toolkit. This is particularly true for simulation, planning and control in robotics. Yet, modern numerical solvers have not reached the level of efficiency and reliability required in practical applications where speed, robustness, and accuracy are all necessary. In this work, we introduce a few variations of the well-established augmented Lagrangian method, specifically for solving QPs, which include heuristics for improving practical numerical performances. Those variants are embedded within an open-source software which includes an efficient C++ implementation, a modular API, as well as best-performing heuristics for our test-bed. Relying on this framework, we present a benchmark studying the practical performances of modern optimization solvers for convex QPs on generic and complex problems of the literature as well as on common robotic scenarios. This benchmark notably highlights that this approach outperforms modern solvers in terms of efficiency, accuracy and robustness for small to medium-sized problems, while remaining competitive for higher dimensions."
Olivito2020,Renato S. Olivito and Saverio Porzio and Carmelo Scuro and Domenico L. Carnì and Francesco Lamonaca,Inventory and monitoring of historical cultural heritage buildings on a territorial scale: A preliminary study of structural health monitoring based on the CARTIS approach,Acta IMEKO,10,1,2020,10.21014/ACTA_IMEKO.V10I1.820,2221870X,"Earthquakes induce dynamic stresses in structures, and past seismic events have demonstrated that existing heritage buildings are highly vulnerable. This vulnerability applies to both reinforced-concrete and masonry buildings, which are concentrated in historic centres throughout Italy. Significant variations in construction account for the inadequacy of existing structures to withstand seismic actions, such as the materials used and the construction details, which can be neglected in building practices. This work focuses on the analysis of heritage buildings through an inventory using the Caratterizzazione TIpologica Strutturale (CARTIS) form developed by the Seismic Engineering University Laboratories Network in conjunction with the Civil Protection Department. On knowing a building framework, structural health monitoring (SHM) systems can be applied on the town compartments (TCs) that are prone to the highest vulnerabilities. A priority criticalities scale can be devised starting from the building inventory by identifying the TCs through the CARTIS-based data. This approach can be used to determine a safety threshold obtained via structural parametrical analysis using commercial software (VEMnl) with different building typologies. The next stage consists of the implementation of appropriate SHM to provide important information regarding the structural integrity of the buildings. The proposed methodology is outlined in this paper with reference to the suggested SHM system."
Ferdows2021,Mohammad Ferdows and Mohammed Shamshuddin and Khairy Zaimi,Computation of steady free convective boundary layer viscous fluid flow and heat transfer towards the moving flat plate subjected to suction/injection effects,CFD Letters,13,3,2021,10.37934/cfdl.13.3.1624,21801363,"The steady free convective boundary layer viscous fluid flow and heat transfer towards the moving flat plate is investigated. It is important to study the fluid flow and heat transfer problems in the presence of suction and injection effects due to an extensive variety of applications in engineering and industry. Thus, the main objective of the present study is to analyse the impacts of the suction and injection parameter on the velocity and temperature of the fluid as well as the skin friction and the Nusselt number coefficients. The problem has coupled partial differential equations which are converted into ordinary differential equations by employing similarity transformation and adopting of the strong wall suction. The ordinary differential equations thus obtain are handled numerically by utilizing Maple software simulation. The main findings concerning the behaviours of velocity and temperature against various values of the emerging physical parameter such as suction/injection are presented clearly in this numerical examination via graphical illustrations whose physical explanation are discussed thoroughly based on a strong theoretical basis. Furthermore, skin friction and Nusselt number results are studied at different values of pressure gradient. The detail geometry reveals that the velocity and temperature of the fluid decreases with increase of suction/injection parameter. Both skin friction coefficient and Nusselt number decreases with an increase of suction/injection parameter."
Mehmood2022,Khalid Mehmood and Yansong Bao and Saifullah and Sadia Bibi and Saad Dahlawi and Muhammad Yaseen and Muhammad Mohsin Abrar and Prashant Srivastava and Shah Fahad and Turki Kh Faraj,Contributions of Open Biomass Burning and Crop Straw Burning to Air Quality: Current Research Paradigm and Future Outlooks,Frontiers in Environmental Science,10,,2022,10.3389/fenvs.2022.852492,2296665X,"Since open biomass burning (OBB) and open crop straw burning (OCSB) could pose a great risk to human health via altering the air quality, these practices have grabbed considerable attention from the scientific community and policymakers in recent years. In order to have a greater and deeper understanding of the contributions of both OBB and OCSB on air quality, a bibliometric analysis was performed using the Web of Science core collection to understand the research developments and future perspectives of these issues between 1991 and 2021. VOSviewer software 1.6.15 and R version 4.0.3 were employed to determine the annual scientific production trend and the role of countries, institutions, authors, and journal metrics network analysis. The findings showed that the interest in the study of OBB and OCSB pollution related to air quality has increased significantly over the last decade. A total of 1,021 publications were retrieved, with English as the most preferably used language. Among all documents, research articles were the most commonly appearing document type, and the researchers mainly emphasized environmental science, meteorology, atmospheric sciences, energy fuels, and environmental engineering fields. In terms of article analysis, Atmospheric Chemistry Physics, followed by Atmospheric Environment, was found to be the leading journal in this research domain, whereas the most frequently utilized keywords in the documents were biomass, biomass burning, and PM2.5. In terms of countries, the United States emerged as the leader with the highest publication rate, followed by China and India. The Chinese Academy of Sciences was ranked first in the list of most productive institutions, followed by the University of Montana and the US Forest Service. Based on the analysis, the finer spatial and temporal resolution and the characterization and understanding of the complex processes that are occurring in the atmosphere, such as clustering, oxidation, surface chemistry, and their impact on air quality, need to be explored in depth. Our research analysis can provide a baseline for future studies in air quality."
SantoshGangappa2020,Goudar Santosh Gangappa and S. Sripad Kulkarni,Experimentation and validation of basalt & jute fiber reinforced in polymer matrix hybrid composites,,38,,2020,10.1016/j.matpr.2020.07.081,22147853,"Hybrid composite is a combination of natural fiber and synthetic fibers. Nowadays, polymer hybrid composites are extensively used in several engineering applications. Basalt fiber, because of its high tensile strength and modulus is considered with jute, as jute possess incredible antistatic properties forms a hybrid with polyester resin. In this proposed work polyester resin is used as a matrix, basalt and jute fibers are used as reinforcement to develop hybrid composites by Compression Molding Technique. The primary mechanical properties such as tensile strength, tensile modulus and compressive strength were experimentally found using universal testing machine in warp and weft direction where parameters are force, displacement, thickness, volume fraction of resin to fill the reinforcement. The relative significance are discussed by subjecting the developed hybrid polymer composite to dynamic behavior, the various parameters like mode shape, damping factor, frequency, acceleration and force are analyzed using Fast Fourier Transform analyzer experimentally and validated by using Lab-view software."
Subramonyam2022,Hariharan Subramonyam and Jane Im and Colleen Seifert and Eytan Adar,Solving Separation-of-Concerns Problems in Collaborative Design of Human-AI Systems through Leaky Abstractions,,,,2022,10.1145/3491102.3517537,,"In conventional software development, user experience (UX) designers and engineers collaborate through separation of concerns (SoC): designers create human interface specifications, and engineers build to those specifications. However, we argue that Human-AI systems thwart SoC because human needs must shape the design of the AI interface, the underlying AI sub-components, and training data. How do designers and engineers currently collaborate on AI and UX design? To find out, we interviewed 21 industry professionals (UX researchers, AI engineers, data scientists, and managers) across 14 organizations about their collaborative work practices and associated challenges. We find that hidden information encapsulated by SoC challenges collaboration across design and engineering concerns. Practitioners describe inventing ad-hoc representations exposing low-level design and implementation details (which we characterize as leaky abstractions) to ""puncture""SoC and share information across expertise boundaries. We identify how leaky abstractions are employed to collaborate at the AI-UX boundary and formalize a process of creating and using leaky abstractions."
Swalmeh2022,Mohammed Z. Swalmeh and Feras Shatat and Firas A. Alwawi and Mohd Asrul Hery Ibrahim and Ibrahim Mohammed Sulaiman and Nusayba Yaseen and Mohammad F.M. Naser,Effectiveness of Radiation on Magneto-Combined Convective Boundary Layer Flow in Polar Nanofluid around a Spherical Shape,Fractal and Fractional,6,7,2022,10.3390/fractalfract6070383,25043110,"Many physical aspects emerging from the local structure and micromotions of liquid particles can be studied by utilizing the governing model of micropolar liquid. It has the ability to explain the behavior of a wide range of real fluids, including polymeric solutions, liquid crystals, lubricants, and animal blood. This earned it a major role in the treatment of many industrial and engineering applications. Radiative heat transmission induced by a combined convection flow of micropolar fluid over a solid sphere, and its enhancement via nanoparticle oxides, are investigated in this study. An applied magnetic field and a constant wall temperature are also considered. The Tiwari–Das model is used to construct the mathematical model. An approximate numerical solution is included using the Keller box method, in which its numerical calculations are performed via MATLAB software, to obtain numerical results and graphic outputs reflecting the effects of critical parameters on the physical quantities associated with heat transfer. The investigation results point out that a weakness in the intensity of the magnetic field, or an increment in the nanoparticle volume fraction, causes an increment in velocity. Raising the radiation parameter promotes energy transport, angular velocity, and velocity."
Medvedev2021,Danny Medvedev and Uri Shani and Dov Dori,Gaining insights into conceptual models: A graph‐theoretic querying approach,Applied Sciences (Switzerland),11,2,2021,10.3390/app11020765,20763417,"Modern complex systems include products and services that comprise many intercon-nected pieces of integrated hardware and software, which are expected to serve humans interacting with them. As technology advances, expectations of a smooth, flawless system operation grow. Model‐based systems engineering, an approach based on conceptual models, copes with this chal-lenge. Models help construct formal system representations, visualize them, understand the design, simulate the system, and discover design flaws early on. Modeling tools can benefit tremendously from querying capabilities that enable gaining deep insights into system aspects that direct model observations do not reveal. Querying mechanisms can unveil and explain cause‐and‐effect phenom-ena, identify central components, and estimate impacts or risks associated with changes. Being con-nected networks of system elements, models can be effectively represented as graphs, to which queries are applied. Capitalizing on established graph‐theoretic algorithms to solve a large variety of problems can elevate the modeling experience to new levels. To utilize this rich set of capabilities, one must convert the model into a graph and store it in a graph database with no significant loss of information. Applying the appropriate algorithms and translating the query response back to the original intelligible and meaningful diagrammatic and textual model representation is most valua-ble. We present and demonstrate a querying approach of converting Object‐Process Methodology (OPM) ISO 19450 models into graphs, storing them in a Neo4J graph database, and performing queries that answer complex questions on various system aspects, providing key insights into the modeled system or phenomenon and helping to improve the system design."
Assad2021,Hatim El Assad and Benaissa Kissi and Rhanim Hassan and Parron Vera Miguel Angel and Rubio Cintas Maria Dolores and Guemimi Chafik and Mariem Kacem-Boureau,Numerical modeling of soil erosion with three wall laws at the soil-water interface,Civil Engineering Journal (Iran),7,9,2021,10.28991/cej-2021-03091742,24763055,"In the area of civil engineering and especially hydraulic structures, we find multiple anomalies that weakens mechanical characteristics of dikes, one of the most common anomalies is erosion phenomenon specifically pipe flow erosion which causes major damage to dam structures. This phenomenon is caused by a hole which is the result of the high pressure of water that facilitate the soil migration between the two sides of the dam. It becomes only a question of time until the diameter of the hole expands and causes destruction of the dam structure. This problem pushed physicist to perform many tests to quantify erosion kinetics, one of the most used tests to have logical and trusted results is the HET (hole erosion test). Meanwhile there is not much research regarding the models that govern these types of tests. Objectives: In this paper we modeled the HET using modeling software based on the Navier Stokes equations, this model tackles also the singularity of the interface structure/water using wall laws for a flow turbulence. Methods/Analysis: The studied soil in this paper is a clay soil, clay soil has the property of containing water more than most other soils. Three wall laws were applied on the soil / water interface to calculate the erosion rate in order to avoid the rupture of such a structure. The modlisitation was made on the ANSYS software. Findings: In this work, two-dimensional modeling was carried of the soil.in contrast of the early models which is one-dimensional model, the first one had shown that the wall-shear stress which is not uniform along the whole wall. Then using the linear erosion law to predict the non-uniform erosion along the whole length. The previous study found that the wall laws have a significant impact on the wall-shear stress, which affects the erosion interface in the fluid/soil, particularly at the hole's extremes. Our experiment revealed that the degraded profile is not uniform."
Tong2021,Zhao Wei Tong and Sami Ullah Khan and Hanumesh Vaidya and Rajashekhar Rajashekhar and Tian Chuan Sun and M. Ijaz Khan and K. V. Prasad and Ronnason Chinram and Ayman A. Aly,Nonlinear thermal radiation and activation energy significances in slip flow of bioconvection of Oldroyd-B nanofluid with Cattaneo-Christov theories,Case Studies in Thermal Engineering,26,,2021,10.1016/j.csite.2021.101069,2214157X,"Recently, nanofluids are an effective source of enhancing the thermal transportation systems associated with industrial and engineering phenomena. With the nanoscale size and effective thermal properties, the nanomaterials convey exclusive beneficial applications in heat exchanges, coolant processes, medical treatment, electronic cooling systems, energy production, etc. Keeping all such motivating significances of nanoparticles in mind, this research presents the thermal aspects of Oldroyd-B nanofluid with applications of bioconvection phenomenon over a convectively heated configuration when the slip effects are more dominant. The bio-convective nanofluid model is further extended by incorporating the thermal radiation relations in nonlinear form and activation energy. The modifications in heat and mass equations are suggested in view of modified Cattaneo-Christov theories. The magnetic force and porous medium applications are also entertained. The convective conditions are imposed on accessing the flow dynamically. The numerical simulations via shooting technique by using MATLAB software are performed with convincing solution accuracy. The physical objective in view of all parameters that govern the flow model is presented in graphs and tables. The obtained theoretical results reflects applications in thermal extrusion processes, power plants, enhancing the heat/mass process, information technology, chemical processes, pharmacological processes, cooling and heating systems, solar energy production, bio-technolgy applications like enzymes, biofules etc."
Zheng2022,Sining Zheng and Guizhen Wu and Jiahao Zhao and Weiqi Chen,Impact of the COVID-19 epidemic anxiety on college students' employment confidence and employment situation perception in China,Frontiers in Psychology,13,,2022,10.3389/fpsyg.2022.980634,16641078,"The psychological problems and employment problems of college students have always been the focus of attention of all sectors of society. The COVID-19 epidemic has a great impact on the mental health and employment of Chinese college students. Under this background, this study discusses how epidemic anxiety affects the employment confidence and perception of employment situation of Chinese college students. Through the online questionnaire survey of 1,132 college students nationwide, and the ordinal logistic regression analysis of the survey data using Stata 16.0 software, the results show that: (1) Epidemic anxiety negatively affects Chinese college students' employment confidence and employment situation perception, and has a significant impact on employment confidence. The three control variables of employment guidance, older age and higher education have a significant positive impact on college students' employment confidence and employment situation perception. College students in the eastern region have stronger employment confidence and more optimistic employment situation perception. But the expected monthly salary is negatively correlated with employment confidence. (2) Male college students and Science and Engineering students' epidemic anxiety have a stronger negative impact on employment confidence and employment situation perception. (3) Employment guidance has a moderating effect on the relationship between epidemic anxiety, employment confidence and employment situation perception. Employment guidance can enhance college students' employment confidence and reduce their sense of employment crisis by alleviating epidemic anxiety. Combined with the research conclusions, it is proposed that the state and schools should pay attention to the psychological counseling of college students, strengthen the employment guidance of colleges and universities, vigorously support the development of small, medium-sized and micro enterprises, and improve the employment and entrepreneurship service system of college students, so as to promote the employment of college students."
Zhang2021,Yuekan Zhang and Meng Yang and Lanyue Jiang and Hui Wang and Jinguang Xu and Junru Yang,High concentration fine particle separation performance in hydrocyclones,Minerals,11,3,2021,10.3390/min11030307,2075163X,"The vast majority of current research on hydrocyclone field centrifugal separation focuses on low concentration fluids having volume fraction less than 3%. For high-concentration fluids having volume fractions greater than 10%, which are often encountered in engineering, the law governing particle motion and the classification mechanism are still unclear. In order to gain insights into the interaction between fine particles in the high concentration hydrocyclone field and to improve the hydrocyclone separation performance of these particles, a Dense Discrete Phase Model (DDPM) of the Euler-Eulerian method under the Ansys Fluent 14.5 software was employed. Numerical simulations were carried out to study the characteristics of the hydrocyclone field of dense particles and the influence of parameters, such as the diameter of the overflow outlet, diameter of the underflow outlet, and material concentration, on separation performance. The trajectories and separation efficiencies of two kinds of fine particles with different densities and six different particle sizes at high concentration were obtained. The results show that for the hydrocyclone classification of high-concentration fine particles, particles with large density and small particle size are more likely to enter the internal cyclone and discharge from the overflow. Particles with small density and large particle size are more likely to enter the external cyclone and discharge from the underflow. The research results of this topic could provide a feasible reference and theoretical basis for the centrifugal separation of high-concentration fine particle fluid."
Jin2020,Wencheng Jin and Jayde Aufrecht and Fernando Patino-Ramirez and Heidy Cabral and Chloé Arson and Scott T. Retterer,Modeling root system growth around obstacles,Scientific Reports,10,1,2020,10.1038/s41598-020-72557-8,20452322,"State-of-the-Art models of Root System Architecture (RSA) do not allow simulating root growth around rigid obstacles. Yet, the presence of obstacles can be highly disruptive to the root system. We grew wheat seedlings in sealed petri dishes without obstacle and in custom 3D-printed rhizoboxes containing obstacles. Time-lapse photography was used to reconstruct the wheat root morphology network. We used the reconstructed wheat root network without obstacle to calibrate an RSA model implemented in the R-SWMS software. The root network with obstacles allowed calibrating the parameters of a new function that models the influence of rigid obstacles on wheat root growth. Experimental results show that the presence of a rigid obstacle does not affect the growth rate of the wheat root axes, but that it does influence the root trajectory after the main axis has passed the obstacle. The growth recovery time, i.e. the time for the main root axis to recover its geotropism-driven growth, is proportional to the time during which the main axis grows along the obstacle. Qualitative and quantitative comparisons between experimental and numerical results show that the proposed model successfully simulates wheat RSA growth around obstacles. Our results suggest that wheat roots follow patterns that could inspire the design of adaptive engineering flow networks."
Li2022,Xuebin Li and Xuesheng Liu and Yunliang Tan and Qing Ma and Baoyang Wu and Honglei Wang,Creep Constitutive Model and Numerical Realization of Coal-Rock Combination Deteriorated by Immersion,Minerals,12,3,2022,10.3390/min12030292,2075163X,"Coal-rock combination refers to the coal and rock as a whole, and the failure of the whole structure of the combination is the main cause for the instability of the deep underground engineering. In deep underground engineering, the coal-rock combination is usually under certain hydrogeological conditions, and it is prone to seepage and rheological failure instability accidents due to the long-term action of water and stress. In this study, the creep constitutive model of coal-rock combination considering the influence of moisture content was established based on the Burgers creep model. According to the experimental results of triaxial creep of rock, the relationship between the moisture content and the parameter of the Burgers creep model was derived, and the correctness of the constitutive model in this study was verified. Then, through the C++ language, the core equation of the model was modified, and the numerical calculation of the model was realized by introducing the coal-rock combination creep model considering the influence of moisture content into FLAC3D numerical simulation software. Finally, the model was used to simulate and study the creep characteristics of coal-rock combination with different moisture contents under triaxial loading. The results showed that the stress environment and moisture content have significant effects on the creep characteristics of the coal-rock combination. Under the same stress state, with the increased of moisture content, the strain rate of the coal-rock combination exhibited a non-linear rapid increase in the constant-velocity creep stage, the limit creep deformation and the instantaneous elastic deformation increased, and the viscosity coefficient was significantly decreased. For example, when the axial stress was 5 MPa and the moisture content increased from 0% to 1.5%, the strain rate increased by 44.06%, the limit creep deformation increased by 20%, the instantaneous elastic deformation increased 10.53%, and the viscosity coefficient decreased by about 50%. When the moisture content is 0%, the axial stress increased from 5 to 14 MPa, and the limit creep deformation increased nearly four times. With the increase of moisture content, this value will further expand. The research conclusions can provide a certain reference basis for the long-term stability control of surrounding rock in underground engineering affected by the water."
Abdelmoety2022,Ahmed K. Abdelmoety and Muntaseer Kainat and Nader Yoosef-Ghodsi and Yong Li and Samer Adeeb,Strain-based reliability analysis of dented pipelines using a response surface method,Journal of Pipeline Science and Engineering,2,1,2022,10.1016/j.jpse.2021.11.002,26671433,"Dent defects can decrease the life span of oil and gas pipelines. Therefore, they need constant monitoring and maintenance to ensure the pipeline's safety and integrity. Subsequently, this paper performs a strain-based reliability analysis on pipe dent defects using a response surface method (RSM) with a quadratic response surface (RS), including the interaction terms between the RS variables. The analyses are performed to determine the factors controlling the dent defects’ probability of failure (POF). Different pipe configurations, pipe lengths, indenter sizes, and dent depths are considered in this study. A suitable finite element (FE) model for the reliability analysis was developed for this study using the FE analysis software ABAQUS. The uncertainties in the pipe wall thickness, the dent depth, the yield strength of the pipe material, and the strain capacity are considered for the reliability analysis. The first-order reliability method (FORM) is used in the RSM as the reliability method to calculate the POF and the most probable point (MPP). The POFs of several dent defects were calculated. It has been found that the POF, which is highly related to the nominal value of the maximum equivalent plastic strains generated in the dent defect, is not only related to the indentation depth or the size of the indenter. Thus, the dent depth criterion used in the engineering practice can lead to inconsistent reliability levels in dented pipes."
Calvo2022,Isidro Calvo and Aitana Espin and Jose Miguel Gil-García and Pablo Fernández Bustamante and Oscar Barambones and Estibaliz Apiñaniz,Scalable IoT Architecture for Monitoring IEQ Conditions in Public and Private Buildings,Energies,15,6,2022,10.3390/en15062270,19961073,"This paper presents a scalable IoT architecture based on the edge–fog–cloud paradigm for monitoring the Indoor Environmental Quality (IEQ) parameters in public buildings. Nowadays, IEQ monitoring systems are becoming important for several reasons: (1) to ensure that temperature and humidity conditions are adequate, improving the comfort and productivity of the occupants; (2) to introduce actions to reduce energy consumption, contributing to achieving the Sustainable Development Goals (SDG); and (3) to guarantee the quality of the air—a key concern due to the COVID-19 worldwide pandemic. Two kinds of nodes compose the proposed architecture; these are the so-called: (1) smart IEQ sensor nodes, responsible for acquiring indoor environmental measures locally, and (2) the IEQ concentrators, responsible for collecting the data from smart sensor nodes distributed along the facilities. The IEQ concentrators are also responsible for configuring the acquisition system locally, logging the acquired local data, analyzing the information, and connecting to cloud applications. The presented architecture has been designed using low-cost open-source hardware and software—specifically, single board computers and microcontrollers such as Raspberry Pis and Arduino boards. WiFi and TCP/IP communication technologies were selected, since they are typically available in corporative buildings, benefiting from already available communication infrastructures. The application layer was implemented with MQTT. A prototype was built and deployed at the Faculty of Engineering of Vitoria-Gasteiz, University of the Basque Country (UPV/EHU), using the existing network infrastructure. This prototype allowed for collecting data within different academic scenarios. Finally, a smart sensor node was designed including low-cost sensors to measure temperature, humidity, eCO2, and VOC."
Chen2023,Yali Chen and Xiaozi Wang and Zhen Liu and Jia Cui and Mohamed Osmani and Peter Demian,Exploring Building Information Modeling (BIM) and Internet of Things (IoT) Integration for Sustainable Building,Buildings,13,2,2023,10.3390/buildings13020288,20755309,"Sustainable development, which has become the priority study of architectural design, is receiving increasing attention with global climate change. At the same time, the building industry is urgently changing towards intelligent and digitalized tendencies. As a result, Building Information Modeling (BIM) and the Internet of Things (IoT) make crucial contributions to the transforming process. However, there is little knowledge of the integration of BIM–IoT in sustainable building from a macro perspective. Moreover, most existing research adopts a literature review method and lacks objective quantitative analysis. Few papers use bibliometric analysis to study the respective BIM and IoT research fields. Furthermore, few studies use Citespace software tools to analyze the integrated application of BIM–IoT. Therefore, this paper aims to investigate the research frontiers and knowledge structure in BIM–IoT integration and the relationship between BIM-IoT and sustainable building and explore the research hotspots, trends, and future research directions. A quick and objective method was proposed to understand the research status of these new and rapidly developing fields. This paper uses topic search in the web of science core collection to obtain relevant literature and then uses Citespace for bibliometric analysis based on the literature review. Controlled terms and subject terms statistics from the Engineering Index core database search results are also used to briefly examine the fields’ research frontiers and hotspots as obtained from Citespace. The results show that: (1) The research on BIM–IoT integration focuses on building intelligence with BIM as the basis of application, and research on BIM–IoT integration within the field of sustainable building is currently focused on the first three phases of the life cycle. (2) The development of sustainable buildings needs to be considered on its human and social dimensions. BIM provides a platform for sharing information and communication among stakeholders involved in the building’s entire life cycle. At the same time, IoT allows occupants to better participate in buildings’ sustainable design and decision making. (3) In the future, more emerging technologies such as cloud computing and big data are required to better promote sustainable buildings and thus realize the construction of sustainable smart cities. At the same time, researchers should also pay attention to the sustainable transformation of existing buildings."
Zheng2020,Xu Zheng and Yejun Gao and Wuxing Jing and Yongsheng Wang,Multidisciplinary integrated design of long-range ballistic missile using PSO algorithm,Journal of Systems Engineering and Electronics,31,2,2020,10.23919/JSEE.2020.000011,16711793,"In the case of the given design variables and constraint functions, this paper is concerned with the rapid overall parameters design of trajectory, propulsion and aerodynamics for long-range ballistic missiles based on the index of the minimum take-off mass. In contrast to the traditional subsystem independent design, this paper adopts the research idea of the combination of the subsystem independent design and the multisystem integration design. Firstly, the trajectory, propulsion and aerodynamics of the subsystem are separately designed by the engineering design, including the design of the minimum energy trajectory, the computation of propulsion system parameters, and the calculation of aerodynamic coefficient and dynamic derivative of the missile by employing the software of missile DATCOM. Then, the uniform design method is used to simplify the constraint conditions and the design variables through the integration design, and the accurate design of the optimized variables would be accomplished by adopting the uniform particle swarm optimization (PSO) algorithm. Finally, the automation design software is written for the three-stage solid ballistic missile. The take-off mass of 29 850 kg is derived by the subsystem independent design, and 20 constraints are reduced by employing the uniform design on the basis of 29 design variables and 32 constraints, and the take-off mass is dropped by 1 850 kg by applying the combination of the uniform design and PSO. The simulation results demonstrate the effectiveness and feasibility of the proposed hybrid optimization technique."
Ding2020,Hao Ding and Xinghong Jiang and Ke Li and Hongyan Guo and Wenfeng Li,Intelligent Classification Method for Tunnel Lining Cracks Based on PFC-BP Neural Network,Mathematical Problems in Engineering,2020,,2020,10.1155/2020/8838216,15635147,"Tunnel lining crack is the most common disease and also the manifestation of other diseases, which widely exists in plain concrete lining structure. Proper evaluation and classification of engineering conditions directly relate to operation safety. Particle flow code (PFC) calculation software is applied in this study, and the simulation reliability is verified by using the laboratory axial compression test and 1: 10 model experiment to calibrate the calculation parameters. Parameter analysis is carried out focusing on the load parameters, structural parameters, dimension, and direction which affect the crack diseases. Based on that, an evaluation index system represented by tunnel buried depth (H), crack position (P), crack length (L), crack width (W), crack depth (D), and crack direction (A) is put forward. The training data of the back propagation (BP) neural network which takes load-bearing safety and crack stability as the evaluation criteria are obtained. An expert system is introduced into the BP neural network for correction of prediction results, realizing classified dynamic optimization of complex engineering conditions. The results of this study can be used to judge the safety state of cracked lining structure and provide guidance to the prevention and control of crack diseases, which is significant to ensure the safety of tunnel operation."
Quiones2022,Rocío Quiñones and Carmen Llatas and Maria Victoria Montes and Isidro Cortés,Quantification of Construction Waste in Early Design Stages Using Bim-Based Tool,Recycling,7,5,2022,10.3390/recycling7050063,23134321,"Construction and demolition waste represents a growing environmental, social, and economic problem, and has become a priority for European and worldwide policies. The early quantification of construction waste is essential for the minimisation of its production and the improvement of waste management. This requires the development of design-based tools that enable a better understanding of the expected waste produced during the construction phase. Building Information Modelling (BIM) methodologies have gained recognition in the Architecture, Engineering, Construction, and Operations (AECO) sector, largely due to their capacity for data simulation, storage, and management during the building design phase. This study presents a software application, called WE-BIM Add-in, to quantify construction waste (CW) while designing the BIM model in Revit. A validated CW quantification model which enables waste types and quantities per building element to be predicted in detail according to the European List of Waste (LoW) is integrated into the Revit workflow. Design alternatives could be effortlessly simulated in real time to assist practitioners in decision-making during the early design stages. Two alternative structural systems of a Spanish residential building were compared: a reinforced concrete structure, Option 1 (O1), and a steel structure, Option 2 (O2). The results were obtained automatically: O2, in addition to reducing 56% of O1′s waste, would have increased the waste recycling rate by 49%; and displayed in Revit, thereby remaining consistent with those of other studies that compare prefabricated systems with in situ systems. This work provides a basis for future research into the automated estimation of construction waste in BIM which could become a useful tool in waste-prevention policies."
Tang2022,Yaming Tang and Zizheng Guo and Li Wu and Bo Hong and Wei Feng and Xiaohong Su and Zhengguo Li and Yuhang Zhu,Assessing Debris Flow Risk at a Catchment Scale for an Economic Decision Based on the LiDAR DEM and Numerical Simulation,Frontiers in Earth Science,10,,2022,10.3389/feart.2022.821735,22966463,"Various risk management measures have been applied to reduce risks associated with the debris flow; however, only a few studies have adopted the economic benefit to evaluate measure effectiveness. The present study sought to explore debris flow risks at a catchment scale and establish the appropriate risk-reducing measures. The Chengbei Gully debris flow in Shanxi province (China) was selected for the case study. High-resolution topographic data of the drainage basin were obtained using the airborne LiDAR technology. FLO-2D software was used to simulate the debris flow process to perform hazard zonation. Vulnerability was estimated based on the location of elements at risk within the hazard zones and the field survey. Several structural and non-structural measures for controlling risks were proposed based on the risk assessment results, and the benefit–cost ratio was used to analyze their effectiveness. The findings indicated that the rainfall event triggering the Chengbei Gully debris flow had an 80-year return period. The total risk under this rainfall condition was 2.3 × 105 $, which was an unacceptable level according to the criteria of tolerance risk. The findings showed that the engineering measure was the best mitigation approach for the Chengbei Gully debris flow with a benefit of 1.35 million $ and a benefit–cost ratio of 6.43."
Xue2022,Gang Xue and Yanjun Liu and Zhenjie Shi and Lei Guo and Zhitong Li,Research on Trajectory Tracking Control of Underwater Vehicle Manipulator System Based on Model-Free Adaptive Control Method,Journal of Marine Science and Engineering,10,5,2022,10.3390/jmse10050652,20771312,"In order to improve the trajectory tracking accuracy of an Underwater Vehicle Manipulator System (UVMS) under uncertain disturbance conditions of ocean current, a Model-free Adaptive Control (MFAC) method was used. Combined with Radial Basis Function Neural Networks (RBFNN), the RBFNN-MFAC method is proposed to improve the performance of the controller. A hydrodynamic model of UVMS was defined in the commercial software, Fluent, to calculate hydrodynamics disturbance, and the mechanism of the dynamic model of UVMS was defined in the commercial software, Adams, to simulate the motion of UVMS. The trajectory tracking performance with various control schemes, including PID (Proportional Integral Derivative), MFAC and RBFNN-MFAC, were analyzed with the Adams and Simulink joint simulation model. The results show that the position tracking accuracy and the speed tracking accuracy with the MFAC control scheme were 68.1% and 81.0% better, respectively, than those with PID control scheme. The position tracking accuracy and the speed tracking accuracy with the RBFNN-MFAC control scheme were 66.3% and 43.1% better, respectively, than those with the MFAC control scheme. The MFAC control scheme and the RBFNN-MFAC control scheme proposed in this paper exhibit good trajectory tracking performance without the precise dynamic model of UVMS, which is of great importance to applications in engineering."
Fedotov2020,A. A. Fedotov and S. M. Sergeev and E. N. Provotorova and T. V. Prozhogina and O. Yu Zaslavskaya,The digital twin of a warehouse robot for Industry 4.0,,862,3,2020,10.1088/1757-899X/862/3/032061,1757899X,"One of the most important components in the modern conception Industry 4.0 is the digital twin. The special feature of the mechanical engineering production is the high proportion of time, spent on an interoperational storage. High efficiency of the enterprise activity is connected with the solution of a flexible automatic warehousing system creation. It is necessary to describe structural and dynamic parameters of a robot-stacker in the math formalisms. This will allow to find the optimal ratio between the load capacity, cargo speed and statistic, dynamic indicators of positioning accuracy. The obtained results of a mathematical modeling can be used as the algorithms for an integrated environment of a software development for various automation systems of warehouse processes."
Morashti2022,Jonathan Asher Morashti and Youra An and Hyunmi Jang,A Systematic Literature Review of Sustainable Packaging in Supply Chain Management,Sustainability (Switzerland),14,9,2022,10.3390/su14094921,20711050,"This exploratory study utilises quantitative analysis to deliver a systematic literature review of published journal papers from 1993 to 2020 with the aim to identify research trends and present a comprehensive overview of research focus conducted in the sustainable packaging domain within the scope of supply chain management. This research is conducted with the data mining software, NetMiner 4, utilising the three analytical tools of statistical analysis, keyword network analysis, and topic analysis. The research also utilises the qualitative method of in-depth interviews in order to investigate current trends and perspectives on the future of sustainable packaging and to validate the analysis results. The research findings reveal that research in the field of ‘sustainable packaging in supply chain management’ field has been extremely limited, and this study acts to address this research gap. The results confirm that the vast majority of research focus has been in the fields of engineering and science. Research on the topic has gained momentum and has significantly increased since 2013 with research trends becoming increasingly diversified and gradually aligned with the concept of circular economy, while the topic of operational management has been highlighted as an area requiring additional attention. The keyword frequency analysis reveals the following highest occurring keywords in TF: life cycle; environmental impact; consumer; transportation; and production. The highest occurring keywords in TF-IDF: production; transportation; consumer; food; and environmental impact. Topic modelling revealed the following six topics: consumer behaviour; environmental pollution; circular economy; waste management; resource conservation; and operational management. This study contributes to understanding past, present, and future research agendas, and can be utilised as foundation for research development, as it provides insight to current research status and trends provided by the keyword network analysis highlighting research focus and trends in ‘sustainable packaging in supply chain management’."
Zhou2022,Jibiao Zhou and Yanyong Guo and Jian Sun and Erze Yu and Rui Wang,Review of bike-sharing system studies using bibliometrics method,Journal of Traffic and Transportation Engineering (English Edition),9,4,2022,10.1016/j.jtte.2021.08.003,20957564,"Compared to motorized modes, bike-sharing systems (BSSs) are generally recognized as an environmentally friendly mode of transport and mobility. Due to the advantages and benefits of BSSs, they have spread globally in the past decade. The mapping knowledge domain (MKD) technique is an important tool for bibliometric analysis that can directly reflect the development status and trends in a research field and has been extensively applied in natural science, medical science, engineering and technology, humanities and the social sciences. In this paper, we conduct a systematic analysis of the development trend in bike-sharing studies taken from Web of Science (WoS) Core Collection articles published between 2010 and 2020 using the MKD software tool VOSviewer and CiteSpace. The results show that the topics of the cited documents can be divided into three categories: (a) development, operation mode and lessons learned; (b) BSS static rebalancing problem; (c) spatiotemporal characteristics and demand prediction. Next, we conduct document co-citation analysis and keyword co-occurrence analysis to visually explore the research trends in bike-sharing studies. Our results also find that, (a) bicycle rebalancing problem, (b) travel behavioral movements and barrier, (c) impact factors and characteristics of the internal usage demand, (d) innovation and sustainability for BSSs in the future, and (e) built environment and land-use, are the five major research areas and interests for bike-sharing studies. Finally, examining the trends in BSSs studies by identifying keyword bursts allows an on-demand characteristics analysis based on multisource data fusion technology. This study expands the application field of the MKD analysis method and promotes the development of BSSs against a background of new technology innovation."
Samimpey2020,Rozita Samimpey and Ehsan Saghatforoush,A systematic review of prerequisites for constructability implementation in infrastructure projects,Civil Engineering Journal (Iran),6,3,2020,10.28991/cej-2020-03091493,24763055,"Success in infrastructure projects requires success in all phases of the project, including design, construction, and operation. One of the necessary actions for developing countries to construct their economic infrastructures, is implementing infrastructure plans. This industry should focus more on the construction process and utilizing creative tools and new concepts for construction development. The reason of it is because of delivering the project with certain quality, in time and with the given budget. Contractors should have new strategies for construction to optimize project completion, and constructability. Accordingly, constructability improvements have become the concern of construction industry practitioners. Considering constructability issues in the early stages of the project enhances identifying design limitations that prevent capabilities of contractors to take part in planning and improving project performance. The purpose of this study is identifying the prerequisites of constructability to resolve the current problems of projects, including inappropriate plans without implementability, poor decision making in design, and lack of sufficient implementation experience in the design engineering team. This study provides a list of prerequisites for constructability implementation in infrastructure projects. Accordingly, it identifies the prerequisites, using Systematic Literature Review (SLR) technique. The NVivo software is used to facilitate the qualitative analyses."
Bano2021,Mukhtiar Bano and Amir Qayyum and Rao Naveed Bin Rais and Syed Sherjeel A. Gilani,Soft-Mesh: A Robust Routing Architecture for Hybrid SDN and Wireless Mesh Networks,IEEE Access,9,,2021,10.1109/ACCESS.2021.3089020,21693536,"Wireless Mesh Networks (WMNs) are considered self-organizing, self-healing, and self-configuring networks. Despite these exciting features, WMNs face several routing challenges including scalability, reliability and link failures, mobility, flexibility, and other network management issues. To address these challenges, WMNs need to make programmable to allow modifications of standard techniques to be configured and implemented through software programs that can be resolved by integrating Software Defined Networking (SDN) architecture. SDN, being a cutting-edge technology promises the facilitation of network management as well as routing issues of wireless mesh networks. However, the evolution of the legacy IP-based network model in its entirety leads to technical, operational, and economic problems that can be mitigated by full interoperability between SDN and existing IP devices. This study introduces a Robust Routing Architecture for Hybrid Software-Defined and Wireless Mesh Networks (Soft-Mesh), by systematic and gradual transitioning of WMNs to SDNs in an efficient manner. The main objective of this paper is to suggest improvements to the architecture of the SDN node that allow the implementation of various network functions such as routing, load balancing, network control, and traffic engineering for the hybrid SDN and IP networks. Mininet-WiFi Simulator is used to perform various experiments to evaluate the performance of proposed architecture by creating a hybrid network topology with a varying number of nodes that is 50, 100, 150, 200, and 250 including SDN hybrid and legacy nodes with varying proportion of SDN hybrid and legacy nodes. Results are taken for the average UDP throughput, end-to-end delay, packet drop ratio, and routing overhead while comparing with traditional routing protocols including Optimized Link State Routing (OLSR) and Better Approach to Mobile Adhoc Networking (BATMAN) and with existing hybrid SDN/IP routing architectures including Hakiri and wmSDN. The analysis of simulation results shows that the proposed architecture Soft-Mesh outperforms in terms of the aforementioned performance metrics than the traditional and exiting hybrid routing protocols. Soft-Mesh gives 50% to 70% improved results concerning the incremental proportion of SDN hybrid nodes. &copy; 2013 IEEE."
Liu2021,Jingyue Jimmy Liu,Advances and Applications of Atomic-Resolution Scanning Transmission Electron Microscopy,Microscopy and Microanalysis,27,5,2021,10.1017/S1431927621012125,14358115,"Although scanning transmission electron microscopy (STEM) images of individual heavy atoms were reported 50 years ago, the applications of atomic-resolution STEM imaging became wide spread only after the practical realization of aberration correctors on field-emission STEM/TEM instruments to form sub-Ångstrom electron probes. The innovative designs and advances of electron optical systems, the fundamental understanding of electron-specimen interaction processes, and the advances in detector technology all played a major role in achieving the goal of atomic-resolution STEM imaging of practical materials. It is clear that tremendous advances in computer technology and electronics, image acquisition and processing algorithms, image simulations, and precision machining synergistically made atomic-resolution STEM imaging routinely accessible. It is anticipated that further hardware/software development is needed to achieve three-dimensional atomic-resolution STEM imaging with single-atom chemical sensitivity, even for electron-beam-sensitive materials. Artificial intelligence, machine learning, and big-data science are expected to significantly enhance the impact of STEM and associated techniques on many research fields such as materials science and engineering, quantum and nanoscale science, physics and chemistry, and biology and medicine. This review focuses on advances of STEM imaging from the invention of the field-emission electron gun to the realization of aberration-corrected and monochromated atomic-resolution STEM and its broad applications."
Stnkel2020,Patrick Stünkel and Harald König and Yngve Lamo and Adrian Rutle,Towards multiple model synchronization with comprehensive systems,,12076 LNCS,,2020,10.1007/978-3-030-45234-6_17,16113349,"Model management is a central activity in Software Engineering. The most challenging aspect of model management is to keep models consistent with each other while they evolve. As a consequence, there has been increasing activity in this area, which has produced a number of approaches to address this synchronization challenge. The majority of these approaches, however, is limited to a binary setting; i.e. the synchronization of exactly two models with each other. A recent Dagstuhl seminar on multidirectional transformations made it clear that there is a need for further investigations in the domain of general multiple model synchronization simply because not every multiary consistency relation can be factored into binary ones. However, with the help of an auxiliary artifact, which provides a global view over all models, multiary synchronization can be achieved by existing binary model synchronization means. In this paper, we propose a novel comprehensive system construction to produce such an artifact using the same underlying base modelling language as the one used to define the models. Our approach is based on the definition of partial commonalities among a set of aligned models. Comprehensive systems can be shown to generalize the underlying categories of graph diagrams and triple graph grammars and can efficiently be implemented in existing tools."
Hosamo2022,Haidar Hosamo Hosamo and Henrik Kofoed Nielsen and Ammar Njeeb Alnmr and Paul Ragnar Svennevig and Kjeld Svidt,A review of the Digital Twin technology for fault detection in buildings,Frontiers in Built Environment,8,,2022,10.3389/fbuil.2022.1013196,22973362,"This study aims to evaluate the utilization of technology known as Digital Twin for fault detection in buildings. The strategy consisted of studying existing applications, difficulties, and possibilities that come with it. The Digital Twin technology is one of the most intriguing newly discovered technologies rapidly evolving; however, some problems still need to be addressed. First, using Digital Twins to detect building faults to prevent future failures and cutting overall costs by improving building maintenance is still ambiguous. Second, how Digital Twin technology may be applied to discover inefficiencies inside the building to optimize energy usage is not well defined. To address these issues, we reviewed 326 documents related to Digital Twin, BIM, and fault detection in civil engineering. Then out of the 326 documents, we reviewed 115 documents related to Digital Twin for fault detection in detail. This study used a qualitative assessment to uncover Digital Twin technology’s full fault detection capabilities. Our research concludes that Digital Twins need more development in areas such as scanner hardware and software, detection and prediction algorithms, modeling, and twinning programs before they will be convincing enough for fault detection and prediction. In addition, more building owners, architects, and engineers need substantial financial incentives to invest in condition monitoring before many of the strategies discussed in the reviewed papers will be used in the construction industry. For future investigation, more research needs to be devoted to exploring how machine learning may be integrated with other Digital Twin components to develop new fault detection methods."
He2022,Jiaying He and Xiaohui Jiang and Yuxin Lei and Wenjuan Cai and J. Zhang,"Temporal and Spatial Variation and Driving Forces of Soil Erosion on the Loess Plateau before and after the Implementation of the Grain-for-Green Project: A Case Study in the Yanhe River Basin, China",International Journal of Environmental Research and Public Health,19,14,2022,10.3390/ijerph19148446,16604601,"To curb soil erosion, the Grain-for-Green Project has been implemented in the Loess Plateau region, and there have been few quantitative evaluations of the impact of ecological engineering on the spatial distribution of soil erosion on the Loess Plateau. In this paper, we used ArcGIS software, the Revised Universal Soil Loss Equation (RUSLE) model and the Geographic Detector (GeoDetector) model to investigate the changes in the spatial distribution of soil erosion and driving forces before and after the implementation of the Grain-for-Green Project in Yanhe River Basin, a typical area on the Loess Plateau. After the implementation of the Grain-for-Green Project, the soil erosion showed a decreasing trend over time and from local improvement to global optimization in space. The implementation of the Grain-for-Green Project led to changes in the dominant driving force of the spatial distribution of soil erosion, with the dominant driving force changing from the slope factor to the vegetation coverage factor. The main driving force of the two-factor interaction on soil erosion spatial differentiation changed from the slope factor and other factors to the vegetation coverage and other factors. The Grain-for-Green Project mainly influenced soil erosion by increasing the vegetation cover. The effect of the Grain-for-Green Project on the spatial distribution of soil erosion had hysteresis and spatial differences, and the direct and indirect driving forces generated by ecological engineering reached more than 50% on average."
Wang2022,You Wang and Ziwei Wang and Tingting Ma and Guowei Li and Huixia Tie,Research on the Realization Path of Railway Intelligent Construction Based on System Engineering,Sustainability (Switzerland),14,11,2022,10.3390/su14116945,20711050,"The implementation of railway intelligent construction is the need of national strategic development and the demand of society. Based on the idea of system engineering, this paper pro-poses a three‐dimensional railway intelligent construction system architecture composed of a full life cycle, management level and technical support. Based on this architecture, a “three‐step” implementation path is proposed. Then, it analyzes the technology support framework required in the architecture based on Building Information Modeling (BIM), incorporating Global Positioning System (GPS) and Geographic Information System (GIS), algorithmic prediction and machine learning technology, Internet of Things (IoT) and artificial intelligence technology, big data and cloud computing technology, and the application of railway intelligent construction system architecture is analyzed by taking a railway tunnel project in Zhejiang Province of China as an example. Finally, it discusses the problems that may be encountered in the implementation of railway intelligent construction and puts forward relevant suggestions. The results show that railway intelligent construction is an essential way. At present, China’s railway intelligent construction is still in the primary stage. The design organization should do a good job in the top‐level design and accumulate sufficient data for the later stage. All parties in the middle stage of construction should do a good job in the induction and integration of information and accumulate sufficient experience. In this way, we can integrate into the advanced stage and give full play to the advantages of software and hardware integrated applications such as BIM, IoT, big data, cloud computing and intelligent devices so as to truly realize the intellectualization and modernization of railway construction."
Wu2021,Yue Wu and Weiguo Qiao and Yanzhi Li and Yabing Jiao and Shuai Zhang and Zonghao Zhang and Huini Liu,Application of Computer Method in Solving Complex Engineering Technical Problems,IEEE Access,9,,2021,10.1109/ACCESS.2021.3073490,21693536,"In order to verify the applicability of the computer numerical simulation method under complex engineering conditions, and solve the problem that it is difficult to guarantee the normal construction of underground projects with a depth of more than 1 km (it is difficult to carry out multiple field attempts under high-depth conditions, and if you simply use ordinary the calculation method, its calculation accuracy is difficult to guarantee). For the construction of complex geological conditions of an underground mine with a depth of 1300 m in the Central Plains of China, based on the finite difference method (FDM) with high calculation accuracy, using FLAC $^\{\mathrm \{3D\}\}$ software based on FDM, and using progressive design method (PDM), design a variety of different schemes and analyze the mechanism of force and deformation of the surrounding rock mass of the project. Apply the obtained optimal scheme in actual engineering, and monitor the actual force and deformation. Comparing the results of the computer numerical simulation with the monitoring results, the results show that the computer numerical simulation method can also have good application in the actual engineering under the geological conditions of ultra-kilometer depth. It also provides theoretical basis and technical guidance for engineering hypothesis under approximate conditions."
Ogras2020,Selman Ogras and Fevzi Onen,Flood Analysis with HEC-RAS: A Case Study of Tigris River,Advances in Civil Engineering,2020,,2020,10.1155/2020/6131982,16878094,"Floods are seen in countries in tropical climatic zones, both in terms of quantity and harm. The non-tropical climate countries such as Turkey are also affected by the floods. The geographical structure of Turkey is extremely complex and varies even at short distance. Therefore, the shape and effects of the floods vary from region to region. Considering the peculiar state of nature, floods, which are the greatest disasters after the earthquake, are unlikely to occur. But floods are becoming more risky for human beings day by day because of the population growth, need of water and settlements, wrong zoning plan, and unplanned engineering practices. Regulation comes at the beginning of measures to be taken to minimize the damages that occur from the floods. To do these studies, it must be specified the changes which bridges on the rivers and hydraulics structures like regulator cause in cross sections and the effects of the changes to water surface profile due to the natural state of the land. In order to determine water surface profiles, many software packages have been developed for facilitating the analysis and calculation. HEC-RAS is one of them. In this study, the floodplain analysis was handled between Diyarbakir-Silvan Highway and historical Ten-Eyed Bridge. There are three bridges, and one of which are historical bridges, as well as fertile agricultural lands, facilities, and hospitals in the Dicle University campus, the Hevsel Gardens on the UNESCO World Cultural Heritage List, and some residential areas on the route under study. The aim of the study we have done in this much important route is to evaluate the flood areas and create a flood hazard map which can predict risky areas. And also contributing to the Tigris River Rehabilitation Project is one of the aims. About methodology, the 1/1000 maps of the study area were digitized using the AutoCAD Civil 3D program and cross sections were made by obtaining the digital elevation models of the region. The obtained cross sections were defined in the HEC-RAS software, and the hydraulic characteristics of the flood bed and the water surface profiles of the Q25, Q50, Q100, and Q500 flood recurring and one-dimensional floodplain analysis of the Tigris River were determined."
Gbadamosi2022,Saheed Lekan Gbadamosi and Fejiro S. Ogunje and Samuel Tita Wara and Nnamdi I. Nwulu,Techno-Economic Evaluation of a Hybrid Energy System for an Educational Institution: A Case Study,Energies,15,15,2022,10.3390/en15155606,19961073,"This study evaluates the technical, economic and environmental benefits of renewable energy resources (RER) for electricity supply to large size buildings in an educational institution. The cost of energy generation coupled with the epileptic power supply has led to the demand for an alternative source of energy supply to an education institution in Nigeria. The essence of renewable energy generation is becoming more glaring and a hybrid energy system (HES) is believed to deliver efficient and sustainable energy for the institutions; this paper aims to analyse the techno-economic assessment of a HES design setup at the College of Engineering, Afe Babalola University Ado-Ekiti for powering the university buildings; this grid connected system was assessed with various system configurations was simulated using hybrid optimization model for electric renewables (HOMER) software and the levelized cost of energy (LCOE) with the consideration of the HES benefits was developed. The results obtained from the simulation indicate that the grid and solar Photovoltaic (PV) system provide an optimal system that adequately meets the load demand with more renewable energy integration and this significantly reduces the cost of energy by 45% and also causes a 32.09% reduction in CO2 emissions; this configuration is environmentally sustainable and financially suitable for electrifying an educational institution."
BarretoJunior2021,Camilo de Lellis Barreto Junior and Alexandre Cardoso and Edgard Afonso Lamounier Júnior and Paulo Camargos Silva and Alexandre Carvalho Silva,Designing virtual reality environments through an authoring system based on cad floor plans: A methodology and case study applied to electric power substations for supervision,Energies,14,21,2021,10.3390/en14217435,19961073,"The adoption of Virtual Reality (RV) technologies in prototype design and process revision has contributed to multiple industry areas. Nonetheless, the development of VR systems for engineering is a complex task, as it involves specialized teams handling low-level code development. Given these problems, the goal of this study is presenting a methodology for designing VR, through an Authoring System based on Computer-Aided Design (CAD). The presented methodology provides an easy integration of electric power substation floor plans and Virtual Reality software (VRS), as well as three-dimensional and symbol modeling conventions. Centralized software architecture was developed, composed of the CAD Editor, input manager and VRS. The methodology was evaluated through a case study applied to the conception (elaboration) of electric power substations (EPS) as part of a Research and Development (R&D) project for training and field assets supervision. The results demonstrated visual precision and high integrity in elaboration of a VR environment from the CAD floor plan. This work also presents a comparative analysis between manual conception and the Authoring System."
Hongqiang2020,Ma Hongqiang and Chen Hongyu and Hou Jinliang and Ge Jieya and Lv Wei,CiteSpace-based visualization analysis on mappings of alkali-activated cementitious materials in the field of engineering research,Case Studies in Construction Materials,12,,2020,10.1016/j.cscm.2020.e00350,22145095,"Alkali-activated cementitious materials (AACM) became the most promising alternative materials of Portland cement due to their merits including early strength, high environmental adaptability and low environmental impact. The scientific knowledge mapping was drawn with CiteSpace Visual Analysis Software and the method combining qualitative and quantitative analysis, the co-citation network based on the core collection of Web of Science is generated, and frontiers and hotspots of studies in AACM are explored to provide theoretical guidance for subsequent researches. The research results showed that the publications with respect to the field of AACM rocketed recently and had become an increasingly interdisciplinary research subject. Relevant researches were most conducted in China, Australia and USA, however, European countries have a high degree of research content centrality and are closely related to other countries. The most studied materials were the fly ash, slag and metakaolin based on the Visual Analysis, and researchers focused on the modification of AACM and studies in new solid wastes in recently, and the studies targeting at practical engineering applications. National or international agencies are to intensify cooperation and improve practical engineering applications of different AACM to improve scientific research significance. The follow-up researches are prospected in this paper. The relations between the research interest, subject, hotspot, author, journal and contributing institutions are highlighted, promoting comprehensive understandings on the field of AACM."
Stocco2022,Thiago Domingues Stocco and Mayara Cristina Moreira Silva and Marcus Alexandre Finzi Corat and Gabriely Gonçalves Lima and Anderson Oliveira Lobo,Towards Bioinspired Meniscus-Regenerative Scaffolds: Engineering a Novel 3D Bioprinted Patient-Specific Construct Reinforced by Biomimetically Aligned Nanofibers,International Journal of Nanomedicine,17,,2022,10.2147/IJN.S353937,11782013,"Introduction: Three of the main requirements that remain major challenges in tissue engineering of the knee meniscus are to engineer scaffolds with compatible anatomical shape, good mechanical properties, and microstructure able to mimic the architecture of the extracellular matrix (ECM). In this context, we presented a new biofabrication strategy to develop a three-dimensional (3D) meniscus-regenerative scaffold with custom-made macroscopic size and microarchitecture bioinspired by the organization of structural fibers of native tissue ECM. Methods: The concept was based on the combination of bioprinted cell-laden hydrogel (type 1 collagen) reinforced by multilayers of biomimetically aligned electrospun nanofibrous mats (polycaprolactone/carbon nanotubes, PCL/CNT), using a patient-specific 3D digital meniscus model reconstructed from MRI data by free and open-source software. Results: The results showed that the incorporation of aligned nanofibers sheets between the hydrogel layers enhanced the scaffold’s structural integrity and shape fidelity compared to the nanofiber-free collagen hydrogel. Furthermore, mechanical compression tests demonstrated that the presence of nanofiber layers significantly improved the mechanical properties of the bioprinted construct. Importantly, the introduction of PCL/CNT nanofibrous mats between the layers of the bioprinted collagen hydrogel did not negatively affect cell viability, in which mesenchymal stem cells remained viable even after 7 days of culture within the scaffold. Conclusion: Overall, these findings evidence that this bioengineering approach offers a promising strategy for fabricating biomimetic meniscus scaffolds for tissue engineering."
Tseng2021,Kuo Hsiung Tseng and Meng Yun Chung and Li Hsien Chen and Pei Yao Chang,Green smart campus monitoring and detection using LoRa,Sensors,21,19,2021,10.3390/s21196582,14248220,"Along with the rapid development of sensing systems and wireless transmission technology, the scope of application of the IoT has substantially increased, and research and innovation that integrate artificial intelligence. This study integrated civil engineering and electrical engineering to establish a universal and modularized long-term sensing system. Aiming at positive construction in civil engineering, the campus of National Taipei University of Technology was used as the experimental site as a green campus. This paper focused on the cooling effect of the green roof and the temperature difference of the solar panel to effectively isolate the direct sunlight on the roof of the building. To achieve long-term monitoring, energy consumption must be minimized. Considering that the distance between sensor nodes in the experimental site was over dozens of feet, LoRa transmission technology was selected for data transmission. LoRa only consumes a small amount of energy during data transmission, and it can freely switch between work modes, achieving optimal power utilization efficiency. The greening-related research results indicated that the shade from solar panels on the rooftop could effectively reduce the temperature increase caused by direct sunlight on concrete surfaces. The temperature reduction effect was positively correlated with whether the solar panels provided shade. After 1 week of monitoring, we observed that having plants on the rooftop for greening negatively correlated with temperature reduction efficiency. Permeable pavement on the ground was positively correlated with temperature reduction efficiency. However, its temperature reduction efficiency was inferior to that of solar panel shading. The temperature difference between high-rise buildings and the ground was approximately 1–2 °C. At the same elevation, the temperature difference between buildings with and without greening was approximately 0.8 °C. Regarding the sensing system designed for this site, both hardware and software could be flexibly set according to the research purposes, precision requirements of the sites, and the measurement scope, thereby enabling their application in more fields."
Hussain2022,Qudeer Hussain and Anat Ruangrassamee and Panuwat Joyklad and Anil C. Wijeyewickrema,Shear Enhancement of RC Beams Using Low-Cost Natural Fiber Rope Reinforced Polymer Composites,Buildings,12,5,2022,10.3390/buildings12050602,20755309,"The aim of this research work is to investigate the efficiency of newly developed Natural Fiber Rope Reinforced Polymer (NFRRP) composites to enhance the shear strength of reinforced concrete (RC) beams. Two types of NFRRP composites were made using low-cost hemp and cotton fiber ropes. The effectiveness of this NFRRP confinement in increasing the shear, energy dissipation, and deformation capacities of concrete beams was studied. The effect of these natural fiber ropes with different configurations on beams was investigated. The responses of seven RC beams with different spacing arrangements of natural fiber ropes were evaluated in terms of shear enhancement, deflection, energy dissipation capacity, effect of strengthening configuration, rope types, and ultimate failure modes. The NFRRP composites exceptionally enhanced the load carrying abilities, energy dissipation, and deformation capabilities of RC beams as compared to the control beam. The ultimate load carrying capacities of natural hemp and cotton Fiber Rope Reinforced Polymer (FRRP) composite confined beams were found to be 63% and 56% higher than that of the control beam, respectively. Thus, the shear strengthening of RC beams using natural fiber ropes is found to be an effective technique. Finite Element Analysis was also carried out by using the Advanced Tool for Engineering Nonlinear Analysis (ATENA) software. The analysis results compare favorably with the tests’ results."
Votinr2021,Patrik Voštinár and Dana Horváthová and Martin Mitter and Martin Bako,The look at the various uses of VR,Open Computer Science,11,1,2021,10.1515/comp-2020-0123,22991093,"Virtual, augmented and mixed reality (VR, AR and MR) infiltrated not only gaming, industry, engineering, live events, entertainment, real estate, retail, military, etc., but as surveys indicate, also healthcare and education. In all these areas there is a lack of software development experts for VR, AR and MR to meet the needs of practice. Therefore, our intention at the Department of Computer Science, Faculty of Natural Sciences, Matej Bel University in Banská Bystrica, Slovakia, is to focus on the education and enlightenment of these areas. The aim of this article is to show the role of interactivity in different VR applications and its impact on users in three different areas: gaming, healthcare and education. In the case of one application of Arachnophobia, we also present the results of the research using a questionnaire."
Arama2021,Zülal Akbay Arama and Aylin Ece Kayabekir and Gebrail Bekdaş and Sanghun Kim and Zong Woo Geem,The usage of the harmony search algorithm for the optimal design problem of reinforced concrete retaining walls,Applied Sciences (Switzerland),11,3,2021,10.3390/app11031343,20763417,"In this paper, the Harmony Search (HS) algorithm is utilized to perform single and multivariate parametric studies to acquire the optimization of both size and cost of reinforced concrete (RC) retaining walls embedded in pure frictional soils. The geotechnical properties of the backfill and foundation soil such as shear strength angle, unit weight, and the ultimate bearing pressure of the soil have been used to create different cases for evaluating the effects of site properties on the size and cost of the wall. The change of depth of excavation and surcharge loading condition is fictionalized for generating different environmental conditions for all envisaged soil profiles to predict possible rates of influences. The unit cost of the concrete has also been evaluated as a variant to show the economic constraints on the selection of structural materials. The results of the analyses represent the integrated influences of different significant parameters on the achievement of minimum cost-dimension optimization. Besides, a well-known commercial geotechnical engineering software is used to compare the appropriateness of the suggested designs in terms of both the attainment of geotechnical stability and the structural requirements. Consequently, this study can guide both researchers and designers to select the proper and optimal sections of RC-retaining wall systems with simultaneous analyses of parameters that are influenced by the design process. Furthermore, the optimization results indicate that a significant cost reduction may be achieved when compared with the traditional pre-design method."
Carlini2020,Maurizio Carlini and Sonia Castellucci and Andrea Mennuni and Sara Selli,Simulation of anaerobic digestion processes: Validation of a novel software tool ADM1-based with AQUASIM,Energy Reports,6,,2020,10.1016/j.egyr.2020.08.030,23524847,"Nowadays pollution and greenhouse gasses emissions led to the definition of action plans and guidelines aimed at improving renewable power production and circular & green economy approaches. Anaerobic digestion of organic wastes represents a meeting point between circular economy and bioenergy exploitation. The huge variety of residual biomass materials for anaerobic digestion makes clear the importance of the simulation process to underline biogas production and critical operating plant conditions for each kind of processed substrate and used reactors. Main target of this work is the definition of a software tool, called ADMS 1.0 (Anaerobic Digestion Model Simulator version 1.0), by which the user can conduct both implementation and simulation of anaerobic digestion biochemical processes (continuous-flow stirred tank reactors and batch reactors). This software tool is composed by an ad-hoc graphic user interface (GUI) developed in Python and a computing core, written in MATLAB. Anaerobic Digestion Model N.1 (ADM1) was implemented. Two simulations were carried out implementing the same scenario both in AQUASIM and ADMS 1.0 to validate the correct functioning and numerical consistency of the proposed tool. The scenario is based on experimental data. The results obtained by both software tools were compared to confirm the consistency of numerical outputs. Differences between these results have been measured by the introduction of a mean of the relative errors associated to the singular simulated biochemical component. This error, expressed in percentage, represents the discrepancy between AQUASIM and ADMS 1.0 results. The relative errors average value is equal to 0.0648%, which is consistently less than the acceptable engineering value of 5%."
Bozkurt2022,Ferhat Bozkurt,A deep and handcrafted features-based framework for diagnosis of COVID-19 from chest x-ray images,Concurrency and Computation: Practice and Experience,34,5,2022,10.1002/cpe.6725,15320634,"Automatic early diagnosis of COVID-19 with computer-aided tools is crucial for disease treatment and control. Radiology images of COVID-19 and other lung diseases like bacterial pneumonia, viral pneumonia have common features. Thus, this similarity makes it difficult for radiologists to detect COVID-19 cases. A reliable method for classifying non-COVID-19 and COVID-19 chest x-ray images could be useful to reduce triage process and diagnose. In this study, we develop an original framework (HANDEFU) that supports handcrafted, deep, and fusion-based feature extraction techniques for feature engineering. The user interactively builds any model by selecting feature extraction technique and classification method through the framework. Any feature extraction technique and model could then be added dynamically to the library of software at a later time upon request. The novelty of this study is that image preprocessing and diverse feature extraction and classification techniques are assembled under an original framework. In this study, this framework is utilized for diagnosing COVID-19 from chest x-ray images on an open-access dataset. All of the experimental results and performance evaluations on this dataset are performed with this software. In experimental studies, COVID-19 prediction is performed by 27 different models through software. The superior performance with accuracy of 99.36% is obtained by LBP+SVM model."
Paulsen2020,Hilko Paulsen and Victoria Zorn and David Inkermann and Nine Reining and Julian Baschin and Thomas Vietor and Simone Kauffeld,Socio technical system analysis and design of virtualisation processes: A report on practice regarding virtual initial start-up,Gruppe. Interaktion. Organisation. Zeitschrift fur Angewandte Organisationspsychologie,51,1,2020,10.1007/s11612-020-00507-z,23666218,"This report on practice for the journal “Group. Interaction. Organization” considers a company’s goal of virtualising initial start-up of machines from a socio-technical perspective. In an increasingly digitalised work environment, opportunities arise to move physical tasks toward virtuality, thus allowing digital copies to be used in virtual simulations rather than having to physically built a product for testing first. Before going ahead with the conventional physical initial start-up of machines, a company from the mechanical and plant engineering sector plans to simulate initial start-up with specialised software. Thus, interaction of control software and machine hardware is simulated and checked with the goal of identifying errors and problems early on in the development process. This new technology and its implementation into product development is deliberated from a socio-technical perspective. In this report, human, technologial and organisational aspects and their interactions are analysed and shaped. Results indicate that virtualising intial start-up encourages concurrent product development processes while working on sales orders as well as a more modular interconnectedness of all engineering disciplines involved in product development. Concurrent processes, however, also require higher levels of media and self management KSAOs from employees."
Olukoya2022,Oluwafemi Olukoya,Assessing frameworks for eliciting privacy & security requirements from laws and regulations,Computers and Security,117,,2022,10.1016/j.cose.2022.102697,01674048,"The processing of personal data has become a prominent concern for stakeholders when selecting software or service providers to serve their needs. Different laws and legislation have been introduced to standardize and strengthen data protection policies across different countries to protect such data. Therefore, businesses and organizations responsible for managing personal data are obligated to implement the privacy and security requirements established by these laws and legislation. Different methods and tools have been provided for eliciting requirements for legally compliant software based on the relevant data protection laws and legislation. However, little has been done in assessing these methodologies on regulations outside the EU and the US. This paper aims to assess these methodologies on other information security laws and regulations beyond the General Data Protection Regulation (GDPR) and Health Insurance Portability and Accountability Act (HIPAA) by eliciting security requirements explicitly focusing on the Nigerian data protection regulation. To investigate the applicability of these methodologies, we use the extracted privacy and security requirements with information communication protocols in verifying compliance in procedural practices of products and services in the financial technology sector. The analysis reports on the completeness, consistency, and utility of the frameworks. Finally, foundational research directions for interoperable standards for eliciting software requirements from legal texts are proposed."
Laciok2020,Vendula Laciok and Ales Bernatik and Michal Lesnak,Experimental implementation of new technology into the area of teaching occupational safety for industry 4.0,International Journal of Safety and Security Engineering,10,3,2020,10.18280/ijsse.100313,2041904X,"The use of new technologies (additive technology, collaborative robotics, virtual or augmented reality) in teaching and preparing for it gives the teacher many different ways to activate students to learn. Therefore, this article focuses on the options for using virtual reality in the field of occupational safety. A work injury scenario was created in the XVR software environment. It was aimed at students studying Occupational and Process Safety at the Faculty of Safety Engineering (FSE), VSB-Technical University of Ostrava. In the future, they will be professionally qualified in risk prevention (Health, Safety, Environment Professional, HSE). The aim was to train students in: an employer's obligations during a work injury, the HSE Professional's job during a work injury, cooperating with the emergency services and the Czech Police."
Horcas2023,José Miguel Horcas and Mónica Pinto and Lidia Fuentes,Empirical analysis of the tool support for software product lines,Software and Systems Modeling,22,1,2023,10.1007/s10270-022-01011-2,16191374,"For the last ten years, software product line (SPL) tool developers have been facing the implementation of different variability requirements and the support of SPL engineering activities demanded by emergent domains. Despite systematic literature reviews identifying the main characteristics of existing tools and the SPL activities they support, these reviews do not always help to understand if such tools provide what complex variability projects demand. This paper presents an empirical research in which we evaluate the degree of maturity of existing SPL tools focusing on their support of variability modeling characteristics and SPL engineering activities required by current application domains. We first identify the characteristics and activities that are essential for the development of SPLs by analyzing a selected sample of case studies chosen from application domains with high variability. Second, we conduct an exploratory study to analyze whether the existing tools support those characteristics and activities. We conclude that, with the current tool support, it is possible to develop a basic SPL approach. But we have also found out that these tools present several limitations when dealing with complex variability requirements demanded by emergent application domains, such as non-Boolean features or large configuration spaces. Additionally, we identify the necessity for an integrated approach with appropriate tool support to completely cover all the activities and phases of SPL engineering. To mitigate this problem, we propose different road map using the existing tools to partially or entirely support SPL engineering activities, from variability modeling to product derivation."
Spinellis2020,Diomidis Spinellis and Zoe Kotti and Audris Mockus,A Dataset for GitHub Repository Deduplication,,,,2020,10.1145/3379597.3387496,,"GitHub projects can be easily replicated through the site's fork process or through a Git clone-push sequence. This is a problem for empirical software engineering, because it can lead to skewed results or mistrained machine learning models. We provide a dataset of 10.6 million GitHub projects that are copies of others, and link each record with the project's ultimate parent. The ultimate parents were derived from a ranking along six metrics. The related projects were calculated as the connected components of an 18.2 million node and 12 million edge denoised graph created by directing edges to ultimate parents. The graph was created by filtering out more than 30 hand-picked and 2.3 million pattern-matched clumping projects. Projects that introduced unwanted clumping were identified by repeatedly visualizing shortest path distances between unrelated important projects. Our dataset identified 30 thousand duplicate projects in an existing popular reference dataset of 1.8 million projects. An evaluation of our dataset against another created independently with different methods found a significant overlap, but also differences attributed to the operational definition of what projects are considered as related."
Carnot2020,Miriam Louise Carnot and Jorge Bernardino and Nuno Laranjeiro and Hugo Gonçalo Oliveira,Applying text analytics for studying research trends in dependability,Entropy,22,11,2020,10.3390/e22111303,10994300,"The dependability of systems and networks has been the target of research for many years now. In the 1970s, what is now known as the top conference on dependability—The IEEE/IFIP International Conference on Dependable Systems and Networks (DSN)—emerged gathering international researchers and sparking the interest of the scientific community. Although it started in niche systems, nowadays dependability is viewed as highly important in most computer systems. The goal of this work is to analyze the research published in the proceedings of well-established dependability conferences (i.e., DSN, International Symposium on Software Reliability Engineering (ISSRE), International Symposium on Reliable Distributed Systems (SRDS), European Dependable Computing Conference (EDCC), Latin-American Symposium on Dependable Computing (LADC), Pacific Rim International Symposium on Dependable Computing (PRDC)), while using Natural Language Processing (NLP) and namely the Latent Dirichlet Allocation (LDA) algorithm to identify active, collapsing, ephemeral, and new lines of research in the dependability field. Results show a strong emphasis on terms, like ‘security’, despite the general focus of the conferences in dependability and new trends that are related with ’machine learning’ and ‘blockchain’. We used the PRDC conference as a use case, which showed similarity with the overall set of conferences, although we also found specific terms, like ‘cyber-physical’, being popular at PRDC and not in the overall dataset."
LeGoues2021,Claire Le Goues and Michael Pradel and Abhik Roychoudhury and Satish Chandra,Automatic Program Repair,IEEE Software,38,4,2021,10.1109/MS.2021.3072577,19374194,"Programming mistakes of all kinds-in source code, configurations, tests, or other artifacts-are a wide-ranging and expensive problem. Developers dedicate a significant proportion of engineering time and effort to finding and fixing bugs in their code, businesses lose market share when vulnerabilities in the software they sell impact customers, and overall productivity is impacted by software that does not work as intended or is prone to vulnerabilities.1 Rapidly finding and fixing bugs and vulnerabilities only grows in importance as software is continuously evolving and deployed and as society becomes increasingly dependent on software systems in all aspects of modern life. Speaking to this general problem, this special issue of IEEE Software addresses recent advances in research and practice in automatic software repair."
Ma2022,Junda Ma and Guoxin Wang and Jinzhi Lu and Hans Vangheluwe and Dimitris Kiritsis and Yan Yan,Systematic Literature Review of MBSE Tool-Chains,Applied Sciences (Switzerland),12,7,2022,10.3390/app12073431,20763417,"Currently, the fundamental tenets of systems engineering are supported by a model-based approach to minimize risks and avoid design changes in late development stages. The models are used to formalize, analyze, design, optimize, and verify system development and artifacts, helping developers integrate engineering development across domains. Although model-based development is well established in specific domains, such as software, mechanical systems, and electrical systems, its role in integrated development from a system perspective is still a challenge for industry. The model-based systems engineering (MBSE) tool-chain is an emerging technique in the area of systems engineering and is expected to become a next-generation approach for supporting model integration across domains. This article presents a literature review to highlight the usage and state of the art to generally specify the current understanding of MBSE tool-chain concepts. Moreover, the results are used for identifying the usage, advantages, barriers, concerns, and trends of tool-chain development from an MBSE perspective."
Sikic2021,Lucija Sikic and Petar Afric and Adrian Satja Kurdija and Marin Silic,Improving Software Defect Prediction by Aggregated Change Metrics,IEEE Access,9,,2021,10.1109/ACCESS.2021.3054948,21693536,"To ensure the delivery of high quality software, it is necessary to ensure that all of its artifacts function properly, which is usually done by performing appropriate tests with limited resources. It is therefore desirable to identify defective artifacts so that they can be corrected before the testing process. So far, researchers have proposed various predictive models for this purpose. Such models are typically trained on data representing previous project versions of a software and then used to predict which of the software artifacts in the new version are likely to be defective. However, the data representing a software project usually consists of measurable properties of the project or its modules, and leaves out information about the timeline of the software development process. To fill this gap, we propose a new set of metrics, namely aggregated change metrics, which are created by aggregating the data of all changes made to the software between two versions, taking into account the chronological order of the changes. In experiments conducted on open source projects written in Java, we show that the stability and performance of commonly used classification models are improved by extending a feature set to include both measurable properties of the analyzed software and the aggregated change metrics."
Claes2020,Maëlick Claes and Mika V. Mäntylä,20-MAD: 20 Years of Issues and Commits of Mozilla and Apache Development,,,,2020,10.1145/3379597.3387487,,"Data of long-lived and high profile projects is valuable for research on successful software engineering in the wild. Having a dataset with different linked software repositories of such projects, enables deeper diving investigations. This paper presents 20-MAD, a dataset linking the commit and issue data of Mozilla and Apache projects. It includes over 20 years of information about 765 projects, 3.4M commits, 2.3M issues, and 17.3M issue comments, and its compressed size is over 6 GB. The data contains all the typical information about source code commits (e.g., lines added and removed, message and commit time) and issues (status, severity, votes, and summary). The issue comments have been pre-processed for natural language processing and sentiment analysis. This includes emoticons and valence and arousal scores. Linking code repository and issue tracker information, allows studying individuals in two types of repositories and provide more accurate time zone information for issue trackers as well. To our knowledge, this the largest linked dataset in size and in project lifetime that is not based on GitHub."
Barriga2020,Angela Barriga and Adrian Rutle and Rogardt Heldal,Improving model repair through experience sharing,Journal of Object Technology,19,2,2020,10.5381/JOT.2020.19.2.A13,16601769,"In model-driven software engineering, models are used in all phases of the development process. These models may get broken due to various editions throughout their life-cycle. There are already approaches that provide an automatic repair of models, however, the same issues might not have the same solutions in all contexts due to different user preferences and business policies. Personalization would enhance the usability of automatic repairs in different contexts, and by reusing the experience from previous repairs we would avoid duplicated calculations when facing similar issues. By using reinforcement learning we have achieved the repair of broken models allowing both automation and personalization of results. In this paper, we propose transfer learning to reuse the experience learned from each model repair. We have validated our approach by repairing models using different sets of personalization preferences and studying how the repair time improved when reusing the experience from each repair."
Mattis2020,Toni Mattis and Patrick Rein and Falco Dürsch and Robert Hirschfeld,RTPTorrent: An Open-source Dataset for Evaluating Regression Test Prioritization,,,,2020,10.1145/3379597.3387458,,"The software engineering practice of automated testing helps programmers find defects earlier during development. With growing software projects and longer-running test suites, frequency and immediacy of feedback decline, thereby making defects harder to repair. Regression test prioritization (RTP) is concerned with running relevant tests earlier to lower the costs of defect localization and to improve feedback. Finding representative data to evaluate RTP techniques is non-trivial, as most software is published without failing tests. In this work, we systematically survey a wide range of RTP literature regarding whether their dataset uses real or synthetic defects or tests, whether they are publicly available, and whether datasets are reused. We observed that some datasets are reused, however, many projects study only few projects and these rarely resemble real-world development activity. In light of these threats to ecological validity, we describe the construction and characteristics of a new dataset, named RTPTorrent, based on 20 open-source Java programs. Our dataset allows researchers to evaluate prioritization heuristics based on version control meta-data, source code, and test results from fine-grained, automated builds over 9 years of development history. We provide reproducible baselines for initial comparisons and make all data publicly available. We see this as a step towards better reproducibility, ecological validity, and long-term availability of studied software in the field of test prioritization."
Birillo2022,Anastasiia Birillo and Ilya Vlasov and Artyom Burylov and Vitalii Selishchev and Artyom Goncharov and Elena Tikhomirova and Nikolay Vyahhi and Timofey Bryksin,Hyperstyle: A Tool for Assessing the Code Quality of Solutions to Programming Assignments,,1,,2022,10.1145/3478431.3499294,,"In software engineering, it is not enough to simply write code that only works as intended, even if it is free from vulnerabilities and bugs. Every programming language has a style guide and a set of best practices defined by its community, which help practitioners to build solutions that have a clear structure and therefore are easy to read and maintain. To introduce assessment of code quality into the educational process, we developed a tool called Hyperstyle. To make it reflect the needs of the programming community and at the same time be easily extendable, we built it upon several existing professional linters and code checkers. Hyperstyle supports four programming languages (Python, Java, Kotlin, and Javascript) and can be used as a standalone tool or integrated into a MOOC platform. We have integrated the tool into two educational platforms, Stepik and JetBrains Academy, and it has been used to process about one million submissions every week since May 2021."
AlRasbi2021,Haitham Al Rasbi and Mohamed Gadi,Energy modelling of traditional and contemporary mosque buildings in oman,Buildings,11,7,2021,10.3390/buildings11070314,20755309,"Building energy efficiency is vital to achieve human thermal comfort with minimum energy consumption. It is a great concern in extremely hot countries such as Oman. This study aims to investigate the thermal performance of contemporary mosque buildings in comparison to traditional mosque buildings in Oman. The research methodology employs energy modelling using EDSL’s Tas Engineering computer simulation software. The analysis focused on how traditional mosque buildings compare to contemporary mosque buildings in terms of dry bulb air temperature and different thermal loads. The outcome showed the traditional mosque building design and construction are better suited for free-running buildings, while contemporary mosque building design and construction achieved less cooling load demand per area."
Britz2021,Wolfgang Britz and Pavel Ciaian and Alexander Gocht and Argyris Kanellopoulos and Dimitrios Kremmydas and Marc Müller and Athanasios Petsakos and Pytrik Reidsma,A design for a generic and modular bio-economic farm model,Agricultural Systems,191,,2021,10.1016/j.agsy.2021.103133,0308521X,"Context: Past reviews of policy impact assessment studies using bio-economic farm models (BEFM) called for the development of a generic and modular implementation that can be maintained by a network of modellers. A main reason for these calls is the project-oriented way in which model developers receive funding. It favours the development of new models with case-study specific features over the maintenance and extension of well-tested, more generic ones which allow comparing results in a consistent way across many case-studies. The demand for more generic tools also reflects the dynamic landscape of policy measures within larger policy frameworks like the Common Agricultural Policy (CAP). These policy frameworks move increasingly away from a ‘one-size-fits-all’ approach of policy design towards more flexible systems, giving greater freedom to shape, implement, and target policy measures to specific regions, farm management systems and farm types. This creates new challenges for model-based impact assessment as applied models have to reflect the variety of policy measures and characteristics of targeted farmers and rural communities. Objective: The aim of this paper is to first address key questions regarding the functionality and implementation of such a modular BEFM that can be maintained and expanded by a user group, and second to develop concrete proposals of necessary model features, model design and shared development. Methods: This paper builds on literature research, including a detailed review of four models that are used extensively for impact assessment within the EU and were developed by multiple teams over a longer period of time. From there, necessary and desirable features of a generic and modular BEFM are identified and requirements for model design regarding modularity, software engineering, and shared development are discussed. Results and conclusions: This feeds into the development of concrete proposals of how modularity and flexibility can be addressed in the development, application and maintenance of a BEFM. At the end, a list of design decisions and implementation steps is proposed to build a modular BEFM that can be maintained by a network of researchers. Significance: The concept for a network-based generic and modular bio-economic farm model responds to the demand for analytical tools in agricultural policy impact analysis. The paper develops a research agenda to overcome observed limitations in the current landscape of such models."
Hbner2020,Paul Hübner and Barbara Paech,Interaction-based creation and maintenance of continuously usable trace links between requirements and source code,Empirical Software Engineering,25,5,2020,10.1007/s10664-020-09831-w,15737616,"Trace links between requirements and code are beneficial for many software engineering tasks such as maintenance, program comprehension, and re-engineering. If trace links are created and used continuously during a project, they need to have high precision and recall to be useful. However, manual trace link creation is cumbersome and existing automatic trace link creation methods are typically only applied retrospectively and to structured requirements. Therefore, they focus on recall and accept manual effort to cope with low precision. Such manual effort is not acceptable continuously. Furthermore, the maintenance of existing links along with changing artefacts in a project is neglected in most automatic trace link creation approaches. Therefore, we developed and evaluated an interaction log-based trace link creation approach IL to continuously provide correct trace links during a project. IL links unstructured requirements specified in an issue tracker and source code managed in a version control system. In the latest version, ILCom, our approach uses the interactions of developers with files in an integrated development environment and issue identifiers provided in commit messages to create trace links continuously after each commit. In this paper, we present ILCom, its most recent evaluation study, and a systematic literature review (SLR) about trace link maintenance (TM). We also present a TM process for ILCom based on two approaches from our SLR. In the evaluation study, we show that precision of ILCom created links is above 90% and recall almost at 80%. In the SLR, we discuss 16 approaches. Our approach is the first trace link creation approach with very good precision and recall and integrated trace maintenance."
Hermann2020,Ben Hermann and Stefan Winter and Janet Siegmund,Community expectations for research artifacts and evaluation processes,,,,2020,10.1145/3368089.3409767,,"Background. Artifact evaluation has been introduced into the software engineering and programming languages research community with a pilot at ESEC/FSE 2011 and has since then enjoyed a healthy adoption throughout the conference landscape. Objective. In this qualitative study, we examine the expectations of the community toward research artifacts and their evaluation processes. Method. We conducted a survey including all members of artifact evaluation committees of major conferences in the software engineering and programming language field since the first pilot and compared the answers to expectations set by calls for artifacts and reviewing guidelines. Results. While we find that some expectations exceed the ones expressed in calls and reviewing guidelines, there is no consensus on quality thresholds for artifacts in general. We observe very specific quality expectations for specific artifact types for review and later usage, but also a lack of their communication in calls. We also find problematic inconsistencies in the terminology used to express artifact evaluation's most important purpose - replicability. Conclusion. We derive several actionable suggestions which can help to mature artifact evaluation in the inspected community and also to aid its introduction into other communities in computer science."
Caldas2020,Ricardo Diniz Caldas and Arthur Rodrigues and Eric Bernd Gil and Genaína Nunes Rodrigues and Thomas Vogel and Patrizio Pelliccione,A hybrid approach combining control theory and AI for engineering self-adaptive systems,,,,2020,10.1145/3387939.3391595,,"Control theoretical techniques have been successfully adopted as methods for self-adaptive systems design to provide formal guarantees about the effectiveness and robustness of adaptation mechanisms. However, the computational effort to obtain guarantees poses severe constraints when it comes to dynamic adaptation. In order to solve these limitations, in this paper, we propose a hybrid approach combining software engineering, control theory, and AI to design for software self-adaptation. Our solution proposes a hierarchical and dynamic system manager with performance tuning. Due to the gap between high-level requirements specification and the internal knob behavior of the managed system, a hierarchically composed components architecture seek the separation of concerns towards a dynamic solution. Therefore, a two-layered adaptive manager was designed to satisfy the software requirements with parameters optimization through regression analysis and evolutionary meta-heuristic. The optimization relies on the collection and processing of performance, effectiveness, and robustness metrics w.r.t control theoretical metrics at the offline and online stages. We evaluate our work with a prototype of the Body Sensor Network (BSN) in the healthcare domain, which is largely used as a demonstrator by the community. The BSN was implemented under the Robot Operating System (ROS) architecture, and concerns about the system dependability are taken as adaptation goals. Our results reinforce the necessity of performing well on such a safety-critical domain and contribute with substantial evidence on how hybrid approaches that combine control and AI-based techniques for engineering self-adaptive systems can provide effective adaptation."
Gonzlez-Santamarta2020,Miguel González-Santamarta and Francisco J. Rodríguez-Lera and Claudia Álvarez-Aparicio and Ángel M. Guerrero-Higueras and Camino Fernández-Llamas,MERLIN a cognitive architecture for service robots,Applied Sciences (Switzerland),10,17,2020,10.3390/app10175989,20763417,"Many social robots deployed in public spaces hide hybrid cognitive architectures for dealing with daily tasks. Mostly, two main blocks sustain these hybrid architectures for robot behavior generation: deliberative and behavioral-based mechanisms. Robot Operating System offers different solutions for implementing these blocks, however, some issues arise when both are released in the robot. This paper presents a software engineering approach for normalizing the process of integrating them and presenting them as a fully cognitive architecture named MERLIN. Providing implementation details and diagrams for established the architecture, this research tests empirically the proposed solution using a variation from the challenge defined in the SciRoc @home competition. The results validate the usability of our approach and show MERLIN as a hybrid architecture ready for short and long-term tasks, showing better results than using a by default approach, particularly when it is deployed in highly interactive scenarios."
Albahli2022,Saleh Albahli and Ghulam Nabi Ahmad Hassan Yar,Defect prediction using Akaike and Bayesian information criterion,Computer Systems Science and Engineering,41,3,2022,10.32604/csse.2022.021750,02676192,"Data available in software engineering for many applications contains variability and it is not possible to say which variable helps in the process of the prediction. Most of the work present in software defect prediction is focused on the selection of best prediction techniques. For this purpose, deep learning and ensemble models have shown promising results. In contrast, there are very few researches that deals with cleaning the training data and selection of best parameter values from the data. Sometimes data available for training the models have high variability and this variability may cause a decrease in model accuracy. To deal with this problem we used the Akaike information criterion (AIC) and the Bayesian information criterion (BIC) for selection of the best variables to train the model. A simple ANN model with one input, one output and two hidden layers was used for the training instead of a very deep and complex model. AIC and BIC values are calculated and combination for minimum AIC and BIC values to be selected for the best model. At first, variables were narrowed down to a smaller number using correlation values. Then subsets for all the possible variable combinations were formed. In the end, an artificial neural network (ANN) model was trained for each subset and the best model was selected on the basis of the smallest AIC and BIC value. It was found that combination of only two variables’ ns and entropy are best for software defect prediction as it gives minimum AIC and BIC values. While, nm and npt is the worst combination and gives maximum AIC and BIC values."
Ahmed2022,Toufique Ahmed and Premkumar Devanbu,Few-shot training LLMs for project-specific code-summarization,,,,2022,10.1145/3551349.3559555,,"Very large language models (LLMs), such as GPT-3 and Codex have achieved state-of-the-art performance on several natural-language tasks, and show great promise also for code. A particularly exciting aspect of LLMs is their knack for few-shot and zero-shot learning: they can learn to perform a task with very few examples. Few-shotting has particular synergies in software engineering, where there are a lot of phenomena (identifier names, APIs, terminology, coding patterns) that are known to be highly project-specific. However, project-specific data can be quite limited, especially early in the history of a project; thus the few-shot learning capacity of LLMs might be very relevant. In this paper, we investigate the use few-shot training with the very large GPT (Generative Pre-trained Transformer) Codex model, and find evidence suggesting that one can significantly surpass state-of-the-art models for code-summarization, leveraging project-specific training."
Okanovi2020,Dušan Okanović and Samuel Beck and Lasse Merz and Christoph Zorn and Leonel Merino and André Van Hoorn and Fabian Beck,Can a chatbot support software engineers with load testing? approach and experiences,,,,2020,10.1145/3358960.3375792,,"Even though load testing is an established technique to assess load-related quality properties of software systems, it is applied only seldom and with questionable results. Indeed, configuring, executing, and interpreting results of a load test require high effort and expertise. Since chatbots have shown promising results for interactively supporting complex tasks in various domains (including software engineering), we hypothesize that chatbots can provide developers suitable support for load testing. In this paper, we present PerformoBot, our chatbot for configuring and running load tests. In a natural language conversation, PerformoBot guides developers through the process of properly specifying the parameters of a load test, which is then automatically executed by PerformoBot using a state-of-the-art load testing tool. After the execution, PerformoBot provides developers a report that answers the respective concern. We report on results of a user study that involved 47 participants, in which we assessed our tool's acceptance and effectiveness. We found that participants in the study, particularly those with a lower level of expertise in performance engineering, had a mostly positive view of PerformoBot."
Eiben2021,Agoston E. Eiben and Emma Hart and Jon Timmis and Andy M. Tyrrell and Alan F. Winfield,Towards autonomous robot evolution,,,,2021,10.1007/978-3-030-66494-7_2,,"We outline a perspective on the future of evolutionary robotics and discuss a long-term vision regarding robots that evolve in the real world. We argue that such systems offer significant potential for advancing both science and engineering. For science, evolving robots can be used to investigate fundamental issues about evolution and the emergence of embodied intelligence. For engineering, artificial evolution can be used as a tool that produces good designs in difficult applications in complex unstructured environments with (partially) unknown and possibly changing conditions. This implies a new paradigm, second-order software engineering, where instead of directly developing a system for a given application, we develop an evolutionary system that will develop the target system for us. Importantly, this also holds for the hardware; with a complete evolutionary robot system, both the software and the hardware are evolved. In this chapter, we discuss the long-term vision, elaborate on the main challenges, and present the initial results of an ongoing research project concerned with the first tangible implementation of such a robot system."
Antn2021,Daniel Antón and José Lázaro Amaro-Mellado,"Engineering graphics for thermal assessment: 3D thermal data visualisation based on infrared thermography, GIS and 3D point cloud processing software",Symmetry,13,2,2021,10.3390/sym13020335,20738994,"Engineering graphics are present in the design stage, but also constitute a way to com-municate, analyse, and synthesise. In the Architecture-Engineering-Construction sector, graphical data become essential in analysing buildings and constructions throughout their lifecycles, such as in the thermal behaviour assessment of building envelopes. Scientific research has addressed the thermal image mapping onto three-dimensional (3D) models for visualisation and analysis. How-ever, the 3D point cloud data creation of buildings’ thermal behaviour directly from rectified infrared thermography (IRT) thermograms is yet to be investigated. Therefore, this paper develops an open-source software graphical method to produce 3D thermal data from IRT images for temperature visualisation and subsequent analysis. This low-cost approach uses both a geographic information system for the thermographic image rectification and the point clouds production, and 3D point cloud processing software. The methodology has been proven useful to obtain, without perspective distortions, 3D thermograms even from non-radiometric raster images. The results also revealed that non-rectangular thermograms enable over 95% of the 3D thermal data generated from IRT against rectangular shapes (over 85%). Finally, the 3D thermal data produced allow further thermal behaviour assessment, including calculating the object’s heat loss and thermal transmit-tance for diverse applications such as energy audits, restoration, monitoring, or product quality control."
Applis2021,Leonhard Applis and Annibale Panichella and Arie Van Deursen,Assessing Robustness of ML-Based Program Analysis Tools using Metamorphic Program Transformations,,,,2021,10.1109/ASE51524.2021.9678706,,"Metamorphic testing is a well-established testing technique that has been successfully applied in various domains, including testing deep learning models to assess their robustness against data noise or malicious input. Currently, metamorphic testing approaches for machine learning (ML) models focused on image processing and object recognition tasks. Hence, these approaches cannot be applied to ML targeting program analysis tasks. In this paper, we extend metamorphic testing approaches for ML models targeting software programs. We present LAMPION, a novel testing framework that applies (semantics preserving) metamorphic transformations on the test datasets. LAMPION produces new code snippets equivalent to the original test set but different in their identifiers or syntactic structure. We evaluate LAMPION against CodeBERT, a state-of-the-art ML model for Code-To-Text tasks that creates Javadoc summaries for given Java methods. Our results show that simple transformations significantly impact the target model behavior, providing additional information on the models reasoning apart from the classic performance metric."
Berriche2020,Aroua Berriche and Faïda Mhenni and Abdelfattah Mlika and Jean Yves Choley,Towards model synchronization for consistency management of mechatronic systems,Applied Sciences (Switzerland),10,10,2020,10.3390/app10103577,20763417,"The development of a mechatronic system involves different designers having various viewpoints on the overall system to handle its complexity. Consequently, multiple models are created from a variety of domains such as mechanical, electronic, and software engineering. These models use different formalisms, modeling languages, and tools to address specific concerns. The major challenge of this approach is to identify and solve any potential inconsistency between models in order to minimize costs and development time before the verification and validation phases. This paper proposes a new collaborative methodology to maintain consistency between different engineering disciplines at an early stage of the development cycle of mechatronic systems based on Model-Based Engineering (MBE). We apply a model synchronization approach to actively check for model consistency in a continuous way during the multidisciplinary design process. As a novel contribution of this paper, we demonstrate how model transformation techniques can be employed; firstly, to abstract various engineering models in a common formalism based on graph theory and, secondly, to update models with appropriate changes evaluated by a project manager. We also show how to detect the differences automatically, and we discuss where designer decisions are essential."
Takada2020,Shingo Takada and Ernesto Cuadros-Vargas and John Impagliazzo and Steven Gordon and Linda Marshall and Heikki Topi and Gerrit van der Veer and Leslie Waguespack,Toward the visual understanding of computing curricula,Education and Information Technologies,25,5,2020,10.1007/s10639-020-10127-1,15737608,"Various computing subdisciplines, such as computer science and software engineering, each have their own curricular guidelines. They can be very difficult to understand and compare for people such as prospective students, industry personnel, and even faculty members. This is compounded by a lack of information surrounding undergraduate computing curricular topics via visual methods. This paper describes two experimental activities where the objective is to explore the possibility of obtaining quantitative data sets necessary for visualization, one based on competencies and the other based on knowledge areas. Both activities were based on surveys. The results from the first activity showed that a consensus interpretation could be obtained for the knowledge, skills, and dispositions implied by the competency descriptions, although not as strongly for dispositions. The second activity resulted in a table of knowledge areas with minimum and maximum weights for six computing subdisciplines. Finally, this paper also shows two examples of how users can explore the various curricular guidelines through visualization."
Siddiq2022,Mohammed Latif Siddiq and Joanna C.S. Santos,BERT-Based GitHub Issue Report Classification,,,,2022,10.1145/3528588.3528660,,"Issue tracking is one of the integral parts of software development, especially for open source projects. GitHub, a commonly used software management tool, provides its own issue tracking system. Each issue can have various tags, which are manually assigned by the project's developers. However, manually labeling software reports is a time-consuming and error-prone task. In this paper, we describe a BERT-based classification technique to automatically label issues as questions, bugs, or enhancements. We evaluate our approach using a dataset containing over 800,000 labeled issues from real open source projects available on GitHub. Our approach classified reported issues with an average F1-score of 0.8571. Our technique outperforms a previous machine learning technique based on FastText."
Javaid2023,Mohd Javaid and Abid Haleem and Ravi Pratap Singh and Rajiv Suman,Sustaining the healthcare systems through the conceptual of biomedical engineering: A study with recent and future potentials,Biomedical Technology,1,,2023,10.1016/j.bmt.2022.11.004,2949723X,"Biomedical engineering is an interdisciplinary branch of engineering & technology that combines biomedical sciences with engineering principles. This discipline covers broad areas where biomedical engineers are involved in the fields of medicine, regenerative medicine & associated areas and in developing better products and services. Biomedical engineering offers software for simulation, 3D motion-catching and printing technologies for computer modelling and engineering. The discipline of biomedical engineering is a fast-moving, cross-disciplinary field covering medicine, biology, chemistry, engineering, nanotechnology and informatics. Innovative medical gadgets, vaccinations, disease control products, robotics, and algorithms that enhance human health worldwide are being developed by bioengineers. Living tissues are formed of bioactive cells and stored in regulated circumstances on biodegradable scaffolds. The use of biomedical engineering concepts is to address issues with healthcare. Biomedical engineers create medical tools and procedures that enhance people's health by combining their understanding of engineering, virology, and healthcare. Blood glucose monitoring, pacemakers, and prosthetic limbs are examples of biomedical equipment. The main purpose of this paper is to study Biomedical Engineering and its need in healthcare. The paper discusses various innovations and research aspects of Biomedical Engineering in the healthcare domain. The paper further identified and discussed significant applications of Biomedical engineering for healthcare. Biomedical engineering is a fascinating field of life science that can change healthcare and open the door to new technologies in prostheses, operating equipment, diagnoses, imaging and more. The multidisciplinary area of biomedical technology provides better possibilities for biological research and engineering and changes how we interact with the world."
Bluemke2021,Ilona Bluemke and Agnieszka Malanowska,Software Testing Effort Estimation and Related Problems,ACM Computing Surveys,54,3,2021,10.1145/3442694,15577341,"Although testing effort estimation is a very important task in software project management, it is rarely described in the literature. There are many difficulties in finding any useful methods or tools for this purpose. Solutions to many other problems related to testing effort calculation are published much more often. There is also no research focusing on both testing effort estimation and all related areas of software engineering. To fill this gap, we performed a systematic literature review on both questions. Although our primary objective was to find some tools or implementable metods for test effort estimation, we have quickly discovered many other interesting topics related to the main one. The main contribution of this work is the presentation of the testing effort estimation task in a very wide context, indicating the relations with other research fields. This systematic literature review presents a detailed overview of testing effort estimation task, including challenges and approaches to automating it and the solutions proposed in the literature. It also exhaustively investigates related research topics, classifying publications that can be found in connection to the testing effort according to seven criteria formulated on the basis of our research questions. We present here both synthesis of our finding and the deep analysis of the stated research problems."
Ciborowska2022,Agnieszka Ciborowska and Kostadin Damevski,Fast Changeset-based Bug Localization with BERT,,2022-May,,2022,10.1145/3510003.3510042,02705257,"Automatically localizing software bugs to the changesets that induced them has the potential to improve software developer efficiency and to positively affect software quality. To facilitate this automation, a bug report has to be effectively matched with source code changes, even when a significant lexical gap exists between natural language used to describe the bug and identifier naming practices used by developers. To bridge this gap, we need techniques that are able to capture software engineering-specific and project-specific semantics in order to detect relatedness between the two types of documents that goes beyond exact term matching. Popular transformer-based deep learning architectures, such as BERT, excel at leveraging contextual information, hence appear to be a suitable candidate for the task. However, BERT-like models are computationally expensive, which precludes them from being used in an environment where response time is important. In this paper, we describe how BERT can be made fast enough to be applicable to changeset-based bug localization. We also explore several design decisions in using BERT for this purpose, including how best to encode changesets and how to match bug reports to individual changes for improved accuracy. We compare the accuracy and performance of our model to a non-contextual baseline (i.e., vector space model) and BERT-based architectures previously used in software engineering. Our evaluation results demonstrate advantages in using the proposed BERT model compared to the baselines, especially for bug reports that lack any hints about related code elements."
Alami2022,Adam Alami and Oliver Krancher,How Scrum adds value to achieving software quality?,Empirical Software Engineering,27,7,2022,10.1007/s10664-022-10208-4,15737616,"Scrum remains the most popular agile software development method implementation for a variety of reasons; one important motive is to improve software quality. Yet many organizations fail to achieve quality improvements through the use of Scrum, and existing research sheds little light on the value-add of Scrum for software quality. More specifically, (1) how notions of software quality among Scrum practitioners relate to established quality perspectives, (2) how Scrum helps teams to achieve higher software quality and (3) why some teams fail to meet the objective of higher quality. We addressed these gaps through a two-phased qualitative study based on 39 interviews and two in-depth case studies. We find that Scrum practitioners emphasize established notions of external quality comprising of conformity to business needs and absence of defects, while they also value internal quality, especially sustainable software design. Our results show that Scrum helps teams achieve both dimensions of quality by promoting some social antecedents (collaboration, psychological safety, accountability, transparency) and process-induced advantages (iterative development, formal inspection, and adaptation). Our findings unveil how these factors contribute to achieving software quality and under what conditions their effects can fail to materialize. These conditions include inconsistent Scrum implementations, cultural constraints, team tensions, and inaccessibility of end-users. In addition, the complexity of the project aggravates the impact of these conditions. Taken together, these findings show that Scrum can complement established quality assurance and software engineering practices by promoting a social environment that is conducive to creating high-quality software. Based on our findings, we provide specific recommendations for how practitioners can create such an environment."
Sarpiri2021,Mona Najafi Sarpiri and Taghi Javdani Gandomani,A case study of using the hybrid model of scrum and six sigma in software development,International Journal of Electrical and Computer Engineering,11,6,2021,10.11591/ijece.v11i6.pp5342-5350,20888708,"The world of software engineering is constantly discovering new ways that lead to an increase in team performance in the production of software products and, at the same time, brings the customer's further satisfaction. With the advent of agile methodologies in software development, these objectives have been considered more seriously by software teams and companies. Due to their very nature, agile methodologies have the potential to be integrated with other methodologies or specific managerial approaches defined in line with agility objectives. One of the cases is Six Sigma, which is used in organizations by focusing on organizational change and process improvement. In the present study, attempts were made to present the hybrid software development approach, including Scrum, as the most common agile and Six Sigma methodology. This approach was practically used in a case study, and the obtained results were analyzed. The results of this evaluation showed that this hybrid method could lead to the increased team performance and customer satisfaction. However, besides these two achievements, an increase in the number of re-works, number of defects discovered, and the duration of the project implementation were also observed. These cases are in line with the main objectives of Scrum and Six Sigma and are justifiable and acceptable due to achieving those objectives."
Bersani2020,Marcello M. Bersani and Matteo Soldo and Claudio Menghi and Patrizio Pelliccione and Matteo Rossi,PuRSUE -from specification of robotic environments to synthesis of controllers,Formal Aspects of Computing,32,2-3,2020,10.1007/s00165-020-00509-0,1433299X,"Developing robotic applications is a complex task, which requires skills that are usually only possessed by highly-qualified robotic developers. While formal methods that help developers in the creation and design of robotic applications exist, they must be explicitly customized to be impactful in the robotics domain and to support effectively the growth of the robotic market. Specifically, the robotic market is asking for techniques that: (i) enable a systematic and rigorous design of robotic applications though high-level languages; and (ii) enable the automatic synthesis of low-level controllers, which allow robots to achieve their missions. To address these problems we present the PuRSUE (Planner for RobotS in Uncontrollable Environments) approach, which aims to support developers in the rigorous and systematic design of high-level run-time control strategies for robotic applications. The approach includes PuRSUE-ML a high-level language that allows for modeling the environment, the agents deployed therein, and their missions. PuRSUE is able to check automatically whether a controller that allows robots to achieve their missions might exist and, then, it synthesizes a controller. We evaluated how PuRSUE helps designers in modeling robotic applications, the effectiveness of its automatic computation of controllers, and how the approach supports the deployment of controllers on actual robots. The evaluation is based on 13 scenarios derived from 3 different robotic applications presented in the literature. The results show that: (i) PuRSUE-ML is effective in supporting designers in the formal modeling of robotic applications compared to a direct encoding of robotic applications in low-level modeling formalisms; (ii) PuRSUE enables the automatic generation of controllers that are difficult to create manually; and (iii) the plans generated with PuRSUE are indeed effective when deployed on actual robots."
vanGelder2021,Pieter van Gelder and Pim Klaassen and Behnam Taebi and Bart Walhout and Ruud van Ommen and Ibo van de Poel and Zoe Robaey and Lotte Asveld and Ruud Balkenende and Frank Hollmann and Erik Jan van Kampen and Nima Khakzad and Robbert Krebbers and Jos de Lange and Wolter Pieters and Karel Terwel and Eelco Visser and Tiny van der Werff and Dick Jung,Safe-by-design in engineering: An overview and comparative analysis of engineering disciplines,International Journal of Environmental Research and Public Health,18,12,2021,10.3390/ijerph18126329,16604601,"In this paper, we provide an overview of how Safe-by-Design is conceived and applied in practice in a large number of engineering disciplines. We discuss the differences, commonalities, and possibilities for mutual learning found in those practices and identify several ways of putting those disciplinary outlooks in perspective. The considered engineering disciplines in the order of historically grown technologies are construction engineering, chemical engineering, aerospace engineering, urban engineering, software engineering, bio-engineering, nano-engineering, and finally cyber space engineering. Each discipline is briefly introduced, the technology at issue is described, the relevant or dominant hazards are examined, the social challenge(s) are observed, and the relevant developments in the field are described. Within each discipline the risk management strategies, the design principles promoting safety or safety awareness, and associated methods or tools are discussed. Possible dilemmas that the designers in the discipline face are highlighted. Each discipline is concluded by discussing the opportunities and bottlenecks in addressing safety. Commonalities and differences between the engineering disciplines are investigated, specifically on the design strategies for which empirical data have been collected. We argue that Safe-by-Design is best considered as a specific elaboration of Responsible Research and Innovation, with an explicit focus on safety in relation to other important values in engineering such as well-being, sustainability, equity, and affordability. Safe-by-Design provides for an intellectual venue where social science and the humanities (SSH) collaborate on technological developments and innovation by helping to proactively incorporate safety considerations into engineering practices, while navigating between the extremes of technological optimism and disproportionate precaution. As such, Safe-by-Design is also a practical tool for policymakers and risk assessors that helps shape governance arrangements for accommodating and incentivizing safety, while fully acknowledging uncertainty."
Shin2020,Seung Yeob Shin and Shiva Nejati and Mehrdad Sabetzadeh and Lionel C. Briand and Chetan Arora and Frank Zimmer,Dynamic adaptation of software-defined networks for IoT systems: A search-based approach,,,,2020,10.1145/3387939.3391603,,"The concept of Internet of Things (IoT) has led to the development of many complex and critical systems such as smart emergency management systems. IoT-enabled applications typically depend on a communication network for transmitting large volumes of data in unpredictable and changing environments. These networks are prone to congestion when there is a burst in demand, e.g., as an emergency situation is unfolding, and therefore rely on configurable software-defined networks (SDN). In this paper, we propose a dynamic adaptive SDN configuration approach for IoT systems. The approach enables resolving congestion in real time while minimizing network utilization, data transmission delays and adaptation costs. Our approach builds on existing work in dynamic adaptive search-based software engineering (SBSE) to reconfigure an SDN while simultaneously ensuring multiple quality of service criteria. We evaluate our approach on an industrial national emergency management system, which is aimed at detecting disasters and emergencies, and facilitating recovery and rescue operations by providing first responders with a reliable communication infrastructure. Our results indicate that (1) our approach is able to efficiently and effectively adapt an SDN to dynamically resolve congestion, and (2) compared to two baseline data forwarding algorithms that are static and non-adaptive, our approach increases data transmission rate by a factor of at least 3 and decreases data loss by at least 70%."
Guaman2020,Danny S. Guaman and Jose M.Del Alamo and Julio C. Caiza,A Systematic Mapping Study on Software Quality Control Techniques for Assessing Privacy in Information Systems,IEEE Access,8,,2020,10.1109/ACCESS.2020.2988408,21693536,"Software Quality Control (SQC) techniques are widely used throughout the software development process with the objective of assessing and detecting anomalies that affect the quality of an information system. Privacy is one quality attribute of software systems for which several SQC techniques have been proposed in recent years. However, research has been carried out from different perspectives and, consequently, it has led to a growing body of knowledge scattered across different domains. To bridge this gap, we have carried out a systematic mapping study to provide practitioners and researchers with an overview of the state-of-the-art techniques to carry out software quality control of information systems focusing on aspects of privacy. Our results show a steady growth in the research efforts in this field. The European General Data Protection Regulation seems to have a significant influence on this growth, since 37% of techniques that focus on assessing compliance derive their assessment criteria from this legal framework. The maturity of the techniques varies between the type of technique: Formal verification techniques exhibit the lowest level of maturity while the combination of techniques has demonstrated its successful application in real-world scenarios. The latter seems a promising avenue of research as it provides better results in terms of coverage, precision and effectiveness than the application of individual, isolated techniques. In this paper, we describe the existing SQC techniques focusing on privacy and provide a suitable basis for identifying future research directions."
Dwitam2020,Ferliana Dwitam and Andre Rusli,User stories collection via interactive chatbot to support requirements gathering,Telkomnika (Telecommunication Computing Electronics and Control),18,2,2020,10.12928/TELKOMNIKA.V18I2.14866,23029293,"Nowadays, software products have become an essential part of human life. To build software, developers must have a good understanding of the requirements of the software. However, software developers tend to jumpstart system construction without having a clear and detailed understanding of the requirements. The user story concept is one of the practices of the requirements elicitation. This paper aims to present the work conducted to develop an Android chatbot application to support the requirements elicitation activity in software engineering, making the work less time-consuming and structured even for users not accustomed to requirements engineering. The chatbot uses Nazief & Adriani stemming algorithm to pre-process the natural language it receives from the users and artificial mark-up language (AIML) as the knowledge base to process the bot's responses. A preliminary acceptance test based on the technology acceptance model results in an 83.03% score for users' behavioral intention to use."
Abugabah2020,Ahed Abugabah and Ahmad Ali Alzubi and Osama Alfarraj and Mohammed Al-Maitah and Waleed S. Alnumay,Intelligent traffic engineering in software-defined vehicular networking based on multi-path routing,IEEE Access,8,,2020,10.1109/ACCESS.2020.2983204,21693536,"This paper addresses traffic engineering (TE) issues in software-defined vehicular networking (SDVN). A brief analysis of the features of SDVN, which improves the efficiency of TE in SDVN, is presented. The feasibility of using multi-path routing with TE is substantiated. A procedure and an example of the formation of multiple routes based on a modified wave routing algorithm are given. Considering the features of the SDVN technology, a modified TE method is proposed, which reduces both the time complexity of forming multiple paths and the path reconfiguration time. The dynamic path reconfiguration algorithm is presented."
Groeneveld2020,Wouter Groeneveld and Hans Jacobs and Joost Vennekens and Kris Aerts,Non-cognitive abilities of exceptional software engineers: A delphi study,,,,2020,10.1145/3328778.3366811,,"Important building blocks of software engineering concepts are without a doubt technical. During the last decade, research and practical interest for non-technicalities has grown, revealing the building blocks to be various skills and abilities beside pure technical knowledge. Multiple attempts to categorise these blocks have been made, but so far little international studies have been performed that identify skills by asking experts from both the industrial and academic world: which abilities are needed for a developer to excel in the software engineering industry? To answer this question, we performed a Delphi study, inviting 36 experts from 11 different countries world-wide, affiliated with 21 internationally renowned institutions. This study presents the 55 identified and ranked skills as classified in four major areas: communicative skills (empathy, actively listening, etc.), collaborative skills (sharing responsibility, learning from each other, etc.), problem solving skills (verifying assumptions, solution-oriented thinking, etc.), and personal skills (curiosity, being open to ideas, etc.), of which a comparison has been made between opinions of technical experts, business experts, and academics. We hope this work inspires educators and practitioners to adjust their training programs, mitigating the gap between the industry and the academic world."
Ahmad2023,Aakash Ahmad and Muhammad Waseem and Peng Liang and Mahdi Fahmideh and Mst Shamima Aktar and Tommi Mikkonen,Towards Human-Bot Collaborative Software Architecting with ChatGPT,,,,2023,10.1145/3593434.3593468,,"Architecting software-intensive systems can be a complex process. It deals with the daunting tasks of unifying stakeholders' perspectives, designers' intellect, tool-based automation, pattern-driven reuse, and so on, to sketch a blueprint that guides software implementation and evaluation. Despite its benefits, architecture-centric software engineering (ACSE) suffers from a multitude of challenges. ACSE challenges could stem from a lack of standardized processes, socio-technical limitations, and scarcity of human expertise etc. that can impede the development of existing and emergent classes of software. Software Development Bots (DevBots) trained on large language models can help synergise architects' knowledge with artificially intelligent decision support to enable rapid architecting in a human-bot collaborative ACSE. An emerging solution to enable this collaboration is ChatGPT, a disruptive technology not primarily introduced for software engineering, but is capable of articulating and refining architectural artifacts based on natural language processing. We detail a case study that involves collaboration between a novice software architect and ChatGPT to architect a service-based software. Future research focuses on harnessing empirical evidence about architects' productivity and explores socio-technical aspects of architecting with ChatGPT to tackle challenges of ACSE."
Casalaro2022,Giuseppina Lucia Casalaro and Giulio Cattivera and Federico Ciccozzi and Ivano Malavolta and Andreas Wortmann and Patrizio Pelliccione,Model-driven engineering for mobile robotic systems: a systematic mapping study,Software and Systems Modeling,21,1,2022,10.1007/s10270-021-00908-8,16191374,"Mobile robots operate in various environments (e.g. aquatic, aerial, or terrestrial), they come in many diverse shapes and they are increasingly becoming parts of our lives. The successful engineering of mobile robotics systems demands the interdisciplinary collaboration of experts from different domains, such as mechanical and electrical engineering, artificial intelligence, and systems engineering. Research and industry have tried to tackle this heterogeneity by proposing a multitude of model-driven solutions to engineer the software of mobile robotics systems. However, there is no systematic study of the state of the art in model-driven engineering (MDE) for mobile robotics systems that could guide research or practitioners in finding model-driven solutions and tools to efficiently engineer mobile robotics systems. The paper is contributing to this direction by providing a map of software engineering research in MDE that investigates (1) which types of robots are supported by existing MDE approaches, (2) the types and characteristics of MRSs that are engineered using MDE approaches, (3) a description of how MDE approaches support the engineering of MRSs, (4) how existing MDE approaches are validated, and (5) how tools support existing MDE approaches. We also provide a replication package to assess, extend, and/or replicate the study. The results of this work and the highlighted challenges can guide researchers and practitioners from robotics and software engineering through the research landscape."
Quezada-Sarmiento2020,Pablo Alejandro Quezada-Sarmiento and Jon A. Elorriaga and Ana Arruarte and Hironori Washizaki,Open BOK on software engineering educational context: A systematic literature review,Sustainability (Switzerland),12,17,2020,10.3390/SU12176858,20711050,"In this review, a Systematic Literature Review (SLR) on Open Body of Knowledge (BOK) is presented. Moreover, the theoretical base to build a model for knowledge description was created, and it was found that there is a lack of guidelines to describe knowledge description because of the dramatically increasing number of requirements to produce an Open BOK, the difficulty of comparing related BOK contents, and the fact that reusing knowledge description is a very laborious task. In this sense, this review can be considered as a first step in building a model that can be used for describing knowledge description in Open BOK. Finally, in order to improve the educational context, a comparison among BOK, structure, and evolution is conducted."
Parri2021,Jacopo Parri and Fulvio Patara and Samuele Sampietro and Enrico Vicario,A framework for Model-Driven Engineering of resilient software-controlled systems,Computing,103,4,2021,10.1007/s00607-020-00841-6,14365057,"Emergent paradigms of Industry 4.0 and Industrial Internet of Things expect cyber-physical systems to reliably provide services overcoming disruptions in operative conditions and adapting to changes in architectural and functional requirements. In this paper, we describe a hardware/software framework supporting operation and maintenance of software-controlled systems enhancing resilience by promoting a Model-Driven Engineering (MDE) process to automatically derive structural configurations and failure models from reliability artifacts. Specifically, a reflective architecture developed around digital twins enables representation and control of system Configuration Items properly derived from SysML Block Definition Diagrams, providing support for variation. Besides, a plurality of distributed analytic agents for qualitative evaluation over executable failure models empowers the system with runtime self-assessment and dynamic adaptation capabilities. We describe the framework architecture outlining roles and responsibilities in a System of Systems perspective, providing salient design traits about digital twins and data analytic agents for failure propagation modeling and analysis. We discuss a prototype implementation following the MDE approach, highlighting self-recovery and self-adaptation properties on a real cyber-physical system for vehicle access control to Limited Traffic Zones."
Cruz-Benito2021,Juan Cruz-Benito and Sanjay Vishwakarma and Francisco Martin-Fernandez and Ismael Faro,Automated Source Code Generation and Auto-Completion Using Deep Learning: Comparing and Discussing Current Language Model-Related Approaches,AI (Switzerland),2,1,2021,10.3390/ai2010001,26732688,"In recent years, the use of deep learning in language models has gained much attention. Some research projects claim that they can generate text that can be interpreted as human writing, enabling new possibilities in many application areas. Among the different areas related to language processing, one of the most notable in applying this type of modeling is programming languages. For years, the machine learning community has been researching this software engineering area, pursuing goals like applying different approaches to auto-complete, generate, fix, or evaluate code programmed by humans. Considering the increasing popularity of the deep learning-enabled language models approach, we found a lack of empirical papers that compare different deep learning architectures to create and use language models based on programming code. This paper compares different neural network architectures like Average Stochastic Gradient Descent (ASGD) Weight-Dropped LSTMs (AWD-LSTMs), AWD-Quasi-Recurrent Neural Networks (QRNNs), and Transformer while using transfer learning and different forms of tokenization to see how they behave in building language models using a Python dataset for code generation and filling mask tasks. Considering the results, we discuss each approach’s different strengths and weaknesses and what gaps we found to evaluate the language models or to apply them in a real programming context."
Darban2021,Saeid Darban and Hosein Ghasemzadeh Tehrani and Nader Karballaeezadeh and Amir Mosavi,Application of analytical hierarchy process for structural health monitoring and prioritizing concrete bridges in iran,Applied Sciences (Switzerland),11,17,2021,10.3390/app11178060,20763417,"This paper proposes a method for monitoring the structural health of concrete bridges in Iran. In this method, the bridge condition index (BCI) of bridges is determined by the analytical hierarchy process (AHP). BCI constitutes eight indices that are scored based on the experts’ views, including structural, hydrology and climate, safety, load impact, geotechnical and seismicity, strategic importance, facilities, and traffic and pavement. Experts’ views were analyzed by Expert Choice software, and the relative importance (weight) of all eight indices were determined using AHP. Moreover, the scores of indices for various conditions were extracted from experts’ standpoints. BCI defines as the sum of weighted scores of indices. Bridge inspectors can examine the bridge, determine the scores of indices, and compute BCI. Higher values of BCI indicate better conditions. Therefore, bridges with lower BCI take priority in maintenance activities. As the case studies, the authors selected five bridges in Iran. Successful implementation of the proposed method for these case studies verified that this method can be applied as an easy-to-use optimization tool in health monitoring and prioritizing programs."
Peters2021,Andrés A. Peters and Francisco J. Vargas and Cristóbal Garrido and Cristóbal Andrade and Felipe Villenas,Pl-toon: A low-cost experimental platform for teaching and research on decentralized cooperative control,Sensors,21,6,2021,10.3390/s21062072,14248220,"In this paper, we present the development of a low-cost multi-agent system experimental platform for teaching, and research purposes. The platform consists of train-like autonomous agents equipped with local speed estimation, distance sensing to their nearest predecessor, and wireless communications with other agents and a central coordinator. The individual agents can be used for simple PID experiments in a classroom or laboratory setting, while a collection of agents are capable of performing decentralized platooning with cooperative adaptive cruise control in a variety of settings, the latter being the main goal of the platform. The agents are built from low cost components and programmed with open source software, enabling teaching experiences and experimental work with a larger number of agents that would otherwise be possible with other existing solutions. Additionally, we illustrate with experimental results some of the teaching activities that the platform is capable of performing."
Wang2022,Wei Wang and Yun Qi and Jiao Liu,Study on multi field coupling numerical simulation of nitrogen injection in goaf and fire-fighting technology,Scientific Reports,12,1,2022,10.1038/s41598-022-22296-9,20452322,"In order to effectively prevent the spontaneous combustion of residual coal in goaf, taking the 10,101 fully mechanized top coal caving face of Baozigou coal mine as research object, the multi-field coupling numerical model of nitrogen injection in goaf is established. The FLUENT software is used to study the variation law of spontaneous combustion zone in goaf under dynamic mining of working face with different nitrogen injection parameters, determining the range of spontaneous combustion zone in stable stage. The fitting curve between nitrogen injection parameters and width of spontaneous combustion zone in goaf is obtained. Results show that with the increase of nitrogen injection depth from 10 to 60 m, the width of spontaneous combustion zone in goaf begins to decrease gradually, yet the width of spontaneous combustion zone tends to expand after more than 40 m. When the nitrogen injection location is 40 m, the spontaneous combustion zone width decreases from 49 to 22 m as the nitrogen injection volume increases from 500 to 1000 m3/h. Nitrogen injection continuously reduces the area of high temperature zone and temperature extreme value. When the nitrogen injection parameter is set to (40 m–1000 m3/h), temperature extreme value decreases by 308.85 K compared with that without nitrogen injection. When the nitrogen injection parameter is (40 m–690 m3/h), it can meet the inert cooling requirements of goaf. The width of spontaneous combustion zone is 31 m and the temperature extreme value is 309.95 K at the moment. Finally, engineering application of the fire prevention technology combining shot-off loss wind and nitrogen injection is used to test effect of spontaneous combustion prevention and verify accuracy of nitrogen injection simulation. CO concentration at the measuring point 1, 3 and 5 are reduced to 0 × 10−3‰, 2 × 10−3‰ and 1.2 × 10−3‰, and temperature are reduced to 295.15 K, 296.15 K and 295.65 K respectively. It shows that the spontaneous combustion of residual coal in goaf has been successfully controlled."
Iqbal2020,Danish Iqbal and Assad Abbas and Mazhar Ali and Muhammad Usman Shahid Khan and Raheel Nawaz,Requirement Validation for Embedded Systems in Automotive Industry through Modeling,IEEE Access,8,,2020,10.1109/ACCESS.2019.2963774,21693536,"Requirement validation contributes significantly toward the success of software projects. Validating requirements is also essential to ensure the correctness of embedded systems in the auto industry. The auto industry emphasizes a lot on the verification of car designs and shapes. Invalid or erroneous requirements lead to inappropriate designs and degraded product quality. Considering the required expertise and time for requirement validation, significant attention is not devoted to verification and validation of requirements in the industry. Currently, the failure ratio of software projects is significantly higher and the key reason for that appears to be the inappropriate and invalidated requirements at the early stages in the projects. To that end, we propose a model-based approach that uses the existing VV model. Through virtual prototyping, the proposed approach eliminates the need to validate the requirements after each stage of the project. Consequently, the model is validated after the design phase and the errors in requirements are detected at the earliest stage. In this research, we performed two different case studies for requirement validation in the auto industry by using a modeling-based approach and formal technique using Petri nets. A benefit of the proposed modeling-based approach is that the projects in the auto industry domain can be completed in less time due to effective requirements validation. Moreover, the modeling-based approach minimizes the development time, cost and increases productivity because the majority of the code is automatically generated using the approach."
Rajvanshi2021,Harsh Rajvanshi and Yashpal Jain and Nidhi Kaintura and Chaitanya Soni and Raja Chandramohan and Ramanathan Srinivasan and Vinay Telasey and Praveen K. Bharti and Deepak Jain and Mangeshi Surve and Sachin Saxena and Vilas Gangamwar and M. S. Anand and Altaf A. Lal,"A comprehensive mobile application tool for disease surveillance, workforce management and supply chain management for Malaria Elimination Demonstration Project",Malaria Journal,20,1,2021,10.1186/s12936-021-03623-3,14752875,"Background: Health care technologies are now offering accountability, quality, robustness, and accuracy in disease surveillance and health care delivery programmes. With the advent of mobile hand-held devices, these technologies have become more accessible and adaptable for use by field staff working in remote areas. The Malaria Elimination Demonstration Project started collection of data and conduct of routine operations using paper-based reporting systems. Observing the need for a robust and quality digital mobile application, a comprehensive mobile application tool was developed that allowed the project to conduct disease surveillance, workforce management and supply chain management. Methods: In June 2017, the project conceptualized a comprehensive mobile application tool in the local language (Hindi) for disease surveillance, human resources management, and supply chain management. The tool is also available in English. Solution for Community Health-workers (SOCH) mobile app is an android native application developed using android SDK and web-based tool using MVC.net framework. Construction of the application started in November 2017 and rolled out its pilot in April 2018, followed by pan-district roll out in July 2018. The application uses self-validation tools to ensure high level of data quality and integrity. Results: The software is available in android based hand-held devices and web-screens with built-in data analytical capabilities. Using SOCH, the project has now successfully digitized its routine surveillance, attendance, tour plans, supply chain management components. The project has documented a reduction in 91% indigenous cases in the district, 60% improvement in stock accountability, and 99.6% accuracy in data collected through the mobile application. Conclusion: SOCH is an excellent and user-friendly tool, which can be customized for any public health management programme. The system ensures accountability and data robustness, which is needed for malaria elimination efforts throughout the country. The mobile application can be adapted for English or any other Indian or international language for use for malaria or any other disease surveillance and control programme. Another expansion feature of this mobile application is incorporation of indicators for Indoor Residual Spraying (IRS), Long-Lasting Insecticidal Nets (LLINs), and minor engineering by the residents of community under surveillance. The authors believe that it would be highly desirable and appropriate for an international organization, such as the World Health Organization (WHO), to conduct an independent comparison of all available mobile e-surveillance tools, so that a high-performing and globally suitable system can be selected for use in malaria elimination programmes. The Foundation of Disease Elimination and Controlof India has decided to make the SOCH mobile application available to anyone who would like to use it for disease surveillance and health care programmes."
Silva2020,Juarez Bento Silva and Isabela Nardi Silva and Simone Bilessimo,"Technological structure for technology integration in the classroom, inspired by the maker culture",Journal of Information Technology Education: Research,19,,2020,10.28945/4532,15393585,"Aim/Purpose This paper presented the framework for the integration of digital technolo-gies in education, implemented in InTecEdu Program, developed by Remote Experimentation Laboratory (RExLab), Federal University of Santa Catarina (UFSC), Brazil. Background The main objective of the model presented is to arouse interest in science and technology among adolescents. Therefore, it sought to develop STEM competencies (Science, Technology, Engineering, and Mathematics) in chil-dren and adolescents. Understanding learning in STAM areas can favor the development of professionals who can supply the demand in related sectors, especially in the scientific-technological scope. To fulfill the main objective, strategies related to students and teachers were developed. With activities aimed at students, it was hoped to promote vocations to scientific-technolog-ical careers and encourage entrepreneurship. On the other hand, the activities related to teachers aimed at training them to integrate technology into their lesson plans. Inspired by the Maker Culture, the model sought to make it possible for teachers to become the main agents in the process of integrating technology in their lesson plans, since they were in charge of building and producing their digital content and other resources to support their didactic activities. The maker movement is a technological extension of the ""Do It Yourself!"" culture, which encourages ordinary people to build, modify, repair, and manufacture their objects, with their own hands. The training actions were preceded by a diagnosis, inspired by the Technological Pedagogical Content Knowledge (TPACK) model, as well as the lesson plans prepared and made available by the teachers. Methodology Methodologically, the framework's work plan was composed of five Work Packages (WP), which include management, resource mapping, strategies re-lated to teachers, strategies related to students, and the dissemination and ex-ploitation of results. In the 2014-2018 period, 367 teachers participated in training activities, intending to integrate technologies into lesson plans. At the end of 2018, 27 Basic Education schools, including an indigenous and a rural school, from the public-school system, in the states of Santa Catarina, Minas Gerais, and the Rio Grande do Sul, in Brazil, using the project's Vir-tual Learning Environment (VLE). In these 70 teachers, 230 classes, and 6,766 students accessed didactic content, produced by teachers, at VLE. Also, 20 laboratories were available in 26 instances, for use in practical activi-ties in disciplines in the STEM areas. Specifically, in the STEM areas, 3,360 students from 98 classes from 9 schools had integrated the Remote Labora-tories, in lesson plans in the subjects of Physics and Biology (High School), Science (Elementary School). Contribution The main results of the application of the framework are related to the train-ing of human resources, knowledge production, and educational innovation. About the training of human resources, we sought to contribute to the train-ing of teachers concerning technology in education and, with that, arouse greater interest on the part of students, as well as obtain improvements in their learning from teaching methodologies supported on the use of digital technologies. On the other hand, the production of knowledge, in the pro-gram and the socialization of research, is favored by the model based on open-source resources, both in terms of software and hardware and with open educational resources. This characteristic favor and expands the poten-tial for reapplying research and, consequently, its contribution to educational innovation. Findings The results, about students, indicated an increase in motivation due to the creation of new teaching and learning opportunities. The fact of extending the classroom and school, through remote laboratories, to support practical activities and the use of VLE, was also pointed out as a very positive factor. On the other hand, the realization of the workshops, inspired by practices of the Maker Culture, provided an approximation of these to the skills of the real world, which will certainly favor their employability. Regarding the teach-ers, it is noticed the continuity and expansion in the use of technological re-sources in the classroom; many sought and have participated in new training actions. Recommendations for Practitioners Provision of a repository of practices for sharing and reuse of lesson plans developed by teachers participating in the research. Technical documents, manuals, and guides for robotics, computer programming, electronics and new technology workshops for students. Recommendations for Researchers Technical documents, manuals, and guides for remote laboratories. Data col-lected in the applied questionnaires. Technical documents, manuals, and guides for robotics, computer programming, electronics and new technology workshops for students. Impact on Society The main results of the framework application are related to human re-sources formation, knowledge production, and educational innovation. Re-garding the formation of human resources, we sought to contribute to the formation of teachers concerning technology in education and, about the stu-dents the creation of teaching and learning opportunities, to extend the class-room and also the school, through the remote laboratories, to support the practical activities and the use of the VLE. Future Research The socialization and reapplication of the framework since it is based on open-source resources, both software and hardware, and with open educa-tional resources."
Urban2022,Harald Urban and Gabriel Pelikan and Christian Schranz,Augmented Reality in AEC Education: A Case Study,Buildings,12,4,2022,10.3390/buildings12040391,20755309,"Augmented Reality (AR) is a Construction 4.0 technology that is seen as a site-extension of Building Information Modelling (BIM). In addition to the practical aspect within the design and construction processes AR can be used to support teaching through visualizations and interaction. This article presents a new AR platform called “AR-supported Teaching”, applicable for both Architecture, Engineering and Construction (AEC) education and as a Construction 4.0 technology. The aim of this project is to increase the amount of AEC AR content available for education and to introduce students to the productive use of AR. During its development, special attention was paid to the needs of the AEC industry. Users can employ BIM models to create AR scenes before adding animations and annotations without requiring programming skills. The AR platform enables interaction with remote experts and is therefore also suitable for distance learning. In a pilot study, use cases were defined and students tested the usability of the applications. The results were positive and additional suggestions for improvement were made. The feedback and motivation of the students indicate that AR has a future in education, especially if enough AEC AR content and practical use cases are available. The latter also concerns the application of AR in AEC practice."
Martin2022,Mariano Martin and Rafiqul Gani and Iqbal M. Mujtaba,"Sustainable process synthesis, design, and analysis: Challenges and opportunities",Sustainable Production and Consumption,30,,2022,10.1016/j.spc.2022.01.002,23525509,"In this perspective paper, we present challenges and opportunities that the chemical, biochemical and related industries pose to the process system engineering community to help deliver reliable and novel sustainable alternatives. More specifically, we highlight the need for a systems approach where model-based sustainable process synthesis, design, and analysis serve as opportunities to tackle the challenges. Three technology areas (interlinked to each other) that impact the sustainability of earth, namely, chemical processes linked with CO2 capture and utilization, biorefineries and water desalination are selected to highlight our views as well as the need for further development of computer-aided tools to efficiently solve the large and complex mathematical systems the problems represent. Analysis of these problems and their reported solutions indicate that opportunity exists for development of a new class of model-based methods and tools and their integration with the currently available ones to obtain the desired sustainability development goals."
Nawaz2022,Yasir Nawaz and Muhammad Shoaib Arif and Kamaleldin Abodayeh and Mairaj Bibi,Finite Element Method for Non-Newtonian Radiative Maxwell NanoFluid Flow under the Influence of Heat and Mass Transfer,Energies,15,13,2022,10.3390/en15134713,19961073,"The recent study was concerned with employing the finite element method for heat and mass transfer of MHD Maxwell nanofluid flow over the stretching sheet under the effects of radiations and chemical reactions. Moreover, the effects of viscous dissipation and porous plate were considered. The mathematical model of the flow was described in the form of a set of partial differential equations (PDEs). Further, these PDEs were transformed into a set of nonlinear ordinary differential equations (ODEs) using similarity transformations. Rather than analytical integrations, numerical integration was used to compute integrals obtained by applying the finite element method. The mesh-free analysis and comparison of the finite element method with the finite difference method are also provided to justify the calculated results. The effect of different parameters on velocity, temperature and concentration profile is shown in graphs, and numerical values for physical quantities of interest are also given in a tabular form. In addition, simulations were carried out by employing software that applies the finite element method for solving PDEs. The calculated results are also portrayed in graphs with varying sheet velocities. The results show that the second-order finite difference method is more accurate than the finite element method with linear interpolation polynomial. However, the finite element method requires less number of iterations than the finite difference method in a considered particular case. We had high hopes that this work would act as a roadmap for future researchers entrusted with resolving outstanding challenges in the realm of enclosures utilized in industry and engineering."
Khandoker2022,Azad Khandoker and Sabine Sint and Guido Gessl and Klaus Zeman and Franz Jungreitmayr and Helmut Wahl and Andreas Wenigwieser and Roland Kretschmer,Towards a logical framework for ideal MBSE tool selection based on discipline specific requirements,Journal of Systems and Software,189,,2022,10.1016/j.jss.2022.111306,01641212,"Model-Based Systems Engineering (MBSE) has emerged with great potential to fulfill the non-linearly rising demand in interdisciplinary engineering, e.g., product development. However, the variety and complexity of MBSE tools pose difficulties in particular industrial applications. This paper tries to serve as a guideline to find the ideal tool for a specific industrial application as well as to highlight the key criteria that an industry might consider. For this purpose, we propose a logical framework for MBSE tool selection, which is based on market research, the approaches of Quality Function Deployment (QFD), and decision matrix. As customers are at the center of any product, accordingly the needs of MBSE tool users are addressed within this research as the fundamental starting point. Market research and extensive discussions with MBSE tool vendors and academia show the current situation of MBSE tools. To compare the performance of the considered tools, a set of user needs is defined. QFD is performed to analyze the user needs with respect to evaluable technical properties. Subsequently each tool performance is assessed using a decision matrix. Through this process, a well-defined functional structure of MBSE tools is sketched, and in order to identify the properties of an ideal tool, all the attributes of different MBSE tools are mapped to a common platform. For the purpose of evaluation, we apply our proposed logical framework to select an exemplary MBSE tool for interdisciplinary application."
Bhandari2022,D. S. Bhandari and Dharmendra Tripathi and O. Anwar Bég,Electro-osmosis modulated periodic membrane pumping flow and particle motion with magnetic field effects,Physics of Fluids,34,9,2022,10.1063/5.0111050,10897666,"Theoretical studies of micro-electro-mechanical systems provide important insight into the mechanisms and optimization of such devices for a range of applications, including biomedical and chemical engineering. Inspired by emerging applications of microfluidics, unsteady viscous flow in a microchannel with periodic membrane pumping modulated by electro-magnetohydrodynamics is analyzed in a mathematical framework. The membrane kinematics induces the pressure inside the microchannel, where an electric field enhances the capability of the pumping flow rate. This model is formulated based on the Navier-Stokes equations, the Poisson equation, and the Maxwell electromagnetic equations and is further simplified using the lubrication approximations and Debye-Hückel linearization. The transformed dimensionless conservation equations under appropriate boundary conditions are analytically solved and the graphical results are illustrated through MATLAB (2019b) software. From the computational results, it is found that the Hartmann number enhances the fluid pressure uniformly throughout the microchannel, while the electric field parameter enforces the direction of the pressure-driven flow. The time-averaged flow rate exhibits a linear decay with axial pressure gradient, and it is strongly elevated with electric field parameter whereas it is weakly increased with electric double layer thickness parameter. It is further observed that the fluid is driven unidirectionally by the membrane contractions via a particle tracking simulation method. This study is relevant to provide the parametric estimation in designing the magnetic field-based microfluidics devices for microlevel transport phenomena."
Bublik2021,Sergey Bublik and Jan Erik Olsen and Varun Loomba and Quinn Gareth Reynolds and Kristian Etienne Einarsrud,A Review of Ferroalloy Tapping Models,Metallurgical and Materials Transactions B: Process Metallurgy and Materials Processing Science,52,4,2021,10.1007/s11663-021-02134-5,10735615,"Tapping is an important furnace operation in the ferroalloy industry and poses a number of complex and coupled challenges of both practical and economical importance. Owing to the hazardous high-temperature conditions surrounding the tap hole, the application of various modeling techniques allows for development and acquisition of both scientific and engineering knowledge of the process through physical or numerical proxies. In this review, earlier work on modeling of ferroalloy tapping is summarized and main principles of the tapping process and multiphase interaction of slag and metal are discussed and summarized. The main focus is on drainage of slag and alloys, but some attention will also be given to metal loss, metal overflow and health, safety and environment. Our review shows that although considerable progress has been made in computational capability over the last decades, However, it is clear that research and development in the field of ferroalloy furnace tapping remains at a relatively nascent stage. The most progress up to date has happened in the area of so called reduced-order models. Such models are robust and simple, and may be easily fitted to process data from a particular operation in order to develop tailored solutions. Such models are more easily combined with software and instruments, ultimately enabling improved automation, process control and ultimately improved tapping consistency."
Manouchehri2020,Fateme Manouchehri and Ali Shahandeh Nookabadi and Mahdi Kadivar,Production routing in perishable and quality degradable supply chains,Heliyon,6,2,2020,10.1016/j.heliyon.2020.e03376,24058440,"Generally, the major goal in perishable supply chains is preserving the product's quality along with improving its logistic performance. In this regard, temperature seems to be the most important and most sensitive factor, since uncontrolled temperature has shown to have great impact on reducing product quality. In this study, an integrated production routing model for a perishable product was developed; considering the production, inventory, storage temperature, routing and vehicle temperature. To solve the problem, a hybrid search algorithm combining the variable of neighborhood search algorithm and the mechanism of the simulated annealing algorithm was designed. In order to evaluate the validity of the proposed algorithm, its results were compared with that of the CPLEX solver in the GAMZ software environment. Comparison of the results of the two methods shows the efficiency of the proposed algorithm to solve this problem. As a case study, the proposed model was also applied in a real industrial case. According to the results, this company can greatly reduce its distribution and inventory costs and also avoid waste by using this program."
LEI2020,Qun LEI and Dingwei WENG and Baoshan GUAN and Lijun MU and Yun XU and Zhen WANG and Ying GUO and Shuai LI,A novel approach of tight oil reservoirs stimulation based on fracture controlling optimization and design,Petroleum Exploration and Development,47,3,2020,10.1016/S1876-3804(20)60080-5,18763804,"To deal with the stress interference caused by simultaneous propagation of multiple fractures and the wettability reversal and physical property changes of the reservoir caused by fracturing fluid getting in during large-volume fracturing of tight oil reservoirs through a horizontal well, a non-planar 3D fracture growth model was built, wettability reversal characterizing parameters and change of relative permeability curve were introduced to correct the production prediction model of fractured horizontal well, a fracturing design optimization software (FrSmart) by integrating geological and engineering data was developed, and a fracturing design optimization approach for tight oil reservoirs based on fracture control was worked out. The adaptability of the method was analyzed and the fracture parameters of horizontal wells in tight oil reservoirs were optimized. The simulation results show that fracturing technology based on fracture control is suitable for tight oil reservoirs, and by optimizing fracture parameters, this technology makes it possible to produce the maximum amount of reserves in the well-controlled unit of unconventional reservoirs. The key points of fracturing design optimization based on fracture control include increasing lateral length of and reducing the row spacing between horizontal wells, increasing perforation clusters in one stage to decrease the spacing of neighboring fractures, and also avoiding interference of old and new fracturing wells. Field tests show that this technology can increase single well production and ultimate recovery. Using this technology in developing unconventional resources such as tight oil reservoirs in China will enhance the economics significantly."
Beylergil2020,Bertan Beylergil,Multi-objective optimal design of hybrid composite laminates under eccentric loading,Alexandria Engineering Journal,59,6,2020,10.1016/j.aej.2020.09.015,11100168,"Laminated composites are being used in many engineering applications since they provide significant weight reduction, better fatigue and corrosion resistance. In this study, design optimization of a hybrid composite laminate subjected to eccentric loading was carried out using multi-objective genetic algorithm (MOGA). The ply material (carbon fiber or E glass) and fiber orientations (−90 ≤ θ ≤ 90) were considered as design variables. The objective function was to minimize cost and weight with the maximization of the stiffness. The design constraints were the maximum Tsai-Wu failure index, first mode natural frequency. The numerical analyses were carried out by using ANSYS software package. Genetic aggregation model was used to generate the response surfaces which were used to obtain the optimum design variables. The optimum lay-up sequence and ply materials were determined for weight saving and cost reduction of hybrid composite plates. It was shown that the hybridization of carbon fiber and E glass fiber provided the optimal designs offers lower cost and higher mechanical performance. MOGA method was very effective to optimize the structural performance of the hybrid composite plates under eccentric loading."
Solak2020,Serdar Solak and Önder Yakut and Emine Dogru Bolat,Design and implementation ofweb-based virtual mobile robot laboratory for engineering education,Symmetry,12,6,2020,10.3390/SYM12060906,20738994,"A web-based virtual and remote laboratory environment is developed, realized and proposed for real time control and monitoring of a mobile robot in an indoor environment. In this laboratory, a real time and continuous video stream of indoor laboratory environment is viewed by wireless IP camera mounted to the ceiling. The localization of the robot is also implemented using this IP camera. In this environment, a virtual target and virtual obstacles are located anywhere on the video image taken by the user. The robot is guaranteed to arrive at the virtual target avoiding virtual obstacles using the shortest path. The video stream of the robot's navigation is monitored through the web environment. The robot is controlled by a BeagleBoard-xM single board computer. The PC web server symmetrically communicates with the other web server on the BeagleBoard-xM, executing developed application software. Since genetic algorithms generate alternative solutions, it is utilized as a path planning algorithm. Parameters such as population size and maximum generation of genetic algorithms applied to get the shortest path for the robot are tuned via the web-based virtual laboratory environment. The robot is also controlled manually through the web environment. At the conclusion of the experiments, the results are monitored on the web-based virtual laboratory environment. A low-cost mobile robot virtual remote laboratory is designed and implemented for engineering education in this paper. Consequently, survey and some experimental works, of the usability and performance of the RRC-Lab (remote robot control-laboratory) system are confirmed by students."
Heyden2020,Emil Heyden and Jan Küchenhof and Erik Greve and Dieter Krause,Development of a Design Education Platform for an Interdisciplinary Teaching Concept,,91,,2020,10.1016/j.procir.2020.02.213,22128271,"Teams of engineering students take part in a problem-based learning course, where they develop physical product variants of a robotic system and take part in a competition. Design theory and methodology are part of the teaching concept, including complexity management by platform strategy and modularization. Skills in mechanical engineering, design, numerical optimization, additive manufacturing and use of CAD, PDM and FEM software are improved. The developed robotic systems are divided into different modules, these modules are attached on the Design Education Platform. This paper shows the in-house development of this platform with regard to the requirements from the design education course and technical specifications. In this context, findings from an earlier developed product family architecture of a robotic system as reference are integrated. The product architecture is shown in a module interface graph, displaying the component variety as well as their linkages. A description of the used mechanical and electrical components and their computational as well as communication possibilities is shown. The problem-based learning course and its interdisciplinary teaching concept are presented. First findings are formulated and the resulting products of student teams of summer semester 2019 are shown. These first product variants are the starting point of a product family, with new modules and resulting product variants each semester. The influence of the Design Education Platform on the course is evaluated, with the focus on decreasing the internal work for the lecturers, while student numbers are growing. The outlook shows further possibilities of tasks to fulfill and the use of the system as a cyber-physical system for future product generations."
Zhou2020,Zhengshu Zhou and Qiang Zhi and Shuji Morisaki and Shuichiro Yamamoto,An Evaluation of Quantitative Non-Functional Requirements Assurance Using ArchiMate,IEEE Access,8,,2020,10.1109/ACCESS.2020.2987964,21693536,"Goal-oriented NFR (Non-Functional Requirement) assurance approaches were used to qualitatively evaluate software architectures. Assurance cases using quantitative method have not been applied to evaluate NFR assurance for software architectures. This paper presents a system architecture evaluation method which is able to conduct quantitative NFR assurance evaluation for system architecture through ArchiMate. The paper also proposes an algorithm to automate the quantitative evaluation process. A questionnaire survey among software engineers and a case study on a vehicular safety monitor system were carried out to verify the necessity of the method. Additionally, we conducted an experimental design with 18 samples divided into 2 groups with the goal of comparing how the independent variables affect the dependent variables. The results of the experiment demonstrate that the proposed method achieves better NFR evaluation effect than the traditional approach. Moreover, compared with the traditional approach, the proposed method shortens the time for NFR evaluation. The proposed method is expected to be used at the early stage of software development projects for system NFR development, such as requirements analysis, system architecture design and system modeling. At present, the method has been applied by software engineers in a practical software project."
Barragn-Villarejo2020,Manuel Barragán-Villarejo and Francisco de Paula García-López and Alejandro Marano-Marcolini and José María Maza-Ortega,Power system hardware in the loop (PSHIL): A holistic testing approach for smart grid technologies,Energies,13,15,2020,10.3390/en13153858,19961073,"The smart-grid era is characterized by a progressive penetration of distributed energy resources into the power systems. To ensure the safe operation of the system, it is necessary to evaluate the interactions that those devices and their associated control algorithms have between themselves and the pre-existing network. In this regard, Hardware-in-the-Loop (HIL) testing approaches are a necessary step before integrating new devices into the actual network. However, HIL is a device-oriented testing approach with some limitations, particularly considering the possible impact that the device under test may have in the power system. This paper proposes the Power System Hardware-in-the-Loop (PSHIL) concept, which widens the focus from a device- to a system-oriented testing approach. Under this perspective, it is possible to evaluate holistically the impact of a given technology over the power system, considering all of its power and control components. This paper describes in detail the PSHIL architecture and its main hardware and software components. Three application examples, using the infrastructure available in the electrical engineering laboratory of the University of Sevilla, are included, remarking the new possibilities and benefits of using PSHIL with respect to previous approaches."
Papp2021,Bálint Papp and Gergely Kristóf and Christof Gromke,Application and assessment of a GPU-based LES method for predicting dynamic wind loads on buildings,Journal of Wind Engineering and Industrial Aerodynamics,217,,2021,10.1016/j.jweia.2021.104739,01676105,"This study presents the assessment of a fast Large Eddy Simulation method for estimating dynamic wind loads on buildings using a GPU-based CFD software, which produces statistically converged results on a nine-million-cell mesh in approximately 6 hours. The surface pressure distribution of a cuboid building model was validated with experimental data obtained in an atmospheric boundary layer wind tunnel and compared with field measurements. Although due to the applied equidistant Cartesian grid the large gradients near the edges are not fully resolved, good overall agreement was found for the mean and fluctuating pressure distributions (correlation coefficient: 0.90/0.73, FAC2: 0.92/0.98, FB: −/0.06, MG: −/0.95, NMSE: −/0.10, VG: −/1.08). It was shown that the numerical model is able to produce matching turbulent spectra in an intermediate frequency range within the inertial subrange, limited by the domain size and the spatial resolution. Mesh refinement for capturing large gradients as well as for expanding the frequency limits can be achieved by using a GPU with higher VRAM capacity for the simulation. The continuing advancement of the presented model is a promising development for estimating dynamic wind loads on buildings and identifying design problems fast enough for the engineering practice, without high-performance computing."
Phan2020,Mai Ha Phan and Ha Quang Thinh Ngo,A multidisciplinary mechatronics program: From project‐based learning to a community‐based approach on an open platform,Electronics (Switzerland),9,6,2020,10.3390/electronics9060954,20799292,"To face contemporary problems, international engineers must be trained in advanced learning environments and with professional skills and knowledge. Sponsored by USAID (US Agency for International Development), the Build‐IT (Building University‐Industry Learning and Development through Innovation and Technology) program leverages the vast capabilities of the implementing partner from Arizona State University and plays a key role as an innovative pioneer in converging personalities from various fields. A well‐educated engineer can contribute to the sustainable development of society. With the aim of building community‐oriented education, an integrated strategy was proposed in which a problem‐based learning method is investigated to apply technical knowledge. In accordance with this strategy, in our proposed method, students from Mechatronics Engineering first had to work together with other learners in the electronics, software, control automation, and mechanics fields, followed by the design of an open platform integrated multi‐disciplinary approach. By collaborating with their peers in developing this hardware, students become better equipped with specialized knowledge. This process also allows students to feel confident in implementing their innovative thinking while still maintaining the core meaning of the instrument. One of the key benefits of this approach is that helping students overcome their problems concurrently enhances the engineer’s function in the community despite missing some specialized skill sets. Based on experimental works using this open framework, the present approach demonstrates that pupils in our program have sufficient ability to contribute to social achievements. Lastly, the feasible, low‐cost, and visually educational instrument made by the participants showcases the value of such a multi‐disciplinary approach."
Swain2023,K. Swain and S. Mohammed Ibrahim and G. Dharmaiah and S. Noeiaghdam,Numerical study of nanoparticles aggregation on radiative 3D flow of maxwell fluid over a permeable stretching surface with thermal radiation and heat source/sink,Results in Engineering,19,,2023,10.1016/j.rineng.2023.101208,25901230,"The flow of fluid past a stretching sheet under heat and mass transfer analysis is significant because it has numerous applications in engineering and technology, including metal spinning, polymer extrusion, the manufacture of glass fibres, and metal casting etc. The current article investigates the effects of thermal radiation and heat source/sink on 3D nanofluid flow over an elongated surface embedded in a porous medium with nanoparticles aggregation. Further, the significance of variable magnetic field is one of the striking features of the present study. We have considered copper (metallic) and titania (metallic oxide) as nanoparticles and water as base fluid. The effective thermal conductivity of nanofluid is predicted by using a modified Maxwell model. Applying similarity transformations, the governing PDEs are transformed into nonlinear ODEs and solved numerically by MATLAB software using bvp4c and the shooting method. Graphs and tables are plotted to depict the significance of the effective parameters on velocity and temperature profiles as well as skin friction coefficient and Nusselt number. The existence of radiative parameter effects is more advantageous for improving heat transmission. The temperature distribution is enhanced due to presence of nanoparticles aggregation. Thus, the impact of nanoparticles aggregation is supportive theoretical tool in future bioengineering and industrial applications."
Bano2020,Tayyaba Bano and Franziska Hegner and Martin Heinrich and Ruediger Schwarze,Investigation of fluid-structure interaction induced bending for elastic flaps in a cross flow,Applied Sciences (Switzerland),10,18,2020,10.3390/APP10186177,20763417,"With the recent increase in the design of light and flexible structures, numerical investigations of fluid and structure together play a significant role in most engineering applications. Therefore, the current study presents an examination of fluid-structure interaction involving flexible structures. The problem is numerically solved by a commercial software ANSYS-Workbench. Two-way coupled three-dimensional transient simulations are carried out for the flexible flaps of different thicknesses in glycerin for a laminar flow and Reynolds number ranging from 3 < Re < 12. The bending line of the flaps is compared with experimental data for different alignments of the flaps relative to the fluid flow. The study reports the computation of the maximum tip-deflection and deformation of flaps fixed at the bottom and mounted normal to the flow. Additionally, drag coefficients for flexible flaps are computed and flow regimes in the wake of the flaps are presented. As well, the study gives an understanding on how the fluid response changes as the structure deforms and the model is appropriate to predict the behavior of thick and comparatively thinner flaps. The results are sufficiently encouraging to consider the present model for analyzing turbulent flow processes against flexible objects."
Anagnostakis2021,Aristidis G. Anagnostakis and Nikolaos Giannakeas and Markos G. Tsipouras and Euripidis Glavas and Alexandros T. Tzallas,Iot micro-blockchain fundamentals,Sensors,21,8,2021,10.3390/s21082784,14248220,"In this paper we investigate the essential minimum functionality of the autonomous blockchain, and the minimum hardware and software required to support it in the micro-scale in the IoT world. The application of deep-blockchain operation in the lower-level activity of the IoT ecosystem, is expected to bring profound clarity and constitutes a unique challenge. Setting up and operating bit-level blockchain mechanisms on minimal IoT elements like smart switches and active sensors, mandates pushing blockchain engineering to the limits. “How deep can blockchain actually go?” “Which is the minimum Thing of the IoT world that can actually deliver autonomous blockchain functionality?” To answer, an experiment based on IoT micro-controllers was set. The “Witness Protocol” was defined to set the minimum essential micro-blockchain functionality. The protocol was developed and installed on a peer, ad-hoc, autonomous network of casual, real-life IoT micro-devices. The setup was tested, benchmarked, and evaluated in terms of computational needs, efficiency, and collective resistance against malicious attacks. The leading considerations are highlighted, and the results of the experiment are presented. Findings are intriguing and prove that fully autonomous, private micro-blockchain networks are absolutely feasible in the smart dust world, utilizing the capacities of the existing low-end IoT devices."
Lopes2021,Rui Pedro Lopes and Bárbara Barroso and Leonel Deusdado and André Novo and Manuel Guimarães and João Paulo Teixeira and Paulo Leitão,Digital technologies for innovative mental health rehabilitation,Electronics (Switzerland),10,18,2021,10.3390/electronics10182260,20799292,"Schizophrenia is a chronic mental illness, characterized by the loss of the notion of reality, failing to distinguish it from the imaginary. It affects the patient in life’s major areas, such as work, interpersonal relationships, or self-care, and the usual treatment is performed with the help of anti-psychotic medication, which targets primarily the hallucinations, delirium, etc. Other symptoms, such as the decreased emotional expression or avolition, require a multidisciplinary approach, including psychopharmacology, cognitive training, and many forms of therapy. In this context, this paper addresses the use of digital technologies to design and develop innovative rehabilitation techniques, particularly focusing on mental health rehabilitation, and contributing for the promotion of well-being and health from a holistic perspective. In this context, serious games and virtual reality allows for creation of immersive environments that contribute to a more effective and lasting recovery, with improvements in terms of quality of life. The use of machine learning techniques will allow the real-time analysis of the data collected during the execution of the rehabilitation procedures, as well as enable their dynamic and automatic adaptation according to the profile and performance of the patients, by increasing or reducing the exercises’ difficulty. It relies on the acquisition of biometric and physiological signals, such as voice, heart rate, and game performance, to estimate the stress level, thus adapting the difficulty of the experience to the skills of the patient. The system described in this paper is currently in development, in collaboration with a health unit, and is an engineering effort that combines hardware and software to develop a rehabilitation tool for schizophrenic patients. A clinical trial is also planned for assessing the effectiveness of the system among negative symptoms in schizophrenia patients."
Qin2021,Fangling Qin and Ying Zhu and Tianqi Ao and Ting Chen,The development trend and research frontiers of distributed hydrological models—visual bibliometric analysis based on citespace,Water (Switzerland),13,2,2021,10.3390/w13020174,20734441,"Based on the bibliometric and data visualization analysis software Citespace, this study carried out document statistics and information mining on the Web of Science database and characterized the distributed hydrological model knowledge system from 1986 to 2019. The results show a few things: (1) from 1986 to 2019, the United States and China accounted for 41% of the total amount of publications, and they were the main force in the field of distributed hydrological model research; (2) field research involves multiple disciplines, mainly covering water resources, geology, earth sciences, environmental sciences, ecology and engineering; (3) the frontier of field research has shifted from using distributed hydrological models in order to simulate runoff and nonpoint source environmental responses to the coupling of technologies and products that can obtain high-precision, high-resolution data with distributed hydrological models. (4) Affected by climate warming, the melting of glaciers has accelerated, and the spatial distribution of permafrost and water resources have changed, which has caused a non-negligible impact on the hydrological process. Therefore, the development of distributed hydrological models suitable for alpine regions and the response of hydrological processes to climate change have also become important research directions at present."
Ore2020,Fredrik Ore and Juan Luis Jiménez Sánchez and Magnus Wiktorsson and Lars Hanson,Design method of human–industrial robot collaborative workstation with industrial application,International Journal of Computer Integrated Manufacturing,33,9,2020,10.1080/0951192X.2020.1815844,13623052,"How to design Human–Industrial Robot Collaborative (HIRC) workstations is one of the key challenges in the realisation of safe and efficient HIRC systems in industry. The aim of this paper is to present a simple method to be used in early phases of HIRC workstation design. The design method requires a simulation tool and is based on systematic design methodologies and its reference work, Pahl and Beitz´s engineering design framework. The proposed HIRC design method consists of four phases: planning and clarifying the work task, conceptual design, embodiment design and detail design, where iteration loops back to previous phases are vital. This design method is applied in an industrial HIRC design case on assembly of a flywheel cover on a heavy vehicle engine block. In this application example, a previously developed HIRC simulation software is used to generate quantitative values on identified evaluation criteria, in this case operation time and biomechanical load. This proposed HIRC design method in combination with any type of simulation tool enables the systematic design of HIRC workstations early in the production development process."
Viana2020,Romeu Viana and Oscar Dias and Davide Lagoa and Mónica Galocha and Isabel Rocha and Miguel Cacho Teixeira,Genome-scale metabolic model of the human pathogen candida albicans: A promising platform for drug target prediction,Journal of Fungi,6,3,2020,10.3390/jof6030171,2309608X,"Candida albicans is one of the most impactful fungal pathogens and the most common cause of invasive candidiasis, which is associated with very high mortality rates. With the rise in the frequency of multidrug-resistant clinical isolates, the identification of new drug targets and new drugs is crucial in overcoming the increase in therapeutic failure. In this study, the first validated genome-scale metabolic model for Candida albicans, iRV781, is presented. The model consists of 1221 reactions, 926 metabolites, 781 genes, and four compartments. This model was reconstructed using the open-source software tool merlin 4.0.2. It is provided in the well-established systems biology markup language (SBML) format, thus, being usable in most metabolic engineering platforms, such as OptFlux or COBRA. The model was validated, proving accurate when predicting the capability of utilizing different carbon and nitrogen sources when compared to experimental data. Finally, this genome-scale metabolic reconstruction was tested as a platform for the identification of drug targets, through the comparison between known drug targets and the prediction of gene essentiality in conditions mimicking the human host. Altogether, this model provides a promising platform for global elucidation of the metabolic potential of C. albicans, possibly guiding the identification of new drug targets to tackle human candidiasis."
Leoni2020,Francesco Leoni and Øystein Grong and Lise Sandnes and Torgeir Welo and Filippo Berto,Finite element modelling of the filler wire feeding in the hybrid metal extrusion & bonding (HYB) process,Journal of Advanced Joining Processes,1,,2020,10.1016/j.jajp.2020.100006,26663309,"HYB is a new solid-state joining method for metals and alloys that utilises continuous extrusion as a technique to enable aluminium filler metal additions. In the present paper a finite element (FE) model for the filler wire feeding inside the HYB PinPoint extruder has been developed and implemented within the commercial software package Deform 3D ™. Based on a comparison with experimental data for the extruder housing temperature, the drive spindle torque and the wire feed rate, it is concluded that the FE model is sufficiently relevant and comprehensive to be used in future developments of the HYB process. In addition to improved wire feeding control, a model-based approach will make it possible to reduce the engineering time and costs involved in case a modification of the design is needed to optimise the extruder performance in a real joining situation."
Yang2020,Yan Yang,Exploration and Practice of Maker Education Mode in Innovation and Entrepreneurship Education,Frontiers in Psychology,11,,2020,10.3389/fpsyg.2020.01626,16641078,"This study was conducted with the purpose of exploring the impact of positive entrepreneurial psychological quality in innovation and entrepreneurship education, as well as the development of maker education in colleges and universities. The questionnaire survey method – The Positive Mental Characters Scale for Chinese College Students – and the SPSS 26.0 mathematical statistical analysis software were adopted to analyze and characterize the development of innovation and entrepreneurship education in colleges and universities, as well as the practice of maker education. The results show that there are differences in the factors that affect the positive entrepreneurial psychological quality of college students studying different majors in the liberal arts and sciences. Family economy has the most obvious impact on liberal arts students, sports activities have the most obvious impact on science students, has and grades have the most obvious impact on engineering students; the average score of college students’ innovation and entrepreneurship ability is around 3.0, showing that the overall innovation and creativity ability is general. Furthermore, there are differences in the development of the maker education model between the eastern and western universities. Overall, the maker faculty of eastern universities are more complete, with a larger number of professors, associate seniors, and intermediate teachers. In addition, the investigation on the positive entrepreneurial psychological quality shows a positive effect on cultivating students’ healthy entrepreneurial quality as well as promoting the development and practice of maker education."
Izci2022,Davut Izci,A novel modified arithmetic optimization algorithm for power system stabilizer design,Sigma Journal of Engineering and Natural Sciences,40,3,2022,10.14744/sigma.2022.00056,13047205,"The development of a novel hybrid algorithm by modifying the arithmetic optimization algorithm (AOA) with the aid of simulated annealing technique is discussed in this paper. The novel algorithm, named modified arithmetic optimization algorithm (mAOA), is proposed as an effective tool for optimizing power system stabilizer (PSS) adopted in a single-machine infinite-bus power system. To perform the assessments, MATLAB/Simulink software was used. The evaluations on the proposed algorithm are initially performed using several benchmark functions that have unimodal and multimodal natures. The results are then compared with five of the other competitive approaches (arithmetic optimization algorithm, simulated annealing algorithm, genetic algorithm, particle swarm optimization and gravitational search algorithm). The comparisons with respect to those algorithms demonstrate the great promise of the constructed hybrid mAOA algorithm. This shows the greater balance between global and local search stages achieved by the mAOA algorithm. The performance of the developed mAOA algorithm is also assessed through designing an optimally performing PSS for further evaluation which allows the observation of its capability for complex real-world engineering problems. To do so, PSS damping controller is formulated as an optimization problem and the constructed mAOA algorithm is used to search for optimal controller parameters to demonstrate the applicability and the greater performance of the proposed hybrid algorithm for such a complex real-world engineering problem. The obtained results for the latter case are compared with the sine-cosine and symbiotic organisms search algorithms as they are the best performing reported algorithms. The comparisons have demonstrated the superiority of the mAOA algorithm over reported best performing algorithms in terms of PSS design, as well."
Arce2022,Elena Arce and Andrés Suárez-García and José Antonio López-Vázquez and María Isabel Fernández-Ibáñez,Design Sprint: Enhancing STEAM and engineering education through agile prototyping and testing ideas,Thinking Skills and Creativity,44,,2022,10.1016/j.tsc.2022.101039,18711871,"Creating project-based learning experiences in the classroom where students learn in a team to solve complex problems and to develop creative and critical thinking is a challenge. Design Sprint (DS) is an agile methodology (implemented in 5 days) with the goal of creating innovative design based on user needs (User Experience). The objective of this work was to develop an Engineering Drawing classroom experience linked to the context of the current COVID-19 pandemic with the Design Sprint methodology. The experience had to involve the integration of theory and practice, the application of knowledge, the development of both hard and soft skills, and the empowerment of students to conduct research. 56 first-year students following three STEAM degrees at the University of A Coruña participated in this experience. The activities were designed for both face-to-face and remote learning. Microsoft Teams and Moodle were used for tutoring and for monitoring student progress. The Moodle Workshop tool was used for the evaluation of the prototypes that were developed and the projects were evaluated by video. The students defended their projects through a presentation in lightning talk format (Ignite). Evaluation rubrics were used following a triple approach: co-evaluation, hetero-evaluation and self-evaluation. The 3D design of the projects was developed with Autodesk software. A total of 18 projects were developed. Once the projects were completed, a survey was administered to evaluate the levels of student satisfaction. The survey results were very positive. The Design Sprint projects also showed positive effects on grades. The Design Sprint method has promoted an interactive learning environment. In addition to its simplicity, a further advantage of DS method is that all student dedication is planned. Students were therefore less likely to feel overloaded, all of which helps with better time management. The DS methodology is multipurpose, so it can be applied to various fields and subjects."
Alarifi2023,Ibrahim M. Alarifi,PETG/carbon fiber composites with different structures produced by 3D printing,Polymer Testing,120,,2023,10.1016/j.polymertesting.2023.107949,01429418,"This research presents a novel methodology for simulating the failure of a 3D-printed engineering design structure. Fused deposition modeling (FDM) of polyethylene terephthalate glycol (PETG)/Carbon fiber (CF) material was utilized to develop and build the structure's topology. The mechanical characteristics of PETG/CF materials were evaluated through modeling, which was quantitatively linked to the experimental results. Scanning electron microscopy (SEM) was used to evaluate the fracture surface material before and after failure testing. The actual tests and numerical studies used five different fabrication structures which were correlated with deformation, force, and failure mode. ANSYS software was used with experimental results and finite element analysis (FEA) under both dynamic and quasi-static conditions. Five 3D printed materials of PETG reinforced with short CFs of approximately 7.6 μm in a weight fraction of 20% were investigated. The overall goal was to create a cost-effective and straightforward material production technology that can retain high mechanical strength while also providing suitable flexibility. The tensile test results of the 3D-printed PETG/CF solid structural design revealed a 23% improvement in yield strength over the other conventional structures. The study illustrates how FEA of 3D printing is used to evaluate the performance of a helmet chinstrap design with different production conditions, hence possibly reducing the product design and development time."
Hollenstein2022,Lena Hollenstein and Stefanie Thurnheer and Franziska Vogt,Problem Solving and Digital Transformation: Acquiring Skills through Pretend Play in Kindergarten,Education Sciences,12,2,2022,10.3390/educsci12020092,22277102,"One of the crucial 21st-century digital skills, in the context of digital transformation, is problem solving—equally so in the fields of science, technology, engineering, and mathematics (STEM). In the context of kindergarten, learning through play is central; therefore, pretend play, and particularly guided pretend play, is suggested as an innovative way to foster skills for digital problem solving. As yet, the potential of pretend play for children’s learning about digital transformation and digital problem-solving processes has hardly been researched. The paper examines how children solve digital problems in guided pretend play. In an explorative intervention study “We play the future”, an information technology center (IT center) is introduced as one of the play corners for pretend play in kindergartens, together with other inputs such as a smart home corner (Internet of Things) or autonomous vehicles. Children’s play was video recorded. From the 15 participating kindergartens, 13 h of sequences involving the IT center were analyzed using content analysis. The findings indicate that children identify problems in a play situation and solve them using problemsolving strategies, such as devising new applications and installing software. Furthermore, the findings show that the kindergarten teacher’s participation in the pretend play is important for enabling longer and more complex problem-solving processes. Consequences for further teacher training to foster problem-solving skills during guided pretend play are discussed."
Agor2023,Chima Dike Agor and Elvis Michael Mbadike and George Uwadiegwu Alaneme,Evaluation of sisal fiber and aluminum waste concrete blend for sustainable construction using adaptive neuro-fuzzy inference system,Scientific Reports,13,1,2023,10.1038/s41598-023-30008-0,20452322,"This research study presents evaluation of aluminum waste-sisal fiber concrete’s mechanical properties using adaptive neuro-fuzzy inference system (ANFIS) to achieve sustainable and eco-efficient engineering works. The deployment of artificial intelligence (AI) tools enables the optimization of building materials combined with admixtures to create durable engineering designs and eliminate the drawbacks encountered in trial-and-error or empirical method. The features of the cement-AW blend's setting time were evaluated in the laboratory and the results revealed that 0–50% of aluminum-waste (AW) inclusion increased both the initial and final setting time from 51–165 min and 585–795 min respectively. The blended concrete mix's flexural strength tests also show that 10% sisal-fiber (SF) substitution results in a maximum flexural strength of 11.6N/mm2, while 50% replacement results in a minimum flexural strength of 4.11N/mm2. Moreover, compressive strength test results show that SF and AW replacements of 0.08% and 0.1%, respectively, resulted in peak outcome of 24.97N/mm2, while replacements of 0.5% and 0.45% resulted in a minimum response of 17.02N/mm2. The ANFIS-model was developed using 91 datasets obtained from the experimental findings on varying replacements of cement and fine-aggregates with AW and SF respectively ranging from 0 to 50%. The ANFIS computation toolbox in MATLAB software was adopted for the model simulation, testing, training and validation of the response function using hybrid method of optimization and grid partition method of FIS at 100 Epochs. The compressive strength behavior is the target response, and the mixture variations of cement-AW and fine aggregates-SF combinations were used as the independent variables. The ANFIS-model performance assessment results obtained using loss function criteria demonstrates MAE of 0.1318, RMSE of 0.412, and coefficient of determination value of 99.57% which indicates a good relationship between the predicted and actual results while multiple linear regression (MLR) model presents a coefficient of determination of 82.46%."
Seibold2021,Heidi Seibold and Severin Czerny and Siona Decke and Roman Dieterle and Thomas Eder and Steffen Fohr and Nico Hahn and Rabea Hartmann and Christoph Heindl and Philipp Kopper and Dario Lepke and Verena Loidl and Maximilian Mandl and Sarah Musiol and Jessica Peter and Alexander Piehler and Elio Rojas and Stefanie Schmid and Hannah Schmidt and Melissa Schmoll and Lennart Schneider and Xiao Yin To and Viet Tran and Antje Völker and Moritz Wagner and Joshua Wagner and Maria Waize and Hannah Wecker and Rui Yang and Simone Zellner and Malte Nalenz,A computational reproducibility study of PLOS ONE articles featuring longitudinal data analyses,PLoS ONE,16,6 June,2021,10.1371/journal.pone.0251194,19326203,"Computational reproducibility is a corner stone for sound and credible research. Especially in complex statistical analyses-such as the analysis of longitudinal data-reproducing results is far from simple, especially if no source code is available. In this work we aimed to reproduce analyses of longitudinal data of 11 articles published in PLOS ONE. Inclusion criteria were the availability of data and author consent. We investigated the types of methods and software used and whether we were able to reproduce the data analysis using open source software. Most articles provided overview tables and simple visualisations. Generalised Estimating Equations (GEEs) were the most popular statistical models among the selected articles. Only one article used open source software and only one published part of the analysis code. Replication was difficult in most cases and required reverse engineering of results or contacting the authors. For three articles we were not able to reproduce the results, for another two only parts of them. For all but two articles we had to contact the authors to be able to reproduce the results. Our main learning is that reproducing papers is difficult if no code is supplied and leads to a high burden for those conducting the reproductions. Open data policies in journals are good, but to truly boost reproducibility we suggest adding open code policies."
Seibert2022,Paul Seibert and Alexander Raßloff and Karl Kalina and Marreddy Ambati and Markus Kästner,Microstructure Characterization and Reconstruction in Python: MCRpy,Integrating Materials and Manufacturing Innovation,11,3,2022,10.1007/s40192-022-00273-4,21939772,"Microstructure characterization and reconstruction (MCR) is an important prerequisite for empowering and accelerating integrated computational materials engineering. Much progress has been made in MCR recently; however, in the absence of a flexible software platform it is difficult to use ideas from other researchers and to develop them further. To address this issue, this work presents MCRpy as an easy-to-use, extensible and flexible open-source MCR software platform. MCRpy can be used as a program with graphical user interface, as a command line tool and as a Python library. The central idea is that microstructure reconstruction is formulated as a modular and extensible optimization problem. In this way, arbitrary descriptors can be used for characterization and arbitrary loss functions combining arbitrary descriptors can be minimized using arbitrary optimizers for reconstructing random heterogeneous media. With stochastic optimizers, this leads to variations of the well-known Yeong–Torquato algorithm. Furthermore, MCRpy features automatic differentiation, enabling the utilization of gradient-based optimizers. In this work, after a brief introduction to the underlying concepts, the capabilities of MCRpy are demonstrated by exemplarily applying it to typical MCR tasks. Finally, it is shown how to extend MCRpy by defining a new microstructure descriptor and readily using it for reconstruction without additional implementation effort."
Sas2020,Amelie Sas and Nicholas Ohs and Esther Tanck and G. Harry van Lenthe,Nonlinear voxel-based finite element model for strength assessment of healthy and metastatic proximal femurs,Bone Reports,12,,2020,10.1016/j.bonr.2020.100263,23521872,"Nonlinear finite element (FE) models can accurately quantify bone strength in healthy and metastatic femurs. However, their use in clinical practice is limited since state-of-the-art implementations using tetrahedral meshes involve a lot of manual work for which specific modelling software and engineering knowledge are required. Voxel-based meshes could enable the transition since they are robust and can be highly automated. Therefore, the aim of this work was to bridge the modelling gap between the tetrahedral and voxel-based approach. Specifically, we validated a nonlinear voxel-based FE method relative to experimental data from 20 femurs with and without artificial metastases that had been mechanically loaded until failure. CT scans of the femurs were segmented and automatically converted into a voxel-based mesh with hexahedral elements. Nonlinear material properties were implemented in an open-source linear voxel-based FE solver by adding an additional loop to the routine such that the material properties could be adapted after each increment. Bone strength, quantified as the maximum force in the force-displacement curve, was evaluated. The results were compared to a previously established nonlinear tetrahedral FE approach as well as to the experimentally measured bone strength. The voxel-based FE model predicted the experimental bone strength very well both for healthy (R2 = 0.90, RMSE = 0.88 kN) and metastatic femurs (R2 = 0.93, RMSE = 0.64 kN). The model precision and accuracy were very similar to the ones obtained with the tetrahedral model (R2 = 0.90/0.93, RMSE = 0.90/0.64 kN for intact/metastatic respectively). The more intuitive voxel-based meshes thus quantified macroscale femoral strength equally well as state-of-the-art tetrahedral models. The robustness, high level of automation and time-efficiency (< 30 min) of the implemented workflow offer great potential for developing FE models to improve fracture risk prediction in clinical practice."
Wall2020,Johan Wall and Marco Bertoni and Tobias Larsson,The model-driven decision arena: Augmented decision-making for product-service systems design,Systems,8,2,2020,10.3390/systems8020022,20798954,"The shift towards Product-Service Systems (PSS) stresses the need to embed new and unique capabilities in Decision Support Systems, with the aim of helping the engineering team in handling the pool of information and knowledge available during decision events. Emerging from a multiple case study in the Swedish manufacturing industry, this paper describes the development of the Model-Driven Decision Arena (MDDA), an environment for collaborative decision-making that focuses on the early design phases of PSS. Based on the findings from multiple case studies, this paper illustrates the main goals of the MDDA, detailing its main functions, its physical environment, and its software architecture and models. This paper demonstrates the use of the MDDA in a case study related to the development of an asphalt compactor, presenting and discussing the results of verification activities conducted with industrial practitioners on the current MDDA prototype."
Alves2022,Thiago Alves and Rogério Sales Gonçalves and Giuseppe Carbone,Serious Games Strategies With Cable-Driven Robots for Bimanual Rehabilitation: A Randomized Controlled Trial With Post-Stroke Patients,Frontiers in Robotics and AI,9,,2022,10.3389/frobt.2022.739088,22969144,"Cable-driven robots can be an ideal fit for performing post-stroke rehabilitation due to their specific features. For example, they have small and lightweight moving parts and a relatively large workspace. They also allow safe human-robot interactions and can be easily adapted to different patients and training protocols. However, the existing cable-driven robots are mostly unilateral devices that can allow only the rehabilitation of the most affected limb. This leaves unaddressed the rehabilitation of bimanual activities, which are predominant within the common Activities of Daily Living (ADL). Serious games can be integrated with cable-driven robots to further enhance their features by providing an interactive experience and by generating a high level of engagement in patients, while they can turn monotonous and repetitive therapy exercises into entertainment tasks. Additionally, serious game interfaces can collect detailed quantitative treatment information such as exercise time, velocities, and force, which can be very useful to monitor a patient’s progress and adjust the treatment protocols. Given the above-mentioned strong advantages of both cable driven robots, bimanual rehabilitation and serious games, this paper proposes and discusses a combination of them, in particular, for performing bilateral/bimanual rehabilitation tasks. The main design characteristics are analyzed for implementing the design of both the hardware and software components. The hardware design consists of a specifically developed cable-driven robot. The software design consists of a specifically developed serious game for performing bimanual rehabilitation exercises. The developed software also includes BiEval. This specific software allows to quantitatively measure and assess the rehabilitation therapy effects. An experimental validation is reported with 15 healthy subjects and a RCT (Randomized Controlled Trial) has been performed with 10 post-stroke patients at the Physiotherapy’s Clinic of the Federal University of Uberlândia (Minas Gerais, Brazil). The RCT results demonstrate the engineering feasibility and effectiveness of the proposed cable-driven robot in combination with the proposed BiEval software as a valuable tool to augment the conventional physiotherapy protocols and for providing reliable measurements of the patient’s rehabilitation performance and progress. The clinical trial was approved by the Research Ethics Committee of the UFU (Brazil) under the CAAE N° 00914818.5.0000.5152 on plataformabrasil@saude.gov.br."
Broekman2021,André Broekman and Petrus Johannes Gräbe,"A low-cost, mobile real-time kinematic geolocation service for engineering and research applications",HardwareX,10,,2021,10.1016/j.ohx.2021.e00203,24680672,"Centimetre accurate geolocation service is beneficial to a wide range of applications, ranging from sports engineering, civil infrastructure, autonomous vehicles, surveying to digitisation of historically significant structures. Previously, these features were confined to prohibitively expensive commercial hardware, requiring technical knowledge and experience to operate. Continued technological advancements have seen the miniaturisation of electronics and antennas, coupled with an increase in the number and performance of global navigation satellite systems (GNSS) by various nations and organisations, providing global signal coverage. This paper demonstrates a low-cost, mobile, real-time kinematic (RTK) geolocation service for engineering and research applications, fabricated from components readily available from commercial suppliers. This solution, consisting of a mobile RTK base station and RTK rover, provides centimetre-accuracy performance up to a distance of 15 km away from the base station. Correction data is transmitted over the internet using free and open software solutions. The small footprint of both the RTK base station and RTK rover, provides versatile applications even in remote locations. The performance of the geolocation service is validated using field experiments, comparing measurements against state-of-the-art photogrammetry, light detection and ranging (LiDAR) and digital level measurement technologies. The authors encourage the adoption of the RTK geolocation solution based on the calibrated results."
Vega2022,Hugo Vega and Enzo Sanez and Percy De La Cruz and Santiago Moquillaza and Johny Pretell,Intelligent System to Predict University Students Dropout,International journal of online and biomedical engineering,18,7,2022,10.3991/ijoe.v18i07.30195,26268493,"The objective of this research is to reduce the dropout rate of students in the Faculty of Systems Engineering and Informatics of the Universidad Nacional Mayor de San Marcos (FISI-UNMSM), through the implementation of an intelligent system with a data mining approach and the autonomous learning algorithm (decision trees) that predicts which students are at risk of dropping out. It was developed in Python and the free software Weka. For this, the data of the students who entered the faculty from 2004 to 2014 have been considered. This solution increases the availability and the level of satisfaction of the faculty; in the learning process, an accuracy percentage of 90.34% and precision of 95.91% was obtained, so the data mining model is considered valid. In addition, it was found that the variables that most influenced students in making the decision to abandon their studies are the historical weighted average their grades, the weighted average their grades of the last cycle, and the number of credits of their approved courses"
Honcharenko2021,Tetyana Honcharenko and Oleksandr Terentyev and Oksana Malykhina and Iryna Druzhynina and Ievgenii Gorbatyuk,BIM-Concept for Design of Engineering Networks at the Stage of Urban Planning,"International Journal on Advanced Science, Engineering and Information Technology",11,5,2021,10.18517/ijaseit.11.5.13687,24606952,"This study describes a new approach to creating a Central Project Database based on the BIM-concept for designing engineering networks at the stage of urban planning. This approach is based on systematic access to the process of developing urban planning documentation at the pre-design stage of construction. Comprehensive information support of the planning territory corresponds to modern trends in the transition of urban planning to digital construction. An information model of the project of planning territory for construction at the stage of urban design is proposed. The content of each layer of the planning territory information model is described in detail. The conceptual BIM model of spatial data for solving design problems of engineering networks is presented in the form of three main blocks. Block 1 is data sources that are taken from the operating system and external sources. Block 2 is data storage in which operational and external sources supply spatial data and metadata. Block 3 is consumers of information that generate requests for data to the means of presenting the information. The BIM structure of the Central Project Database for comprehensive information support of the design of engineering networks is presented. As the study results are shown, a summary plan of engineering networks was made using BIM-based design software and a digital 3D model of the consolidated plan of engineering networks, made using AutoCAD Civil 3D. The use of BIM-concept at the stage of urban planning design is a new direction of its development. The implication for further research studies on the development of complex BIM-based design for urban infrastructure."
Eghbali2022,Aryaz Eghbali and Michael Pradel,CrystalBLEU: Precisely and Efficiently Measuring the Similarity of Code,,,,2022,10.1145/3551349.3556903,,"Recent years have brought a surge of work on predicting pieces of source code, e.g., for code completion, code migration, program repair, or translating natural language into code. All this work faces the challenge of evaluating the quality of a prediction w.r.t. some oracle, typically in the form of a reference solution. A common evaluation metric is the BLEU score, an n-gram-based metric originally proposed for evaluating natural language translation, but adopted in software engineering because it can be easily computed on any programming language and enables automated evaluation at scale. However, a key difference between natural and programming languages is that in the latter, completely unrelated pieces of code may have many common n-grams simply because of the syntactic verbosity and coding conventions of programming languages. We observe that these trivially shared n-grams hamper the ability of the metric to distinguish between truly similar code examples and code examples that are merely written in the same language. This paper presents CrystalBLEU, an evaluation metric based on BLEU, that allows for precisely and efficiently measuring the similarity of code. Our metric preserves the desirable properties of BLEU, such as being language-agnostic, able to handle incomplete or partially incorrect code, and efficient, while reducing the noise caused by trivially shared n-grams. We evaluate CrystalBLEU on two datasets from prior work and on a new, labeled dataset of semantically equivalent programs. Our results show that CrystalBLEU can distinguish similar from dissimilar code examples 1.9-4.5 times more effectively, when compared to the original BLEU score and a previously proposed variant of BLEU for code."
Saito2020,Kazuya Saito and Ricardo Pérez-De La Fuente and Kôichi Arimoto and Seong Young ah and Hitoshi Aonuma and Ryuma Niiyama and Zhong You,Earwig fan designing: Biomimetic and evolutionary biology applications,Proceedings of the National Academy of Sciences of the United States of America,117,30,2020,10.1073/pnas.2005769117,10916490,"Technologies to fold structures into compact shapes are required in multiple engineering applications. Earwigs (Dermaptera) fold their fanlike hind wings in a unique, highly sophisticated manner, granting them the most compact wing storage among all insects. The structural and material composition, in-flight reinforcement mechanisms, and bistable property of earwig wings have been previously studied. However, the geometrical rules required to reproduce their complex crease patterns have remained uncertain. Here we show the method to design an earwig-inspired fan by considering the flat foldability in the origami model, as informed by X-ray microcomputed tomography imaging. As our dedicated designing software shows, the earwig fan can be customized into artificial deployable structures of different sizes and configurations for use in architecture, aerospace, mechanical engineering, and daily use items. Moreover, the proposed method is able to reconstruct the wing-folding mechanism of an ancient earwig relative, the 280-million-year-old Protelytron permianum. This allows us to propose evolutionary patterns that explain how extant earwigs acquired their wing-folding mechanism and to project hypothetical, extinct transitional forms. Our findings can be used as the basic design guidelines in biomimetic research for harnessing the excellent engineering properties of earwig wings, and demonstrate how a geometrical designing method can reveal morphofunctional evolutionary constraints and predict plausible biological disparity in deep time."
Xu2021,Bowen Xu and Kee Won Lee and Wenjie Li and Michael J. Yaszemski and Lichun Lu and Yabin Yang and Shanfeng Wang,A comparative study on cylindrical and spherical models in fabrication of bone tissue engineering scaffolds: Finite element simulation and experiments,Materials and Design,211,,2021,10.1016/j.matdes.2021.110150,18734197,"Tissue engineering scaffolds have been used for curing bone defects. Poly(propylene fumarate) (PPF) is promising in bone tissue engineering. The ideal scaffolds should have high porosity and sufficient mechanical properties. In this comparative study, two models with cylindrical and spherical pore structures have been designed in the Abaqus software based on the pore opening size (L) to strut length (D) ratio (L/D). Structural analyses including compression, shear, and torsion simulation were performed using finite element analysis (FEA). Compression experiments on the PPF scaffolds fabricated using projection micro-stereolithography (PμSL) were conducted with digital image correlation (DIC). Fluid simulation was further performed to investigate the fluid permeability of the scaffolds. The porosity and surface area (Sp) to volume (Vt) ratio (Sp/Vt) are found to be generally larger in the spherical pore unit cells than in the cylindrical ones. At the same L/D or porosity, the cylindrical pore unit cells have higher compression/shear modulus with better stress distribution, higher torsional rigidity, and higher hydraulic permeability than the spherical ones. This research provides guidance to the design of bone tissue engineering scaffolds as the bulk properties and fluid permeability of the scaffolds could be adjusted by using different pore structures with varied microstructure parameters."
OToole2020,Saoirse O'Toole and David Bartlett and Andrew Keeling and John McBride and Eduardo Bernabe and Luuk Crins and Bas Loomans,Influence of scanner precision and analysis software in quantifying three-dimensional intraoral changes: Two-factor factorial experimental design,Journal of Medical Internet Research,22,11,2020,10.2196/17150,14388871,"Background: Three-dimensional scans are increasingly used to quantify biological topographical changes and clinical health outcomes. Traditionally, the use of 3D scans has been limited to specialized centers owing to the high cost of the scanning equipment and the necessity for complex analysis software. Technological advances have made cheaper, more accessible methods of data capture and analysis available in the field of dentistry, potentially facilitating a primary care system to quantify disease progression. However, this system has yet to be compared with previous high-precision methods in university hospital settings. Objective: The aim of this study was to compare a dental primary care method of data capture (intraoral scanner) with a precision hospital-based method (laser profilometer) in addition to comparing open source and commercial software available for data analysis. Methods: Longitudinal dental wear data from 30 patients were analyzed using a two-factor factorial experimental design. Bimaxillary intraoral digital scans (TrueDefinition, 3M, UK) and conventional silicone impressions, poured in type-4 dental stone, were made at both baseline and follow-up appointments (mean 36 months, SD 10.9). Stone models were scanned using precision laser profilometry (Taicaan, Southampton, UK). Three-dimensional changes in both forms of digital scans of the first molars (n=76) were quantitatively analyzed using the engineering software Geomagic Control (3D Systems, Germany) and freeware WearCompare (Leeds Digital Dentistry, UK). Volume change (mm3) was the primary measurement outcome. The maximum point loss (μm) and the average profile loss (μm) were also recorded. Data were paired and skewed, and were therefore compared using Wilcoxon signed-rank tests with Bonferroni correction. Results: The median (IQR) volume change for Geomagic using profilometry and using the intraoral scan was -0.37 mm3 (-3.75-2.30) and +0.51 mm3 (-2.17-4.26), respectively (P<.001). Using WearCompare, the median (IQR) volume change for profilometry and intraoral scanning was -1.21 mm3 (-3.48-0.56) and -0.39 mm3 (-3.96-2.76), respectively (P=.04). WearCompare detected significantly greater volume loss than Geomagic regardless of scanner type. No differences were observed between groups with respect to the maximum point loss or average profile loss. Conclusions: As expected, the method of data capture, software used, and measurement metric all significantly influenced the measurement outcome. However, when appropriate analysis was used, the primary care system was able to quantify the degree of change and can be recommended depending on the accuracy needed to diagnose a condition. Lower-resolution scanners may underestimate complex changes when measuring at the micron level."
Qin2021,Rongjun Qin and Armin Gruen,The role of machine intelligence in photogrammetric 3D modeling–an overview and perspectives,International Journal of Digital Earth,14,1,2021,10.1080/17538947.2020.1805037,17538955,"The process of modern photogrammetry converts images and/or LiDAR data into usable 2D/3D/4D products. The photogrammetric industry offers engineering-grade hardware and software components for various applications. While some components of the data processing pipeline work already automatically, there is still substantial manual involvement required in order to obtain reliable and high-quality results. The recent development of machine learning techniques has attracted a great attention in its potential to address complex tasks that traditionally require manual inputs. It is therefore worth revisiting the role and existing efforts of machine learning techniques in the field of photogrammetry, as well as its neighboring field computer vision. This paper provides an overview of the state-of-the-art efforts in machine learning in bringing the automated and ‘intelligent’ component to photogrammetry, computer vision and (to a lesser degree) to remote sensing. We will primarily cover the relevant efforts following a typical 3D photogrammetric processing pipeline: (1) data acquisition (2) geo-referencing/interest point matching (3) Digital Surface Model generation (4) semantic interpretations, followed by conclusions and our insights."
Kallis2022,Rafael Kallis and Oscar Chaparro and Andrea Di Sorbo and Sebastiano Panichella,NLBSE'22 Tool Competition,,,,2022,10.1145/3528588.3528664,,"We report on the organization and results of the first edition of the Tool Competition from the International Workshop on Natural Language-based Software Engineering (NLBSE'22). This year, five teams submitted multiple classification models to automatically classify issue reports as bugs, enhancements, or questions. Most of them are based on BERT (Bidirectional Encoder Representations from Transformers) and were fine-tuned and evaluated on a benchmark dataset of 800k issue reports. The goal of the competition was to improve the classification performance of a baseline model based on fastText. This report provides details of the competition, including its rules, the teams and contestant models, and the ranking of models based on their average classification performance across the issue types."
Singh2021,Pooja Singh and Lalit Kumar Singh,Instrumentation and control systems design for nuclear power plant: An interview study with industry practitioners,Nuclear Engineering and Technology,53,11,2021,10.1016/j.net.2021.05.025,2234358X,"Instrumentation and Control systems (I&C) play a significant role in nuclear power plants (NPP) and other safety critical systems (SCS). We have conducted a rigorous study and discussions with experienced practitioners worldwide the strategy for the development of I&C systems to investigate the several aspects related to their dependability. We discussed with experienced practitioners that work on nuclear domain with the intention of knowing their approach, they use day-to-day for the development of such systems. The aim of this research is to obtain to provide guidance to those building I&C systems of NPP and have implications on state engineering licensure boards, in the determination of legal liability, and in risk assessment for policymakers, corporate governors, and insurance executives."
Schffer2020,Eike Schäffer and Volker Stiehl and Peter K. Schwab and Andreas Mayr and Josef Lierhammer and Jörg Franke,Process-Driven Approach within the Engineering Domain by Combining Business Process Model and Notation (BPMN) with Process Engines,,96,,2020,10.1016/j.procir.2021.01.076,22128271,"Digitization within the framework of Industry 4.0 is considered the biggest and fastest driver of change in history of manufacturing industry. While the size of a company is becoming less essential, the ability to adapt quickly to changing market conditions and new technologies is more important than ever. This trend particularly applies to the companies' software landscapes, where individual sub-processes and services must be orchestrated, seamlessly integrated, and iteratively renewed according to the ever-increasing user requirements. However, inflexible, closed monolithic software applications as well as self-programmed stand-alone tools that are difficult to integrate are still predominant in the engineering domain. A complete reimplementation of existing, proprietary engineering tools and their integration into monolithic applications of large software providers is often not economically feasible, especially for small and medium-sized machinery and plant manufacturers. In this context, the so-called Process-Driven Approach (PDA) offers a sustainable and tool-neutral opportunity for process and tool orchestration, enabling an easy integration of individual software applications by consistent utilization of the separation of concerns principle. The PDA, originating from business informatics, is mainly based on the standardized and machine-executable visual modeling language Business Process Model and Notation (BPMN). Using the semantic enhancements found in version 2.0, BPMN is not just used to model the business processes but also to model and execute the integration processes between different systems. After the PDA has already been successfully applied to large-scale projects in business informatics, it is now being transferred to the engineering domain. As shown in this paper, PDA allows to orchestrate the different processes in engineering and to integrate the underlying software tools, such as e-mail or spreadsheet applications, engineering tools, or custom microservices, using standardized interfaces like REST API. In doing so, engineering processes can be made more transparent, monitored, and optimized by means of appropriate key figures. The concept is validated by a prototypical implementation of a minimum functional PDA architecture for the engineering domain."
Chacn2021,Rolando Chacón,Designing construction 4.0 activities for aec classrooms,Buildings,11,11,2021,10.3390/buildings11110511,20755309,"This article describes the outcomes of the development of the project MATES to STEAM. The project is aimed at integrating Construction 4.0 content to a recently started new degree on Technologies on Civil Engineering. This integration is underpinned by the creation of STEAM-rich activities that can complement such degree. The philosophical design of these activities followed three requirements: (i) the activities should infuse Construction 4.0-related technologies, (ii) the activities should foster motivation among students with a STEAM vision by-design and (iii) the activities should be designed with a hardware-software independent perspective (open-source, accessible, affordable). Cornerstone and capstone projects as well as a set of workshops represent the demonstrators of these activities. All these demonstrators are knitted together in a single path in which an educational attempt to fill the identified Construction 4.0 gaps is proposed. The STEAM perspective provides completeness to the whole development. During the last two years, the project was developed and the design, the development and implementation of several demonstrators were completed. In the years to come, a systematic deployment and analysis of such demonstrators is expected when a full implementation of the new degree of Technologies in Civil Engineering will be addressed."
ThomasVanBinsbergen2020,L. Thomas Van Binsbergen and Mauricio Verano Merino and Pierre Jeanjean and Tijs Van Der Storm and Benoit Combemale and Olivier Barais,A principled approach to REPL interpreters,,,,2020,10.1145/3426428.3426917,,"Read-eval-print-loops (REPLs) allow programmers to test out snippets of code, explore APIs, or even incrementally construct code, and get immediate feedback on their actions. However, even though many languages provide a REPL, the relation between the language as is and what is accepted at the REPL prompt is not always well-defined. Furthermore, implementing a REPL for new languages, such as DSLs, may incur significant language engineering cost. In this paper we survey the domain of REPLs and investigate the (formal) principles underlying REPLs. We identify and define the class of sequential languages, which admit a sound REPL implementation based on a definitional interpreter, and present design guidelines for extending existing language implementations to support REPL-style interfaces (including computational notebooks). The obtained REPLs can then be generically turned into an exploring interpreter, to allow exploration of the user's interaction. The approach is illustrated using three case studies, based on MiniJava, QL (a DSL for questionnaires), and eFLINT (a DSL for normative rules). We expect sequential languages, and the consequent design principles, to be stepping stones towards a better understanding of the essence of REPLs."
Wittmann2022,Bruce J. Wittmann and Kadina E. Johnston and Patrick J. Almhjell and Frances H. Arnold,EvSeq: Cost-Effective Amplicon Sequencing of Every Variant in a Protein Library,ACS Synthetic Biology,11,3,2022,10.1021/acssynbio.1c00592,21615063,"Widespread availability of protein sequence-fitness data would revolutionize both our biochemical understanding of proteins and our ability to engineer them. Unfortunately, even though thousands of protein variants are generated and evaluated for fitness during a typical protein engineering campaign, most are never sequenced, leaving a wealth of potential sequence-fitness information untapped. Primarily, this is because sequencing is unnecessary for many protein engineering strategies; the added cost and effort of sequencing are thus unjustified. It also results from the fact that, even though many lower-cost sequencing strategies have been developed, they often require at least some access to and experience with sequencing or computational resources, both of which can be barriers to access. Here, we present every variant sequencing (evSeq), a method and collection of tools/standardized components for sequencing a variable region within every variant gene produced during a protein engineering campaign at a cost of cents per variant. evSeq was designed to democratize low-cost sequencing for protein engineers and, indeed, anyone interested in engineering biological systems. Execution of its wet-lab component is simple, requires no sequencing experience to perform, relies only on resources and services typically available to biology labs, and slots neatly into existing protein engineering workflows. Analysis of evSeq data is likewise made simple by its accompanying software (found at github.com/fhalab/evSeq, documentation at fhalab.github.io/evSeq), which can be run on a personal laptop and was designed to be accessible to users with no computational experience. Low-cost and easy-to-use, evSeq makes the collection of extensive protein variant sequence-fitness data practical."
Le2022,Thai Le and Noseong Park and Dongwon Lee,SHIELD: Defending Textual Neural Networks against Multiple Black-Box Adversarial Attacks with Stochastic Multi-Expert Patcher,,1,,2022,10.18653/v1/2022.acl-long.459,0736587X,"Even though several methods have proposed to defend textual neural network (NN) models against black-box adversarial attacks, they often defend against a specific text perturbation strategy and/or require re-training the models from scratch. This leads to a lack of generalization in practice and redundant computation. In particular, the state-of-the-art transformer models (e.g., BERT, RoBERTa) require great time and computation resources. By borrowing an idea from software engineering, in order to address these limitations, we propose a novel algorithm, SHIELD, which modifies and re-trains only the last layer of a textual NN, and thus it “patches” and “transforms” the NN into a stochastic weighted ensemble of multi-expert prediction heads. Considering that most of current black-box attacks rely on iterative search mechanisms to optimize their adversarial perturbations, SHIELD confuses the attackers by automatically utilizing different weighted ensembles of predictors depending on the input. In other words, SHIELD breaks a fundamental assumption of the attack, which is a victim NN model remains constant during an attack. By conducting comprehensive experiments, we demonstrate that all of CNN, RNN, BERT, and RoBERTa-based textual NNs, once patched by SHIELD, exhibit a relative enhancement of 15%-70% in accuracy on average against 14 different black-box attacks, outperforming 6 defensive baselines across 3 public datasets. Source code will be published at github.com/lethaiq/shield-defend-adversarial-texts."
Almars2022,Abdulqader M. Almars and Malik Almaliki and Talal H. Noor and Majed M. Alwateer and Elsayed Atlam,HANN: Hybrid Attention Neural Network for Detecting Covid-19 Related Rumors,IEEE Access,10,,2022,10.1109/ACCESS.2022.3146712,21693536,"In the age of social media, the spread of rumors is becoming easier due to the proliferation of communication and information dissemination platforms. Detecting rumors is a major problem with significant consequences for the economy, democracy, and public safety. Deep learning approaches were used to classify rumors and have yielded state-of-the-art results. Nevertheless, the majority of techniques do not attempt to explain why or how decisions are made. This paper introduces a hybrid attention neural network (HANN) to identify rumors from social media. The advantage of HANN is that it will allow the main user to capture the relative and important features between different classes as well as provide an explanation of the model's decisions. Two deep neural networks are included in the proposal: CNNs and Bidirectional Long Short Term Memory (Bi-LSTM) networks with attention modules. In this paper, the model is trained using a benchmark dataset containing 3612 distinct tweets crawled from Twitter including several types of rumors related to COVID-19. Each subset of data has a balanced label distribution with 1480 rumors tweets (46.87%) and 1677 non-rumors tweets (53.12%). The experimental results demonstrate that the new approach (HANN model) performs better results in terms of performance and accuracy (about 0.915%) than many contemporary models (AraBERT, MARBEART, PCNN, LSTM, LSTM-PCNN and Attention LSTM). Moreover, a number of software engineering features such as followers, friends, and registration age are used to enhance the model's accuracy."
Marcher2021,Melissa Høegh Marcher and Ingrid Maria Christensen and Paweł Grabarczyk and Therese Graversen and Claus Brabrand,Computing Educational Activities Involving People Rather Than Things Appeal More to Women (CS1 Appeal Perspective),,,,2021,10.1145/3446871.3469761,,"Prior research on recruitment of women to computing has established that computing tasks involving People rather than Things have been perceived as much more appealing by female high-school students (potentially recruitable as university computing students). This paper changes the focus from prospective to current university students and presents the results of a new experiment that advances and moves beyond earlier research in two crucial respects. First of all, the participants of the experiment are N=152 university students, who already study computing, rather than general high-school students. Second of all, the choice between a People-themed versus an isomorphic Things-themed version of an educational task now pertains to real (in fact, mandatory) assignments that the students had to perform, rather than hypothetical tasks. The change of experimental context, design, and methodology allows us to complement previous findings related to recruitment with suggestions significant for computing educational activities. The overall findings of the new experiment are consistent with that of the previous one. We find that, also at university, there is a visible preference for choosing People themed over Things themed computing tasks amongst women. The results also expose considerable variation between tasks in the effect of gender observed. At the same time, male students, in general, seem to be either indifferent to the themes or to slightly prefer People versions. This suggests that educators should consider favoring People themed assignments over ones involving Things."
BenMahria2021,Bilal Ben Mahria and Ilham Chaker and Azeddine Zahi,A novel approach for learning ontology from relational database: from the construction to the evaluation,Journal of Big Data,8,1,2021,10.1186/s40537-021-00412-2,21961115,"The aim of converting relational database into Ontology is to provide applications that are based on the semantic representation of the data. Whereas, representing the data using ontologies has shown to be a useful mechanism for managing and exchanging data. This is the reason why bridging the gap between relational databases and ontologies has attracted the interest of the ontology community from early years, and it is commonly referred to as the database-to-ontology mapping problem. In this paper, we: (1) propose a new life cycle for ontology learning from RDBs based on the software engineering requirements; (2) describe a new method for building ontology from Relational database based on the predefined life cycle; (3) add three new semantics that can be extracted from RDB; (4) we suggest an evaluation process based on two categories of metrics: (i) conceptual ontology (TBox) evaluation metrics; (ii) factual ontology (ABox) evaluation metrics."
Wang2020,Jiawei Wang and Li Li and Andreas Zeller,"Better Code, Better Sharing: On the Need of Analyzing Jupyter Notebooks",,,,2020,10.1145/3377816.3381724,02705257,"By bringing together code, text, and examples, Jupyter notebooks have become one of the most popular means to produce scientific results in a productive and reproducible way. As many of the notebook authors are experts in their scientific fields, but laymen with respect to software engineering, one may ask questions on the quality of notebooks and their code. In a preliminary study, we experimentally demonstrate that Jupyter notebooks are inundated with poor quality code, e.g., not respecting recommended coding practices, or containing unused variables and deprecated functions. Considering the education nature of Jupyter notebooks, these poor coding practices, as well as the lacks of quality control, might be propagated into the next generation of developers. Hence, we argue that there is a strong need to programmatically analyze Jupyter notebooks, calling on our community to pay more attention to the reliability of Jupyter notebooks. CCS CONCEPTS • Software and its engineering ? Software verification and validation."
Politowski2020,Cristiano Politowski and Fabio Petrillo and Gabriel Cavalheiro Ullmann and Josias De Andrade Werly and Yann Gaël Guéhéneuc,Dataset of Video Game Development Problems,,,,2020,10.1145/3379597.3387486,,"Different from traditional software development, there is little information about the software-engineering process and techniques in video-game development. One popular way to share knowledge among the video-game developers' community is the publishing of postmortems, which are documents summarizing what happened during the video-game development project. However, these documents are written without formal structure and often providing disparate information. Through this paper, we provide developers and researchers with grounded dataset describing software-engineering problems in video-game development extracted from postmortems. We created the dataset using an iterative method through which we manually coded more than 200 postmortems spanning 20 years (1998 to 2018) and extracted 1,035 problems related to software engineering while maintaining traceability links to the postmortems. We grouped the problems in 20 different types. This dataset is useful to understand the problems faced by developers during video-game development, providing researchers and practitioners a starting point to study video-game development in the context of software engineering."
Panichella2020,Sebastiano Panichella and Nik Zaugg,An Empirical Investigation of Relevant Changes and Automation Needs in Modern Code Review,Empirical Software Engineering,25,6,2020,10.1007/s10664-020-09870-3,15737616,"Recent research has shown that available tools for Modern Code Review (MCR) are still far from meeting the current expectations of developers. The objective of this paper is to investigate the approaches and tools that, from a developer’s point of view, are still needed to facilitate MCR activities. To that end, we first empirically elicited a taxonomy of recurrent review change types that characterize MCR. The taxonomy was designed by performing three steps: (i) we generated an initial version of the taxonomy by qualitatively and quantitatively analyzing 211 review changes/commits and 648 review comments of ten open-source projects; then (ii) we integrated into this initial taxonomy, topics, and MCR change types of an existing taxonomy available from the literature; finally, (iii) we surveyed 52 developers to integrate eventually missing change types in the taxonomy. Results of our study highlight that the availability of new emerging development technologies (e.g., Cloud-based technologies) and practices (e.g., Continuous delivery) has pushed developers to perform additional activities during MCR and that additional types of feedback are expected by reviewers. Our participants provided recommendations, specified techniques to employ, and highlighted the data to analyze for building recommender systems able to automate the code review activities composing our taxonomy. We surveyed 14 additional participants (12 developers and 2 researchers), not involved in the previous survey, to qualitatively assess the relevance and completeness of the identified MCR change types as well as assess how critical and feasible to implement are some of the identified techniques to support MCR activities. Thus, with a study involving 21 additional developers, we qualitatively assess the feasibility and usefulness of leveraging natural language feedback (automation considered critical/feasible to implement) in supporting developers during MCR activities. In summary, this study sheds some more light on the approaches and tools that are still needed to facilitate MCR activities, confirming the feasibility and usefulness of using summarization techniques during MCR activities. We believe that the results of our work represent an essential step for meeting the expectations of developers and supporting the vision of full or partial automation in MCR."
Idowu2022,Samuel Idowu and Daniel Strüber and Thorsten Berger,Asset Management in Machine Learning: State-of-research and State-of-practice,ACM Computing Surveys,55,7,2022,10.1145/3543847,15577341,"Machine learning components are essential for today's software systems, causing a need to adapt traditional software engineering practices when developing machine-learning-based systems. This need is pronounced due to many development-related challenges of machine learning components such as asset, experiment, and dependency management. Recently, many asset management tools addressing these challenges have become available. It is essential to understand the support such tools offer to facilitate research and practice on building new management tools with native supports for machine learning and software engineering assets. This article positions machine learning asset management as a discipline that provides improved methods and tools for performing operations on machine learning assets. We present a feature-based survey of 18 state-of-practice and 12 state-of-research tools supporting machine-learning asset management. We overview their features for managing the types of assets used in machine learning experiments. Most state-of-research tools focus on tracking, exploring, and retrieving assets to address development concerns such as reproducibility, while the state-of-practice tools also offer collaboration and workflow-execution-related operations. In addition, assets are primarily tracked intrusively from the source code through APIs and managed via web dashboards or command-line interfaces (CLIs). We identify asynchronous collaboration and asset reusability as directions for new tools and techniques."
Abbas2020,Nadeem Abbas and Jesper Andersson and Danny Weyns,ASPLe: A methodology to develop self-adaptive software systems with systematic reuse,Journal of Systems and Software,167,,2020,10.1016/j.jss.2020.110626,01641212,"More than two decades of research have demonstrated an increasing need for software systems to be self-adaptive. Self-adaptation manages runtime dynamics, which are difficult to predict before deployment. A vast body of knowledge to develop Self-Adaptive Software Systems (SASS) has been established. However, we discovered a lack of process support to develop self-adaptive systems with reuse. The lack of process support may hinder knowledge transfer and quality design. To that end, we propose a domain-engineering based methodology, Autonomic Software Product Lines engineering (ASPLe), which provides step-by-step guidelines for developing families of SASS with systematic reuse. The evaluation results from a case study show positive effects on quality and reuse for self-adaptive systems designed using the ASPLe compared to state-of-the-art engineering practices."
Li2023,Fuyang Li and Wanpeng Lu and Jacky Wai Keung and Xiao Yu and Lina Gong and Juan Li,The impact of feature selection techniques on effort-aware defect prediction: An empirical study,IET Software,17,2,2023,10.1049/sfw2.12099,17518814,"Effort-Aware Defect Prediction (EADP) methods sort software modules based on the defect density and guide the testing team to inspect the modules with high defect density first. Previous studies indicated that some feature selection methods could improve the performance of Classification-Based Defect Prediction (CBDP) models, and the Correlation-based feature subset selection method with the Best First strategy (CorBF) performed the best. However, the practical benefits of feature selection methods on EADP performance are still unknown, and blindly employing the best-performing CorBF method in CBDP to pre-process the defect datasets may not improve the performance of EADP models but possibly result in performance degradation. To assess the impact of the feature selection techniques on EADP, a total of 24 feature selection methods with 10 classifiers embedded in a state-of-the-art EADP model (CBS+) on the 41 PROMISE defect datasets were examined. We employ six evaluation metrics to assess the performance of EADP models comprehensively. The results show that (1) The impact of the feature selection methods varies in classifiers and datasets. (2) The four wrapper-based feature subset selection methods with forwards search, that is, AdaBoost with Forwards Search, Deep Forest with Forwards Search, Random Forest with Forwards Search, and XGBoost with Forwards Search (XGBF) are better than other methods across the studied classifiers and the used datasets. And XGBF with XGBoost as the embedded classifier in CBS+ performs the best on the datasets. (3) The best-performing CorBF method in CBDP does not perform well on the EADP task. (4) The selected features vary with different feature selection methods and different datasets, and the features noc (number of children), ic (inheritance coupling), cbo (coupling between object classes), and cbm (coupling between methods) are frequently selected by the four wrapper-based feature subset selection methods with forwards search. (5) Using AdaBoost, deep forest, random forest, and XGBoost as the base classifiers embedded in CBS+ can achieve the best performance. In summary, we recommend the software testing team should employ XGBF with XGBoost as the embedded classifier in CBS+ to enhance the EADP performance."
Sun2020,Zhe Sun and Chi Hu and Chunlei Li and Linbo Wu,Domain ontology construction and evaluation for the entire process of software testing,IEEE Access,8,,2020,10.1109/ACCESS.2020.3037188,21693536,"As an important part of software engineering, software testing is a knowledge-intensive work. In the process of software testing, inconsistent knowledge expression, diverse knowledge carriers, and a few experienced people have mastered most of the knowledge, which hinders the transfer and sharing of domain knowledge. Ontology is widely used in various stages of software engineering to define the semantic relationship between relevant information and knowledge. To solve the problem of knowledge silo in the process of software testing, this article forms an Entire Process Ontology on Software Testing (EPOST). EPOST covers the concepts and relationships of software testing process information, software test object information, and software defect information. The concepts and terms in the ontology are extracted from ISTQB, SWEBOK, IEEE std.829-2008 standard, and IEEE std.610.12-1990 standard. We adopt a comprehensive ontology construction method based on Dev. 101 method and Methontology method. The developed ontology is successfully evaluated by using validation and verification tests. Ontology verification uses an improved FOCA evaluation method by adding a cohesion metric. The evaluation result infers that EPOST has a high quality of ontology and good domain coverage, and achieves the purpose of ontology construction. Finally, we make a case study on the role of EPOST in software testing process. The results show that ontology-based application in the software testing process can promote the sharing and transmission of domain knowledge, and improve the testing process and testing quality."
VonSolms2020,Sune Von Solms and Lynn A. Futcher,Adaption of a Secure Software Development Methodology for Secure Engineering Design,IEEE Access,8,,2020,10.1109/ACCESS.2020.3007355,21693536,"With the rapid advancement of technologies in the era of Industry 4.0, the inter-connected nature of operations and systems is introducing a rapidly changing landscape of digitized and connected systems. Cybercrime is considered as possibly the greatest threat to connected systems worldwide, and therefore there exists a large drive in engineering to include cybersecurity in the design, development and maintenance of smart cyber-physical systems. Traditionally, the cybersecurity space was considered the responsibility of Information Technology (IT) professionals, where the greater IT infrastructure was required to keep these engineering systems safe. However, through the evolution of engineering and control systems, the IT infrastructure has started to become more integrated with these systems, improving the efficiency of the systems, but also making them more susceptible to cyber-attacks. These changes mean that securing these systems cannot remain the sole responsibility of the IT professionals, as systems must be designed with cybersecurity in mind. Considering that engineers are designing and developing more integrated systems, there exists a knowledge gap in the field of cybersecurity engineering and engineers' understanding of their cybersecurity responsibilities. This study aimed to determine the level of security that is currently considered in standard electrical engineering projects in a typical academic environment. This baseline serves as a motivation to develop a practical approach to assist engineering students in considering cybersecurity when developing engineering systems and products."
Mayor2021,Jesus Mayor and Daniel López-Fernández,Scrum vr: Virtual reality serious video game to learn scrum,Applied Sciences (Switzerland),11,19,2021,10.3390/app11199015,20763417,"Education is crucial for the growth of society, and the usage of effective learning methods is key to transmit knowledge to young students. Some initiatives present Virtual Reality technologies as a promising medium to provide active, effective, and innovative teaching. In turn, the use of this technology seems to be very attractive to students, making it possible to acquire knowledge through it. On the other hand, agile methodologies have taken an essential role within information technologies and they are key in Software Engineering education. This paper combines both areas and presents prior research about Virtual Reality experiences with educational purposes and introduces a serious VR video game that aims to promote the learning of agile methodologies in Software Engineering education, specifically the Scrum methodology. This application tries to bring students closer to their first days of work within a software development team that uses the Scrum methodology. Two evaluation processes performed with university teachers and students indicate that the developed video game meets the proposed objectives and looks promising."
Shahzad2021,Basit Shahzad and Iqra Javed and Asadullah Shaikh and Adel Sulaiman and Ahsanullah Abro and Muhammad Ali Memon,Reliable requirements engineering practices for covid-19 using blockchain,Sustainability (Switzerland),13,12,2021,10.3390/su13126748,20711050,"Improvement in the requirements for engineering practices is needed in areas such as requirement elicitation, validation, prioritization, and negotiations between stakeholders to create successful projects for COVID-19 (coronavirus disease 2019) software. Many algorithms and techniques are used to create quality software projects, but they still need more improvement to work effectively for global pandemic COVID-19 software. By improving the reliability of requirement engineering practices using blockchain-based technology, the software will be reliable and will make it easier for the users working in a lockdown situation because of COVID-19. Therefore, our purpose is to identify the factors for reliable software engineering practices using blockchain-oriented technology for COVID-19 software. A systematic literature review is conducted to identify challenges and offer solutions. Through using blockchain-based technology for requirement engineering practices, the requirements will be gathered accurately and validated, and the conflicts between stakeholders will also be solved. It will improve the quality and reliability of COVID-19 software projects, which will help society work effectively from home. Improvement in the quality and reliability of COVID-19 software will improve users’ interest, and their working capacity will be increased."
Shrikanth2020,N. C. Shrikanth and Tim Menzies,Assessing practitioner beliefs about software defect prediction,,,,2020,10.1145/3377813.3381367,02705257,"Just because software developers say they believe in ""X"", that does not necessarily mean that ""X"" is true. As shown here, there exist numerous beliefs listed in the recent Software Engineering literature which are only supported by small portions of the available data. Hence we ask what is the source of this disconnect between beliefs and evidence?. To answer this question we look for evidence for ten beliefs within 300,000+ changes seen in dozens of open-source projects. Some of those beliefs had strong support across all the projects; specifically, ""A commit that involves more added and removed lines is more bug-prone"" and ""Files with fewer lines contributed by their owners (who contribute most changes) are bug-prone"". Most of the widely-held beliefs studied are only sporadically supported in the data; i.e. large effects can appear in project data and then disappear in subsequent releases. Such sporadic support explains why developers believe things that were relevant to their prior work, but not necessarily their current work. Our conclusion will be that we need to change the nature of the debate with Software Engineering. Specifically, while it is important to report the effects that hold right now, it is also important to report on what effects change over time."
Ganesan2020,Madhubala Ganesan and Ah Lian Kor and Colin Pattinson and Eric Rondeau,Green cloud software engineering for big data processing,Sustainability (Switzerland),12,21,2020,10.3390/su12219255,20711050,"Internet of Things (IoT) coupled with big data analytics is emerging as the core of smart and sustainable systems which bolsters economic, environmental and social sustainability. Cloud-based data centers provide high performance computing power to analyze voluminous IoT data to provide invaluable insights to support decision making. However, multifarious servers in data centers appear to be the black hole of superfluous energy consumption that contributes to 23% of the global carbon dioxide (CO2) emissions in ICT (Information and Communication Technology) industry. IoT-related energy research focuses on low-power sensors and enhanced machine-to-machine communication performance. To date, cloud-based data centers still face energy-related challenges which are detrimental to the environment. Virtual machine (VM) consolidation is a well-known approach to affect energy-efficient cloud infrastructures. Although several research works demonstrate positive results for VM consolidation in simulated environments, there is a gap for investigations on real, physical cloud infrastructure for big data workloads. This research work addresses the gap of conducting real physical cloud infrastructure-based experiments. The primary goal of setting up a real physical cloud infrastructure is for the evaluation of dynamic VM consolidation approaches which include integrated algorithms from existing relevant research. An open source VM consolidation framework, Openstack NEAT is adopted and experiments are conducted on a Multi-node Openstack Cloud with Apache Spark as the big data platform. Open sourced Openstack has been deployed because it enables rapid innovation, and boosts scalability as well as resource utilization. Additionally, this research work investigates the performance based on service level agreement (SLA) metrics and energy usage of compute hosts. Relevant results concerning the best performing combination of algorithms are presented and discussed."
Wiecher2020,Carsten Wiecher and Sergej Japs and Lydia Kaiser and Joel Greenyer and Roman Dumitrescu and Carsten Wolff,Scenarios in the loop: Integrated requirements analysis and automotive system validation,,,,2020,10.1145/3417990.3421264,,"The development of safety-relevant systems in the automotive industry requires the definition of high-quality requirements and tests for the coordination and monitoring of development activities in an agile development environment. In this paper we describe a Scenarios in the Loop (SCIL) approach. SCIL combines (1) natural language requirements specification based on Behavior-Driven Development (BDD) with (2) formal and test-driven requirements modeling and analysis, and (3) integrates discipline-specific tools for software and system validation during development. A central element of SCIL is a flexible and executable scenario-based modeling language, the Scenario Modeling Language for Kotlin (SMLK). SMLK allows for an intuitive requirements formalization, and supports engineers to move iteratively, and continuously aided by automated checks, from stakeholder requirements to the validation of the implemented system. We evaluated the approach using a real example from the field of e-mobility."
Carpegna2020,Giorgia Carpegna and Mario Alovisi and Davide Salvatore Paolino and Andrea Marchetti and Umberto Gibello and Nicola Scotti and Damiano Pasqualini and Alessandro Scattina and Giorgio Chiandussi and Elio Berutti,Evaluation of pressure distribution against root canal walls of NiTi rotary instruments by finite element analysis,Applied Sciences (Switzerland),10,8,2020,10.3390/APP10082981,20763417,"The aim of this study was to evaluate the contact pressure distribution of two different nickel-titanium (NiTi) endodontic rotary instruments against the root canal walls and to virtually predict their centering ability during shaping with finite element analysis (FEA). Resin blocks simulating root canals were used. One was shaped with ProGlider and ProTaper Next (PTN) X1-X2 and one with ScoutRace and BioRace (BR) 1, 2 and 3. Both resin blocks were virtually replicated with computer-aided design (CAD) software. The endodontic instruments ProTaper Next (PTN) X2 and BioRace BR3 were also replicated with CAD. The NiTi instruments and the shaped blocks geometries were discretized and exported for FEA. The instrument rotation in the root canals was simulated. The finite element simulation was performed by applying an insertion and extraction force of 2.5 N with a constant rotational speed (300 rpm). To highlight possible differences between pressure distributions against the root canal portions outside and inside the canal curvature, the parameter Var was originally defined. Var values were systematically lower for PTN X2, revealing a better centering ability. FEA proved effective for the virtual prediction of the centering ability of NiTi instruments during an early design phase without the use of prototypes."
Ma2021,Junbiao Ma and Ning Jiang and Xujun Wang and Xiaodong Jia and Dehao Yao,Numerical study of the strength and characteristics of sandstone samples with combined double hole and double fissure defects,Sustainability (Switzerland),13,13,2021,10.3390/su13137090,20711050,"To explore the failure mechanism of rock with holes and fissures, uniaxial compression tests of sandstone samples with combined double hole and double fissure defects were carried out using Particle Flow Code 2D (PFC2D) numerical simulation software. The failure behaviour and mechanical properties of the sandstone samples with combined double hole and double fissure defects at different angles were analysed, and the evolution results of the stress field and crack propagation were studied. The results show that with a decrease in fissure angle, the crack initiation stress, damage stress, elastic modulus and peak stress of the defective rock decrease, while the peak strain increases, and the brittleness of the rock is weakened. Rocks with combined double hole and double fissure defects at different angles lead to different failure modes, crack initiation positions and crack development directions. After uniaxial compression, both compressive stress and tensile stress concentration areas are produced in the defective rock, but the compressive stress concentration is of primary importance. The concentration area is mainly distributed around the holes and fissures and the defect connecting line, and the stress concentration area decreases with the decreasing fissure angle. This study can correctly predict the mechanical properties of rock with combined double hole and double fissure defects at different angles and provide a reference for actual rock engineering."
Chirumamilla2021,Aparna Chirumamilla and Guttorm Sindre,E-exams in Norwegian higher education: Vendors and managers views on requirements in a digital ecosystem perspective,Computers and Education,172,,2021,10.1016/j.compedu.2021.104263,03601315,"E-assessment has been supported in Learning Management Systems for decades. More recently, dedicated e-exam systems have emerged on the market, more specifically supporting the workflow and security needs surrounding high stakes exams. For instance, in Norway, LMS's Canvas and Blackboard are only used for ungraded assessment tasks, while e-exam systems like WISEflow and Inspera Assessment are used for graded ones. Since the systems are mass-market software, vendors must satisfy the needs of several customers, and needs that are specific to only one or a few customers will receive low priority, perhaps forcing teachers to adapt their assessments to what the tool supports, rather than having the tool adapt to the preferred pedagogy. So far, there has been considerable research on views of students and teachers on e-exam systems, much less on the views of vendors and managers. In this paper, we investigate what these stakeholder groups consider to be the key features of e-exam systems, and by what process they are determined. An exploratory case study was conducted, based on interviews with 12 participants belonging to three different groups: vendors, process manager and system managers in Norwegian universities. Our findings indicate much agreement among these groups about key features of e-exam systems, though observing that not all functionality requested by end-users will be prioritized. Also, there was much agreement that a movement towards standardization, open interfaces and digital ecosystems would allow smoother integration with other information systems in the higher education sector, and easier addition of plug-ins for specific functionality – but that there still is a way to go to reach the ambitions of a flexible ecosystem. Currently, vendors give more priority for adding functional features in e-exam systems rather than better interoperability, and integration with third-party tools remains a challenge."
Chaterji2021,Somali Chaterji and Nathan Delay and John Evans and Nathan Mosier and Bernard Engel and Dennis Buckmaster and Michael R. Ladisch and Ranveer Chandra,"Lattice: A Vision for Machine Learning, Data Engineering, and Policy Considerations for Digital Agriculture at Scale",IEEE Open Journal of the Computer Society,2,,2021,10.1109/OJCS.2021.3085846,26441268,"Digital agriculture, with the incorporation of Internet-of-Things (IoT)-based technologies, presents the ability to control a system at multiple levels (individual, local, regional, and global) and generates tools that allow for improved decision making and higher productivity. Recent advances in IoT hardware, e.g., networks of heterogeneous embedded devices, and software, e.g., lightweight computer vision algorithms and cloud optimization solutions, make it possible to efficiently process data from diverse sources in a connected (smart) farm. By interconnecting these IoT devices, often across large geographical distances, it is possible to collect data at different time scales, including in near real-time (i.e., with delays of only a few tens of seconds). This data can then be used for actionable insights, e.g., precise applications of soil supplements and reduced environmental footprint. Through LATTICE, we present an integrated vision for IoT solutions, data processing, and actionable analytics for digital agriculture. We couple this with discussion of economics and policy considerations that will underlie adoption of such IoT and ML technologies. Our paper starts off with the types of datasets in typical field operations, followed by the lifecycle for the data and storage, cloud and edge analytics, and fast information-retrieval solutions. We discuss what algorithms are proving to be most impactful in this space, e.g., approximate data analytics and on-device/in-network processing. We conclude by discussing analytics for alternative agriculture for generation of biofuels and policy challenges in the implementation of digital agriculture in the wild."
Parvin2020,Shahanaz Parvin and Siti Suzilliana Putri Mohamed Isa and Norihan Md Arifin and Fadzilah Md Ali,"The magnetohydrodynamics casson fluid flow, heat and mass transfer due to the presence of assisting flow and buoyancy ratio parameters",CFD Letters,12,8,2020,10.37934/cfdl.12.8.6475,21801363,"The assisting boundary layer flow, heat and mass transfer have wide applications in engineering devices and in nature: for example, nuclear reactors, heat exchangers, solar receivers, atmospheric flow and lake circulation. Therefore, the numerical study of boundary layer flow, heat and mass transfer on Newtonian or non-Newtonian fluid has to be developed, as a reference to experimental works. Therefore, the mathematical modelling and numerical solutions of boundary layer flow, heat and mass transfer on magneto-hydrodynamics Casson fluid are reported in this paper. The model problem is subjected to the presence of mixed convection with assisting flow, together with the buoyant feature. The Casson fluid is assumed to flow over an exponentially stretching sheet, together with the exponential variations of fluid temperature and fluid concentration. The momentum, energy and concentration equations are formed as the controlling equations and written as partial differential equations (PDE). Subsequently, these equations were transformed into the ordinary differential equations (ODE) by using the similarity transformation. Finally, the ODE are solved numerically by bvp4c program in MATLAB software. The graphs of velocity, temperature and concentration profiles and the numerical values of skin friction coefficient, local Nusselt number and local Sherwood number are presented. These results are obtained due to the controlling parameter, namely as magnetic field, assisting flow and buoyancy ratio parameters. As a result, the increment and decrement of the velocity, temperature, concentration, skin friction coefficient, local Nusselt number and local Sherwood number are influenced by magnetic field, assisting flow and buoyancy ratio parameters."
Abichandani2022,Pramod Abichandani and Vidhusheidhaa Sivakumar and Deepan Lobo and Craig Iaboni and Prateek Shekhar,"Internet-of-Things Curriculum, Pedagogy, and Assessment for STEM Education: A Review of Literature",IEEE Access,10,,2022,10.1109/ACCESS.2022.3164709,21693536,"The exponentially growing Internet of Things (IoT) market presents compelling opportunities for innovators to develop and create meaningful applications affecting everyday life. This shifting IoT paradigm is placing new demands on educational institutions to train students with IoT-relevant skills. Numerous education researchers continue to document the implementation of IoT-based curriculum, pedagogy, and assessment techniques for STEM education. In this paper, we systematically reviewed this literature and identified 60 journal articles and conference papers that have reported implementing IoT curriculum and associated instructional approaches, educational technologies, and assessment strategies for K-12 and university students. The curriculum, pedagogy, and assessment strategies presented in these studies were analyzed in the context of the sensing, networking, services, and interface layers of the IoT technology paradigm. The review identifies best educational practices and synthesizes actionable strategies for educators to implement effective IoT learning experiences for students. These strategies leverage low-cost IoT hardware, open-source IoT software, active learning-based instructional approaches, and direct/indirect assessment methods. As IoT education becomes increasingly pervasive, the review and strategies provided in this paper may serve as a guide for future educational efforts."
Silva2020,Ramon Silva and Welington V. Silva and Jonas Yamashita de Farias and Marcos Aires A. Santos and Leonardo O. Neiva,Experimental and numerical analyses of the failure of prestressed concrete railway sleepers,Materials,13,7,2020,10.3390/ma13071704,19961944,"This paper carries out the assessment of load-carrying capacity of prestressed concrete sleepers, in accordance with Brazilian Standard (ABNT NBR 11709) and AREMA Standard. In a lot of railways around the world, many prestressed concrete sleepers have failed due to Rail Seat Abrasion (RSA) and corrosion. RSA is the wear degradation underneath the rail on the surface of prestressed concrete sleepers. In this paper, a numerical study was carried out to evaluate the load-carrying capacity of the prestressed concrete sleepers, using ABAQUS software. The nonlinear using Concrete Damage Plasticity model was validated by 18 experimental results, in accordance to standards. Using the validated model, the influence of different wear depth RSA, combined with corrosion of the prestressed wires, is investigated."
Al-Zubaidi2021,Abdou Al-Zubaidi and Mubbashar Nazeer and Khadija Khalid and Sidra Yaseen and Salman Saleem and Farooq Hussain,"Thermal analysis of blood flow of Newtonian, pseudo-plastic, and dilatant fluids through an inclined wavy channel due to metachronal wave of cilia",Advances in Mechanical Engineering,13,9,2021,10.1177/16878140211049060,16878140,"This paper is organized to study the heat and mass transfer analyses by considering the motion of cilia for Newtonian, Pseudo-plastic, and Dilatant fluids through a horizontally inclined channel in the presence of metachronal waves and variable liquid properties. A non-Newtonian Rabinowitsch model is used to study the flow of peristalsis through ciliated walls. The slip and convective boundary conditions at the channel walls are taken into account. The mathematical model is developed in the form of complex nonlinear partial differential equations then transformed into simplified form by using the definition of low-Reynolds number with lubrication theory. The analytical solution is obtained by using the perturbation method due to its low computational cost and good accuracy. The graphical outcome is based on the behavior of certain physical parameters on velocity, temperature, and concentration profiles for all three types of fluid. A symbolic software named MATHEMATICA 12.0 is used to find the analytical expression and construct the graphical behavior of all profiles that are taken under discussion. The important results in this study depict that the velocity profile tends to increase in the central region of the channel for Newtonian and Pseudo-plastic fluids and decreases for Dilatant fluid while a reverse behavior is observed near the channel walls. A smaller wavelength causes the wavenumber to accelerate and it tends to decelerate for a larger wavelength. The current study will help to understand the use of the complex rheological behavior of biological fluids in engineering and medical science."
Nuruzzaman2021,Md Nuruzzaman and Dipankar Kumar and Gour Chandra Paul,Fractional low-pass electrical transmission line model: Dynamic behaviors of exact solutions with the impact of fractionality and free parameters,Results in Physics,27,,2021,10.1016/j.rinp.2021.104457,22113797,"In this study, we focus on extracting soliton solutions to the differential equation of fractional order governing wave propagation in low-pass electrical transmission lines by the (G'/G2)-expansion method. By a simple linear fractional transformation, we convert the model equation to an ordinary differential equation. Thereupon, through the use of the (G'/G2)-expansion method, various types of solitary wave solutions to the equation of interest are achieved. All the received solutions are verified using Maple software. To show the dynamical behavior of some of the acquired solutions representing singular-periodic, singular, anti-kink, periodic, and bright soliton solutions, their two and three-dimensional profiles are displayed by selecting suitable values of the solutions’ free parameters. Impacts of the fractionality and free parameters on the dynamical behavior of the attained soliton solutions are presented graphically and discussed elaborately. A comparison of our explored solutions with those obtained in the literature is also made. The comparison, wherever available, shows no noticeable difference between our attained results and the published ones. The results that came out through this investigation expose the productivity and powerfulness of the adoptive method for excerpting a diversity of wave solutions to nonlinear evolution equations arising in the fields of mathematics, engineering, and physics."
Pan2021,Hong Gang Pan and Yun Shi Wu and Jian Nan Zhou and Yan Ming Fu and Xin Liang and Tian Yu Zhao,Free vibration analysis of a graphene-reinforced porous composite plate with different boundary conditions,Materials,14,14,2021,10.3390/ma14143879,19961944,"Plates are commonly used in many engineering disciplines, including aerospace. With the continuous improvement in the capacity of high value-added airplanes, large transport aircrafts, and fighter planes that have high strength, high toughness, and corrosion resistance have gradually become the development direction of airplane plate structure production and research. The strength and stability of metal plate structures can be improved by adding reinforced materials. This paper studies graphene platelets (GPLs) reinforced with a free vibration porous composite plate. The porous plate is constructed with a multi-layer model in a metal matrix containing uniform or non-uniformly distributed open-cell internal pores. Considering the random and directional arrangement of graphene platelets in the matrix, the elastic modulus of graphene composites was estimated using the Halpin–Tsai micromechanical model, and the vibration frequencies of graphene composite were calculated using the differential quadrature method. The effects of the total number of layers, GPL distribution pattern, porosity coefficient, GPL weight fraction, and boundary conditions on the free vibration frequency of GPLs reinforced porous composite plates are studied, and the accuracy of the conclusions are verified by the finite element software."
Zago2020,Mattia Zago and Stefano Longari and Andrea Tricarico and Michele Carminati and Manuel Gil Pérez and Gregorio Martínez Pérez and Stefano Zanero,ReCAN – Dataset for reverse engineering of Controller Area Networks,Data in Brief,29,,2020,10.1016/j.dib.2020.105149,23523409,"This article details the methodology and the approach used to extract and decode the data obtained from the Controller Area Network (CAN) buses in two personal vehicles and three commercial trucks for a total of 36 million data frames. The dataset is composed of two complementary parts, namely the raw data and the decoded ones. Along with the description of the data, this article also reports both hardware and software requirements to first extract the data from the vehicles and secondly decode the binary data frames to obtain the actual sensors’ data. Finally, to enable analysis reproducibility and future researches, the code snippets that have been described in pseudo-code will be publicly available in a code repository. Motivated enough actors may intercept, interact, and recognize the vehicle data with consumer-grade technology, ultimately refuting, once-again, the security-through-obscurity paradigm used by the automotive manufacturer as a primary defensive countermeasure."
Fendji2022,Jean Louis K.E. Fendji and Diane C.M. Tala and Blaise O. Yenke and Marcellin Atemkeng,Automatic Speech Recognition Using Limited Vocabulary: A Survey,Applied Artificial Intelligence,36,1,2022,10.1080/08839514.2022.2095039,10876545,"Automatic Speech Recognition (ASR) is an active field of research due to its large number of applications and the proliferation of interfaces or computing devices that can support speech processing. However, the bulk of applications are based on well-resourced languages that overshadow under-resourced ones. Yet, ASR represents an undeniable means to promote such languages, especially when designing human-to-human or human-to-machine systems involving illiterate people. An approach to design an ASR system targeting under-resourced languages is to start with a limited vocabulary. ASR using a limited vocabulary is a subset of the speech recognition problem that focuses on the recognition of a small number of words or sentences. This paper aims to provide a comprehensive view of mechanisms behind ASR systems as well as techniques, tools, projects, recent contributions, and possible future directions in ASR using a limited vocabulary. This work consequently provides a way forward when designing an ASR system using limited vocabulary. Although an emphasis is put on limited vocabulary, most of the tools and techniques reported in this survey can be applied to ASR systems in general. AbbreviationsACC: Accuracy; AM: Acoustic Model; ASR: Automatic Speech Recognition; BD-4SK-ASR: Basic Dataset for Sorani Kurdish Automatic Speech Recognition; CER: Character Error Rate; CMU: Carnegie Mellon University; CNN: Convolutional Neural Network; CNTK: CogNitive ToolKit; CUED: Cambridge University Engineering Department; DCT:Discrete Cosine Transformation; DL: Deep Learning; DNN: Deep Neural Network; DRL: Deep Reinforcement Learning; DWT: Discrete Wavelet Transform; FFT: Fast Fourier Transformation; GMM: Gaussian Mixture Model; HMM: Hidden Markov Model; HTK: Hidden Markov Model ToolKit; JASPER: Just Another Speech Recognizer; LDA: Linear Discriminant Analysis; LER: Letter Error Rate; LGB: Light Gradient Boosting Machine; LM:Language Model; LPC: Linear Predictive Coding; LVCSR: Large Vocabulary Continuous Speech Recognition; LVQ: Learning Vector Quantization Algorithm; MFCC: Mel-Frequency Cepstrum Coefficient; ML: Machine Learning; PCM:Pulse-Code Modulation; PPVT: Peabody Picture Vocabulary Test; RASTA: RelAtive SpecTral; RLAT: Rapid Language Adaptation Toolkit; S2ST: Speech-to-Speech Translation; SAPI: Speech Application Programming Interface; SDK: Software Development Kit; SVASR:Small Vocabulary Automatic Speech Recognition; WER: Word Error Rate."
Kim2020,Sunjun Kim and Byungjoo Lee and Thomas Van Gemert and Antti Oulasvirta,Optimal Sensor Position for a Computer Mouse,,,,2020,10.1145/3313831.3376735,,"Computer mice have their displacement sensors in various locations (center, front, and rear). However, there has been little research into the effects of sensor position or on engineering approaches to exploit it. This paper first discusses the mechanisms via which sensor position affects mouse movement and reports the results from a study of a pointing task in which the sensor position was systematically varied. Placing the sensor in the center turned out to be the best compromise: improvements over front and rear were in the 11-14% range for throughput and 20 - 23% for path deviation. However, users varied in their personal optima. Accordingly, variable-sensor-position mice are then presented, with a demonstration that high accuracy can be achieved with two static optical sensors. A virtual sensor model is described that allows software-side repositioning of the sensor. Individual-specific calibration should yield an added 4% improvement in throughput over the default center position."
Teo2020,Jonathan J.Y. Teo and Rahul Sarpeshkar,The Merging of Biological and Electronic Circuits,iScience,23,11,2020,10.1016/j.isci.2020.101688,25890042,"Biological circuits and systems within even a single cell need to be represented by large-scale feedback networks of nonlinear, stochastic, stiff, asynchronous, non-modular coupled differential equations governing complex molecular interactions. Thus, rational drug discovery and synthetic biological design is difficult. We suggest that a four-pronged interdisciplinary approach merging biology and electronics can help: (1) The mapping of biological circuits to electronic circuits via quantitatively exact schematics; (2) The use of existing electronic circuit software for hierarchical modeling, design, and analysis with such schematics; (3) The use of cytomorphic electronic hardware for rapid stochastic simulation of circuit schematics and associated parameter discovery to fit measured biological data; (4) The use of bio-electronic reporting circuits rather than bio-optical circuits for measurement. We suggest how these approaches can be combined to automate design, modeling, analysis, simulation, and quantitative fitting of measured data from a synthetic biological operational amplifier circuit in living microbial cells."
Curto2021,Domenico Curto and Vincenzo Franzitta and Andrea Guercio and Domenico Panno,Energy retrofit. A case study—santi romano dormitory on the palermo university,Sustainability (Switzerland),13,24,2021,10.3390/su132413524,20711050,"Electrical and thermal consumption related to buildings, whether civil, commercial, public, or of any other kind, is very much in focus today. With today’s targets for energy savings, reduction of consumption, and environmental impact, it is necessary to carry out energy retrofits to modernize installations and their management. The realization of an effective improvement requires a careful analysis of the case study because each category of building has different requirements such as different load profiles and different installations and needs. This was carried out by studying the electrical and thermal load profiles. A good initial energy audit can provide the retrofit solutions capable of achieving the reduction of energy consumption and the emission of climate-changing gases into the atmosphere. A case study, carried out by the Department of Engineering of Palermo, showed how it is possible to perform an energy retrofit to modernize the energy system of the student dormitory at the University of Palermo. The paper presented a study carried out by programming a series of interlinked calculations in Microsoft Excel. In order to quantify the energy savings of the structure under examination, it is necessary to enter some input data, thanks to which all the formulas implemented in the calculation software were automatically completed. The programming of the calculations makes it possible to carry out an energy retrofit with interventions on the building envelope and the installations. The desire to program an automated calculation by modifying only the input data is intended to replicate a study on other buildings with the same peculiarities. In this way, it is possible to verify which retrofit hypotheses would be useful to upgrade old public administration buildings. In the analyzed case study, 65% of electrical energy and 33% of thermal energy could be saved by replacing generation systems, installing a co-generator, replacing windows, and replacing lamps with LED ones."
Abbas2022,Shajar Abbas and Mudassar Nazar and Zaib Un Nisa and Muhammad Amjad and Sayed M.El Din and Agaeb Mahal Alanzi,Heat and Mass Transfer Analysis of MHD Jeffrey Fluid over a Vertical Plate with CPC Fractional Derivative,Symmetry,14,12,2022,10.3390/sym14122491,20738994,"Free convection flow of non-Newtonian fluids over flat, heated surfaces is an important natural phenomenon that also occurs in human-made engineering processes under various physical and mechanical situations. In the current study, the free convection magnetohydrodynamic flow of Jeffrey fluid with heat and mass transfer over an infinite vertical plate is examined. Mathematical modeling is performed using Fourier’s and Fick’s laws, and heat and momentum equations have been obtained. The non-dimensional partial differential equations for energy, mass, and velocity fields are determined using the Laplace transform method in a symmetric manner. Later on, the Laplace transform method is employed to evaluate the results for the temperature, concentration, and velocity fields with the support of Mathcad software. The governing equations, as well as the initial and boundary conditions, satisfy these results. The impacts of fractional and physical characteristics have been shown by graphical illustrations. The obtained fractionalized results are generalized by a more decaying nature. By taking the fractional parameter (Formula presented.), the classical results with the ordinary derivatives are also recovered, making this a good direction for symmetry analysis. The present work also has applications with engineering relevance, such as heating and cooling processes in nuclear reactors, the petrochemical sector, and hydraulic apparatus where the heat transfers through a flat surface. Moreover, the magnetized fluid is also applicable for controlling flow velocity fluctuations."
Leppla2021,Lynn Leppla and Anja Schmid and Sabine Valenta and Juliane Mielke and Sonja Beckmann and Janette Ribaut and Alexandra Teynor and Fabienne Dobbels and Nathalie Duerinckx and Robert Zeiser and Monika Engelhardt and Sabine Gerull and Sabina De Geest and Dora Bolliger and Yves Chalandon and Sabina De DGeest and Sabine Degen and Margerita Fürmann and Florian Grossmann and Monika Hasemann and Philipp Heidegger and Anja Hermann and Sandra Hobelsberger and Mylen Husel and Katharina Koehly and Marina Lemcke and Birgit Maier and Anne Claire Mamez and Stavoula Masouridi and Gyathri Nair and Daniela Neupert and Jakob Passweg and Stefan Pschenitza and Sigrun Reitwiessner and Dennis Rockstein and Urs Schanz and Helen Schoemans and Tobias Schulz and Vanessa Schumacher and Yulia Senft and Viktor Werlitz and Verena Witzig-Brändli,Development of an integrated model of care for allogeneic stem cell transplantation facilitated by eHealth—the SMILe study,Supportive Care in Cancer,29,12,2021,10.1007/s00520-021-06328-0,14337339,"Purpose: Allogeneic stem cell transplantation would benefit from re-engineering care towards an integrated eHealth-facilitated care model. With this paper we aim to: (1) describe the development of an integrated care model (ICM) in allogeneic SteM-cell-transplantatIon faciLitated by eHealth (SMILe) by combining implementation, behavioral, and computer science methods (e.g., contextual analysis, Behavior Change Wheel, and user-centered design combined with agile software development); and (2) describe that model’s characteristics and its application in clinical practice. Methods: The SMILe intervention’s development consisted of four steps, with implementation science methods informing each: (1) planning its set-up within a theoretical foundation; (2) using behavioral science methods to develop the content; (3) choosing and developing its delivery method (human/technology) using behavioral and computer science methods; and (4) describing its characteristics and application in clinical practice. Results: The SMILe intervention is embedded within the eHealth enhanced Chronic Care Model, entailing four self-management intervention modules, targeting monitoring and follow-up of important medical and symptom-related parameters, infection prevention, medication adherence, and physical activity. Interventions are delivered partly face-to-face by a care coordinator embedded within the transplant team, and partly via the SMILeApp that connects patients to the transplant team, who can monitor and rapidly respond to any relevant changes within 1 year post-transplant. Conclusion: This paper provides stepwise guidance on how implementation, behavioral, and computer science methods can be used to develop interventions aiming to improve care for stem cell transplant patients in real-world clinical settings. This new care model is currently being tested in a hybrid I effectiveness-implementation trial."
Chernov2020,Vladimir Anatolyevich Chernov,Implementation of digital technologies in financial management,Economy of Regions,16,1,2020,10.17059/2020-1-21,24111406,"The transition to digital economy based on new technologies becomes one of the main tools of innovative technological development in Russia. This research examines the means for implementing digital technologies that effectively enable rapid innovative development of the Russian economy. I hypothesise it is necessary to develop a methodological system of techniques and algorithms of artificial intelligence filling neural networks of digital technologies for managing finance in the conditions of uncertainty and risk. A wide variety of methods was used in the study, including dialectic and integrated approaches, fundamental economic laws, deductive and inductive methods, statistical comparisons, descriptive methods of analysis. Moreover, I apply such methodologies as the theory of constraints (TOC), methods of lean manufacturing, Kanban-methods as a tool of IT-management, scrum methodology for software development, and other. The statistical and empirical data confirm the validity of the conclusions and results. The paper considers how to create capital in the form of innovative products and technologies in the digital environment. As a result, the study has revealed relations between digital technologies and engineering account, economic analysis, material production, highly skilled creative work, environmental protection, and sustainable development. The article explains how to include automated information and analytical functions for the compensation and generation of missing information into digital technologies. The transition from the database to the knowledge base, which fills neural networks with the artificial intelligence, provides the generation of management decisions and sustainable innovative development in the conditions of uncertainty and risk. The paper recommends currencies for digital payments. I conclude that applying the suggested recommendations is necessary for providing digital technologies with the algorithms of extraction and implementation of knowledge from the data asset, as well as for compensating for the missing information, generating information flows, and making decisions in the conditions of uncertainty and risk. The research can be of use for PhD students, doctoral students, scholars and practitioners in the field of economy."
Hong2021,Li Hong and Rui Sun and Zhongchao Qiu and Zhiming Han and Yanan Li,A multi-cantilever beam low-frequency FBG acceleration sensor,Scientific Reports,11,1,2021,10.1038/s41598-021-98055-z,20452322,"The acquisition of 2–50 Hz low-frequency vibration signals is of great significance for the monitoring researches on engineering seismology, bridges & dams, oil & gas exploration, etc. A multi-cantilever beam low-frequency FBG acceleration sensor is proposed against the low sensitivity that predominates in the low-frequency vibration measurement by FBG acceleration sensors. Structural parameters of the sensor is subjected to simulation analysis and optimization design using the ANSYS software; the real sensor is developed based on the simulation results in the following manner: Three rectangular of the cantilever beams are evenly arranged around the mass block at 120°to improve the sensitivity and alleviate the transverse crosstalk of sensor; in the end, a performance test is performed on the sensor. According to the research findings, the sensor, whose natural frequency is approximately 64 Hz, is applicable for monitoring the low-frequency vibration signals within the range 16–54 Hz. The sensor sensitivity is approximately 87.955pm/ms-2, the linearity being greater than 99%, the transverse interference immunity being lower than 2.58%, and the dynamic range being up to 86 dB. The findings offer a reference for developing sensor of the same type and further improving the sensitivity of fiber optic acceleration sensor."
Nedi2020,Teodora M. Nedić and Aco J. Janićijević and Koviljka Dj Stanković and Nenad M. Kartalović,Efficient replacement of the radioactive sources in the gas-filled surge arresters construction for the insulation co-ordination at the low voltage level,Nuclear Technology and Radiation Protection,35,2,2020,10.2298/NTRP2002130N,14513994,"The possibility of the classical response rate engineering and the long-term deconditioning of the gas-filled surge arresters is considered in this paper. The obtained results are compared with the same results obtained with the gas-filled surge arrester with application of an alpha radioactive source. The work is of an experimental nature. Experiments were performed on the multipurpose gas-filled surge arrester model under well-controlled conditions. The measurement uncertainty of the experimental procedure was less than 8 %. The results were pro-cessed by statistical data processing software. To draw impulse characteristics, the law of con-stancy of surfaces below the impulse voltage in the volt-second area, was used."
Shavarskyi2022,Iaroslav Shavarskyi and Volodymyr Falshtynskyi and Roman Dychkovskyi and Oleksandr Akimov and Dariusz Sala and Valentyn Buketov,Management of the longwall face advance on the stress-strain state of rock mass,Mining of Mineral Deposits,16,3,2022,10.33271/mining16.03.078,24153443,"Purpose is to study influence of a longwall face advance on the geomechanical situation in the neighbourhood of a mining site based upon determination of changes in standard and critical subsidence of the immediate roof rocks. Methods. To study a geomechanical situation in the neighbourhood of a mining site the authors have applied software product GeoDenamics Lite developed at Dnipro University of Technology. The software product relies upon a calculation procedure of stress-strain state of rocks by Professor O.V. Savostianov. Expediency of the software selection is based upon the supported control and adaptation of a coal mining technique to changes in geodynamic stress fields in the anisotropic rock-coal medium impacting temporal and spatial changes in the technological parameters. Findings. The basic problems have been singled out connected with certain changes in a longwall face advance. For the first time, an analytical scheme of tangential stresses within the immediate roof rocks has been developed for Lisova mine of SE Lvivvuhillia under the conditions of coal seam mining by means of the paired longwalls which makes it possible to deter-mine both physical and geometrical parameters of standard loads within the formation. Originality. Dependencies of temporal and spatial changes in subsidences and horizontal displacements of rock layers of the immediate roof have been defined being 5.2 m for the upper rock pack and 3.9 m for the lower pack if the longwall longwall face advance is 1.9 up to 4.8 m/day. Both physical and geometrical parameters of the reference pressure have been defined as well as the parameters of lower sandstone pack in the process of the main roof subsidence. Impact of the extra pressure forces on the immediate roof rocks has been analyzed at the moment of critical lowerings of the immediate roof rocks. In this context, standard loading from the overlying formation in addition to tangential stresses in the roof result in rock fai lure due to vertical cracks above a longwall face. Practical implications. The engineering methods have been developed making it possible to identify impact parameters of a longwall face advance on the geomechanical situation in the neighbourhood of a mining site. In future, it will help forecast changes in the reference pressure around a longwall face while preventing emergency settlement of the powered support."
Paolone2020,Gaetanino Paolone and Martina Marinelli and Romolo Paesani and Paolino Di Felice,Automatic code generation of mvc web applications,Computers,9,3,2020,10.3390/computers9030056,2073431X,"As Web applications become more and more complex, the development costs are increasing as well. A Model Driven Architecture (MDA) approach is proposed in this paper since it simplifies modeling, design, implementation, and integration of applications by defining software mainly at the model level. We adopt the The Unified Modeling Language (UML), as modeling language. UML provides a set of diagrams to model structural and behavioral aspects of the Web applications. Automatic translation of UML diagrams to the Object-Oriented code is highly desirable because it eliminates the chances of introducing human errors. Moreover, automatic code generation helps the software designers delivering of the software on time. In our approach, the automatic transformations across the MDA’s levels are based on meta-models for two of the most important constructs of UML, namely Use Cases and classes. A proprietary tool (called xGenerator) performs the transformations up to the Java source code. The architecture of the generated Web applications respects a variant of the well-known Model-View-Controller (MVC) pattern."
Bilal2022,Sardar Bilal and Noor Zeb Khan and Imtiaz Ali Shah and Jan Awrejcewicz and Ali Akgül and Muhammad Bilal Riaz,Numerical Study of Natural Convection of Power Law Fluid in a Square Cavity Fitted with a Uniformly Heated T-Fin,Mathematics,10,3,2022,10.3390/math10030342,22277390,"Flow of a liquid in an enclosure with heat transfer has drawn special focus of researchers due to the abundant thermal engineering applications. So, the aim of present communication is to explore thermal characteristics of natural convective power-law liquid flow in a square enclosure rooted with a T-shaped fin. The formulation of the problem is executed in the form of partial differential expressions by incorporating the rheological relation of the power-law fluid. The lower wall of the enclosure along with the fin is uniformly heated and vertical walls are prescribed with cold temperature. For effective heat transfer within the cavity the upper boundary is considered thermally insulated. A finite element based commercial software known as COMSOL is used for simulations and discretization of differential equations and is executed incorporating a weak formulation. Domain discretization is performed by dividing it into triangular and rectangular elements at different refinement levels. A grid independence test is accomplished for quantities of engineering interest like local and average Nusselt numbers to attain accuracy and validity in results. Variation in the momentum and thermal distributions against pertinent parameters is analyzed through stream lines and isothermal contour plots. Measurement of the heat flux coefficient along with the calculation of kinetic energy against involved parameters is displayed through graphs and tables. After the comprehensive overview of attained results it is deduced that kinetic energy elevates against the upsurging magnitude of the Rayleigh number, whereas contrary behavior is encapsulated versus power-law index (n). Elevation in the Nusselt number for the shear thinning case (i.e., n = 0.5) adheres as compared to Newtonian (i.e., n = 1) and shear thickening cases (i.e., n = 1.5). It is perceived that by the upsurging power-law index viscosity augmentations and circulation zones increases. Heat is transferred quickly against Rayleigh number (Ra) due to production of temperature difference in flow domain."
Fathy2022,Ahmed Fathy and Ahmed Ben Atitallah and Dalia Yousri and Hegazy Rezk and Mujahed Al-Dhaifallah,A new implementation of the MPPT based raspberry Pi embedded board for partially shaded photovoltaic system,Energy Reports,8,,2022,10.1016/j.egyr.2022.04.035,23524847,"The operation of photovoltaic (PV) module under partial shadow conditions considers a big challenge for most researchers due to power loss and hot spots that reduce the amount of extracted power. In such an operation, the panel voltage–power curve has a unique global maximum power (GMP) to be tracked. Therefore, this paper proposes a new maximum power point tracker (MPPT) implemented by Raspberry Pi 4-based embedded board programmed via two metaheuristic approaches of cuckoo search (CS) and particle swarm optimizer (PSO). The approaches are developed using python software programming language to adapt the duty cycle fed to the MOSFET of DC/DC boost converter connected to the panel terminals. The panel is simulated in Simulink/Matlab library to identify the GMP in each studied case. An experimental setup is conducted in the lab room of the college of Engineering, Jouf University, Saudi Arabia to assess the proposed tracker. Moreover, eight shade patterns are considered via covering 10% to 80% with step 10% of panel with shadow. Furthermore, statistical tests of the Wilcoxson sign rank test and ANOVA are conducted to assess the validity of the proposed tracker. The obtained results are compared to perturb and observe (P&O) and gray​ wolf optimizer (GWO). The PSO-based tracker achieved the best efficiency of 96.92%, the CS achieved 93.62%, and GWO get an efficiency of 93.15%. Additionally, on the side of Wilcoxson sign rank and ANOVA tests, the PSO outperformed CS and GWO. The results confirmed the superiority of the proposed Raspberry Pi system programmed via PSO over that of CS and GWO in enhancing the power generated from the panel operated at different partial shades."
Zaharieva2022,Maya Margaritova Zaharieva and Dimitrina Zheleva-Dimitrova and Snezhana Rusinova-Videva and Yana Ilieva and Anna Brachkova and Vessela Balabanova and Reneta Gevrenova and Tanya Chan Kim and Mila Kaleva and Almira Georgieva and Milka Mileva and Krassimira Yoncheva and Niko Benbassat and Hristo Najdenski and Alexander Dimitrov Kroumov,Antimicrobial and Antioxidant Potential of Scenedesmus obliquus Microalgae in the Context of Integral Biorefinery Concept,Molecules,27,2,2022,10.3390/molecules27020519,14203049,"Small-scale photobioreactors (PBRs) in the inoculum stage were designed with internal (red or green) and external white LED light as an initial step of a larger-scale installation aimed at fulfilling the integral biorefinery concept for maximum utilization of microalgal biomass in a multifunctional laboratory. The specific growth rate of Scenedesmus obliquus (Turpin) Kützing biomass for given cultural conditions was analyzed by using MAPLE software. For the determination of total polyphenols, flavonoids, chlorophyll “a” and “b”, carotenoids and lipids, UHPLC-HRMS, ISO-20776/1, ISO-10993-5 and CUPRAC tests were carried out. Under red light growing, a higher content of polyphenols was found, while the green light favoured the flavonoid accumulation in the biomass. Chlorophylls, carotenoids and lipids were in the same order of magnitude in both samples. The dichloromethane extracts obtained from the biomass of each PBR synergistically potentiated at low concentrations (0.01–0.05 mg/mL) the antibacterial activity of penicillin, fluoroquinolones or oregano essential oil against the selected food-borne pathogens (Staphylococcus aureus, Escherichia coli and Salmonella typhimurium) without showing any in vitro cytotoxicity. Both extracts exhibited good cupric ion-reducing antioxidant capacity at concentrations above 0.042–0.08 mg/mL. The UHPLC-HRMS analysis revealed that both extracts contained long chain fatty acids and carotenoids thus explaining their antibacterial and antioxidant potential. The applied engineering approach showed a great potential to modify microalgae metabolism for the synthesis of target compounds by S. obliquus with capacity for the development of health-promoting nutraceuticals for poultry farming."
Sonawane2022,Chandrakant R. Sonawane and Hitesh N. Panchal and Siamak Hoseinzadeh and Mohammad Hadi Ghasemi and Ali Jawad Alrubaie and Ali Sohani,Bibliometric Analysis of Solar Desalination Systems Powered by Solar Energy and CFD Modelled,Energies,15,14,2022,10.3390/en15145279,19961073,"Solar desalination is a sustainable approach to producing fresh water from saline water. Researchers have tried modifications to solar desalination systems to enhance the distillate output. This survey aims to recognize the characteristics of research work for investigating solar desalination systems. The essential terms to be focused on are computational fluid dynamics (CFD) and solar radiation consideration for the investigation. The data for this bibliometric study was taken from Scopus, with 1932 publications. The characteristics of the research work were analyzed by identifying the research publications, research subject areas, journals, most contributed countries, and data from the authors. The research trend was investigated utilizing yearly research growth, geographical contributions, source quality of publications, and participation of various institutions. VOSviewer software was used for network analysis of essential keywords used in relevant research works and to understand collaborations between the authors and co-authors. About 76% of the total publications belong to the document type articles. It illuminates research tendencies related to the topic under consideration. Results show that desalination research work concerned with CFD or solar radiation was mainly investigated in Engineering and Environmental Science, with their share of more than 50% of the total publications. Further study of relevant research works was assessed using network analysis that helped to link different keywords and authors’ collaborations. This survey helps to spot the increasing research trends and the necessity of further research."
Preechayasomboon2021,Pornthep Preechayasomboon and Eric Rombokas,Haplets: Finger-Worn Wireless and Low-Encumbrance Vibrotactile Haptic Feedback for Virtual and Augmented Reality,Frontiers in Virtual Reality,2,,2021,10.3389/frvir.2021.738613,26734192,"We introduce Haplets, a wearable, low-encumbrance, finger-worn, wireless haptic device that provides vibrotactile feedback for hand tracking applications in virtual and augmented reality. Haplets are small enough to fit on the back of the fingers and fingernails while leaving the fingertips free for interacting with real-world objects. Through robust physically-simulated hands and low-latency wireless communication, Haplets can render haptic feedback in the form of impacts and textures, and supplements the experience with pseudo-haptic illusions. When used in conjunction with handheld tools, such as a pen, Haplets provide haptic feedback for otherwise passive tools in virtual reality, such as for emulating friction and pressure-sensitivity. We present the design and engineering for the hardware for Haplets, as well as the software framework for haptic rendering. As an example use case, we present a user study in which Haplets are used to improve the line width accuracy of a pressure-sensitive pen in a virtual reality drawing task. We also demonstrate Haplets used during manipulation of objects and during a painting and sculpting scenario in virtual reality. Haplets, at the very least, can be used as a prototyping platform for haptic feedback in virtual reality."
Lu2021,Jonathan Lu and Bianca Dumitrascu and Ian C. McDowell and Brian Jo and Alejandro Barrera and Linda K. Hong and Sarah M. Leichter and Timothy E. Reddy and Barbara E. Engelhardt,Causal network inference from gene transcriptional time-series response to glucocorticoids,PLoS Computational Biology,17,1,2021,10.1371/JOURNAL.PCBI.1008223,15537358,"Gene regulatory network inference is essential to uncover complex relationships among gene pathways and inform downstream experiments, ultimately enabling regulatory network re-engineering. Network inference from transcriptional time-series data requires accurate, interpretable, and efficient determination of causal relationships among thousands of genes. Here, we develop Bootstrap Elastic net regression from Time Series (BETS), a statistical framework based on Granger causality for the recovery of a directed gene network from transcriptional time-series data. BETS uses elastic net regression and stability selection from bootstrapped samples to infer causal relationships among genes. BETS is highly parallelized, enabling efficient analysis of large transcriptional data sets. We show competitive accuracy on a community benchmark, the DREAM4 100-gene network inference challenge, where BETS is one of the fastest among methods of similar performance and additionally infers whether causal effects are activating or inhibitory. We apply BETS to transcriptional time-series data of differentially-expressed genes from A549 cells exposed to glucocorticoids over a period of 12 hours. We identify a network of 2768 genes and 31,945 directed edges (FDR ≤ 0.2). We validate inferred causal network edges using two external data sources: Overexpression experiments on the same glucocorticoid system, and genetic variants associated with inferred edges in primary lung tissue in the Genotype-Tissue Expression (GTEx) v6 project. BETS is available as an open source software package at https://github.com/lujonathanh/BETS. Copyright:"
Botejara-Antnez2022,Manuel Botejara-Antúnez and Jaime González-Domínguez and Justo García-Sanz-Calcedo,Comparative analysis of flat roof systems using life cycle assessment methodology: Application to healthcare buildings,Case Studies in Construction Materials,17,,2022,10.1016/j.cscm.2022.e01212,22145095,"Environmental impact reduction, structural security, and material resource optimization are basic aspects in selecting a construction system. In this study the environmental impact of the 10 predominant flat roof systems in the Spanish infrastructure was evaluated. For this purpose, the systems were subjected to life-cycle assessment, and a single-score damage category analysis was carried out for the midpoint and endpoint stages using the ReCiPe 2016 method, the Ecoinvent 3.5 environmental database, the LCA software SimaPro 9.0 and the “cradle-to-grave” perspective. The cost and installation time per functional unit were also taken into account. Results show that trafficable flat roof systems with fixed deck obtained the highest scores in all dimensions analyzed, with impacts in the range 19.15 pt/m²–22.59 pt/m². Moreover, it was determined that the non-trafficable flat green roof systems have lower cost and labour time values, as well as higher environmental impact values. It was concluded that the non-trafficable roof system with gravel finish and PVC membrane is the optimal solution. This roof typology presents the most favorable results in 20 of 22 impact categories and in the three areas of damage, obtaining the global environmental impact values (7.68 pt/m²), as well as acceptable values in the dimensions of cost (US$66.4/m²) and installation time (1.69 h/m²). Generated knowledge will provide engineering managers with a more detailed perspective of the environmental impact of healthcare infrastructures, increasing the socioeconomic and environmental benefits."
Su2021,Cui Su and Jun Pang,CABEAN: A software for the control of asynchronous Boolean networks,Bioinformatics,37,6,2021,10.1093/bioinformatics/btaa752,14602059,"Summary: Direct cell reprogramming, also called transdifferentiation, has great potential for tissue engineering and regenerative medicine. Boolean networks, a popular modelling framework for gene regulatory networks, make it possible to identify intervention targets for direct cell reprogramming with computational methods. In this work, we present our software, CABEAN, for the control of asynchronous Boolean networks. CABEAN identifies efficacious nodes, whose perturbations can drive the dynamics of a network from a source attractor (the initial cell type) to a target attractor (the desired cell type). CABEAN provides several control methods integrating practical constraints. Thus, it has the ability to provide a rich set of control sets, such that biologists can select suitable ones for validation based on specific experimental settings."
Boubekeur2020,Younes Boubekeur and Gunter Mussbacher and Shane McIntosh,Automatic assessment of students' software models using a simple heuristic and machine learning,,,,2020,10.1145/3417990.3418741,,"Software models are increasingly popular. To educate the next generation of software engineers, it is important that they learn how to model software systems well, so that they can design them effectively in industry. It is also important that instructors have the tools that can help them assess students' models more effectively. In this paper, we investigate how a tool that combines a simple heuristic with machine learning techniques can be used to help assess student submissions in model-driven engineering courses. We apply our proposed technique to first identify submissions of high quality and second to predict approximate letter grades. The results are comparable to human grading and a complex rule-based technique for the former and surprisingly accurate for the latter."
Zhang2022,Shiwen Zhang and Meiling Huang and Jincao Zhi and Shanhong Wu and Yan Wang and Fei Pei,Research Hotspots and Trends of Peripheral Nerve Injuries Based on Web of Science From 2017 to 2021: A Bibliometric Analysis,Frontiers in Neurology,13,,2022,10.3389/fneur.2022.872261,16642295,"Background: Peripheral nerve injury (PNI) is very common in clinical practice, which often reduces the quality of life of patients and imposes a serious medical burden on society. However, to date, there have been no bibliometric analyses of the PNI field from 2017 to 2021. This study aimed to provide a comprehensive overview of the current state of research and frontier trends in the field of PNI research from a bibliometric perspective. Methods: Articles and reviews on PNI from 2017 to 2021 were extracted from the Web of Science database. An online bibliometric platform, CiteSpace, and VOSviewer software were used to generate viewable views and perform co-occurrence analysis, co-citation analysis, and burst analysis. The quantitative indicators such as the number of publications, citation frequency, h-index, and impact factor of journals were analyzed by using the functions of “Create Citation Report” and “Journal Citation Reports” in Web of Science Database and Excel software. Results: A total of 4,993 papers was identified. The number of annual publications in the field remained high, with an average of more than 998 publications per year. The number of citations increased year by year, with a high number of 22,272 citations in 2021. The United States and China had significant influence in the field. Johns Hopkins University, USA had a leading position in this field. JESSEN KR and JOURNAL OF NEUROSCIENCE were the most influential authors and journals in the field, respectively. Meanwhile, we found that hot topics in the field of PNI focused on dorsal root ganglion (DRG) and satellite glial cells (SGCs) for neuropathic pain relief and on combining tissue engineering techniques and controlling the repair Schwann cell phenotype to promote nerve regeneration, which are not only the focus of research now but is also forecast to be of continued focus in the future. Conclusion: This is the first study to conduct a comprehensive bibliometric analysis of publications related to PNI from 2017 to 2021, whose bibliometric results can provide a reliable source for researchers to quickly understand key information in this field and identify potential research frontiers and hot directions."
Diara2020,Filippo Diara and Fulvio Rinaudo,Ifc classification for foss hbim: Open issues and a schema proposal for cultural heritage assets,Applied Sciences (Switzerland),10,23,2020,10.3390/app10238320,20763417,"The IFC (Industry Foundation Classes) open format has been developed by BuildingSMART and regularized through ISO standards. It has been implemented into a BIM (Building Information Modeling) informative system for the AEC industry (Architecture Engineering and Construction). The IFC format has changed interoperability processes concerning architectural and technical entities in a semantic way. However, because this standard open format was specifically designed for the modern AEC industry, it may not cater to the demands of cultural heritage assets. Since IFC classification is fundamental for informative systems, it should become a standard also concerning heritage assets, even if nowadays there is no regularized IFC classification for historical existing buildings. Specific cultural heritage peculiarities therefore need semantic classification based on historical asset families. For this reason, this work is based on a proposal and experimental IFC classification implemented inside an HBIM open source software (FreeCAD), whereby limitations of IFC standards can be overcome thanks to the freedom of access to libraries and codes. Moreover, this work is based on IFC objects management outside the platform for interoperability purposes."
Nair2020,Prashant R. Nair,Increasing employability of Indian engineering graduates through experiential learning programs and competitive programming: Case study,,172,,2020,10.1016/j.procs.2020.05.119,18770509,"With regard to engineering, there have been serious concerns about the employability of Indian graduates. There are alarming statistics in a NASSCOM report, which estimates that, of the 3 million joining the IT workforce, only twenty five percent of graduates with engineering background are employable. The figures are grave in the context of graduates from sciences and humanities, which is less than fifteen percent. Aspiring Minds has been administering a computer-based test called AMCAT to lakhs of students in 650+ engineering institutions measure employability of technical graduates. This considers parameters like Business Communication & English, Logical & Numerical skills, analytical & problem-resolution skills and coding. The results are a revelation, 47% of graduates cannot be employed in any domain or sector of the knowledge economy. 17.91%, 3.67% and 40.57% are the employability figures for software services, software products and BPO. Only 3.84% graduates are start-up ready and 6.56% are design job ready and the same trend for other core engineering jobs. There is an urgent need to improve employability of our engineering graduates. This calls for lateral thinking and out-of-the-box initiatives such as experiential learning programs and competitive programming; implementations of which we explore in a top-ranking private university as a case study. Statement of Contribution/Methods The case study of implementation of initiatives in experiential learning programs and competitive programming in a private university is highlighted. An Experiential learning program titled Live-in-Labs as part of the curriculum is explored. This program is student-centric, learner-centric, participatory and hands-on and they provide students an avenue to apply their acquired engineering knowledge, concepts and skills and deploy on a real-time basis in India's villages. It's Course Outcomes (CO) such as human-centered design concepts to document observations and user experiences, user-needs assessment and prioritization are enlisted. A very strong mapping to several Program Outcomes (PO) is observed unlike various regular courses in curriculum. Structured competitive programming initiative in which students compete with others in a contest environment in parameters such as program correctness, execution time, and development time is yet another effort towards student-centered learning. Platforms such as CodeChef, HackerRank and contests such International Collegiate Programming Contest (ICPC), which is considered as the Olympics of Collegiate Programming with annual participation of 50,000 students in 2000+ universities in 100+ countries are efforts in this direction. Competitive Programming initiative's learning objectives also spans several POs. Results, Discussion and Conclusions. Strong mapping of COs to POs inherent in the experiential learning programs improves the employability as also the progression and prospects of the students. Live-in-Labs® program exposes students to pressing issues confronted by village communities in India, through experiential learning opportunities, in order to apply theoretical concepts into application & deployment, by the devising of innovative technology remedies, and facilitation of crucial and collaborative problem-resolution capabilities of the students participating in the program. The participation of several students from foreign universities also enriches learning, collaboration and diversity. Every Live-in-Labs project results in a student paper published in reputed journals and conferences. It also improves student progression in terms of higher studies and high-paying jobs. Competitive programming dramatically improves student skills and capabilities in problem solving, coding, team work, innovation and creativity. It is also observed that code geeks from competitive programming initiatives are invariably the ones to secure the highest paying jobs in dream companies like Google, Amazon and Facebook."
Pan2022,Zhouzhou Pan and Laurence Brassart,Constitutive modelling of hydrolytic degradation in hydrogels,Journal of the Mechanics and Physics of Solids,167,,2022,10.1016/j.jmps.2022.105016,00225096,"Biodegradable synthetic hydrogels have emerged as promising materials for tissue engineering and drug/cell delivery applications. However, their successful implementation requires precise understanding of the degradation response in terms of mechanical properties, swelling, and mass loss. In this work, we develop a thermodynamically-consistent continuum framework and constitutive models for coupled large deformation and hydrolytic degradation in hydrogels. In particular, we propose constitutive models for the evolution of elastic modulus and polymer mass loss based on a description of the evolving network topology during hydrolysis. The theory is validated against experimental data for two model hydrogel systems with different network architectures, namely biodegradable Tetra-PEG hydrogels and PLA-b-PEG-b-PLA hydrogels. We have also implemented our model in a finite element software. We show that our model is capable of simulating degradation-induced heterogeneous swelling in scenarios relevant to biomedical applications. Our theory and constitutive models could be useful for the design of hydrogels with controlled degradation behaviour."
Vazquez-Vilar2021,Marta Vazquez-Vilar and Víctor Garcia-Carpintero and Sara Selma and Joan M. Bernabé-Orts and Javier Sanchez-Vicente and Blanca Salazar-Sarasua and Arianna Ressa and Carmine de Paola and María Ajenjo and Jose Carlos Quintela and Asun Fernández-del-Carmen and Antonio Granell and Diego Orzáez,"The GB4.0 Platform, an All-In-One Tool for CRISPR/Cas-Based Multiplex Genome Engineering in Plants",Frontiers in Plant Science,12,,2021,10.3389/fpls.2021.689937,1664462X,"CRISPR/Cas ability to target several loci simultaneously (multiplexing) is a game-changer in plant breeding. Multiplexing not only accelerates trait pyramiding but also can unveil traits hidden by functional redundancy. Furthermore, multiplexing enhances dCas-based programmable gene expression and enables cascade-like gene regulation. However, the design and assembly of multiplex constructs comprising tandemly arrayed guide RNAs (gRNAs) requires scarless cloning and is still troublesome due to the presence of repetitive sequences, thus hampering a more widespread use. Here we present a comprehensive extension of the software-assisted cloning platform GoldenBraid (GB), in which, on top of its multigene cloning software, we integrate new tools for the Type IIS-based easy and rapid assembly of up to six tandemly-arrayed gRNAs with both Cas9 and Cas12a, using the gRNA-tRNA-spaced and the crRNA unspaced approaches, respectively. As stress tests for the new tools, we assembled and used for Agrobacterium-mediated stable transformation a 17 Cas9-gRNAs construct targeting a subset of the Squamosa-Promoter Binding Protein-Like (SPL) gene family in Nicotiana tabacum. The 14 selected genes are targets of miR156, thus potentially playing an important role in juvenile-to-adult and vegetative-to-reproductive phase transitions. With the 17 gRNAs construct we generated a collection of Cas9-free SPL edited T1 plants harboring up to 9 biallelic mutations and showing leaf juvenility and more branching. The functionality of GB-assembled dCas9 and dCas12a-based CRISPR/Cas activators and repressors using single and multiplexing gRNAs was validated using a Luciferase reporter with the Solanum lycopersicum Mtb promoter or the Agrobacterium tumefaciens nopaline synthase promoter in transient expression in Nicotiana benthamiana. With the incorporation of the new web-based tools and the accompanying collection of DNA parts, the GB4.0 genome edition turns an all-in-one open platform for plant genome engineering."
Sargent2021,Noah Sargent and Mason Jones and Richard Otis and Andrew A. Shapiro and Jean Pierre Delplanque and Wei Xiong,Integration of processing and microstructure models for non-equilibrium solidification in additive manufacturing,Metals,11,4,2021,10.3390/met11040570,20754701,"Integration of models that capture the complex physics of solidification on the macro and microstructural scale with the flexibility to consider multicomponent materials systems is a significant challenge in modeling additive manufacturing processes. This work aims to link process variables, such as energy density, with non-equilibrium solidification by integrating additive manufacturing process simulations with solidification models that consider thermodynamics and diffusion. Temperature histories are generated using a semi-analytic laser powder bed fusion process model and feed into a CALPHAD-based ICME (CALPHAD: Calculation of Phase Diagrams, ICME: Integrated Computational Materials Engineering) framework to model non-equilibrium solidification as a function of both composition and processing parameters. Solidification cracking susceptibility is modeled as a function of composition, cooling rate, and energy density in Al-Cu Alloys and stainless steel 316L (SS316L). Trends in solidification cracking susceptibility predicted by the model are validated by experimental solidification cracking measurements of Al-Cu alloys. Non-equilibrium solidification in additively manufactured SS316L is investigated to determine if this approach can be applied to commercial materials. Modeling results show a linear relationship between energy density and solidification cracking susceptibility in additively manufactured SS316L. This work shows that integration of process and microstructure models is essential for modeling solidification during additive manufacturing."
Donnici2020,Giampiero Donnici and Leonardo Frizziero and Alfredo Liverani and Giulio Buscaroli and Luna Raimondo and Eleonora Saponaro and Giorgia Venditti,A new car concept developed with stylistic design engineering (SDE),Inventions,5,3,2020,10.3390/inventions5030030,24115134,"In this work, a structured design method, the Stylistic Design Engineering (SDE), is applied for the construction of a new minivan car, in particular a new city car, which we will call FIAT 600 Omega. The SDE, or Stylistic Design Engineering, is a structured engineering method for carrying out automotive design projects. The SDE method consists of six different phases: (1) Analysis of stylistic trends; (2) Sketches; (3) 2D Computer Aided Design (CAD) drawings; (4) 3D CAD models; (5) Rendering; (6) Solid stylistic model (also called style maquette). This project deals with the external redesign of the Fiat 600 multiple, a small minivan which was very successful in the 1950s and 1960s. SDE is a methodology consisting of various technologies and design methodologies that will be further explained in detail, such as the Pininfarina method, the Quality Function Deployment (QFD) method, Benchmarking (BM), and Top Flop Analysis (TPA). The work was organized according to the different phases. Initially, the Fiat style was studied, in particular the style of the FIAT 600 MULTI PURPOUSE VEHICLE (MPV). This step is essential to better understand the characteristics of the brand and also the main characteristics carried out over the decades. Then we moved on to the freehand sketching phase, based on what we learned in the previous phase of the study. When a satisfactory shape was found for the new car, by analyzing and discarding the different proposals of the various types of style, we proceeded to the evaluation of the proportions and dimensions through two-dimensional drawings and finally we obtained the three-dimensional shape of the new car thanks to 3D CAD software and rendering software. Many advantages in the industrial world SDE takes together with its development. In fact, until the early 2000s, car design and styling was considered quite a craft activity, not a technical or scientific one, mostly based on the great capability of famous car designers and masters, just like Giugiaro, Zagato, Bertone, Pininfarina, Stephenson, Bangle, etc. Then, thanks to the industrial activity of Eng. Lorenzo Ramacciotti, former CEO of Pininfarina Spa and Mechanical Engineer, and also thanks to the academic studies developed at ALMA MATER STUDIORUM University of Bologna, SDE became the object of attention, because it is able to systematize the car design process and reduce costs. With SDE, a good design research or an industrial product development team can complete a car design project, also without the presence of a mentor. Car Design Process finally becomes with SDE a scientific method; Car Design becomes with SDE an industrial method. Industrial needs are nice products made in a short time; SDE is structured to attend these issues. Industrial challenges follow innovation, in shape and functionality; SDE is able to recognize innovation. Industrial benefits can be reached with SDE, ensuring beautiful aesthetic projects are realized systematically and with low costs."
Mostofa2021,Nafisa Mostofa and Christopher Feltner and Kelly Fullin and Jonathan Guilbe and Sharare Zehtabian and Salih Safa Bacanlı and Ladislau Bölöni and Damla Turgut,A smart walker for people with both visual and mobility impairment,Sensors,21,10,2021,10.3390/s21103488,14248220,"In recent years, significant work has been done in technological enhancements for mobility aids (smart walkers). However, most of this work does not cover the millions of people who have both mobility and visual impairments. In this paper, we design and study four different configurations of smart walkers that are specifically targeted to the needs of this population. We investigated different sensing technologies (ultrasound-based, infrared depth cameras and RGB cameras with advanced computer vision processing), software configurations, and user interface modalities (haptic and audio signal based). Our experiments show that there are several engineering choices that can be used in the design of such assistive devices. Furthermore, we found that a holistic evaluation of the end-to-end performance of the systems is necessary, as the quality of the user interface often has a larger impact on the overall performance than increases in the sensing accuracy beyond a certain point."
He2020,Jian He and Shengli Cao and Hulin Zhang,Cylinder-based hybrid rotary nanogenerator for harvesting rotational energy from axles and self-powered tire pressure monitoring,Energy Science and Engineering,8,2,2020,10.1002/ese3.560,20500505,"Tire pressure monitoring plays a pivotal role in vehicle safety system. However, as a conventional battery-operated electronic system, regularly replacing battery remains a great inconvenience in wide-distributed tire pressure sensing. Here, we introduce a self-powered tire pressure monitor by using a rotary cylinder-based hybrid nanogenerator as a sustainable power source. The designed energy harvester, by hybridizing a triboelectric nanogenerator (TENG) and electromagnetic generator (EMG), can scavenge rotational energy from rolling axles. Integrating with transformers, the hybrid nanogenerator can achieve an open-circuit voltage of 16 V and short-circuit current of 0.1 mA at the rotation rate of 150 rpm, respectively, with the maximal output power of about 1.8 mW at the loading of 20 kΩ. Via a programmable software, the hybrid device can operate as a self-powered counter and timer for potential speed detecting. Further, it has been demonstrated that the hybrid nanogenerator is capable of triggering a transmitter-integrated tire pressure sensor for self-powered monitoring tire pressure in real time. This study expands applications based on TENGs in automobile engineering, which might promote the development of intelligent driving and traffic safety engineering."
Xu2022,Ji Xu and Peng Zhao and Yong Zhang and Junwu Wang and Wei Ge,Discrete particle methods for engineering simulation: Reproducing mesoscale structures in multiphase systems,Resources Chemicals and Materials,1,1,2022,10.1016/j.recm.2022.01.002,27724433,"Most natural resources are processed as particle-fluid multiphase systems in chemical, mineral and material industries, therefore, discrete particles methods (DPM) are reasonable choices of simulation method for engineering the relevant processes and equipments. However, direct application of these methods is challenged by the complex multiscale behavior of such systems, which leads to enormous computational cost or otherwise qualitatively inaccurate description of the mesoscale structures. The coarse-grained DPM based on the energy-minimization multi-scale (EMMS) model, or EMMS-DPM, was proposed to reduce the computational cost by several orders while maintaining an accurate description of the mesoscale structures, which paves the way for its engineering applications. Further empowered by the high-efficiency multi-scale DEM software DEMms and the corresponding customized heterogeneous supercomputing facilities with graphics processing units (GPUs), it may even approach realtime simulation of industrial reactors. This short review will introduce the principle of DPM, in particular, EMMS-DPM, and the recent developments in modeling, numerical implementation and application of large-scale DPM which aims to reach industrial scale on one hand and resolves mesoscale structures critical to reaction-transport coupling on the other hand. This review finally prospects on the future developments of DPM in this direction."
Jzsa2021,T. I. Józsa and R. M. Padmos and W. K. El-Bouri and A. G. Hoekstra and S. J. Payne,On the Sensitivity Analysis of Porous Finite Element Models for Cerebral Perfusion Estimation,Annals of Biomedical Engineering,49,12,2021,10.1007/s10439-021-02808-w,15739686,"Computational physiological models are promising tools to enhance the design of clinical trials and to assist in decision making. Organ-scale haemodynamic models are gaining popularity to evaluate perfusion in a virtual environment both in healthy and diseased patients. Recently, the principles of verification, validation, and uncertainty quantification of such physiological models have been laid down to ensure safe applications of engineering software in the medical device industry. The present study sets out to establish guidelines for the usage of a three-dimensional steady state porous cerebral perfusion model of the human brain following principles detailed in the verification and validation (V&V 40) standard of the American Society of Mechanical Engineers. The model relies on the finite element method and has been developed specifically to estimate how brain perfusion is altered in ischaemic stroke patients before, during, and after treatments. Simulations are compared with exact analytical solutions and a thorough sensitivity analysis is presented covering every numerical and physiological model parameter. The results suggest that such porous models can approximate blood pressure and perfusion distributions reliably even on a coarse grid with first order elements. On the other hand, higher order elements are essential to mitigate errors in volumetric blood flow rate estimation through cortical surface regions. Matching the volumetric flow rate corresponding to major cerebral arteries is identified as a validation milestone. It is found that inlet velocity boundary conditions are hard to obtain and that constant pressure inlet boundary conditions are feasible alternatives. A one-dimensional model is presented which can serve as a computationally inexpensive replacement of the three-dimensional brain model to ease parameter optimisation, sensitivity analyses and uncertainty quantification. The findings of the present study can be generalised to organ-scale porous perfusion models. The results increase the applicability of computational tools regarding treatment development for stroke and other cerebrovascular conditions."
Boden2021,Brigitte Boden and Jan Flink and Niklas Först and Robert Mischke and Kathrin Schaffert and Alexander Weinert and Annika Wohlan and Andreas Schreiber,RCE: An Integration Environment for Engineering and Science,SoftwareX,15,,2021,10.1016/j.softx.2021.100759,23527110,"Engineering complex systems such as air- and spacecraft is a multidisciplinary effort that requires the collaboration of engineers from a multitude of specializations working in concert. Typically, each engineer uses one or more specialized software tools to analyze some data set and passes, in an ad-hoc manner, the results on to their colleagues who require these results as input for their respective tools. This process is time-consuming, error-prone, and not replicable. To alleviate this problem, we present RCE (Remote Component Environment), an open-source application developed primarily at DLR, that enables its users to intuitively integrate disciplinary tools, to define dependencies between them via an easy-to-use graphical interface, and to execute the resulting multidisciplinary engineering workflow. All data produced are stored centrally for provenance, subsequent analysis, and post-processing. Hence, RCE makes it easy for collaborating engineers to contribute their individual disciplinary tools to a multidisciplinary design or analysis, and simplifies the analysis of the workflow's results."
Popescu2022,Daniela Popescu and Mihai Dragomir and Sorin Popescu and Diana Dragomir,Building Better Digital Twins for Production Systems by Incorporating Environmental Related Functions—Literature Analysis and Determining Alternatives,Applied Sciences (Switzerland),12,17,2022,10.3390/app12178657,20763417,"The digital twin solution is an industry 4.0 specific tool that has grown in the past decade, stemming from the modelling and simulation approaches that existed before, complemented by new sensor capabilities, cloud processing, big data analytics, and implementation mechanisms. As it is being used mostly in the present by manufacturing companies, the primary focus of the solution is to enhance productivity and reduce costs by optimizing processes and enabling real-time problem-solving, sometimes based on decision-making systems and artificial intelligence. However, as companies are being faced with an increasingly steep list of environmental requirements and regulations, ranging from the classical pollution control and waste recycling to full-scale economic models based on circular economy and transformative carbon dioxide elimination programs, the features of the manufacturing digital twins must also evolve to provide an appropriate answer to these challenges. In this paper, the authors propose a framework for building better digital twins for production systems by incorporating environmental-related functions. The demarches start from analysing existing solutions presented in literature from the point of view of environmental suitability, based on the use of the MoSCoW method for differentiating attributes (into Must have, Should have, Could have, Will not have elements) and determining development alternatives based on the employment of Multi-Criteria Decision Analysis (MCDA) for feature selection, and the TRIZ method (Theory of Inventive Problem-Solving) for application guidelines. The MCDA was performed within a focus group of nine production specialists from regionally successful sectors. We arrive at the conclusion that environmental-related functions are poorly implemented in the digital twins of the present (although more so in integrated solutions and custom-built applications) and that the development of the proper tools, databases, and interpretation keys should proceed immediately in the fields of production engineering, industrial ecology, and software development to support them."
Diehl2020,Martin Diehl and Ding Wang and Chuanlai Liu and Jaber Rezaei Mianroodi and Fengbo Han and Duancheng Ma and Piet J.J. Kok and Franz Roters and Pratheek Shanthraj,Solving Material Mechanics and Multiphysics Problems of Metals with Complex Microstructures Using DAMASK—The Düsseldorf Advanced Material Simulation Kit,Advanced Engineering Materials,22,3,2020,10.1002/adem.201901044,15272648,"Predicting process–structure and structure–property relationships are the key tasks in materials science and engineering. Central to both research directions is the internal material structure. In the case of metallic materials for structural applications, this internal structure, the microstructure, is the collective ensemble of all equilibrium and nonequilibrium lattice imperfections. Continuum models to derive process–structure and structure–property relationships are based on two ingredients: 1) quantitative state variables that capture the essential features of the material's state and 2) kinetic equations for the state that describe its evolution under load. Successful models, that is, models that are of practical use, depend on state variables and corresponding evolution laws that are sufficiently representative for the microstructure and that are able to describe the phenomena of interest. The development of software tools capable to integrate these different aspects to get a holistic view of process–structure–property relationships requires joint efforts from specialists in different disciplines and a long-term perspective. The Düsseldorf Advanced Material Simulation Kit (DAMASK) is such a tool. In this overview article published on the occasion of Advanced Engineering Materials' 20th anniversary, some representative application examples which demonstrate how DAMASK can be used to study metallic microstructures at different length scales are presented."
Silva2023,Francisco Tardelli da Silva and Ismael Cristofer Baierle and Ricardo Gonçalves de Faria Correa and Miguel Afonso Sellitto and Fernanda Araujo Pimentel Peres and Liane Mahlmann Kipper,Open Innovation in Agribusiness: Barriers and Challenges in the Transition to Agriculture 4.0,Sustainability (Switzerland),15,11,2023,10.3390/su15118562,20711050,"Industry 4.0 digital technologies in agribusiness will enable traditional farming systems to migrate to Agriculture 4.0. Open innovation emerges as an enabler for implementing these technologies and increased sector competitiveness. However, there are still doubts and questions about how technologies and open innovation relate to and will drive Agriculture 4.0. This study identified which digital technologies of Industry 4.0 have more adherence to agribusiness, what the barriers and facilitators for using these technologies are, and how open innovation can increase the competitiveness of agribusiness. The results show that of the Industry 4.0 technologies related to agribusiness, the Internet of Things (IoT) is the most prominent. The main barriers are the users’ need for more knowledge and advanced skills, which evidences the need for investment in training operators. Among the facilitators stand the pre-existence of several technologies, which bring with them already defined basic structures, control of the technology, and communication between systems. To overcome the barriers and enhance the migration to Agriculture 4.0, developing devices, tools, systems, software, and machines is essential. More stakeholders, managers, and practitioners may share such opportunities for innovation in agribusiness through the concept of Open Innovation. To benefit from it, facilitators, managers, and practitioners of agribusiness should search for alternatives for their problems with engineering solutions providers."
Khan2021,Muhammad Israr Khan and Shuhong Wang,Slope stability analysis to correlate shear strength with slope angle and shear stress by considering saturated and unsaturated seismic conditions,Applied Sciences (Switzerland),11,10,2021,10.3390/app11104568,20763417,"Assessment and analysis of soil slope stability is an important part of geotechnical engineering at all times. This paper examines the assessment of soil slope stability in fine-grained soils. The effect of change in shear strength (τ), shear stress (σ) and slope angle (β) on the factor of safety has been studied. It correlates shear strength with slope angle and shear stress by considering the horizontal seismic coefficients in both saturated and unsaturated conditions. The slope failure surface was considered a circular slip surface. Statistical package for social sciences (SPSS) and Slide, numerical modeling software and limit equilibrium slope stability analysis software, respectively, are used to find out the correlations between the three basic parameters. The slope angle varied from 70 to 88 degrees, which are the most critical values for slope angles, and a total of 200 analyses were performed. τ, β and σ are correlated, and the correlations are provided in the results section. The results indicate that the correlations developed between the parameters have a very close relationship. The applicability of the developed equations is above 99%. These correlations are applicable in any type of soil slope stability analysis, where the value of shear strength and factor of safety is required with the variation of slope angle and shear stress."
Donmez2022,Mustafa Borga Donmez and Vinicius Rizzo Marques and Gülce Çakmak and Hakan Yilmaz and Martin Schimmel and Burak Yilmaz,Congruence between the meshes of a combined healing abutment-scan body system acquired with four different intraoral scanners and the corresponding library file: An in vitro analysis,Journal of Dentistry,118,,2022,10.1016/j.jdent.2021.103938,03005712,"Objectives: To investigate the congruence between the meshes of a combined healing abutment-scan body (CHA-SB) system acquired with four different intraoral scanners and the corresponding library file. Material and methods: A CHA-SB was fixed to an implant at the right first molar position in a dentate mandibular model and digitized by using 4 different intraoral scanners (IOSs) [TRIOS 3 (T3), Omnicam (OC), Primescan (PS), and Virtuo Vivo (VV)] (n = 8) and an industrial grade optical scanner (ATOS Core 80) (n = 1) to generate standard tessellation language (STL) files of the test scans (CHA-SB-STLs) and the master reference model scan (MRM-STL). A reverse engineering software (Studio Geomagic X) was used to superimpose the proprietary library file of the CHA-SB over the generated STL files. Root mean square (RMS) values representing the deviations between the library file and the superimposed STL files were statistically analyzed by using 1-way ANOVA (α=0.05). Qualitative analysis of the deviations was performed by visual inspection. Results: Differences between the congruence of the library file and the CHA-SB scans among different IOSs were nonsignificant (F = 1.619, df= 3, P = .207). The single best result was 29 ± 28.9 µm for OC, 30.8 ± 29.6 µm for VV, 35.6 ± 35.5 µm for T3, and 39.5 ± 39.2 µm for PS, which were all above the deviation value of the scan performed by using the industrial-grade scanner (23.2 ± 23.2 µm). Conclusion: The dimensional congruence between the library file and the STL file of the CHA-SB system scans was similar when intraoral scanners with different acquisition technologies were used to scan a model with an implant. Clinical significance: Scans of the tested intraoral scanners may result in crowns with similar positional accuracy, given the similarities in congruence of their scans with the library file."
Pavlenko2022,Ivan Pavlenko and Ján Pitel’ and Vitalii Ivanov and Kristina Berladir and Jana Mižáková and Vitalii Kolos and Justyna Trojanowska,Using Regression Analysis for Automated Material Selection in Smart Manufacturing,Mathematics,10,11,2022,10.3390/math10111888,22277390,"In intelligent manufacturing, the phase content and physical and mechanical properties of construction materials can vary due to different suppliers of blanks manufacturers. Therefore, evaluating the composition and properties for implementing a decision-making approach in material selection using up-to-date software is a topical problem in smart manufacturing. Therefore, the article aims to develop a comprehensive automated material selection approach. The proposed method is based on the comprehensive use of normalization and probability approaches and the linear regression procedure formulated in a matrix form. As a result of the study, analytical dependencies for automated material selection were developed. Based on the hypotheses about the impact of the phase composition on physical and mechanical properties, the proposed approach was proven qualitatively and quantitively for carbon steels from AISI 1010 to AISI 1060. The achieved results allowed evaluating the phase composition and physical properties for an arbitrary material from a particular group by its mechanical properties. Overall, an automated material selection approach based on decision-making criteria is helpful for mechanical engineering, smart manufacturing, and industrial engineering purposes."
Gambo2021,Nuru Gambo and Innocent Musonda,Effect of the Fourth Industrial Revolution on Road Transport Asset Management Practice in Nigeria,Journal of Construction in Developing Countries,26,1,2021,10.21315/jcdc2021.26.1.2,21804222,"Poor management practices of road transport assets posed a challenge to the sustainable development of the transport system in developing countries like Nigeria. Studies in the past focused mainly on the performance of road construction process. However, few studies have evaluated the effect of the fourth Industrial Revolution (4.0IR) on the road transport assets in developing countries such as Nigeria. The current study aimed at assessing the effect of the 4.0IR towards improving the management practice of road transport assets. Survey instruments were administered to project and facility managers in the Nigerian road construction sector of the economy using a proportionate random sampling technique. Partial least square structural equation modelling was used for data analysis utilising the Warp 7.0 partial least squares-structural equation modelling (PLS-SEM) software algorithm. The software calculates p-values with WarpPLS based on non-parametric algorithms, resampling or stable algorithms and thus does not require that the variables to be normally distributed. The study concluded that the 4.0IR drivers have a moderate effect change on the management practice of road transport assets in Nigeria at the moment. The findings imply that management of road assets in Nigeria would moderately improve due to the 4.0IR technologies resulting in transport, safety and general efficiency and effectiveness of road networks in Nigeria. The study identified the 4.0IR drivers to include robotics, mobility, virtual and augmented reality, Internet of things and cloud computing, machine learning, artificial intelligence, blockchain, three-dimensional (3D) printing drones that are built with an attached 3D printer (the drone hangs a 3D printing nozzle that has fed plastic, concrete mix or other material from a tube connected to the top of the drone's printing path that precisely plotted by software, for a promised printing accuracy of 0.1 mm) and digital engineering. This study emanated from the government reports and past studies in the area of road transport asset management practice which the study investigated the major causes of poor practices and assessed the effect of the 4.0IR on the practice"
Khalyasmaa2020,Alexandra I. Khalyasmaa and Stanislav A. Eroshenko and Valeriy A. Tashchilin and Hariprakash Ramachandran and Teja Piepur Chakravarthi and Denis N. Butusov,Industry experience of developing day-ahead photovoltaic plant forecasting system based on machine learning,Remote Sensing,12,20,2020,10.3390/rs12203420,20724292,"This article highlights the industry experience of the development and practical implementation of a short-term photovoltaic forecasting system based on machine learning methods for a real industry-scale photovoltaic power plant implemented in a Russian power system using remote data acquisition. One of the goals of the study is to improve photovoltaic power plants generation forecasting accuracy based on open-source meteorological data, which is provided in regular weather forecasts. In order to improve the robustness of the system in terms of the forecasting accuracy, we apply newly derived feature introduction, a factor obtained as a result of feature engineering procedure, characterizing the relationship between photovoltaic power plant energy production and solar irradiation on a horizontal surface, thus taking into account the impacts of atmospheric and electrical nature. The article scrutinizes the application of different machine learning algorithms, including Random Forest regressor, Gradient Boosting Regressor, Linear Regression and Decision Trees regression, to the remotely obtained data. As a result of the application of the aforementioned approaches together with hyperparameters, tuning and pipelining of the algorithms, the optimal structure, parameters and the application sphere of different regressors were identified for various testing samples. The mathematical model developed within the framework of the study gave us the opportunity to provide robust photovoltaic energy forecasting results with mean accuracy over 92% for mostly-sunny sample days and over 83% for mostly cloudy days with different types of precipitation."
Marcoline2022,Frank V. Marcoline and John Furth and Smita Nayak and Michael Grabe and Robert I. Macey,Berkeley Madonna Version 10–A simulation package for solving mathematical models,CPT: Pharmacometrics and Systems Pharmacology,11,3,2022,10.1002/psp4.12757,21638306,"Berkeley Madonna is a software program that provides an easy and intuitive environment for graphically building and numerically solving mathematical equations. Our users range from college undergraduates with little or no mathematical experience to academic researchers and professionals building and simulating sophisticated mathematical models that represent complex systems in the biological, chemical, and engineering fields. Here we briefly describe our recent advances including a new Java-based user interface introduced in Version 9 and our transition from a 32- to 64-bit architecture with the release of Version 10. We take the reader through an example tutorial that illustrates how to construct a mathematical model in Berkeley Madonna while highlighting some of the recent changes to the software. Specifically, we construct a standard pharmacokinetic model of the antifungal medication amphotericin B taken from the literature and discuss aspects related to model building, key numerical considerations, data fitting, and graphical visualization. We end by discussing planned functionality and features intended for future releases."
Mendoza-Pitti2021,Luis Mendoza-Pitti and Huriviades Calderon-Gomez and Miguel Vargas-Lombardo and Jose Manuel Gomez-Pulido and Jose Luis Castillo-Sequera,Towards a Service-Oriented Architecture for the Energy Efficiency of Buildings: A Systematic Review,IEEE Access,9,,2021,10.1109/ACCESS.2021.3057543,21693536,"Currently, smart buildings generate large amounts of data due to the many devices and equipment available. Hence, buildings implement building management systems (BMSs), which monitor, control, manage and analyze each of these components. However, current BMSs are incapable of managing a massive amount of data (big data) and therefore cannot extract knowledge or make intelligent decisions in quasi real time. In addition, there are serious limitations to integrating BMSs with other services since they generally use proprietary software. In this sense, service-oriented architecture (SOA) is an architectural style that allows one to build distributed systems and provide functionalities such as services to end users or other types of services. Therefore, an SOA has the great advantage of allowing the expansion of the functionalities of BMSs. In fact, there are several studies that address SOAs for building management. However, we have not found any description or systematic analysis in the literature that allows the development of a versatile and interoperable SOA focused on the energy efficiency of buildings and that can integrate massive data analysis features. For these reasons, this study seeks to fill this knowledge gap and, more specifically, to identify and analyze the various software requirements proposed in the literature and the characteristics of big data that allow for improving the energy efficiency of buildings. To this end, we performed an in-depth review of the literature according to the methodology proposed by Kitchenham. As a result of this review, we provide researchers with a specific vision of the requirements and characteristics to consider for software development aimed at the energy efficiency of unique or historic buildings."
Lpez2022,José Antonio Hernández López and Javier Luis Cánovas Izquierdo and Jesús Sánchez Cuadrado,ModelSet: a dataset for machine learning in model-driven engineering,Software and Systems Modeling,21,3,2022,10.1007/s10270-021-00929-3,16191374,"The application of machine learning (ML) algorithms to address problems related to model-driven engineering (MDE) is currently hindered by the lack of curated datasets of software models. There are several reasons for this, including the lack of large collections of good quality models, the difficulty to label models due to the required domain expertise, and the relative immaturity of the application of ML to MDE. In this work, we present ModelSet, a labelled dataset of software models intended to enable the application of ML to address software modelling problems. To create it we have devised a method designed to facilitate the exploration and labelling of model datasets by interactively grouping similar models using off-the-shelf technologies like a search engine. We have built an Eclipse plug-in to support the labelling process, which we have used to label 5,466 Ecore meta-models and 5,120 UML models with its category as the main label plus additional secondary labels of interest. We have evaluated the ability of our labelling method to create meaningful groups of models in order to speed up the process, improving the effectiveness of classical clustering methods. We showcase the usefulness of the dataset by applying it in a real scenario: enhancing the MAR search engine. We use ModelSet to train models able to infer useful metadata to navigate search results. The dataset and the tooling are available at https://figshare.com/s/5a6c02fa8ed20782935c and a live version at http://modelset.github.io."
Chia2022,Aleena Chia,"The metaverse, but not the way you think: game engines and automation beyond game development",Critical Studies in Media Communication,39,3,2022,10.1080/15295036.2022.2080850,14795809,"The production of videogames routinely uses automated techniques to generate content, rig animations, map light, and script behaviors. The automation of programming and artistic functions is increasingly baked into game engines that work with other software applications in 3D production ecosystems, which are laying the foundations for what is being pitched by platform companies as the future metaverse. Platform studies has analyzed automated decision-making through the politics of classification. Game studies has investigated engines such as Unreal and Unity as platform tools that consolidate power through asymmetries of interconnectivity and interoperability. This commentary discusses the automaticity of game engines as platform tools for designing and simulating interactive 3D worlds within and beyond games. Outlining the structuring force of game engines from game development and entertainment media to architecture, engineering, construction, and manufacturing, I speculate on the implications of engines for game workers and game studies."
Ahmed2022,Zahoor Ahmed and Hasan Zulfiqar and Abdullah Aman Khan and Ijaz Gul and Fu Ying Dao and Zhao Yue Zhang and Xiao Long Yu and Lixia Tang,iThermo: A Sequence-Based Model for Identifying Thermophilic Proteins Using a Multi-Feature Fusion Strategy,Frontiers in Microbiology,13,,2022,10.3389/fmicb.2022.790063,1664302X,"Thermophilic proteins have important application value in biotechnology and industrial processes. The correct identification of thermophilic proteins provides important information for the application of these proteins in engineering. The identification method of thermophilic proteins based on biochemistry is laborious, time-consuming, and high cost. Therefore, there is an urgent need for a fast and accurate method to identify thermophilic proteins. Considering this urgency, we constructed a reliable benchmark dataset containing 1,368 thermophilic and 1,443 non-thermophilic proteins. A multi-layer perceptron (MLP) model based on a multi-feature fusion strategy was proposed to discriminate thermophilic proteins from non-thermophilic proteins. On independent data set, the proposed model could achieve an accuracy of 96.26%, which demonstrates that the model has a good application prospect. In order to use the model conveniently, a user-friendly software package called iThermo was established and can be freely accessed at http://lin-group.cn/server/iThermo/index.html. The high accuracy of the model and the practicability of the developed software package indicate that this study can accelerate the discovery and engineering application of thermally stable proteins."
Atoum2021,Issa Atoum and Mahmoud Khalid Baklizi and Izzat Alsmadi and Ahmed Ali Otoom and Taha Alhersh and Jafar Ababneh and Jameel Almalki and Saeed Masoud Alshahrani,Challenges of Software Requirements Quality Assurance and Validation: A Systematic Literature Review,IEEE Access,9,,2021,10.1109/ACCESS.2021.3117989,21693536,"Validation of software requirements is a primary phase in requirements engineering that ensures requirements match the target system with the intended needs of the acquirer. It aims to detect and correct errors that prevail in the specified requirements. Although there are tremendous requirements validation approaches, some software may fail because of limited or ineffective requirements validation techniques and unreliable requirements' quality characteristics. In this study, a systematic literature review of requirements validation is performed. The study analyzes the most adopted validation techniques, reports requirements quality characteristics, and discovers significant challenges of validation techniques. The review identified 66 relevant primary studies analyzed to derive deep insights into the following aspects of requirements validation: trends of requirements validation methods, including their subtechnique strengths and weaknesses, requirements quality characteristics categories, and adopted tools and datasets in these techniques. We grouped validation techniques into categories: prototyping, inspection, knowledge-oriented, test-oriented, modeling and assessment, and formal models. The analysis reported 19 validation techniques, 27 tools, new requirements validation characteristics, and several challenges that prevailed through validation techniques. The trend of validation techniques is to those methods that apply machine learning techniques with knowledge from dictionaries and ontologies. Most challenges are about how to express the requirements and how to revert clients' feedback. There is a strong relationship between validation techniques, software application domain, and requirements validation quality attributes. Thus, there is an immense need to unify the quality characteristics and domain-specific validation methods."
Alshaer2020,Hamada Alshaer and Harald Haas,Software-Defined Networking-Enabled Heterogeneous Wireless Networks and Applications Convergence,IEEE Access,8,,2020,10.1109/ACCESS.2020.2986132,21693536,"A software-defined networking (SDN) architecture is capable of integrating all radio frequency and optical wireless small cell networks (e.g. fifth generation (5G), long-term evolution (LTE) femtocell, wireless fidelity (WiFi), light fidelity (LiFi)) in one network domain. This paper considers a SDN-enabled heterogeneous network (HetNet) comprised of LiFi, LTE femtocell and WiFi access points (APs). The HetNet control plane maintains the state of the network topology and wireless resources, which can support the development of intelligent service provisioning and efficient data communications in x generation (xG) wireless networks. The SDN applications use the network state to provide services in the data plane. However, when the state of network and wireless resources constantly changes, the SDN applications cannot provide reliable and guaranteed services to the wireless user equipments. This paper develops a queuing theoretic framework, which provides a performance evaluation for the SDN-enabled HetNet and applications convergence. A traffic engineering (TE) scheme is developed to support dynamic agnostic downlink flows routing to APs and differentiated granular services across the HetNet. Network and user centric policies are developed to make applications aware of network resource availability on the northbound and southbound interfaces of a SDN controller. Numerical models are introduced to study the impact of the computation and communication resources of northbound and southbound interfaces on the SDN-enabled HetNet scalability and the quality-of-service (QoS) guarantee of applications. Also, simulation scenarios are conducted to evaluate the performance of the TE scheme in provisioning effective and reliable services for subscribers."
Silveira2021,Marcos Paulo Motta Silveira and Larissa Mendes Campaner and Marco Antonio Bottino and Renato Sussumu Nishioka and Alexandre Luiz Souto Borges and João Paulo Mendes Tribst,Influence of the dental implant number and load direction on stress distribution in a 3-unit implant-supported fixed dental prosthesis,Dental and Medical Problems,58,1,2021,10.17219/dmp/130847,23009020,"Background. The choice between 2 or 3 implants to support a 3-unit implant-supported fixed dental prosthesis (FDP) still generates doubt in clinical practice. Objectives. The aim of this study was to evaluate stress distribution in 3-unit implant-supported FDPs according to the implant number and load direction. Material and methods. A numerical simulation was performed to analyze stress and strain according to the implant number (2 or 3) and load direction (axial or oblique). A model of a jaw was created by means of the modeling software Rhinoceros, v. 5.0 SR8. External hexagon implants, micro-conical abutments and screws were also modeled. The final geometries were exported to the computer-aided engineering (CAE) software Ansys, v. 17.2, and all materials were considered homogeneous, isotropic and elastic. Different load directions were applied for each model (300 N) at the center of the prosthesis. Results. The von Mises stress and strain values were obtained for the titanium structures and the bone, respectively. The implant number influenced the prosthesis biomechanics, with higher stress and strain concentrations when 2 implants were simulated. The oblique load also affected the mechanical response, showing higher stress and strain in comparison with the axial load, regardless of the implant number. Conclusions. It was concluded that for a 3-unit implant-supported FDP, a greater number of implants associated with axial loads can result in a better mechanical response during chewing."
Shambour2022,Qusai Y. Shambour and Abdelrahman H. Hussein and Qasem M. Kharma and Mosleh M. Abualhaj,Effective hybrid content-based collaborative filtering approach for requirements engineering,Computer Systems Science and Engineering,40,1,2022,10.32604/CSSE.2022.017221,02676192,"Requirements engineering (RE) is among the most valuable and critical processes in software development. The quality of this process significantly affects the success of a software project. An important step in RE is requirements elicitation, which involves collecting project-related requirements from different sources. Repositories of reusable requirements are typically important sources of an increasing number of reusable software requirements. However, the process of searching such repositories to collect valuable project-related requirements is time-consuming and difficult to perform accurately. Recommender systems have been widely recognized as an effective solution to such problem. Accordingly, this study proposes an effective hybrid content-based collaborative filtering recommendation approach. The proposed approach will support project stakeholders in mitigating the risk of missing requirements during requirements elicitation by identifying related requirements from software requirement repositories. The experimental results on the RALIC dataset demonstrate that the proposed approach considerably outperforms baseline collaborative filtering-based recommendation methods in terms of prediction accuracy and coverage in addition to mitigating the data sparsity and cold-start item problems."
Elyasaf2021,Achiya Elyasaf,Context-Oriented Behavioral Programming,Information and Software Technology,133,,2021,10.1016/j.infsof.2020.106504,09505849,"Context: Modern systems require programmers to develop code that dynamically adapts to different contexts, leading to the evolution of new context-oriented programming languages. These languages introduce new software-engineering challenges, such as: how to maintain the separation of concerns of the codebase? how to model the changing behaviors? how to verify the system behavior? and more. Objective: This paper introduces Context-Oriented Behavioral Programming (COBP) — a novel paradigm for developing context-aware systems, centered on natural and incremental specification of context-dependent behaviors. As the name suggests, we combine behavioral-programming (BP) — a scenario-based modeling paradigm — with context idioms that explicitly specify when scenarios are relevant and what information they need. The core idea is to connect the behavioral model with a data model that represents the context, allowing an intuitive connection between the models via update and select queries. Combining behavioral-programming with context-oriented programming brings the best of the two worlds, solving issues that arise when using each of the approaches in separation. Methods: We begin with providing abstract semantics for COBP and two implementations for the semantics, laying the foundations for applying reasoning algorithms to context-aware behavioral programs. Next, we exemplify the semantics with formal specifications of systems, including a variant of Conway's Game of Life. Then, we provide two case studies of real-life context-aware systems (one in robotics and another in IoT) that were developed using this tool. Throughout the examples and case studies, we provide design patterns and a methodology for coping with the above challenges. Results: The case studies show that the proposed approach is applicable for developing real-life systems, and presents measurable advantages over the alternatives — behavioral programming alone and context-oriented programming alone. Conclusion: We present a paradigm allowing programmers and system engineers to capture complex context-dependent requirements and align their code with such requirements."
Tsolakis2022,Ioannis A. Tsolakis and William Papaioannou and Erofili Papadopoulou and Maria Dalampira and Apostolos I. Tsolakis,Comparison in Terms of Accuracy between DLP and LCD Printing Technology for Dental Model Printing,Dentistry Journal,10,10,2022,10.3390/dj10100181,23046767,"Background: The aim of this study is to evaluate the accuracy of a Liquid Crystal Display (LCD) 3D printer compared to a Direct Light Processing (DLP) 3D printer for dental model printing. Methods: Two different printers in terms of 3D printing technology were used in this study. One was a DLP 3D printer and one an LCD 3D printer. The accuracy of the printers was evaluated in terms of trueness and precision. Ten STL reference files were used for this study. For trueness, each STL file was printed once with each 3D printer. For precision, one randomly chosen STL file was printed 10 times with each 3D printer. Afterward, the models were scanned with a model scanner, and reverse engineering software was used for the STL comparisons. Results: In terms of trueness, the comparison between the LCD 3D printer and DLP 3D printer was statistically significant, with a p-value = 0.004. For precision, the comparison between the LCD 3D printer and the DLP 3D printer was statistically significant, with a p-value = 0.011. Conclusions: The DLP 3D printer is more accurate in terms of dental model printing than the LCD 3D printer. However, both DLP and LCD printers can accurately be used to print dental models for the fabrication of orthodontic appliances."
Heboyan2022,Artak Heboyan and Roberto Lo Giudice and Les Kalman and Muhammad Sohail Zafar and João Paulo Mendes Tribst,Stress Distribution Pattern in Zygomatic Implants Supporting Different Superstructure Materials,Materials,15,14,2022,10.3390/ma15144953,19961944,"The aim of this study was to assess and compare the stress–strain pattern of zygomatic dental implants supporting different superstructures using 3D finite element analysis (FEA). A model of a tridimensional edentulous maxilla with four dental implants was designed using the computer-aided design (CAD) software. Two standard and two zygomatic implants were positioned to support the U-shaped bar superstructure. In the computer-aided engineering (CAE) software, different materials have been simulated for the superstructure: cobalt–chrome (CoCr) alloy, titanium alloy (Ti), zirconia (Zr), carbon-fiber polymers (CF) and polyetheretherketone (PEEK). An axial load of 500 N was applied in the posterior regions near the zygomatic implants. Considering the mechanical response of the bone tissue, all superstructure materials resulted in homogeneous strain and thus could reconstruct the edentulous maxilla. However, with the aim to reduce the stress in the zygomatic implants and prosthetic screws, stiffer materials, such Zr, CoCr and Ti, appeared to be a preferable option."
DeStefano2020,Manuel De Stefano and Fabiano Pecorelli and Damian A. Tamburri and Fabio Palomba and Andrea De Lucia,Splicing Community Patterns and Smells: A Preliminary Study,,,,2020,10.1145/3387940.3392204,,"Software engineering projects are now more than ever a community effort. In the recent past, researchers have shown that their success may not only depend on source code quality, but also on other aspects like the balance of distance, culture, global engineering practices, and more. In such a scenario, understanding the characteristics of the community around a project and foresee possible problems may be the key to develop successful systems. In this paper, we focus on this research problem and propose an exploratory study on the relation between community patterns, i.e., recurrent mixes of organizational or social structure types, and smells, i.e., sub-optimal patterns across the organizational structure of a software development community that may be precursors of some sort of social debt. We exploit association rule mining to discover frequent relations between them. Our findings show that different organizational patterns are connected to different forms of socio-technical problems, possibly suggesting that practitioners should put in place specific preventive actions aimed at avoiding the emergence of community smells depending on the organization of the project."
Stefanovic2021,Sandra Stefanovic and Elena Klochkova,Digitalisation of teaching and learning as a tool for increasing students’ satisfaction and educational efficiency: Using smart platforms in efl,Sustainability (Switzerland),13,9,2021,10.3390/su13094892,20711050,"This manuscript aims to present possibilities for developing mobile and smart platforms and systems in teaching and learning the English language for engineering professionals in different engineering study programs. Foreign language teaching and learning processes are based on tradi-tional methods, while in engineering and technical sciences, teaching and learning processes include different digital platforms. Therefore, the following hypotheses were stated. (H1) It is possible to develop a software solution for mobile platforms that can have a higher level of interactivity, and it may lead to better learning outcomes, especially in the field of adopting engineering vocabulary. (H2 ) Implementation of the developed solution increases motivation for learning and leads to a higher level of satisfaction with the learning process as a part of the quality of life. (H3 ) Students who have digital and mobile platforms in the learning process could have higher achievement values. This manuscript presents software application development and its implementation in teaching English as a foreign language for engineering and technical study programs on the bachelor level. Initial results in implementation and satisfaction of end users point to the justification of implementing such solutions."
Hu2022,Xing Hu and Xin Xia and David Lo and Zhiyuan Wan and Qiuyuan Chen and Thomas Zimmermann,Practitioners' Expectations on Automated Code Comment Generation,,2022-May,,2022,10.1145/3510003.3510152,02705257,"Good comments are invaluable assets to software projects, as they help developers understand and maintain projects. However, due to some poor commenting practices, comments are often missing or inconsistent with the source code. Software engineering practitioners often spend a significant amount of time and effort reading and understanding programs without or with poor comments. To counter this, researchers have proposed various techniques to au-tomatically generate code comments in recent years, which can not only save developers time writing comments but also help them better understand existing software projects. However, it is unclear whether these techniques can alleviate comment issues and whether practitioners appreciate this line of research. To fill this gap, we performed an empirical study by interviewing and surveying practitioners about their expectations of research in code comment generation. We then compared what practitioners need and the current state-of-the-art research by performing a literature review of papers on code comment generation techniques pub-lished in the premier publication venues from 2010 to 2020. From this comparison, we highlighted the directions where researchers need to put effort to develop comment generation techniques that matter to practitioners."
Heeman2022,Wido Heeman and Jasper Vonk and Vasilis Ntziachristos and Brian W. Pogue and Rudi A.J.O. Dierckx and Schelto Kruijff and Gooitzen M. Van Dam,A Guideline for Clinicians Performing Clinical Studies with Fluorescence Imaging,Journal of Nuclear Medicine,63,5,2022,10.2967/jnumed.121.262975,2159662X,"Fluorescence imaging is an emerging imaging technique that has shown many benefits for clinical care. Currently, the field is in rapid clinical translation, and an unprecedented number of clinical trials are performed. Clinicians are inundated with numerous opportunities and combinations of different imaging modalities. To streamline this process, a multidisciplinary approach is needed with drug discovery, software and systems engineering, and translational medicine. Here, we discuss the main constituents of a uniform fluorescence imaging protocol to match the clinical need and ensure consistent study designs and reliable data collection in clinical trials. In an era in which the potential of fluorescence imaging has become evident, consistent conduct of studies, data analysis, and data interpretation is essential for implementation into the standard of care. COPYRIGHT"
Rafique2020,Wajid Rafique and Wanchun Dou and Khalid Hussain and Khurshid Ahmed,Factors influencing programming expertise in a web-based e-learning paradigm,Online Learning Journal,24,1,2020,10.24059/olj.v24i1.1956,24725730,"Modern internet technologies have revolutionized traditional education by providing flexible and resourceful e-learning opportunities in all fields of life. Programming is an integral part of the undergraduate curriculum in computer sciences where an adequate level of programming expertise is expected from the graduates. In this paper, we explore and examine the key factors that contribute to developing programming skills among undergraduate students in e-learning. We propose that programming education follows the Technology Acceptance Model (TAM), which affects the students’ attitude toward learning. We extend the TAM by integrating the factors of teaching practices, intrinsic factors, perceived usefulness, and efficacy problems with the learning intentions in our research framework. This research involves the responses of the 460 final year students studying for a Bachelor of Computer Science and Software Engineering at an e-learning institution. Structural Equation Modelling (SEM) and Confirmatory Factor Analysis (CFA) have been employed to evaluate the relationship between factors of the model. Experimental results demonstrate that teaching practices, intrinsic factors, and perceived usefulness play a key role in endorsing learning intentions in the students. Further analysis reveals that learning intentions positively influence the programming expertise whereas an adverse impact has been observed from the efficacy problems. The results proclaim that perceived usefulness, teaching practices, and intrinsic factors develop adequate learning intentions in the students which overcome the efficacy problems and lead to better programming expertise. This research provides critical implications for policymakers to effectively implement computer science programs in an e-learning paradigm."
Xie2020,Chunli Xie and Xia Wang and Cheng Qian and Mengqi Wang,A source code similarity based on Siamese neural network,Applied Sciences (Switzerland),10,21,2020,10.3390/app10217519,20763417,"Finding similar code snippets is a fundamental task in the field of software engineering. Several approaches have been proposed for this task by using statistical language model which focuses on syntax and structure of codes rather than deep semantic information underlying codes. In this paper, a Siamese Neural Network is proposed that maps codes into continuous space vectors and try to capture their semantic meaning. Firstly, an unsupervised pre-trained method that models code snippets as a weighted series of word vectors. The weights of the series are fitted by the Term Frequency-Inverse Document Frequency (TF-IDF). Then, a Siamese Neural Network trained model is constructed to learn semantic vector representation of code snippets. Finally, the cosine similarity is provided to measure the similarity score between pairs of code snippets. Moreover, we have implemented our approach on a dataset of functionally similar code. The experimental results show that our method improves some performance over single word embedding method."
Tao2020,Hongwei Tao and Yixiang Chen and Hengyang Wu,A reallocation approach for software trustworthiness based on trustworthy attributes,Mathematics,8,1,2020,10.3390/math8010014,22277390,"Software trustworthiness is an important research field in software engineering. In order to appropriately evaluate it, some different measurement approaches have been proposed, which have important guiding significance for improving software trustworthiness. Recently, we have investigated attributes-based approaches. That is, how to maximize trustworthy degree of some software satisfying a given threshold by adjusting every attribute value such that the cost is minimal, i.e., the sum of all attribute values is as small as possible. The work is helpful to improve the software quality under the same cost. This paper continues this work and considers a reallocation approach to dealing with the problem that the threshold and the minimal constraints of every attribute values dynamically increase. In this process, the costs of trustworthiness improvement should be ensured to be minimal. For this purpose, we firstly define a reallocation model by mathematical programming. Then we introduce the notion of growth function. Based on this, a polynomial reallocation algorithm is designed to solve the above reallocation model. Finally, we verify our work on spacecraft softwares and the results show that this work is valid."
Kang2020,Sungmin Kang and Robert Feldt and Shin Yoo,SINVAD: Search-based Image Space Navigation for DNN Image Classifier Test Input Generation,,,,2020,10.1145/3387940.3391456,,"The testing of Deep Neural Networks (DNNs) has become increasingly important as DNNs are widely adopted by safety critical systems. While many test adequacy criteria have been suggested, automated test input generation for many types of DNNs remains a challenge because the raw input space is too large to randomly sample or to navigate and search for plausible inputs. Consequently, current testing techniques for DNNs depend on small local perturbations to existing inputs, based on the metamorphic testing principle. We propose new ways to search not over the entire image space, but rather over a plausible input space that resembles the true training distribution. This space is constructed using Variational Autoencoders (VAEs), and navigated through their latent vector space. We show that this space helps efficiently produce test inputs that can reveal information about the robustness of DNNs when dealing with realistic tests, opening the field to meaningful exploration through the space of highly structured images."
Zhang2020,Yuwei Zhang and Dahai Jin and Ying Xing and Yunzhan Gong,Automated defect identification via path analysis-based features with transfer learning,Journal of Systems and Software,166,,2020,10.1016/j.jss.2020.110585,01641212,"Recently, artificial intelligence techniques have been widely applied to address various specialized tasks in software engineering, such as code generation, defect identification, and bug repair. Despite the diffuse usage of static analysis tools in automatically detecting potential software defects, developers consider the large number of reported alarms and the expensive cost of manual inspection to be a key barrier to using them in practice. To automate the process of defect identification, researchers utilize machine learning algorithms with a set of hand-engineered features to build classification models for identifying alarms as actionable or unactionable. However, traditional features often fail to represent the deep syntactic structure of alarms. To bridge the gap between programs’ syntactic structure and defect identification features, this paper first extracts a set of novel fine-grained features at variable-level, called path-variable characteristic, by applying path analysis techniques in the feature extraction process. We then raise a two-stage transfer learning approach based on our proposed features, called feature ranking-matching based transfer learning, to increase the performance of cross-project defect identification. Our experimental results for eight open-source projects show that the proposed features at variable-level are promising and can yield significant improvement on both within-project and cross-project defect identification."
Tocto-Cano2020,Esteban Tocto-Cano and Sandro Paz Collado and Javier Linkolk López-Gonzales and Josué E. Turpo-Chaparro,A systematic review of the application of maturity models in universities,Information (Switzerland),11,10,2020,10.3390/info11100466,20782489,"A maturity model is a widely used tool in software engineering and has mostly been extended to domains such as education, health, energy, finance, government, and general use. It is valuable for evaluations and continuous improvement of business processes or certain aspects of organizations, as it represents a more organized and systematic way of doing business. In this paper, we only focus on college higher education. For this reason, we present a novel approach that allows detecting some gaps in the existing maturity models for universities, as they are not models that address the dimensions in their entirety. To identify these models and their validities, as well as a classification of models that were identified in universities, we carried out a systematic literature review on 27,289 articles retrieved with respect to maturity models and published in peer-reviewed journals between 2007 and 2020. We found 23 articles that find maturity models applied in universities, through exclusion and inclusion criteria. We then grouped these items into nine categories with specific purposes. We concluded that maturity models used in Universities move towards agility, which is supported by the semantic web."
Hugues2020,Jerome Hugues and Anton Hristosov and John J. Hudak and Joe Yankel,TwinOps - DevOps meets model-based engineering and digital twins for the engineering of CPS,,,,2020,10.1145/3417990.3421446,,"The engineering of Cyber-Physical Systems (CPS) requires a large set of expertise to capture the system requirements and to derive a correct solution. Model-based Engineering and DevOps aim to efficiently deliver software with increased quality. Model-based Engineering relies on models as first-class artifacts to analyze, simulate, and ultimately generate parts of a system. DevOps focuses on software engineering activities, from early development to integration, and then improvement through the monitoring of the system at run-time. We claim these can be efficiently combined to improve the engineering process of CPS. In this paper, we present TwinOps, a process that unifies Model-based Engineering, Digital Twins, and DevOps practice in a uniform workflow. TwinOps illustrates how to leverage several best practices in MBE and DevOps for the engineering Cyber-Physical systems. We illustrate our contribution using a Digital Twins case study to illustrate TwinOps benefits, combining AADL and Modelica models, and an IoT platform."
Fekete2020,Alexander Fekete and Jakob Rhyner,Sustainable digital transformation of disaster risk—integrating new types of digital social vulnerability and interdependencies with critical infrastructure,Sustainability (Switzerland),12,22,2020,10.3390/su12229324,20711050,"This article explores the relationship between digital transformation and disaster risk. Vulnerability studies aim at differentiating impacts and losses by using fine-grained information from demographic, social, and personal characteristics of humans. With ongoing digital development, these characteristics will transform and result in new traits, which need to be identified and integrated. Digital transformations will produce new social groups, partly human, semi-human, or non-human—some of which already exist, and some which can be foreseen by extrapolating from recent developments in the field of brain wearables, robotics, and software engineering. Though involved in the process of digital transformation, many researchers and practitioners in the field of Disaster Risk Reduction or Climate Change Adaptation are not yet aware of the repercussions for disaster and vulnerability assessments. Emerging vulnerabilities are due to a growing dependency on digital services and tools in the case of a severe emergency or crisis. This article depicts the different implications for future theoretical frameworks when identifying novel semi-human groups and their vulnerabilities to disaster risks. Findings include assumed changes within common indicators of social vulnerability, new indicators, a typology of humans, and human interrelations with digital extensions and two different perspectives on these groups and their dependencies with critical infrastructure."
Snchez-Gordn2020,Mary Sánchez-Gordón and Laxmi Rijal and Ricardo Colomo-Palacios,Beyond Technical Skills in Software Testing: Automated versus Manual Testing,,,,2020,10.1145/3387940.3392238,,"Software testing is not a purely technical, but rather socio-technical activity. Although there are a few studies on this topic, to the best of our knowledge there is a lack of research focusing specifically on skills, in particular soft skills needed for automated and manual testing. In both cases, software testing is a challenging task that requires considerable effort by practitioners. The aim of this study is to identify what are the most valued skills with regards to these different types of testing. To do so, a survey was applied among software practitioners and 72 responses were received. The questionnaire covers 35 skills grouped in technical (hard) and non-technical (soft) skills. The results of this exploratory study provide empirical evidence that reveals the importance that software practitioners give to hard and soft skills alike."
Simmons2020,Andrew J. Simmons and Scott Barnett and Jessica Rivera-Villicana and Akshat Bajaj and Rajesh Vasa,A large-scale comparative analysis of Coding Standard conformance in Open-Source Data Science projects,,,,2020,10.1145/3382494.3410680,19493789,"Background: Meeting the growing industry demand for Data Science requires cross-disciplinary teams that can translate machine learning research into production-ready code. Software engineering teams value adherence to coding standards as an indication of code readability, maintainability, and developer expertise. However, there are no large-scale empirical studies of coding standards focused specifically on Data Science projects. Aims: This study investigates the extent to which Data Science projects follow code standards. In particular, which standards are followed, which are ignored, and how does this differ to traditional software projects? Method: We compare a corpus of 1048 Open-Source Data Science projects to a reference group of 1099 non-Data Science projects with a similar level of quality and maturity. Results: Data Science projects suffer from a significantly higher rate of functions that use an excessive numbers of parameters and local variables. Data Science projects also follow different variable naming conventions to non-Data Science projects. Conclusions: The differences indicate that Data Science codebases are distinct from traditional software codebases and do not follow traditional software engineering conventions. Our conjecture is that this may be because traditional software engineering conventions are inappropriate in the context of Data Science projects."
Liu2022,Zhe Liu and Chunyang Chen and Junjie Wang and Yuekai Huang and Jun Hu and Qing Wang,Guided Bug Crush: Assist Manual GUI Testing of Android Apps via Hint Moves,,,,2022,10.1145/3491102.3501903,,"Mobile apps are indispensable for people's daily life. Complementing with automated GUI testing, manual testing is the last line of defence for app quality. However, the repeated actions and easily missing of functionalities make manual testing time-consuming and inefficient. Inspired by the game candy crush with flashy candies as hint moves for players, we propose an approach named NaviDroid for navigating testers via highlighted next operations for more effective and efficient testing. Within NaviDroid, we construct an enriched state transition graph with the triggering actions as the edges for two involved states. Based on it, we utilize the dynamic programming algorithm to plan the exploration path, and augment the GUI with visualized hints for testers to quickly explore untested activities and avoid duplicate explorations. The automated experiments demonstrate the high coverage and efficient path planning of NaviDroid and a user study further confirms its usefulness. The NaviDroid can help us develop more robust software that works in more mission-critical settings, not only by performing more thorough testing with the same effort that has been put in before, but also by integrating these techniques into different parts of development pipeline."
Geiger2021,R. Stuart Geiger and Dorothy Howard and Lilly Irani,The Labor of Maintaining and Scaling Free and Open-Source Software Projects,Proceedings of the ACM on Human-Computer Interaction,5,CSCW1,2021,10.1145/3449249,25730142,"Free and/or open-source software (or F/OSS) projects now play a major and dominant role in society, constituting critical digital infrastructure relied upon by companies, academics, non-profits, activists, and more. As F/OSS has become larger and more established, we investigate the labor of maintaining and sustaining those projects at various scales. We report findings from an interview-based study with contributors and maintainers working in a wide range of F/OSS projects. Maintainers of F/OSS projects do not just maintain software code in a more traditional software engineering understanding of the term: fixing bugs, patching security vulnerabilities, and updating dependencies. F/OSS maintainers also perform complex and often-invisible interpersonal and organizational work to keep their projects operating as active communities of users and contributors. We particularly focus on how this labor of maintaining and sustaining changes as projects and their software grow and scale across many dimensions. In understanding F/OSS to be as much about maintaining a communal project as it is maintaining software code, we discuss broadly applicable considerations for peer production communities and other socio-technical systems more broadly."
Kanbach2024,Dominik K. Kanbach and Louisa Heiduk and Georg Blueher and Maximilian Schreiter and Alexander Lahmann,The GenAI is out of the bottle: generative artificial intelligence from a business model innovation perspective,Review of Managerial Science,18,4,2024,10.1007/s11846-023-00696-z,18636691,"The introduction of ChatGPT in November 2022 by OpenAI has stimulated substantial discourse on the implementation of artificial intelligence (AI) in various domains such as academia, business, and society at large. Although AI has been utilized in numerous areas for several years, the emergence of generative AI (GAI) applications such as ChatGPT, Jasper, or DALL-E are considered a breakthrough for the acceleration of AI technology due to their ease of use, intuitive interface, and performance. With GAI, it is possible to create a variety of content such as texts, images, audio, code, and even videos. This creates a variety of implications for businesses requiring a deeper examination, including an influence on business model innovation (BMI). Therefore, this study provides a BMI perspective on GAI with two primary contributions: (1) The development of six comprehensive propositions outlining the impact of GAI on businesses, and (2) the discussion of three industry examples, specifically software engineering, healthcare, and financial services. This study employs a qualitative content analysis using a scoping review methodology, drawing from a wide-ranging sample of 513 data points. These include academic publications, company reports, and public information such as press releases, news articles, interviews, and podcasts. The study thus contributes to the growing academic discourse in management research concerning AI's potential impact and offers practical insights into how to utilize this technology to develop new or improve existing business models."
Abraho2021,Silvia Abrahão and Emilio Insfran and Arthur Sluÿters and Jean Vanderdonckt,Model-based intelligent user interface adaptation: challenges and future directions,Software and Systems Modeling,20,5,2021,10.1007/s10270-021-00909-7,16191374,"Adapting the user interface of a software system to the requirements of the context of use continues to be a major challenge, particularly when users become more demanding in terms of adaptation quality. A considerable number of methods have, over the past three decades, provided some form of modelling with which to support user interface adaptation. There is, however, a crucial issue as regards in analysing the concepts, the underlying knowledge, and the user experience afforded by these methods as regards comparing their benefits and shortcomings. These methods are so numerous that positioning a new method in the state of the art is challenging. This paper, therefore, defines a conceptual reference framework for intelligent user interface adaptation containing a set of conceptual adaptation properties that are useful for model-based user interface adaptation. The objective of this set of properties is to understand any method, to compare various methods and to generate new ideas for adaptation. We also analyse the opportunities that machine learning techniques could provide for data processing and analysis in this context, and identify some open challenges in order to guarantee an appropriate user experience for end-users. The relevant literature and our experience in research and industrial collaboration have been used as the basis on which to propose future directions in which these challenges can be addressed."
Laaber2021,Christoph Laaber and Mikael Basmaci and Pasquale Salza,Predicting unstable software benchmarks using static source code features,Empirical Software Engineering,26,6,2021,10.1007/s10664-021-09996-y,15737616,"Software benchmarks are only as good as the performance measurements they yield. Unstable benchmarks show high variability among repeated measurements, which causes uncertainty about the actual performance and complicates reliable change assessment. However, if a benchmark is stable or unstable only becomes evident after it has been executed and its results are available. In this paper, we introduce a machine-learning-based approach to predict a benchmark’s stability without having to execute it. Our approach relies on 58 statically-computed source code features, extracted for benchmark code and code called by a benchmark, related to (1) meta information, e.g., lines of code (LOC), (2) programming language elements, e.g., conditionals or loops, and (3) potentially performance-impacting standard library calls, e.g., file and network input/output (I/O). To assess our approach’s effectiveness, we perform a large-scale experiment on 4,461 Go benchmarks coming from 230 open-source software (OSS) projects. First, we assess the prediction performance of our machine learning models using 11 binary classification algorithms. We find that Random Forest performs best with good prediction performance from 0.79 to 0.90, and 0.43 to 0.68, in terms of AUC and MCC, respectively. Second, we perform feature importance analyses for individual features and feature categories. We find that 7 features related to meta-information, slice usage, nested loops, and synchronization application programming interfaces (APIs) are individually important for good predictions; and that the combination of all features of the called source code is paramount for our model, while the combination of features of the benchmark itself is less important. Our results show that although benchmark stability is affected by more than just the source code, we can effectively utilize machine learning models to predict whether a benchmark will be stable or not ahead of execution. This enables spending precious testing time on reliable benchmarks, supporting developers to identify unstable benchmarks during development, allowing unstable benchmarks to be repeated more often, estimating stability in scenarios where repeated benchmark execution is infeasible or impossible, and warning developers if new benchmarks or existing benchmarks executed in new environments will be unstable."
Nair2020,Aravind Nair and Avijit Roy and Karl Meinke,FuncGNN: A graph neural network approach to program similarity,,,,2020,10.1145/3382494.3410675,19493789,"Background: Program similarity is a fundamental concept, central to the solution of software engineering tasks such as software plagiarism, clone identification, code refactoring and code search. Accurate similarity estimation between programs requires an in-depth understanding of their structure, semantics and flow. A control flow graph (CFG), is a graphical representation of a program which captures its logical control flow and hence its semantics. A common approach is to estimate program similarity by analysing CFGs using graph similarity measures, e.g. graph edit distance (GED). However, graph edit distance is an NP-hard problem and computationally expensive, making the application of graph similarity techniques to complex software programs impractical. Aim: This study intends to examine the effectiveness of graph neural networks to estimate program similarity, by analysing the associated control flow graphs. Method: We introduce funcGNN1, which is a graph neural network trained on labeled CFG pairs to predict the GED between unseen program pairs by utilizing an effective embedding vector. To our knowledge, this is the first time graph neural networks have been applied on labeled CFGs for estimating the similarity between highlevel language programs. Results:We demonstrate the effectiveness of funcGNN to estimate the GED between programs and our experimental analysis demonstrates how it achieves a lower error rate (1.94 ×10-3), with faster (23 times faster than the quickest traditional GED approximation method) and better scalability compared with state of the art methods. Conclusion: funcGNN posses the inductive learning ability to infer program structure and generalise to unseen programs. The graph embedding of a program proposed by our methodology could be applied to several related software engineering problems (such as code plagiarism and clone identification) thus opening multiple research directions."
Giaimo2020,Federico Giaimo and Hugo Andrade and Christian Berger,Continuous experimentation and the cyber–physical systems challenge: An overview of the literature and the industrial perspective,Journal of Systems and Software,170,,2020,10.1016/j.jss.2020.110781,01641212,"Context: New software development patterns are emerging aiming at accelerating the process of delivering value. One is Continuous Experimentation, which allows to systematically deploy and run instrumented software variants during development phase in order to collect data from the field of application. While currently this practice is used on a daily basis on web-based systems, technical difficulties challenge its adoption in fields where computational resources are constrained, e.g., cyber–physical systems and the automotive industry. Objective: This paper aims at providing an overview of the engagement on the Continuous Experimentation practice in the context of cyber–physical systems. Method: A systematic literature review has been conducted to investigate the link between the practice and the field of application. Additionally, an industrial multiple case study is reported. Results: The study presents the current state-of-the-art regarding Continuous Experimentation in the field of cyber–physical systems. The current perspective of Continuous Experimentation in industry is also reported. Conclusions: The field has not reached maturity yet. More conceptual analyses are found than solution proposals and the state-of-practice is yet to be achieved. However it is expected that in time an increasing number of solutions will be proposed and validated."
Naqvi2022,Muhammad Raza Naqvi and Muhammad Waseem Iqbal and Muhammad Usman Ashraf and Shafiq Ahmad and Ahmed T. Soliman and Shahzada Khurram and Muhammad Shafiq and Jin Ghoo Choi,Ontology driven testing strategies for IoT applications,"Computers, Materials and Continua",70,3,2022,10.32604/cmc.2022.019188,15462226,"Internet-of-Things (IoT) has attained a major share in embedded software development. The new era of specialized intelligent systems requires adaptation of customized software engineering approaches. Currently, software engineering has merged the development phases with the technologies provided by industrial automation. The improvements are still required in testing phase for the software developed to IoT solutions. This research aims to assist in developing the testing strategies for IoT applications, therein ontology has been adopted as a knowledge representation technique to different software engineering processes. The proposed ontological model renders 101 methodology by using Protégé. After completion, the ontology was evaluated in three-dimensional view by the domain experts of software testing, IoT and ontology engineering. Satisfied results of the research are showed in interest of the specialists regarding proposed ontology development and suggestions for improvements. The Proposed reasoning-based ontological model for development of testing strategies in IoT application contributes to increase the general understanding of tests in addition to assisting for the development of testing strategies for different IoT devices."
Monney2020,Isaac Monney and Emmanuel Amponsah Donkor and Richard Buamah,"Clean vehicles, polluted waters: empirical estimates of water consumption and pollution loads of the carwash industry",Heliyon,6,5,2020,10.1016/j.heliyon.2020.e03952,24058440,"Carwash stations use large volumes of water and release harmful chemicals into the environment through their operations. While a significant body of literature has focused on exploring water use in the carwash industry, none has provided comprehensive information on both the pollution loads of the wastewater emanating from this industry and water consumption. Understanding how much water is used and the pollution loads of wastewater from this industry is useful to ensure adoption of water conservation measures and design wastewater recycling systems given the dwindling freshwater resources globally. This study estimated the freshwater quantities used to wash different vehicle types and the pollution loads of the resulting wastewater in the Kumasi Metropolis. Seven proxy carwash stations were purposively selected and monitored to estimate the water used to wash six different categories of vehicles. Composite wastewater samples from three carwash stations were analysed for concentrations of different contaminants which were used to compute pollution loads. Using R software, one-way ANOVA with Tukey's (HSD) post-hoc testing and 2-sample t-test at 95% confidence interval were employed to test statistical differences. After an 8-week monitoring campaign involving 3,667 vehicles, the study showed that average water used for each vehicle type were in the order: Motorbike - 97L (95% CI: 90–103L); Salon car - 158L (95% CI: 154–161L); SUV - 197L (95% CI:191–203L); Buses/Coaches - 370L (95%CI:351–381L); Articulated truck 1,139L (95% CI:916–1,363L); Graders/Loaders - 1405L (95% CI:327–2,483L). Overall, the carwash industry in the Metropolis uses about 1000m3 of freshwater daily and discharges the resulting wastewater into waterways untreated. The wastewater has a low Biodegradability Index (0.3–0.4) and is characterized by a mildly alkali pH (7.6–8.6) with high levels of Sulphates (40.8–69.8 mg/L), COD (990–1413 mg/L), TSS (1260–3417 mg/L) and E. coli (2.3–4.7 × 103 CFU/100mL). Pollution loads of BOD and COD were up to 2tons/year and 6tons/year respectively. Stipulated effluent discharge guideline values were mostly exceeded – in some cases by up to 68 times. To avert the unbridled wastage of freshwater, the study recommends enforcement of wastewater recycling for all carwash stations and promulgation of a tax system that rewards stations that recycle wastewater and surchages those wasting freshwater."
Shmatko2021,Alexey Shmatko and Sergey Barykin and Sergey Sergeev and Anuphat Thirakulwanich,Modeling a logistics hub using the digital footprint method—the implication for open innovation engineering,"Journal of Open Innovation: Technology, Market, and Complexity",7,1,2021,10.3390/joitmc7010059,21998531,"Optimizing the cargo flows through the nodes of a digital transport corridor is a crucial problem; solving it allows to introduce modern management methods in logistics. This study examines the optimization of the technology of a distribution center as the base node of a third-party logistics network operator. Our objective consisted in theoretically substantiating the application of mathematical formalisms to describing the passage of stochastic goods flows of a complex structure through a node of a logistics network. To solve the problem, we constructed a mathematical model intended as a decision-making block in the Warehouse Management System software. This paper presents the results of calculations carried out using a computer according to the proposed algorithm. Preliminary results allow to conclude that there is a significant resource for reducing construction and maintenance costs of the distribution center. Quantitatively, the savings will range from 10% to 40%, depending on the terms of delivery of goods and the degree of market uncertainty. An example of a practical calculation using the developed mathematical model is given. The calculations were performed for Huawei, a key global provider of ICT infrastructure and smart terminals. The algorithms were designed to account for uncertainty, which allows to use the results for risk management applications. In practice, this theoretical concept can serve as a basis for digital logistics platforms, increasing the speed of delivery of goods and cargo and the profitability of logistics as a result. The solution to this problem will allow to consider the nodes of logistics networks as smart independent divisions with an information interface for interaction built into a digital logistics platform. The role of these nodes is to make the best decisions in handling cargo flows. As the next stage of research, we plan to develop algorithms for coordinating the information flows reflecting transport activities and forming the material flows entering and leaving the distribution centers."
Nigischer2021,Christian Nigischer and Sébastien Bougain and Rainer Riegler and Heinz Peter Stanek and Manfred Grafinger,Multi-domain simulation utilizing SysML: State of the art and future perspectives,,100,,2021,10.1016/j.procir.2021.05.073,22128271,"Increasing system complexity requires adapted methodologies to tackle the challenges that come along with multi-domain systems development. Model-Based Systems Engineering (MBSE) provides significant support by using models to describe different aspects of an examined system already in the early stages of the development process. The Systems Modeling Language (SysML) can be utilized to establish a common system information basis on a comparatively abstract level for all participating stakeholders. Although SysML models are able to encompass system information like requirements, structure, behaviour and parametrics, in many cases additional specialized simulation or computation models are needed, especially for early design decision-making and verification activities. Ideally, the input data and the calculated results are automatically exchanged between the SysML modelling editor and the involved simulation tools. Hence, tool chain integration is a crucial factor to provide the necessary interconnectivity to achieve multi-domain simulation. In general, the integration of simulation environments with SysML tools is mainly limited by two factors. First, as the tools available for modelling with SysML are legion, interface solutions are usually developed for a specific tool and therefore, their usage is restricted to that particular software. Second, evolving standards like the Functional Mock-up Interface (FMI) provide capabilities to ease model data exchange and co-simulation, but the implementation of different versions of various standards in the tools causes incompatibilities. In this work a state of the art of the integration between simulation environments and available SysML modelling tools with respect to utilized standards and their implementation maturity is presented. Additionally, the capabilities of the reviewed tools to support co-simulation are evaluated. Furthermore, existing challenges are highlighted, and potential improvements are discussed."
Andrzejewski2020,Grzegorz Andrzejewski and Wojciech Zajac and Kazimierz Krzywicki and Tomasz Królikowski,On some aspects of Concurrent Control Processes Modelling and Implementation in LAD Diagram Language with Use of New Generation Engineering Software,,176,,2020,10.1016/j.procs.2020.09.254,18770509,"In the paper there is discussed the problem of constructing and implementation the mathematical models for control algorithms of concurrent processes. For the control object example there was taken an example of 3D Cartesian machine. The control algorithm is mathematically described with use of Finite State Machine methodology in canonical and concurrent variants. Implementation examples are given, problem of resources consumption and operation time is discussed and the conclusion is d drawn."
Waqas2021,Hassan Waqas and Shan Ali Khan and Taseer Muhammad and Syed Muhammad Raza Shah Naqvi,Heat transfer enhancement in stagnation point flow of ferro-copper oxide/water hybrid nanofluid: A special case study,Case Studies in Thermal Engineering,28,,2021,10.1016/j.csite.2021.101615,2214157X,"This present work is to investigate the behavior of Fe3O4−CuO/H2O hybrid-type nanofluid flow toward a magnetic field for enhancing the thermal transfer by horizontal stretchable surface. In the base fluid different tiny-sized nanoparticles collides and enhanced the heat transformation in the absence of magnetic field. Here the combinations of Fe3O4−CuO/H2O as hybrid nanofluids and CuO/H2O as nanofluid are implemented. The Ferro and Copper oxide are dispersed in water base fluid. Such Ferro nanomaterials are more fruitful in production technology, protein absorption, catalysis, medical technology and environment. The implementation of Fe3O4 NPs in medical science mainly involves targeted drug/gene delivery, biosensor, magnetic resonance imaging (MRI), contrast improvement and hyperthermia, biophotonics, detection of cancer cells, diagnosis and magnetic field-assisted radiotherapy and tissue engineering. Copper oxide used as a catalyst for increasing the rate of combustion in rocket propellant. It may significantly increase the homogenous propellant burning rate, lower the pressure index, and boost the effectiveness of the AP composite propellant as a catalyst. In additional, the governing coupled dimensional equations are reduced into the ordinary once with associated the suitable similarities. The resultant dimension-less expressions are programmed in the computational MATLAB software via bvp4c solver with shooting scheme. The important outcomes of this investigation are the behavior of numerous good parameters including stretching ratio parameterA∗, magnetic parameterβ, volumetric frictions for nanoparticlesφ, suction/injection parameterfw and thermal radiation parameterRd. The physical behavior of above discussed sundry parameters is described through figures. The dispersion of Fe3O4 and CuO in water-based fluid significantly improved the phenomenon of heat transfer. Results found that the velocity field is escalates when the stretching ratio parameter is enhanced. Furthermore, larger values of the suction/injection parameter causes a reduction in velocity profile for A<1 and A>1. The induced magnetic field is improved with larger estimations of the nanoparticle fraction. Thermal field is escalates by growing thermal radiation parameter and Eckert number. It is further observed that temperature profile is reduces by increasing stretching ratio parameter. From the analysis we noted that skin friction is raises via larger estimations of magnetic parameter."
Harten2020,Paul Harten and Todd Martin and Michael Gonzalez and Douglas Young,"The software tool to find greener solvent replacements, PARIS III",Environmental Progress and Sustainable Energy,39,1,2020,10.1002/ep.13331,19447450,"PARIS III (Program for Assisting the Replacement of Industrial Solvents III, Version 1.4.0) is a pollution prevention solvent substitution software tool used to find mixtures of solvents that are less harmful to the environment than the industrial solvents to be replaced. By searching extensively though hundreds of millions of possible solvent combinations, mixtures that perform the same as the original solvents may be found. Greener solvent substitutes may then be chosen from those mixtures that behave similarly but have less environmental impact. These extensive searches may be enhanced by fine-tuning impact weighting factors to better reflect regional environmental concerns; and by adjusting how close the properties of the replacement must be to those of the original solvent. Optimal replacements can then be compared again and selected for better performance, but less environmental impact. This method can be a very effective way of finding greener replacements for harmful solvents used by industry."
Li2020,Tong Li and Shiheng Wang and David Lillis and Zhen Yang,Combining machine learning and logical reasoning to improve requirements traceability recovery,Applied Sciences (Switzerland),10,20,2020,10.3390/app10207253,20763417,"Maintaining traceability links of software systems is a crucial task for software management and development. Unfortunately, dealing with traceability links are typically taken as afterthought due to time pressure. Some studies attempt to use information retrieval-based methods to automate this task, but they only concentrate on calculating the textual similarity between various software artifacts and do not take into account the properties of such artifacts. In this paper, we propose a novel traceability link recovery approach, which comprehensively measures the similarity between use cases and source code by exploring their particular properties. To this end, we leverage and combine machine learning and logical reasoning techniques. On the one hand, our method extracts features by considering the semantics of the use cases and source code, and uses a classification algorithm to train the classifier. On the other hand, we utilize the relationships between artifacts and define a series of rules to recover traceability links. In particular, we not only leverage source code’s structural information, but also take into account the interrelationships between use cases. We have conducted a series of experiments on multiple datasets to evaluate our approach against existing approaches, the results of which show that our approach is substantially better than other methods."
Mahdi2020,Esraa S. Mahdi and Mohammed Z. Mohamedmeki,Analysis of rainfall intensity-duration-frequency (IDF) curves of Baghdad city,,888,1,2020,10.1088/1757-899X/888/1/012066,1757899X,"An appropriate formula of rainfall data is the Intensity Duration Frequency (IDF) relationship. The rainfall IDF relationship is one of the tools used considerably in water resources engineering, either in planning, designing and operating projects of water resources, or in flood control projects. The purpose of this paper is to update rainfall IDF curves of Baghdad city developed in previously. The frequency analysis of available rainfall data is perform by three statistical methods, namely, the Gumbel Distribution Theory, the Log Pearson Type III and Log Normal Distribution to attain rainfall intensities for various short durations (0.25, 0.5, 1, 2, 3, 6, 12 and 24) in hours and return periods (2, 5, 10, 25, 50 and 100) in years. IDF equation was derived based on Bernard equation and the results of three distributions were compared using Kolmogorov-Smirnov goodness of fit test with help of the Easy fit software 5.6. The results of three methods were close and accepted at 10% significant level. The maximum rainfall intensity of 118.052mm/hr was happened at the duration 0.25hr of the return period 100yr, whereas the minimum rainfall intensity of 1.257mm/hr was happened at the duration 24hr of the return period 2yr."
Gabry2020,Marta Gabryś and Lukasz Ortyl,Georeferencing of multi-channel GPR-accuracy and efficiency of mapping of underground utility networks,Remote Sensing,12,18,2020,10.3390/RS12182945,20724292,"Due to the capabilities of non-destructive testing of inaccessible objects, GPR (Ground Penetrating Radar) is used in geology, archeology, forensics and increasingly also in engineering tasks. The wide range of applications of the GPR method has been provided by the use of advanced technological solutions by equipment manufacturers, including multi-channel units. The acquisition of data along several profiles simultaneously allows time to be saved and quasi-continuous information to be collected about the subsurface situation. One of the most important aspects of data acquisition systems, including GPR, is the appropriate methodology and accuracy of the geoposition. This publication aims to discuss the results of GPR measurements carried out using the multi-channel Leica Stream C GPR (IDS GeoRadar Srl, Pisa, Italy). The significant results of the test measurement were presented the idea of which was to determine the achievable accuracy depending on the georeferencing method using a GNSS (Global Navigation Satellite System) receiver, also supported by time synchronization PPS (Pulse Per Second) and a total station. Methodology optimization was also an important aspect of the discussed issue, i.e., the effect of dynamic changes in motion trajectory on the positioning accuracy of echograms and their vectorization products was also examined. The standard algorithms developed for the dedicated software were used for post-processing of the coordinates and filtration of echograms, while the vectorization was done manually. The obtained results provided the basis for the confrontation of the material collected in urban conditions with the available cartographic data in terms of the possibility of verifying the actual location of underground utilities. The urban character of the area limited the possibility of the movement of Leica Stream C due to the large size of the instrument, however, it created the opportunity for additional analyses, including the accuracy of different location variants around high-rise buildings or the agreement of the amplitude distribution at the intersection of perpendicular profiles."
Al-Mamoori2020,Sohaib Kareem Al-Mamoori and Laheab A. Jasem Al-Maliki and Ahmed H. Al-Sulttani and Khaled El-Tawil and Hussain M. Hussain and Nadhir Al-Ansari,"Horizontal and Vertical Geotechnical Variations of Soils According to USCS Classification for the City of An-Najaf, Iraq Using GIS",Geotechnical and Geological Engineering,38,2,2020,10.1007/s10706-019-01139-x,15731529,"The unified soil classification system (USCS) first proposed by Casagrande and subsequently developed by the Army Corps of Engineers. It widely used in many building codes and books. An-Najaf city is the most important city in Iraq due to its religious and spiritual value in the Muslim world, so it is fast expanding and continuous developing city in Iraq. The data from 464 boreholes in the study area for depths of 0–26 m have been used. 13 Soil samples were collected from each borehole with 13 depths level (0–26) m with 2 m intervals. The USCS was applied to the soil samples from 13 depth levels borehole. This research aims to create a geodatabase for soil properties for An-Najaf. The ArcGIS 10.5 software was used to interpolate the spatial data to produce 33 geotechnical maps for fine soil, coarse soil and USCS for 13 depth levels. For numerical soil data, Ordinary Kriging has been used for interpolation mapping of Fine and Coarse percentage data for each depth. For non-numerical (nominal) soil data (USCS class), the Indicator Kriging method is used. The results show that the coarse soil occupied 85–95% for depth 0–16 m and consist of (SP, SP-SM, SM) while fine soil occupied 5–15% consisting of (OL, CH, ML) subsequently, this soil when compacted has a permeability of pervious to semi impervious, good shearing strength, low to very low compressibility and acceptable workability as a construction material. The results also show that after 16 m depths until 26 m, the fine soil percentage increased to 40% with a coarse soil percentage of 60%, indicating changes in soil characteristics as the permeability became semi-pervious to impervious, fair shearing strength, medium compressibility and fair workability as a construction material. The study results will provide help and saving time, efforts and money in preliminary engineering designs."
Zadorozhna2021,D. Benavides Zadorozhna and O. Benavides and J. Sierra Grajeda and S. Figueroa Ramirez and L. de la Cruz May,A parametric study of the effect of leading edge spherical tubercle amplitudes on the aerodynamic performance of a 2D wind turbine airfoil at low Reynolds numbers using computational fluid dynamics,Energy Reports,7,,2021,10.1016/j.egyr.2021.06.093,23524847,"The majority of wind power is currently produced on high wind speed sites by large wind turbine, whereas small wind turbines often operate in light wind conditions. Small capacity wind turbines have not received the same engineering attention as their larger counterparts. This is partially due to a number of unique problems that small wind turbines experience. The most relevant are: low operating Reynolds number (Re<500,000), and poor performance at high angles of attack. Low and medium wind speed sites (Class II–IV) are more common than high wind speed sites, meaning there is a large source of energy not being taken advantage of. Several studies have suggested that flow control devices such as the spherical tubercle could be used to increase lift before stall and generate more power in such situations. The aim of this study is to determine the effect of tubercle amplitude on aerodynamic performance of an airfoil at low-Re numbers (Re=300,000&Re=400,000). Three amplitudes were considered in this study: A1=0.005c, A2=0.01c, and A3=0.03c. A detailed 2D simulation study is carried out, using FLUENT (a commercial CFD software) and the TransitionSSTk−ω turbulence model, to obtain aerodynamic coefficients and flow characteristics. Results indicate that small tubercles perform better overall than larger tubercles. The airfoil with the smallest tubercle outperforms the unmodified airfoil at both studied Reynolds numbers at angles of attack 0° – 4 °. Moreover, the airfoil with the largest tubercle outperformed all of the airfoils at an angle of attack of 0° and Re=300,000. The analysis of the aerodynamic coefficients indicates that the improvement of the aerodynamic performance of airfoils with tubercles is due to the reduction of the drag coefficient. Pressure, intermittency and wall shear stress contours suggest that the overall drag reduction is achieved through the decrease of friction drag. The decrease in friction drag is attributed to the thickening of the laminar boundary layer, caused by a more favorable pressure distribution around the airfoils with the aerodynamic improvements. Moreover, the drastic deterioration in aerodynamic performance at higher angles of attack is attributed to the turbulence generated by the tubercles. This study suggests that spherical tubercles could have a potential application in small wind turbines."
Ariyadasa2020,Subhash Ariyadasa and Subha Fernando and Shantha Fernando,Detecting phishing attacks using a combined model of LSTM and CNN,International Journal of Advanced and Applied Sciences,7,7,2020,10.21833/ijaas.2020.07.007,23133724,"Phishing, a social engineering crime which has been existing for more than two decades, has gained significant research attention to find better solutions to face against the very dynamic strategies of phishing. The financial sector is the primary target of phishing, and there are many different approaches to combat phishing attacks. Software-based detection approaches are more prominent in phishing detection; however, still, there is no robust solution that can stable for a long period. The primary purpose of this paper is to propose a novel solution to detect phishing attacks using a combined model of LSTM and CNN deep networks with the use of both URLs and HTML pages. The URLs are learned using an LSTM network with 1D convolutional, and another 1D convolutional network is used to learn the HTML features. These two networks were trained separately and combined through a sigmoid layer by dropping the last layer of each model to have the proposed model. The proposed model reached 98.34% in terms of accuracy, and that is above the previously recorded highest accuracy of 97.3% among the detection models used both URL and HTML features in the explored literature. The solution requires feature extraction only with HTML pages, and URLs were directly fed with a minimum pre-processing. Although the proposed solution uses extracted HTML features, those do not depend on third-party services. Therefore, an efficient real-time application can be implemented using the proposed model to detect phishing attacks to safeguard Internet users."
Wang2020,Xin Wang and Jianwei Jiang and Shengjie Sun and Jianbing Men and Shuyou Wang,Investigation on the spatial distribution characteristics of behind-armor debris formed by the perforation of EFP through steel target,Defence Technology,16,,2020,10.1016/j.dt.2019.05.016,22149147,"The behind-armor debris (BAD) formed by the perforation of an EFP is the main damage factor for the secondary destruction to the behind-armor components. Aiming at investigating the BAD caused by EFP, flash X-ray radiography combined with an experimental witness plate test method was used, and the FEM-SPH adaptive conversion algorithm in LS-DYNA software was employed to model the perforation process. The simulation results of the debris cloud shape and number of debris were in good agreement with the flash X-ray radiographs and perforated holes on the witness plate, respectively. Three-dimensional numerical simulations of EFP's penetration under various impact conditions were conducted. The results show that, an ellipsoidal debris cloud, with the major-to-minor axis radio (a/b) smaller than that caused by shaped charge jets, was formed behind the target. With the increase of target thickness (h) and decrease of impact velocity (v0) and obliquity (θ), the value of a/b decreases. The number of debris ejected from target is significantly higher than that from EFP. Based on the statistical analysis of the spatial distribution of the BAD, An engineering calculation model was established considering the influence of h, v0 and θ. The model can with reasonable accuracy predict the quantity and velocity distribution characteristics of BAD formed by EFP."
Lund2023,Liaquat Ali Lund and Ubaidullah Yashkun and Nehad Ali Shah,Magnetohydrodynamics streamwise and cross flow of hybrid nanofluid along the viscous dissipation effect: Duality and stability,Physics of Fluids,35,2,2023,10.1063/5.0135361,10897666,"One of the most pressing issues in contemporary applied mathematics is the regulation of energy transfer via the application of external forces. The processes of heat transfer are affected by magnetic force, which has many practical uses in industry, engineering, and medicine. This research explores the magnetohydrodynamics (MHD) three-dimensional stable axisymmetric boundary layer over a permeable moving plate, which consists of water as a base liquid and binary distinct nanoparticles to generate a hybrid nanofluid. In all of these, flow beyond the boundary layer area might be calculated by a small crosswise velocity. As a result of its high thermal conductivity, a pair of distinct kinds of nanoparticles have been considered, namely alumina and copper, which are integrated into the base water. The mathematical model is built within a boundary of specified geometry and then converted into a set of ordinary differential equations (ODEs). Resultant ODEs are solved numerically using the technique of three-stage Lobatto IIIa in bvp4c solver in 2017, MATLAB software. Results revealed that two branches exist in certain ranges of moving parameter. The impacts of an increasing physical parameter on profiles of velocities and temperature with skin friction as well as with heat transfer rate are represented in graphs. Furthermore, as the volume fraction of copper increases, so does the skin friction coefficient in the positive direction of λ. The effect of viscous dissipation on the temperature profile in the z-direction has the same rising results as observed in the x-direction. According to the results of the temporal stability analysis, the upper branch is realizable and stable."
Valente2020,Marco Valente and Matteo Sambucci and Abbas Sibai and Ettore Musacchi,Multi-physics analysis for rubber-cement applications in building and architectural fields: A preliminary analysis,Sustainability (Switzerland),12,15,2020,10.3390/su12155993,20711050,"Generally, in most countries, there are no strict regulations regarding tire disposal. Hence, tires end up thrown in seas and lands as well as being burnt, harming the living beings, and are therefore considered a very dangerous pollution source for the environment. Over the past few years, several researchers have worked on incorporating shredded/powdered rubber tires into cement-based material. This strategy shows a dual functionality: Economic-environmental benefits and technological functionalization of the building material. Rubber-modified cement materials show interesting engineering and architectural properties due to the physical-chemical nature of the tire rubber aggregates. However, the abovementioned performances are affected by type, size, and content of polymer particles used in the cement-based mixtures production. Whereas an increase in the rubber content in the cement mix will negatively affect the mechanical properties of the material as a decrease in its compression strength. This aspect is crucial for the use of the material in building applications, where proper structural integrity must be guaranteed. In this context, the development of innovative manufacturing technologies and the use of multi-physics simulation software represent useful approaches for the study of shapes and geometries designed to maximize the technological properties of the material. After an overview on the performances of 3D printable rubber-cement mixtures developed in our research laboratory, a preliminary experimental Finite Element Method (FEM) analysis will be described. The modeling work aims to highlight how the topology optimization allows maximizing of the physical-mechanical performances of a standard rubber-cement component for building-architectural applications."
Jabardi2020,Mohammed Jabardi and Asaad Sabah Hadi,Twitter fake account detection and classification using ontological engineering and semantic web rule language,Karbala International Journal of Modern Science,6,4,2020,10.33640/2405-609X.2285,24056103,"Nowadays, Twitter has become one of the fastest-growing Online Social Networks (OSNs) for data sharing frameworks and microblogging. It attracts millions of users worldwide where subscribers communicate with each through posts and messages known as ""tweets"". The open structure and behaviour of Twitter cause it to be vulnerable to attacks from fake accounts and a large number of automated software, known as 'bots'. Bots are regarded to be malicious as they send spam to users of social networks over the internet. Data security and privacy are among the most critical issues of social network users, as the protection and fulfilment of these requirements strengthen the network's interest and, ultimately, its credibility. To overcome these issues, we need to build an efficient model to detect and classify fake twitter accounts. This paper presents a new approach with dual functions, namely to identify and classify the twitter bots based on ontological engineering and Semantic Web Rule Language (SWRL) rules. Web Ontology Language (OWL), Semantic Web Rule Language (SWRL) rules, and reasoners are deployed to inductively learn the rules that distinguish a fake account (bot) from a real one, as well as to classify fake accounts into fake followers or spam bot. Our approach could properly identify the false account with an accuracy of (97%) in the first stage, after which these fake accounts were classified into spam or fake follower bots with an accuracy rate of (94.9%). Furthermore, it has been found that he ontology classifier is a more interpretable model that offers straightforward and human-interpretable decision rules, as compared to other machine learning classifiers."
Barriga2020,Angela Barriga and Ludovico Iovino and Adrian Rutle and Rogardt Heldal,Model repair with quality-based reinforcement learning,Journal of Object Technology,19,2,2020,10.5381/JOT.2020.19.2.A17,16601769,"Domain modeling is a core activity in Model-Driven Engineering, and these models must be correct. A large number of artifacts may be constructed on top of these domain models, such as instance models, transformations, and editors. Similar to any other software artifact, domain models are subject to the introduction of errors during the modeling process. There are a number of existing tools that reduce the burden of manually dealing with correctness issues in models. Although various approaches have been proposed to support the quality assessment of modeling artifacts in the past decade, the quality of the automatically repaired models has not been the focus of repairing processes. In this paper, we propose the integration of an automatic evaluation of domain models based on a quality model with a framework for personalized and automatic model repair. The framework uses reinforcement learning to find the best sequence of actions for repairing a broken model."
Hayat2022,Asif Ullah Hayat and Ikram Ullah and Hassan Khan and Wajaree Weera and Ahmed M. Galal,Numerical Simulation of Entropy Optimization in Radiative Hybrid Nanofluid Flow in a Variable Features Darcy–Forchheimer Curved Surface,Symmetry,14,10,2022,10.3390/sym14102057,20738994,"Studies associated with ethylene glycol (EG) have great significance in various engineering sectors because EG is more useful as a cooling agent in various engines. Furthermore, fluid inspection using two distinct nanoparticles has applications in mechanical systems, electronic devices, medical apparatus, and the diagnosis and treatment of disease. Therefore, present comminution explored the entropy production in magnetized hybrid nanomaterials flowing via Darcy–Forchheimer space with varying permeability. Hybrid nano liquid is synthesized by adding cobalt ferrite and gold nanoparticles to ethylene glycol and water. Effects of thermal radiation, Joule heating, heat sources, and an exponential heat source are considered in the energy expression. The assumed problem is modeled in the form of nonlinear PDEs. Such types of problems have mostly occurred in symmetrical phenomena and are applicable in engineering, physics, and applied mathematics. The obtained system is converted to ODEs using suitable substitution transformations. Resultant ODEs are numerically computed with the help of the NDSolve technique using Mathematica software. Their outcomes are displayed through figures and tables. Obtained results reveal that variable permeability and curvature parameters improve the velocity profile, while an exponential heat source (EHS) enhances the thermal effect. It is also observed that entropy optimization improves with the increment in magnetic parameter."
Pereira2020,Lucas Pereira and Nuno Nunes,Understanding the practical issues of deploying energy monitoring and eco-feedback technology in the wild: Lesson learned from three long-term deployments,Energy Reports,6,,2020,10.1016/j.egyr.2019.11.025,23524847,"This paper reports on the different engineering, social and financial challenges behind the building and deploying electric energy monitoring and eco-feedback technology in real-world scenarios, which despite being relevant to the research community are seldom reported in the literature. The objectives of this paper are two-fold: First, discuss the technical and social constraints of real-world deployments. This includes, for example, hardware and software requirements, and issues related to security and intrusiveness of the monitoring solutions. Second, identify and understand the costs associated with developing and deploying such systems. These include hardware costs and consumed energy. To this end, we rely on over five years of experience developing and improving a non-intrusive energy monitoring research platform to enable the deployment of long and short-term studies of eco-feedback technology. During this time, two versions of that platform were deployed in 50 homes for periods that lasted between 6 and 18 consecutive months. By iteratively developing and deploying our sensing and eco-feedback infrastructures, we managed to build upon previous findings and lessons learned to understand how to create, deploy, and maintain such systems. Concurrently, we gained insights regarding what are some of the most relevant costs associated with running such experiments."
Melo2021,Pablo F.S. Melo and Eduardo P. Godoy and Paolo Ferrari and Emiliano Sisinni,Open source control device for industry 4.0 based on RAMI 4.0,Electronics (Switzerland),10,7,2021,10.3390/electronics10070869,20799292,"The technical innovation of the fourth industrial revolution (Industry 4.0—I4.0) is based on the following respective conditions: horizontal and vertical integration of manufacturing systems, decentralization of computing resources and continuous digital engineering throughout the product life cycle. The reference architecture model for Industry 4.0 (RAMI 4.0) is a common model for systematizing, structuring and mapping the complex relationships and functionalities required in I4.0 applications. Despite its adoption in I4.0 projects, RAMI 4.0 is an abstract model, not an implementation guide, which hinders its current adoption and full deployment. As a result, many papers have recently studied the interactions required among the elements distributed along the three axes of RAMI 4.0 to develop a solution compatible with the model. This paper investigates RAMI 4.0 and describes our proposal for the development of an open‐source control device for I4.0 applications. The control device is one of the elements in the hierarchy‐level axis of RAMI 4.0. Its main contribution is the integration of open‐source solutions of hardware, software, communication and programming, covering the relationships among three layers of RAMI 4.0 (assets, integration and communication). The implementation of a proof of concept of the control device is discussed. Experiments in an I4.0 scenario were used to validate the operation of the control device and demonstrated its effectiveness and robustness without interruption, failure or communication problems during the experiments."
Zeleniakiene2020,Daiva Zeleniakiene and Gediminas Monastyreckis and Andrey Aniskevich and Paulius Griskevicius,Deformation and failure of MXene nanosheets,Materials,13,5,2020,10.3390/ma13051253,19961944,"This work is aimed at the development of finite element models and prediction of the mechanical behavior of MXene nanosheets. Using LS-Dyna Explicit software, a finite element model was designed to simulate the nanoindentation process of a two-dimensional MXene Ti3C2Tz monolayer flake and to validate the material model. For the evaluation of the adhesive strength of the free-standing Ti3C2Tz-based film, the model comprised single-layered MXene nanosheets with a specific number of individual flakes, and the reverse engineering method with a curve fitting approach was used. The interlaminar shear strength, in-plane stiffness, and shear energy release rate of MXene film were predicted using this approach. The results of the sensitivity analysis showed that interlaminar shear strength and in-plane stiffness have the largest influence on the mechanical behavior of MXene film under tension, while the shear energy release rate mainly affects the interlaminar damage properties of nanosheets."
Wang2021,Pengyu Wang and Shuhong Wang and Alipujiang Jierula,Automatic identification and location of tunnel lining cracks,Advances in Civil Engineering,2021,,2021,10.1155/2021/8846442,16878094,"The lining crack is common for the tunnel in the stage of operation, which has seriously influenced the service life and safety of tunnel engineering. It is a new trend to use computer vision to detect tunnel cracks over the past few years in China and foreign countries. By image processing technology and intelligent algorithm, the computer has a hominine visual perception system which understands, analyzes, and determines input image information, thus recognizing and detecting specific objectives. However, the effect of image recognition for tunnel crack now cannot satisfy the demands of practical engineering. SSD algorithm has been used when analyzing features of lining surface image, while comparison analysis has been made from image recognition results, error rate, and running time. The results indicate that the SSD algorithm can accurately and rapidly detect and mark the position of the tunnel crack. The tunnel information obtained from image recognition is subsequently imported into the team independently developed software GeoSMA-3D, which is useful for determining tunnel grade."
Ruf2020,Matthias Ruf and Holger Steeb,"An open, modular, and flexible micro X-ray computed tomography system for research",Review of Scientific Instruments,91,11,2020,10.1063/5.0019541,10897623,"In this paper, a modular and open micro X-ray Computed Tomography (μXRCT) system is presented, which was set up during the last years at the Institute of Applied Mechanics (CE) of the University of Stuttgart and earlier at the Institute of Computational Engineering of Ruhr-University Bochum. The system is characterized by its intrinsic flexibility resulting from the modular and open design on each level and the opportunity to implement advanced experimental in situ setups. On the one hand, the presented work is intended to support researchers interested in setting up an experimental XRCT system for the microstructural characterization of materials. On the other hand, it aims to support scientists confronted with the decision to set up a system on their own or to buy a commercial scanner. In addition to the presentation of the various hardware components and the applied modular software concept, the technical opportunities of the open and modular hard- A nd software design are demonstrated by implementing a simple and reliable method for the compensation of bad detector pixels to enhance the raw data quality of the projections. A detailed investigation of the performance of the presented system with regard to the achievable spatial resolution is presented. XRCT datasets of three different applications are finally shown and discussed, demonstrating the wide scope of options of the presented system."
Sun2020,Qihao Sun and Fengshan Ma and Jie Guo and Guang Li and Xuelei Feng,Deformation failure mechanism of deep vertical shaft in Jinchuan mining area,Sustainability (Switzerland),12,6,2020,10.3390/su12062226,20711050,"Vertical shafts play an important role in the safe operation of mines. The stability of vertical shafts has always been a difficult problem in mining engineering, especially with the increasing of mining depth. In order to keep the engineering works stable, it is necessary to make the deformation failure mechanism of vertical shafts clear. This paper describes a case study of the deformation and failure mechanisms of the vertical shaft in Jinchuan No. 2 mining area in Gansu Province, China. Long-term deformation trends and characteristics of the vertical shaft are analyzed through ground movement and GPS monitoring of ground fissures. Then, based on previous research experience and a detailed field investigation, the modes, influencing factors, and mechanisms of deformation failure in the vertical shaft are studied. To further investigate the mechanism of shaft deformation failure and damage process under mining, the Universal Discrete Element Code (UDEC) software with Trigon approach is employed to build the numerical model of the vertical shaft. Through displacement, stress, and damage analysis, the time, location, and cause of vertical shaft failure during mining are explicitly illustrated. The results suggest that the upper and lower parts of the fault are activated in different ways. The damage to the vertical shaft is still developing at a very high rate under mining with filling method. All our results throw light on the nature of deformation and failure of vertical shafts, and several suggestions for the engineering of deep vertical shafts are finally put forward, providing a reference for other engineering."
Zhang2021,Zhenping Zhang and Xiaodong Fu and Qian Sheng and Dawei Yin and Yongqiang Zhou and Juehao Huang,Effect of Rainfall Pattern and Crack on the Stability of a Red Bed Slope: A Case Study in Yunnan Province,Advances in Civil Engineering,2021,,2021,10.1155/2021/6658211,16878094,"Red bed slopes in the southwest of China are associated with a grant number of geological hazards, such as landslides, mud-rock flows, and rock blocks falling, which are vital problems in geotechnical engineering. The damage can be induced or triggered due to a series of human and environmental activities, such as excavation, concentrated or long-term rainfall, earthquake, and fluctuation of groundwater level. According to the field observations and geological exploration results, a small-scale landslide was observed on January 10, 2016, after excavation along XiaoMo highway in Yunnan Province. A numerical model in actual size using GeoStudio software based on this typical red bed engineering slope was established in this study. Back analyses and laboratory tests were used to obtain the mechanical parameters of the geomaterial inside the slope. The historic rainfall data of Mengla County from July to September in 2016 was utilized as the flux boundary in analyzing the seepage variation features and the stability of the engineering slope in the rainy season. One major tension crack was set in the shallow region of the silty clay according to the geology survey to perform the disturbance of excavation on the geomorphology of the slope. Attempts were made to establish the anisotropic permeability of the crack induced by the complex fillings, and differences in the hydraulic response between the cracking and completed slope during the rainfall process were discussed. The result shows that the factor of safety of the slope without crack before the rainfall is 1.076, and the slope is considered in the state of the critical limit equilibrium, which is in accordance with the previous state of the slope under real conditions. The pore water pressure variations of the monitor points in the shallow region of the completed slope present close compliance with the rainfall intensity subjected to different rainfall patterns, which also controls the distribution of the plastic zone in the slope after rainfall. The comparisons in the seepage field and plastic zone between the cracking and completed slope reveal that the crack can shorten the infiltration path effectively, and the higher the permeability coefficient in the vertical direction is, the larger the pore water pressure increasing zone is and the higher the underground water level is, which should be paid more attention in highway constructions."
Herrada2020,Rosario I. Herrada and Raúl Baños and Alfredo Alcayde,Student response systems: A multidisciplinary analysis using visual analytics,Education Sciences,10,12,2020,10.3390/educsci10120348,22277102,"In recent years, several innovations have emerged in the field of education, including Blended-Learning, Massive Open Online Courses, Flipped Classroom and Gamification. In particular, several investigations have highlighted the effectiveness of student response systems, or clickers, in different subjects and disciplines. Although some literature reviews have been published on this subject, none of them offer a review of a large volume of publications from a multidisciplinary approach. Similarly, in the literature there are no studies that have analyzed scientific collaborations on this subject. To respond to these concerns, we proposed the use of a bot to retrieve information from a large number of papers (1696 documents co-authored by a total of 4091 researchers) included in the Scopus database. The disciplines covered include natural sciences, engineering and technology, medical and health sciences, agricultural and veterinary sciences, social sciences and humanities, and the arts. The review of the literature reveals that student response systems are generally well-perceived by teachers and students in all the disciplines. Another interesting result achieved from visual data obtained using network visualization software and word clouds is that student response systems are mainly used in some disciplines, such as physics, chemistry, medicine, and nursing. It is clearly observed that the relationship between researchers from the same country is stronger than between researchers from different countries. Finally, some reflections are included on the role of student response systems in online teaching, especially regarding the changes experienced after the COVID-19 pandemic."
Wang2021,Rui Wang and Rundong Zhao and Emily Ribando-Gros and Jiahui Chen and Yiying Tong and Guo Wei Wei,HERMES: PERSISTENT SPECTRAL GRAPH SOFTWARE,Foundations of Data Science,3,1,2021,10.3934/fods.2021006,26398001,"Persistent homology (PH) is one of the most popular tools in topological data analysis (TDA), while graph theory has had a significant impact on data science. Our earlier work introduced the persistent spectral graph (PSG) theory as a unified multiscale paradigm to encompass TDA and geometric analysis. In PSG theory, families of persistent Laplacian matrices (PLMs) corresponding to various topological dimensions are constructed via a filtration to sample a given dataset at multiple scales. The harmonic spectra from the null spaces of PLMs offer the same topological invariants, namely persistent Betti numbers, at various dimensions as those provided by PH, while the non-harmonic spectra of PLMs give rise to additional geometric analysis of the shape of the data. In this work, we develop an open-source software package, called highly efficient robust multidimensional evolutionary spectra (HERMES), to enable broad applications of PSGs in science, engineering, and technology. To ensure the reliability and robustness of HERMES, we have validated the software with simple geometric shapes and complex datasets from three-dimensional (3D) protein structures. We found that the smallest nonzero eigenvalues are very sensitive to data abnormality."
Hafez2023,N. M. Hafez and A. M. Abd-Alla and T. M.N. Metwaly,Influences of rotation and mass and heat transfer on MHD peristaltic transport of Casson fluid through inclined plane,Alexandria Engineering Journal,68,,2023,10.1016/j.aej.2023.01.038,11100168,"In the present analysis, we discussed the influence of heat and mass transfer on the hydro-magnetic peristaltic flow of a Casson fluid in a rotating inclined system through an asymmetric channel. The model governing equations are obtained, simplified by applying long wavelength and assumptions of low Reynolds number, and then addressed analytically. DSolve command of Mathematica software was used to solve these equations. The numerical results regarding the impacts of embedded parameters on axial velocity, secondary velocity, stream function, temperature, concentration, and streamline patterns are graphed and explained. It has been discovered that velocity slip reduces the size of the trapped bolus while the permeability parameter increases it. The velocity and temperature profiles decrease as the Casson fluid parameter and magnetic field strength increase. In the fields of biomedical engineering and crude oil refinement, the current problem is quite significant. The trapping phenomenon for different parameters is presented. The visual discussion of numerical results for various values of relevant physical parameters. The current work, which focuses on electromagnetic peristaltic pumps, offers several applications in biomedical engineering."
Holmes2021,Thomas D. Holmes and Rachael H. Rothman and William B. Zimmerman,Graph Theory Applied to Plasma Chemical Reaction Engineering,Plasma Chemistry and Plasma Processing,41,2,2021,10.1007/s11090-021-10152-z,15728986,"This work explores the following applications of graph theory to plasma chemical reaction engineering: assembly of a weighted directional graph with the key addition of reaction nodes, from a published set of reaction data for air; graph visualisation for probing the reaction network for potentially useful or problematic reaction pathways; running Dijkstra’s algorithm between all species nodes; further analysis of the graph for useful engineering information such as which conditions, reactions, or species could be enhanced or supressed to favour particular outcomes, e.g. targeted chemical formation. The use of reaction-nodes combined with derived parameters allowed large amounts of key information regarding the plasma chemical reaction network to be assessed simultaneously using a leading open source graph visualisation software (Gephi). A connectivity matrix of Dijkstra’s algorithm between each two species gave a measure of the relative potential of species to be created and destroyed under specific conditions. Further investigation into using the graph for key reaction engineering information led to the development of a graph analysis algorithm to quantify demand for conditions for targeted chemical formation: Optimal Condition Approaching via Reaction-In-Network Analysis (OCARINA). Predictions given by running OCARINA display significant similarities to a well-known electric field strength regime for optimal ozone production in air. Time dependent 0D simulations also showed preferential formation for O· and O3 using the respective conditions generated by the algorithm. These applications of graph theory to plasma chemical reaction engineering show potential in identifying promising simulations and experiments to devote resources."
Schmidt2020,Thomas Schmidt and Miriam Schlindwein and Katharina Lichtner and Christian Wolff,Investigating the Relationship between Emotion Recognition Software and Usability Metrics,i-com,19,2,2020,10.1515/icom-2020-0009,21966826,"Due to progress in affective computing, various forms of general purpose sentiment/emotion recognition software have become available. However, the application of such tools in usability engineering (UE) for measuring the emotional state of participants is rarely employed. We investigate if the application of sentiment/emotion recognition software is beneficial for gathering objective and intuitive data that can predict usability similar to traditional usability metrics. We present the results of a UE project examining this question for the three modalities text, speech and face. We perform a large scale usability test (N = 125) with a counterbalanced within-subject design with two websites of varying usability. We have identified a weak but significant correlation between text-based sentiment analysis on the text acquired via thinking aloud and SUS scores as well as a weak positive correlation between the proportion of neutrality in users' voice and SUS scores. However, for the majority of the output of emotion recognition software, we could not find any significant results. Emotion metrics could not be used to successfully differentiate between two websites of varying usability. Regression models, either unimodal or multimodal could not predict usability metrics. We discuss reasons for these results and how to continue research with more sophisticated methods."
Binder2021,Christoph Binder and Christian Neureiter and Arndt Lüder,Towards a domain-specific approach enabling tool-supported model-based systems engineering of complex industrial internet-of-things applications,Systems,9,2,2021,10.3390/systems9020021,20798954,"Contemporary manufacturing systems are undergoing a major change promoted by emerging technologies such as Cyber-physical Systems (CPS) or the Internet of Things (IoT). This trend, nowadays widely known by the term “Industry 4.0”, leads to a new kind of automated production. However, the rising number of dynamically interconnected elements in industrial production lines results in such a system being transformed into a complex System of Systems (SoS). Due to the increasing complexity and the challenges accompanied by this change, conventional engineering methods using generic principles reach their limits when developing this type of systems. With varying approaches only trying to find a solution for small-scaled areas of this problem statement, the need for a holistic methodology becomes more and more obvious. Having recognized this issue, one of the most promising approaches has been introduced with the Reference Architecture Model Industry 4.0 (RAMI 4.0). However, in the current point of view, this domain-specific architecture framework is missing specifications to address all aspects of such a critical infrastructure. Thus, this paper introduces a comprehensive modeling approach utilizing methods applied in Model-Based Systems Engineering (MBSE) and including domain-specific particularities as well as architectural concepts with the goal to enable mutual engineering of current and future industrial systems. The resulting artifacts, a domain-specific language (DSL), an architecture definition and a development process, are thereby consolidated in a ready to use software framework, whose applicability was evaluated by a real-world case study."
Tribst2021,João Paulo Mendes Tribst and Roberto Lo Giudice and Alison Flavio Campos Dos Santos and Alexandre Luiz Souto Borges and Laís Regiane Silva-Concílio and Marina Amaral and Giuseppe Lo Giudice,Lithium disilicate ceramic endocrown biomechanical response according to different pulp chamber extension angles and filling materials,Materials,14,5,2021,10.3390/ma14051307,19961944,"The purpose of this study is to evaluate the effect of pulp chamber extension angles and filling material mechanical properties on the biomechanical response of a ceramic endocrown. A 3D model of maxillary molar that underwent endodontically treatment was exported to computer aided design software to conduct finite element analysis (FEA). The endocrown model was modified considering different pulp chamber extension angles (right angle; 6°, 12° and 18° of axial divergence). The solids were imported into the computer aided engineering software in Standard for the Exchange of Product Data (STEP) format. Nine different filling materials were simulated to seal the orifice of the root canal system under each endocrown restoration (resin composite, bulk-fill resin composite, alkasite, flowable resin composite, glass ionomer cement, autocured resin-reinforced glass ionomer cement, resin cement, bulk-fill flowable resin composite, zinc oxide cement), totaling 36 models. An axial load (300 N) was applied at the occlusal surface. Results were determined by colorimetric graphs of von-Misses stress (VMS) and Maximum Principal Stress (MPS) on tooth, cement layer, and endocrown restorations. VMS distribution showed a similar pattern between the models, with more stress at the load region for the right-angled endocrowns. The MPS showed that the endocrown intaglio surface and cement layer showed different mechanical responses with different filing materials and pulp chamber angles. The stress peaks plotted in the dispersion plot showed that the filling material stiffness is proportional to the stress magnitude in the endocrown, cement layer and tooth adhesive surface. In addition, the higher the pulp chamber preparation angle, the higher the stress peak in the restoration and tooth, and the lower the stress in the cement layer. Therefore, 6° and 12° pulp chamber angles showed more promising balance between the stresses of the adhesive interface structures. Under the conditions of this study, rigid filling materials were avoided to seal the orifice of root canal system when an endocrown restoration was planned as rehabilitation. In addition, the pulp chamber axial walls were prepared between 6° and 12° of divergence to balance the stress magnitude in the adhesive interface for this treatment modality."
Reksowardojo2020,Iman K. Reksowardojo and Rafi R. Arya and Bentang A. Budiman and Metha Islameka and Sigit P. Santosa and Poetro L. Sambegoro and Abdul R.A. Aziz and Ezrann Z.Z. Abidin,Energy management system design for good delivery electric trike equipped with different powertrain configurations,World Electric Vehicle Journal,11,4,2020,10.3390/wevj11040076,20326653,"This paper demonstrates the design of an electric trike’s energy management system for a goods delivery service via various possible component configurations. A model of the energy management system was first developed based on general engineering vehicles’ equations using Matlab software. Various component configurations, such as the usage of two battery types (lithium iron phosphate (LFP) and lithium nickel cobalt aluminum oxide (NCA)), implementation of three braking strategies (full mechanical, parallel, and series strategies), the presence of a range extender (RE), and various masses of range extenders were simulated by using the model. The driving cycle of the e-trike as input data in the simulation was obtained by driving the vehicle around Bandung City. Speed, distance, and elevation were obtained by using GPS-based software. The simulation results showed that the most efficient and effective component configuration was to use the serial regenerative braking strategy with no RE equipped. This configuration achieved an efficiency of 18.07 km/kWh. However, for a longer route, the usage of a 20-kg RE was required to prevent the state of charge drop below 30%. The NCA with serial regenerative braking and 20-kg RE had an efficiency of 17.47 km/kWh for the complete route."
Brooks2021,Sam Brooks and Rajkumar Roy,An overview of self-engineering systems,Journal of Engineering Design,32,8,2021,10.1080/09544828.2021.1914323,14661837,"Failures in high value, safety-critical, inaccessible, and productivity-critical systems is an area of significant research interest. Despite advances in predictive and continuous maintenance services, there is still a need for a more ambitious approach to preserve a system’s functions despite degradation and damage. This paper presents the concept of a self-engineering (SE) system which utilises techniques such as self-healing, self-repairing, self-adapting and self-reconfiguration to enable a system to respond autonomously to a loss or potential loss in its function. Two types of SE systems are outlined, systems with control and systems without control. A taxonomy of these key techniques and related concepts is presented. This review focuses primarily on physical SE systems, rather than the control and software systems where SE concepts are well advanced. Technology within mechanical, civil, electrical and electronics, mechatronics engineering disciplines and repair robotics is reviewed. Finally, key research gaps identified are discussed."
Rus-Casas2020,Catalina Rus-Casas and Dolores Eliche-Quesada and Juan D. Aguilar-Peña and Gabino Jiménez-Castillo and M. Dolores La Rubia,The impact of the entrepreneurship promotion programs and the social networks on the sustainability entrepreneurial motivation of engineering students,Sustainability (Switzerland),12,12,2020,10.3390/SU12124935,20711050,"This paper presents the results of three academic courses in which Entrepreneurship Promotion Programs (EPP) have been developed for engineering students at the University of Jaén. This study describes the activities and how they have been promoted using the social networks Facebook and Twitter. Grytics for Analytics software was used for monitoring Facebook activity. The use of these tools has also allowed the collaborative development of the Engineering degree competencies related to sustainability and entrepreneurship through the Materials Science disciplines. The study is based on questionnaires before and after the EPP which involved a sample of 459 engineering students. The Kaiser-Meyer-Olkin test, Kolmogorov-Smirnov test and Pearson's correlation were used. The questionnaires show which factors have the strongest influence on the intention to undertake entrepreneurial activity. Motivation (MO), personal requirements (PR), perception of the environment (PE) and the background requirements (BR) were the factors considered. The statistical study shows that PE and PR have a strong influence on MO. Finally, through the study before and after the EPP, the success of the activities and the use of social networks have been demonstrated. The results indicate that the activities of the EPP influence the perception of the environment and the motivation of the engineering students. However, the personal requirements are not affected by the activities."
Stanek2022,Bartosz Stanek and Daniel Węcel and Łukasz Bartela and Sebastian Rulik,Solar tracker error impact on linear absorbers efficiency in parabolic trough collector – Optical and thermodynamic study,Renewable Energy,196,,2022,10.1016/j.renene.2022.07.021,18790682,"The parabolic-trough solar concentrating systems are often considered element of heat generation systems from renewable energy sources. It is important to use appropriate optical systems and solar trackers because parabolic trough collectors utilize only direct solar radiation. The paper presents the results of an analysis of the effect of solar tracker error on the optical and thermodynamic efficiency of linear absorbers. The results are presented for low concentrated geometry, popular among scientific institutions, consistent with the existing facility operating under natural conditions. Two diameters of the linear receiver were considered in the analysis: 33.7 mm and 21.3 mm. The research was carried out in optical engineering software based on the Monte Carlo ray-tracing method. Thermodynamic analysis was performed based on the mathematical model of the radiation absorption phenomenon by heat transfer fluid. The maximum angle error of the solar tracker for the tested geometry, not significantly affecting the performance of the linear absorber for a diameter of 33.7 mm is 1.5°, and for 21.3 mm is 0.9°. The reduction in efficiency for 2° error is 4.7% points, and 42.5% points respectively. The analysis also presents the results of the radiation distribution on the surface of the absorber tube."
Obidallah2020,Waeal J. Obidallah and Bijan Raahemi and Umar Ruhi,Clustering and Association Rules for Web Service Discovery and Recommendation: A Systematic Literature Review,SN Computer Science,1,1,2020,10.1007/s42979-019-0026-8,26618907,"The purpose of this study is to identify, summarize, and systematically compare various clustering and association rule techniques for web service discovery and recommendation, identify the most common data sets used in the extant literature, and highlight current trends and future research directions. Following the methodology of Kitchenham and Charters (Guidelines for performing Systematic Literature reviews in Software Engineering, 2007) for a systematic literature review (SLR), a set of research questions are designed. Six digital databases are searched. A total of 4581 papers were initially retrieved, and a rigorous two-stage scanning process resulted in 66 relevant papers. Based on the selection criteria and data extraction, 57 final studies were selected. These papers are summarized and compared, and the relevant information is extracted to answer the research questions. The synthesis resulted in knowledge of currently proposed methods for web service discovery and recommendation based on clustering and association rule techniques. Furthermore, it identifies algorithms, similarity measures, evaluation metrics, and data sets. Also identifies challenges, research gaps, trends, and future directions. We propose a classification of web service discovery and recommendation methods and map the 57 final selected papers into these classes. This review will help researchers to understand the current state-of-the-art in clustering and association rules techniques for web service discovery and recommendation, and also recognize trends and future directions for improvement. Future studies should broaden the basis of discovery and recommendation by including various types of web service descriptions including plain text that are currently used in web APIs. An opportunity for improvement by utilizing modern techniques based on big data analytics and social network analysis."
Gupta2020,Varun Gupta and Jose Maria Fernandez-Crehuet and Thomas Hanne,Freelancers in the software development process: A systematic mapping study,Processes,8,10,2020,10.3390/pr8101215,22279717,"[Context] Freelancers could catalyze the software development process by providing their niche skills to generate high quality outputs. They could help companies (including startups) to foster innovations by suggesting creative ideas and providing their expertise in implementing them (for instance, designing solutions, coding solutions etc.). Freelancers could effectively and efficiently work as a virtual member of the software development team. The company must make informed decisions about which task to allot to the freelancer, which freelancer to select, pricing the task, and evaluating the submitted work. On the other hand, the freelancer should make an informed decision about evaluating the monetary value of the task to be charged, trusting the requester, analyzing the skills requirement of the task (finding matches between skill requirement and skills processed), selecting the best task, and maintaining the highest level of reputation. However, the literature does not provide freelancers and the companies the guidelines that support their decision making. However, if freelancers are selected carefully for the most suitable task, the companies will benefit a lot in terms of improved software development metrics. [Objectives] The objective of this paper is to provide the research community the research trends in freelancer-supported software development. This helps to understand that which software development areas have higher concentrations of research efforts, which area has the support of empirical evidence to support management decision makings, and which area requires the research attention. [Method] The systematic study is conducted by planning the mapping protocol, executing the protocol, and reporting the findings using various visualization tools like bar charts and pie charts. The search process was planned to be executed using set of inclusion and exclusion conditions on four bibliographic databases (IEEExplore, Springerlink, Sciencedirect, and ACM digital library). The relevant papers are selected by applying inclusion and exclusion conditions. The google citations of the relevant papers are subject to the inclusion and exclusion conditions again to include the more relevant papers. Finally, the systematic schema was created and populated after analyzing the studies abstracts. [Results] The results indicate the following (a) The research focus is on generic software development (78%) rather on individual life cycle activities. (b) The number of empirical studies is limited (25%). (c) A number of studies proposing solutions and evaluating on live cases in industrial settings are missing from the literature. This is in comparison to the validation approaches (72%) i.e., solutions tested in laboratory settings. (d) At present, the literature has limited ability to provide the software companies (including startups) with the guidelines (in the form of opinions and experience reports) for involving freelancers in the software development process. (e) The reported challenges include Collaboration and Coordination (33%), Developer Recommendation (or selection) (19%), Team Formulation (14%), Task Recommendation (allocation) (14%), Task Decomposition (11%), Privacy and Security (Confidentiality) (11%), Budget Estimation (8%), Recognition (8%), Trust Issues (8%), Market Dynamism (6%), Intellectual Property Issues (6%), Participation of Crowd Worker (6%), and Capacity Utilization (3%). These challenges are highly interactive, and each challenge impacts all other challenges. (e) Recent focus of the researchers (total 7 studies in 2019) is on generic software development handling the collaboration and coordination (3 studies out of 7), Developer recommendation (2 studies out of 7), and task recommendation (2 studies out of 7). [Conclusion] The freelancer-driven software engineering research area has got the attraction of the researchers, but it will take a long time to gain maturity. This puts an urgent call for more empirical studies and evaluation-based solution research that could help companies (including startups) to foster innovations. Further, the research focus should be well distributed among the various development phases to address the unique challenges associated with individual activities. The accurate management of the freelancer in the software development could help companies and startups to foster innovations and remain competitive in the marketplace."
Huang2020,Yu Huang and Kevin Leach and Zohreh Sharafi and Nicholas McKay and Tyler Santander and Westley Weimer,"Biases and differences in code review using medical imaging and eye-tracking: Genders, humans, and machines",,,,2020,10.1145/3368089.3409681,,"Code review is a critical step in modern software quality assurance, yet it is vulnerable to human biases. Previous studies have clarified the extent of the problem, particularly regarding biases against the authors of code,but no consensus understanding has emerged. Advances in medical imaging are increasingly applied to software engineering, supporting grounded neurobiological explorations of computing activities, including the review, reading, and writing of source code. In this paper, we present the results of a controlled experiment using both medical imaging and also eye tracking to investigate the neurological correlates of biases and differences between genders of humans and machines (e.g., automated program repair tools) in code review. We find that men and women conduct code reviews differently, in ways that are measurable and supported by behavioral, eye-tracking and medical imaging data. We also find biases in how humans review code as a function of its apparent author, when controlling for code quality. In addition to advancing our fundamental understanding of how cognitive biases relate to the code review process, the results may inform subsequent training and tool design to reduce bias."
Fujii2020,Gaku Fujii and Koichi Hamada and Fuyuki Ishikawa and Satoshi Masuda and Mineo Matsuya and Tomoyuki Myojin and Yasuharu Nishi and Hideto Ogawa and Takahiro Toku and Susumu Tokumoto and Kazunori Tsuchiya and Yasuhiro Ujita,Guidelines for Quality Assurance of Machine Learning-Based Artificial Intelligence,,30,11-12,2020,10.1142/S0218194020400227,02181940,"Significant effort is being put into developing industrial applications for artificial intelligence (AI), especially those using machine learning (ML) techniques. Despite the intensive support for building ML applications, there are still challenges when it comes to evaluating, assuring, and improving the quality or dependability. The difficulty stems from the unique nature of ML, namely, system behavior is derived from training data not from logical design by human engineers. This leads to black-box and intrinsically imperfect implementations that invalidate many principles and techniques in traditional software engineering. In light of this situation, the Japanese industry has jointly worked on a set of guidelines for the quality assurance of AI systems (in the Consortium of Quality Assurance for AI-based Products and Services) from the viewpoint of traditional quality-assurance engineers and test engineers. We report on the second version of these guidelines, which cover a list of quality evaluation aspects, catalogue of current state-of-the-art techniques, and domain-specific discussions in five representative domains. The guidelines provide significant insights for engineers in terms of methodologies and designs for tests driven by application-specific requirements."
Hammad2020,Muhammad Hammad and Hamid Abdul Basit and Stan Jarzabek and Rainer Koschke,A systematic mapping study of clone visualization,Computer Science Review,37,,2020,10.1016/j.cosrev.2020.100266,15740137,"Knowing code clones (similar code fragments) is helpful in software maintenance and re-engineering. As clone detectors return huge numbers of clones, visualization techniques have been proposed to make cloning information more comprehensible and useful for programmers. We present a mapping study of clone visualization techniques, classifying visualizations in respect to the user goals to be achieved by means of clone visualizations and relevant clone-related information needs. Our mapping study will aid tool users in selecting clone visualization tools suitable for the task at hand, tool vendors in improving capabilities of their tools, and researchers in identifying open problems in clone visualization research."
Glazunova2021,O. G. Glazunova and O. V. Parhomenko and V. I. Korolchuk and T. V. Voloshyna,The effectiveness of GitHub cloud services for implementing a programming training project: Students' point of view,,1840,1,2021,10.1088/1742-6596/1840/1/012030,17426596,"In today's IT industry, it is important to develop the ability of IT students to collaboratively develop software, professional and personal skills. An effective method for developing such skills in future IT specialists is to organize different types of educational projects related to different programming technologies during the execution of mini projects, group and individual project assignments, term papers, academic training within the academic disciplines. The paper summarizes the results of a pedagogical study involving 29 expert students who study Computer Science and Software Engineering and used cloud service for GitHub collaborative IT development projects. The research findings testify, the most effective characteristics of this service, according to experts, identified the possibility of collaborative development of software (i1), the convenience of bug tracking (i3) and the convenience of the code editor (i7). It offers examples and results of using GitHub cloud service in the process of executing educational projects by future IT specialists."
Samhi2022,Jordan Samhi and Jun Gao and Nadia Daoudi and Pierre Graux and Henri Hoyez and Xiaoyu Sun and Kevin Allix and Tegawende F. Bissyande and Jacques Klein,JuCify: A Step Towards Android Code Unification for Enhanced Static Analysis,,2022-May,,2022,10.1145/3510003.3512766,02705257,"Native code is now commonplace within Android app packages where it co-exists and interacts with Dex bytecode through the Java Native Interface to deliver rich app functionalities. Yet, state-of-the-art static analysis approaches have mostly overlooked the presence of such native code, which, however, may implement some key sensitive, or even malicious, parts of the app behavior. This limitation of the state of the art is a severe threat to validity in a large range of static analyses that do not have a complete view of the executable code in apps. To address this issue, we propose a new advance in the ambitious research direction of building a unified model of all code in Android apps. The JUCIFY approach presented in this paper is a significant step towards such a model, where we extract and merge call graphs of native code and bytecode to make the final model readily-usable by a common Android analysis framework: in our implementation, JUCIFY builds on the Soot internal intermediate representation. We performed empirical investigations to highlight how, without the unified model, a significant amount of Java methods called from the native code are 'unreachable' in apps' callgraphs, both in goodware and malware. Using JUCIFY, we were able to enable static analyzers to reveal cases where malware relied on native code to hide invocation of payment library code or of other sensitive code in the Android framework. Additionally, JUCIFY'S model enables state-of-the-art tools to achieve better precision and recall in detecting data leaks through native code. Finally, we show that by using JUCIFY we can find sensitive data leaks that pass through native code."
Chen2020,Boyuan Chen and Zhen Ming Jack Jiang,Studying the use of java logging utilities in the wild,,,,2020,10.1145/3377811.3380408,02705257,"Software logging is widely used in practice. Logs have been used for a variety of purposes like debugging, monitoring, security compliance, and business analytics. Instead of directly invoking the standard output functions, developers usually prefer to use logging utilities (LUs) (e.g., SLF4J), which provide additional functionalities like thread-safety and verbosity level support, to instrument their source code. Many of the previous research works on software logging are focused on the log printing code. There are very few works studying the use of LUs, although new LUs are constantly being introduced by companies and researchers. In this paper, we conducted a large-scale empirical study on the use of Java LUs in the wild. We analyzed the use of 3, 856 LUs from 11, 194 projects in GitHub and found that many projects have complex usage patterns for LUs. For example, 75.8% of the large-sized projects have implemented their own LUs in their projects. More than 50% of these projects use at least three LUs. We conducted further qualitative studies to better understand and characterize the complex use of LUs. Our ndings show that dierent LUs are used for a variety of reasons (e.g., internationalization of the log messages). Some projects develop their own LUs to satisfy project-specic logging needs (e.g., dening the logging format). Multiple uses of LUs in one project are pretty common for large and very largesized projects mainly for context like enabling and conguring the logging behavior for the imported packages. Interviewing with 13 industrial developers showed that our ndings are also generally true for industrial projects and are considered as very helpful for them to better congure and manage the logging behavior for their projects. The ndings and the implications presented in this paper will be useful for developers and researchers who are interested in developing and maintaining LUs."
Mafarja2023,Majdi Mafarja and Thaer Thaher and Mohammed Azmi Al-Betar and Jingwei Too and Mohammed A. Awadallah and Iyad Abu Doush and Hamza Turabieh,Classification framework for faulty-software using enhanced exploratory whale optimizer-based feature selection scheme and random forest ensemble learning,Applied Intelligence,53,15,2023,10.1007/s10489-022-04427-x,15737497,"Software Fault Prediction (SFP) is an important process to detect the faulty components of the software to detect faulty classes or faulty modules early in the software development life cycle. In this paper, a machine learning framework is proposed for SFP. Initially, pre-processing and re-sampling techniques are applied to make the SFP datasets ready to be used by ML techniques. Thereafter seven classifiers are compared, namely K-Nearest Neighbors (KNN), Naive Bayes (NB), Linear Discriminant Analysis (LDA), Linear Regression (LR), Decision Tree (DT), Support Vector Machine (SVM), and Random Forest (RF). The RF classifier outperforms all other classifiers in terms of eliminating irrelevant/redundant features. The performance of RF is improved further using a dimensionality reduction method called binary whale optimization algorithm (BWOA) to eliminate the irrelevant/redundant features. Finally, the performance of BWOA is enhanced by hybridizing the exploration strategies of the grey wolf optimizer (GWO) and harris hawks optimization (HHO) algorithms. The proposed method is called SBEWOA. The SFP datasets utilized are selected from the PROMISE repository using sixteen datasets for software projects with different sizes and complexity. The comparative evaluation against nine well-established feature selection methods proves that the proposed SBEWOA is able to significantly produce competitively superior results for several instances of the evaluated dataset. The algorithms’ performance is compared in terms of accuracy, the number of features, and fitness function. This is also proved by the 2-tailed P-values of the Wilcoxon signed ranks statistical test used. In conclusion, the proposed method is an efficient alternative ML method for SFP that can be used for similar problems in the software engineering domain."
Dewangan2022,Seema Dewangan and Rajwant Singh Rao and Alok Mishra and Manjari Gupta,Code Smell Detection Using Ensemble Machine Learning Algorithms,Applied Sciences (Switzerland),12,20,2022,10.3390/app122010321,20763417,"Code smells are the result of not following software engineering principles during software development, especially in the design and coding phase. It leads to low maintainability. To evaluate the quality of software and its maintainability, code smell detection can be helpful. Many machine learning algorithms are being used to detect code smells. In this study, we applied five ensemble machine learning and two deep learning algorithms to detect code smells. Four code smell datasets were analyzed: the Data class, the God class, the Feature-envy, and the Long-method datasets. In previous works, machine learning and stacking ensemble learning algorithms were applied to this dataset and the results found were acceptable, but there is scope of improvement. A class balancing technique (SMOTE) was applied to handle the class imbalance problem in the datasets. The Chi-square feature extraction technique was applied to select the more relevant features in each dataset. All five algorithms obtained the highest accuracy—100% for the Long-method dataset with the different selected sets of metrics, and the poorest accuracy, 91.45%, was achieved by the Max voting method for the Feature-envy dataset for the selected twelve sets of metrics."
Xu2020,Hui Xu and Yangfan Zhou and Jiang Ming and Michael Lyu,Layered obfuscation: a taxonomy of software obfuscation techniques for layered security,Cybersecurity,3,1,2020,10.1186/s42400-020-00049-3,25233246,"Software obfuscation has been developed for over 30 years. A problem always confusing the communities is what security strength the technique can achieve. Nowadays, this problem becomes even harder as the software economy becomes more diversified. Inspired by the classic idea of layered security for risk management, we propose layered obfuscation as a promising way to realize reliable software obfuscation. Our concept is based on the fact that real-world software is usually complicated. Merely applying one or several obfuscation approaches in an ad-hoc way cannot achieve good obscurity. Layered obfuscation, on the other hand, aims to mitigate the risks of reverse software engineering by integrating different obfuscation techniques as a whole solution. In the paper, we conduct a systematic review of existing obfuscation techniques based on the idea of layered obfuscation and develop a novel taxonomy of obfuscation techniques. Following our taxonomy hierarchy, the obfuscation strategies under different branches are orthogonal to each other. In this way, it can assist developers in choosing obfuscation techniques and designing layered obfuscation solutions based on their specific requirements."
Schwendicke2021,F. Schwendicke and J. Krois,Better Reporting of Studies on Artificial Intelligence: CONSORT-AI and Beyond,Journal of Dental Research,100,7,2021,10.1177/0022034521998337,15440591,"An increasing number of studies on artificial intelligence (AI) are published in the dental and oral sciences. The reporting, but also further aspects of these studies, suffer from a range of limitations. Standards towards reporting, like the recently published Consolidated Standards of Reporting Trials (CONSORT)-AI extension can help to improve studies in this emerging field, and the Journal of Dental Research (JDR) encourages authors, reviewers, and readers to adhere to these standards. Notably, though, a wide range of aspects beyond reporting, located along various steps of the AI lifecycle, should be considered when conceiving, conducting, reporting, or evaluating studies on AI in dentistry."
Lindohf2021,Robert Lindohf and Jacob Krüger and Erik Herzog and Thorsten Berger,Software product-line evaluation in the large,Empirical Software Engineering,26,2,2021,10.1007/s10664-020-09913-9,15737616,"Software product-line engineering is arguably one of the most successful methods for establishing large portfolios of software variants in an application domain. However, despite the benefits, establishing a product line requires substantial upfront investments into a software platform with a proper product-line architecture, into new software-engineering processes (domain engineering and application engineering), into business strategies with commercially successful product-line visions and financial planning, as well as into re-organization of development teams. Moreover, establishing a full-fledged product line is not always possible or desired, and thus organizations often adopt product-line engineering only to an extent that deemed necessary or was possible. However, understanding the current state of adoption, namely, the maturity or performance of product-line engineering in an organization, is challenging, while being crucial to steer investments. To this end, several measurement methods have been proposed in the literature, with the most prominent one being the Family Evaluation Framework (FEF), introduced almost two decades ago. Unfortunately, applying it is not straightforward, and the benefits of using it have not been assessed so far. We present an experience report of applying the FEF to nine medium- to large-scale product lines in the avionics domain. We discuss how we tailored and executed the FEF, together with the relevant adaptations and extensions we needed to perform. Specifically, we elicited the data for the FEF assessment with 27 interviews over a period of 11 months. We discuss experiences and assess the benefits of using the FEF, aiming at helping other organizations assessing their practices for engineering their portfolios of software variants."
Legetth2021,Oscar Legetth and Johan Rodhe and Stefan Lang and Parashar Dhapola and Mattias Wallergård and Shamit Soneji,CellexalVR: A virtual reality platform to visualize and analyze single-cell omics data,iScience,24,11,2021,10.1016/j.isci.2021.103251,25890042,"Single-cell RNAseq is a routinely used method to explore heterogeneity within cell populations. Data from these experiments are often visualized using dimension reduction methods such as UMAP and tSNE, where each cell is projected in two or three dimensional space. Three-dimensional projections can be more informative for larger and complex datasets because they are less prone to merging and flattening similar cell-types/clusters together. However, visualizing and cross-comparing 3D projections using current software on conventional flat-screen displays is far from optimal as they are still essentially 2D, and lack meaningful interaction between the user and the data. Here we present CellexalVR (www.cellexalvr.med.lu.se), a feature-rich, fully interactive virtual reality environment for the visualization and analysis of single-cell experiments that allows researchers to intuitively and collaboratively gain an understanding of their data."
Hasselbring2020,Wilhelm Hasselbring and Leslie Carr and Simon Hettrick and Heather Packer and Thanassis Tiropanis,Open Source Research Software,Computer,53,8,2020,10.1109/MC.2020.2998235,15580814,"Reports on the need to make make software open source. It should be both archived for reproducibility and actively maintained for reusability. In computational and computer science, research software is a central asset for development activities. For good scientific practice, the resulting research software should be open source. Established open source software licenses provide sufficient options for granting permissions such that it should be the rare exception to keep research software closed. Proper engineering is required for obtaining reusable and sustainable research software. This way, software engineering methods may improve research in other disciplines. However, research in software engineering and computer science itself will also benefit when programs are reused. To study the state of the art in this field, we analyzed research software publishing practices in computer and computational science and observed significant differences: computational science emphasizes reproducibility, while computer science emphasizes reuse."
Vogel-Heuser2021,Birgit Vogel-Heuser and Fandi Bi,Interdisciplinary effects of technical debt in companies with mechatronic products — a qualitative study,Journal of Systems and Software,171,,2021,10.1016/j.jss.2020.110809,01641212,"Digitalization of products and production systems requires a fusion of mechatronic disciplines, where interfaces between mechanical, electrical, and software engineering are inevitable. The increasingly rapid pace of innovations in mechatronic systems triggers decisions being taken under time and cost pressure. At times, compromises in technical solutions are made, neglecting their long-term damage to the system. Technical debt (TD), a concept from software engineering, refers to short-term benefits that lead to long-term negative consequences, e.g., in the form of more difficult maintainability or evolvability. This also applies to mechatronic systems, yet the knowledge of TD characteristics and correlations in the interdisciplinary life cycle has only received little attention. This first comprehensive survey investigates TD in mechatronics systematically and across sectors. 50 experts, of whom 42% hold positions as department heads, from 21 renowned companies and 10 sectors in the German-speaking region supported this study with real scenarios where TD caused damage to their system. 94 informative TD incidents that were classified into twelve TD types were recorded, of which 2/3 have not yet been eliminated and posed a potential risk to the system. TD emerges most frequently in the first three stages of the life cycle, where the consequences rarely remain isolated at their source but are forwarded to later phases and disciplines in the life cycle. In contrast to the research focus in software engineering, the multi-domain analysis of mechatronic TD issues reveals that software engineers are most burdened by Requirements TD and Infrastructure TD in the interdisciplinary environment."
Hilderbrand2020,Claudia Hilderbrand and Christopher Perdriau and Lara Letaw and Jillian Emard and Zoe Steine-Hanson and Margaret Burnett and Anita Sarma,Engineering gender-inclusivity into software: Ten teams' tales from the trenches,,,,2020,10.1145/3377811.3380371,02705257,"Although the need for gender-inclusivity in software is gaining attention among SE researchers and SE practitioners, and at least one method (GenderMag) has been published to help, little has been reported on how to make such methods work in real-world settings. Real-world teams are ever-mindful of the practicalities of adding new methods on top of their existing processes. For example, how can they keep the time costs viable How can they maximize impacts of using it What about controversies that can arise in talking about gender To find out how software teams in the trenches handle these and similar questions, we collected the GenderMag-based processes of 10 real-world software teams- more than 50 people-for periods ranging from 5 months to 3.5 years. We present these teams' insights and experiences in the form of 9 practices, 2 potential pitfalls, and 2 open issues, so as to provide their insights to other real-world software teams trying to engineer gender-inclusivity into their software products."
Koch2022,Matthias Koch and Daniel Krohmer and Matthias Naab and Dominik Rost and Marcus Trapp,A matter of definition: Criteria for digital ecosystems,Digital Business,2,2,2022,10.1016/j.digbus.2022.100027,26669544,"Digital ecosystems like Airbnb or Uber have fundamentally changed their respective domains, and an increasing number of organizations are now considering how they can benefit from the underlying concepts. While several examples of systems that are generally accepted as digital ecosystems exist in practice, there is no common understanding yet of what exactly constitutes a digital ecosystem. This lack of a clear definition and understanding substantially impedes any well-founded discussion, analysis, design, and establishment of digital ecosystems. Digital ecosystems are strongly driven by practice, and research is following in a descriptive and analytical role. In search of a precise definition that matches the examples we see in practice, we analyzed the state of the art of digital ecosystems and closely related terms, such as \{platform, business, software\} ecosystems as well as the \{platform, sharing\} economy. While we found several common key properties in the literature, we identified a lack of a clear definition and a common understanding of the term. To address this problem, we propose a definition of digital ecosystems based on a set of well-defined criteria. Our definition builds upon the traditional understanding of an economic service and extends it successively with digital and ecosystem characteristics. The resulting model is precise and consistent, and matches the general conception of digital ecosystems in practice. It allows clearly classifying systems as digital ecosystems as well as reasoning about their core service and assets, which we demonstrate using Uber as a prominent example. With this, our proposed model lays the foundation for the systematic development of specific digital ecosystem engineering approaches."
Jimnez2020,Fabián R.L. Jiménez and Ilber A.R. Ruge and Andrés F.L. Jiménez,Modeling and control of a two wheeled auto-balancing robot: A didactic platform for control engineering education,,,,2020,10.18687/LACCEI2020.1.1.556,24146390,"This paper describes the modeling, instrumentation and control of a Two Wheeled Automatic Balancing Robot (TWABR). This mechatronic system has two independently driven wheels to balance in the gravity center above the axis of the wheels rotation. Its dynamic behavior has also served to illustrate fundamental concepts of stability, nonlinear dynamics, and modern control theory. The TWABR was designed using the ESP32 microcontroller as a digital control unit and the MPU6050 Inertial Measurement Unit as the main sensor. The dynamic model of the TWABR was analyzed through its representation in nonlinear differential equations and its linearized representation in the state space. With the linearized mathematical model around the equilibrium point, a classic PID controller and an optimal LQR controller were designed and simulated. The control objective was to balance the TWABR in the vertical equilibrium position, even when it is subjected to disturbances. The two control algorithms were simulated in the Matlab® / Simulink™ software platforms and implemented digitally on the physical TWABR system. As a result, the experimental comparison of the performance of the implemented controllers was performed, where its stabilization, control robustness and adequate dynamic response at the equilibrium point of the TWABR were evaluated."
Alenazi2020,Mounifah Alenazi and Nan Niu and Juha Savolainen,A novel approach to tracing safety requirements and state-based design models,,,,2020,10.1145/3377811.3380332,02705257,"Traceability plays an essential role in assuring that software and systems are safe to use. Automated requirements traceability faces the low precision challenge due to a large number of false positives being returned and mingled with the true links. To overcome this challenge, we present a mutation-driven method built on the novel idea of proactively creating many seemingly correct tracing targets (i.e., mutants of a state machine diagram), and then exploiting model checking within process mining to automatically verify whether the safety requirement's properties hold in the mutants. A mutant is killed if its model checking fails; otherwise, it is survived. We leverage the underlying killed-survived distinction, and develop a correlation analysis procedure to identify the traceability links. Experimental evaluation results on two automotive systems with 27 safety requirements show considerable precision improvements compared with the state-of-the-art."
Bonatto2020,Daniele Bonatto and Sarah Fachada and Gauthier Lafruit,RaViS: Real-time accelerated View Synthesizer for immersive video 6DoF VR,,2020,13,2020,10.2352/ISSN.2470-1173.2020.13.ERVR-382,24701173,"MPEG-I, the upcoming standard for immersive video, has steadily explored immersive video technology for free navigation applications, where any virtual viewpoint to the scene is created using Depth Image-Based Rendering (DIBR) from any number of stationary cameras positioned around the scene. This exploration has recently evolved towards a rendering pipeline using camera feeds, as well as a standard file format, containing all information for synthesizing a virtual viewpoint to a scene. We present an acceleration of our Reference View Synthesis software (RVS) that enables the rendering in real-time of novel views in a head mounted display, hence supporting virtual reality (VR) with 6 Degrees of Freedom (6DoF) including motion parallax within a restricted viewing volume. In this paper, we explain its main engineering challenges."
Luchnikov2021,Ilia A. Luchnikov and Mikhail E. Krechetov and Sergey N. Filippov,Riemannian geometry and automatic differentiation for optimization problems of quantum physics and quantum technologies,New Journal of Physics,23,7,2021,10.1088/1367-2630/ac0b02,13672630,"Optimization with constraints is a typical problem in quantum physics and quantum information science that becomes especially challenging for high-dimensional systems and complex architectures like tensor networks. Here we use ideas of Riemannian geometry to perform optimization on the manifolds of unitary and isometric matrices as well as the cone of positive-definite matrices. Combining this approach with the up-to-date computational methods of automatic differentiation, we demonstrate the efficacy of the Riemannian optimization in the study of the low-energy spectrum and eigenstates of multipartite Hamiltonians, variational search of a tensor network in the form of the multiscale entanglement-renormalization ansatz, preparation of arbitrary states (including highly entangled ones) in the circuit implementation of quantum computation, decomposition of quantum gates, and tomography of quantum states. Universality of the developed approach together with the provided open source software enable one to apply the Riemannian optimization to complex quantum architectures well beyond the listed problems, for instance, to the optimal control of noisy quantum systems."
Pamplin2020,Jeremy Pamplin and Christopher P. Nemeth and Maria L. Serio-Melvin and Sarah J. Murray and Gregory T. Rule and Elizabeth S. Veinott and Sena R. Veazey and Anthony J. Hamilton and Craig A. Fenrich and Dawn E. Laufersweiler and Jose Salinas,Improving Clinician Decisions and Communication in Critical Care Using Novel Information Technology,Military Medicine,185,1-2,2020,10.1093/milmed/usz151,1930613X,"Introduction: The electronic medical record (EMR) is presumed to support clinician decisions by documenting and retrieving patient information. Research shows that the EMR variably affects patient care and clinical decision making. The way information is presented likely has a significant impact on this variability. Well-designed representations of salient information can make a task easier by integrating information in useful patterns that clinicians use to make improved clinical judgments and decisions. Using Cognitive Systems Engineering methods, our research team developed a novel health information technology (NHIT) that interfaces with the EMR to display salient clinical information and enabled communication with a dedicated text-messaging feature. The software allows clinicians to customize displays according to their role and information needs. Here we present results of usability and validation assessments of the NHIT. Materials and Methods: Our subjects were physicians, nurses, respiratory therapists, and physician trainees. Two arms of this study were conducted, a usability assessment and then a validation assessment. The usability assessment was a computer-based simulation using deceased patient data. After a brief five-minute orientation, the usability assessment measured individual clinician performance of typical tasks in two clinical scenarios using the NHIT. The clinical scenarios included patient admission to the unit and patient readiness for surgery. We evaluated clinician perspective about the NHIT after completing tasks using 7-point Likert scale surveys. In the usability assessment, the primary outcome was participant perceptions about the system's ease of use compared to the legacy system. A subsequent cross-over, validation assessment compared performance of two clinical teams during simulated care scenarios: one using only the legacy IT system and one using the NHIT in addition to the legacy IT system. We oriented both teams to the NHIT during a 1-hour session on the night before the first scenario. Scenarios were conducted using high-fidelity simulation in a real burn intensive care unit room. We used observations, task completion times, semi-structured interviews, and surveys to compare user decisions and perceptions about their performance. The primary outcome for the validation assessment was time to reach accurate (correct) decision points. Results: During the usability assessment, clinicians were able to complete all tasks requested. Clinicians reported the NHIT was easier to use and the novel information display allowed for easier data interpretation compared to subject recollection of the legacy EMR. In the validation assessment, a more junior team of clinicians using the NHIT arrived at accurate diagnoses and decision points at similar times as a more experienced team. Both teams noted improved communication between team members when using the NHIT and overall rated the NHIT as easier to use than the legacy EMR, especially with respect to finding information. Conclusions: The primary findings of these assessments are that clinicians found the NHIT easy to use despite minimal training and experience and that it did not degrade clinician efficiency or decision-making accuracy. These findings are in contrast to common user experiences when introduced to new EMRs in clinical practice."
Cao2022,Zhengzheng Cao and Yue Wang and Haixiao Lin and Qiang Sun and Xiaogang Wu and Xiaoshuai Yang,Hydraulic Fracturing Mechanism of Rock Mass under Stress-Damage-Seepage Coupling Effect,Geofluids,2022,,2022,10.1155/2022/5241708,14688123,"According to the damage evolution model of rock mass under stress-seepage coupling effect, the representative element theory is employed to describe the change law of rock mesostructure. Based on the theory of elasticity and Weibull distribution, the statistical damage constitutive model of rock mass and the finite element numerical algorithm are established, by adopting the COMSOL Multiphysics numerical software and MATLAB program. Besides, the validity of the statistical damage constitutive model of rock mass is verified by the triaxial compression test. Besides, the hydraulic fracturing processes of rock mass under equal and unequal in situ stresses are numerically simulated, and the mechanical behavior of rock mass during hydraulic fracturing in complex underground environment is also studied. Under the condition of equal in situ stress, the stress distribution of surrounding rock of circular hole is annular, which is similar to the elastic stress distribution of surrounding rock. Under the condition of unequal in situ stress, the stress distribution tends to be circular with the increase of lateral pressure coefficient, and the stress distribution along the diagonal decreases. The simulation results are in good agreement with the theoretical results, which indicates that the damage mechanical model and the numerical model have correlation and certain accuracy. By analyzing the size and direction of horizontal in situ stress, the shape and extension direction of cracks are judged, which provides an important theoretical basis for water inrush prediction and engineering protection."
Chen2020,Bai Ling Chen and Wuttichai Mhuantong and Shih Hsin Ho and Jo Shu Chang and Xin Qing Zhao and Feng Wu Bai,"Genome sequencing, assembly, and annotation of the self-flocculating microalga Scenedesmus obliquus AS-6-11",BMC Genomics,21,1,2020,10.1186/s12864-020-07142-4,14712164,"Background: Scenedesmus obliquus belongs to green microalgae and is widely used in aquaculture as feed, which is also explored for lipid production and bioremediation. However, genomic studies of this microalga have been very limited. Cell self-flocculation of microalgal cells can be used as a simple and economic method for harvesting biomass, and it is of great importance to perform genome-scale studies for the self-flocculating S. obliquus strains to promote their biotechnological applications. Results: We employed the Pacific Biosciences sequencing platform for sequencing the genome of the self-flocculating microalga S. obliquus AS-6-11, and used the MECAT software for de novo genome assembly. The estimated genome size of S. obliquus AS-6-11 is 172.3 Mbp with an N50 of 94,410 bp, and 31,964 protein-coding genes were identified. Gene Ontology (GO) and KEGG pathway analyses revealed 65 GO terms and 428 biosynthetic pathways. Comparing to the genome sequences of the well-studied green microalgae Chlamydomonas reinhardtii, Chlorella variabilis, Volvox carteri and Micractinium conductrix, the genome of S. obliquus AS-6-11 encodes more unique proteins, including one gene that encodes D-mannose binding lectin. Genes encoding the glycosylphosphatidylinositol (GPI)-anchored cell wall proteins, and proteins with fasciclin domains that are commonly found in cell wall proteins might be responsible for the self-flocculating phenotype, and were analyzed in detail. Four genes encoding both GPI-anchored cell wall proteins and fasciclin domain proteins are the most interesting targets for further studies. Conclusions: The genome sequence of the self-flocculating microalgal S. obliquus AS-6-11 was annotated and analyzed. To our best knowledge, this is the first report on the in-depth annotation of the S. obliquus genome, and the results will facilitate functional genomic studies and metabolic engineering of this important microalga. The comparative genomic analysis here also provides new insights into the evolution of green microalgae. Furthermore, identification of the potential genes encoding self-flocculating proteins will benefit studies on the molecular mechanism underlying this phenotype for its better control and biotechnological applications as well."
Belardo2021,Marika Belardo and Aniello Daniele Marano and Jacopo Beretta and Gianluca Diodati and Mario Graziano and Mariacarmela Capasso and Pierpaolo Ariola and Salvatore Orlando and Francesco Di Caprio and Nicola Paletta and Luigi Di Palma,Wing structure of the next-generation civil tiltrotor: From concept to preliminary design,Aerospace,8,4,2021,10.3390/aerospace8040102,22264310,"The main objective of this paper is to describe a methodology to be applied in the preliminary design of a tiltrotor wing based on previously developed conceptual design methods. The reference vehicle is the Next-Generation Civil Tiltrotor Technology Demonstrator (NGCTR-TD) developed by Leonardo Helicopters within the Clean Sky research program framework. In a previous work by the authors, based on the specific requirements (i.e., dynamics, strength, buckling, functional), the first iteration of design was aimed at finding a wing structure with a minimized structural weight but at the same time strong and stiff enough to comply with sizing loads and aeroelastic stability in the flight envelope. Now, the outcome from the first design loop is used to build a global Finite Element Model (FEM), to be used for a multi-objective optimization performed by using a commercial software environment. In other words, the design strategy, aimed at finding a first optimal solution in terms of the thickness of composite components, is based on a two-level optimization. The first-level optimization is performed with engineering models (non-FEA-based), and the second-level optimization, discussed in this paper, within an FEA environment. The latter is shown to provide satisfactory results in terms of overall wing weight, and a zonal optimization of the composite parts, which is the starting point of an engineered model and a detailed FEM (beyond the scope of the present work), which will also take into account manufacturing, assembly, installation, accessibility and maintenance constraints."
Uteshov2021,Yerzhan Uteshov and Kanay Rysbekov and Daniyar Galiyev and Dilda Nаuryzbayeva and Seitgali Galiyev,Potential for increasing the efficiency of design processes for mining the solid mineral deposits based on digitalization and advanced analytics,Mining of Mineral Deposits,15,2,2021,10.33271/mining15.02.102,24153443,"Purpose. The research purpose is to develop and adapt the existing scientific-methodological, as well as software and information base for managing the geotechnological complexes to implement the process approach at the level of design and operation of mining-and-transport complexes during opencast mining. The development is associated with the coverage of more and more relevant options for excavating-conveying, as well as excavating-automobile-conveying mining-and-transport systems used in quarries. Methods. The methodology for managing the geotechnological complexes involves an adequate step-by-step accounting and appropriate operational regulation of all the main factors in specific mining-engineering, mining-geometrical, mining-and-geological, economic and organizational conditions. The method of simulation logical-statistical modelling of mining-and-transport processes is used as the main method. Findings. The results indicate that simplified analytical approaches and inadequately considered factors that have a significant impact on the efficiency of mining operations lead to significant errors (10-20% or more) that determine the real practical profitability of the mineral deposits development. Originality. Scientific novelty is an integrated approach to a single research object, which is a geotechnological complex, a unique simulation logical-statistical modelling of mining-and-transport processes, the economics of process management, as well as integrated and corporate process management. Practical implications. The obtained results of an integrated technical and technological audit of a project for mining the coal deposit, with sufficient accuracy for practical application, will ensure the methodological base development for the designing, planning and management of mining and mining-transport operations, as well as an increase in the efficiency of scientific and scientific-practical research in this direction when solving the practical tasks of mining sector."
Gao2021,Wenwei Gao and Hairong Yang and Le Wang and Ruilin Hu,Numerical simulations of the soil–rock mixture mechanical properties considering the influence of rock block proportions by pfc2d,Materials,14,18,2021,10.3390/ma14185442,19961944,"Soil–rock mixtures (S-RMs), as a kind of special engineering geological material, need to be studied because of the special structure and complex movement mechanism of their rock blocks, their physical and mechanical properties, and the factors underlying rock block movement in the process of their deformation and failure. In this paper, a series of discrete-element numerical models are constructed in particle flow code software (PFC2D). First, the random structure numerical models of S-RMs with different rock block proportions are established. Then, the parameters of the soil meso-structure are inversed by the biaxial simulation test, and a series of biaxial compressive tests are performed. The characteristics of stress and strain, deformation and failure, and rock block rotation and energy evolution are systematically investigated. The results show the following. (1) As the rock block proportion (confining pressure 0.5 MPa) increases, the peak strength of increases, the fluctuations of the post-peak become more obvious, and the dilatancy of the sample increases. (2) As the rock block proportion increases, the width of the shear band increases, the distribution of cracks becomes more complex and dispersed, and the range of the shear zone increases. (3) The number of rock blocks with rotation also increases significantly as rock block proportion increases, and the rotation angles are mostly between −5° and 5°. (4) The strain energy of S-RMs with different rock block proportions follows the same change rule as axial strain, showing a trend of first increasing and then decreasing, like the stress–strain curve."
Rosaidi2022,Nor Alifah Rosaidi and Nurul Hidayah Ab Raji and Siti Nur Hidayatul Ashikin Ibrahim and Mohd Rijal Ilias,Aligned Magnetohydrodynamics Free Convection Flow of Magnetic Nanofluid over a Moving Vertical Plate with Convective Boundary Condition,Journal of Advanced Research in Fluid Mechanics and Thermal Sciences,93,2,2022,10.37934/arfmts.93.2.3749,22897879,"Due to its substantial applications in physics, chemistry, and engineering, some emphasis has been given in recent years to explore the boundary layer flow of magnetohydrodynamic (MHD) nanofluids. The numerical study is conducted to investigate the behaviour of MHD free convection flow of magnetic nanofluids over a moving vertical plate with convective boundary conditions by taking a few types of parameters into consideration. The similarity transformation was used to reduce the partial differential governing equations into ordinary differential equations. Then, the reduced equations were solved using fourth-fifth order Runge–Kutta–Fehlberg and coded into Maple Software. The results of velocity and temperature profiles were illustrated graphically while the results of skin friction coefficient and Nusselt number were presented in tabulated data. As a result, inclination angle of magnetic field parameter, magnetic interaction parameter, Grashof number and Biot number improve the velocity field and lowers the momentum boundary layer thickness. However, the nanoparticle volume fraction parameter, and the Biot number parameter boost the temperature field and raise the thermal boundary layer thickness. The Nusselt number of the moving plate with the flow is the highest, whereas the skin friction coefficient of the moving plate against the flow is the highest. Fe3O4-kerosene has a better influence to the velocity and temperature profiles as well as skin friction coefficient and Nusselt number."
Tlelo-Cuautle2022,Esteban Tlelo-Cuautle and Astrid Maritza González-Zapata and Jonathan Daniel Díaz-Muñoz and Luis Gerardo de la Fraga and Israel Cruz-Vega,Optimization of fractional-order chaotic cellular neural networks by metaheuristics,European Physical Journal: Special Topics,231,10,2022,10.1140/epjs/s11734-022-00452-6,19516401,"Artificial neural networks have demonstrated to be very useful in solving problems in artificial intelligence. However, in most cases, ANNs are considered integer-order models, limiting the possible applications in recent engineering problems. In addition, when dealing with fractional-order neural networks, almost any work shows cases when varying the fractional order. In this manner, we introduce the optimization of a fractional-order neural network by applying metaheuristics, namely: differential evolution (DE) and accelerated particle swarm optimization (APSO) algorithms. The case study is a chaotic cellular neural network (CNN), for which the main goal is generating fractional orders of the neurons whose Kaplan–Yorke dimension is being maximized. We propose a method based on Fourier transform to evaluate if the generated time series is chaotic or not. The solutions that do not have chaotic behavior are not passed to the time series analysis (TISEAN) software, thus saving execution time. We show the best solutions provided by DE and APSO of the attractors of the fractional-order chaotic CNNs."
Cheng2022,Fan Cheng and Liangzhong Yao and Jian Xu and Yongning Chi and Yuanzhang Sun and Zhibing Wang and Yan Li,A comprehensive AC fault ride-through strategy for HVDC link with serial-connected LCC-VSC hybrid inverter,CSEE Journal of Power and Energy Systems,8,1,2022,10.17775/CSEEJPES.2020.03510,20960042,"An HVDC link with line-commutated converter (LCC) as a rectifier at the power sending end and with a serial-connected LCC and voltage source converter (VSC) hybrid inverter (SLVHI) at the power receiving end, has been adopted for the forthcoming Baihetan HVDC engineering project in China. To realize the AC fault ride-through (FRT) of SLVHI, a new strategy based on DC chopper (DCC) is proposed in this paper. Firstly, the mathematical model is built to investigate the VSC DC overvoltage mechanism after SLVHI commutation failure (CF) and related factors. Secondly, a modified DCC topology of SLVHI is designed to adjust the unbalanced power dissipation based on voltage drop depth. Thirdly, an enhanced voltage-dependent current order limiter (VDCOL) is proposed to deal with the unbalanced power after DCC switch-off. With the cooperation between modified DCC and enhanced VDCOL, the proposed FRT strategy can realize CF mitigation and VSC DC overvoltage suppression simultaneously. Finally, the inverter side AC FRT performances of the HVDC link with SLVHI were studied using PSCAD software/EMTDC algorithm. The simulation results validate the effectiveness and superiority of the proposed strategy, with better CF mitigation and VSC DC overvoltage suppression abilities than offered by other existing FRT strategies under different fault scenarios."
Kolozvari2021,Kristijan Kolozvari and Kamiar Kalbasi and Kutay Orakcal and John Wallace,Three-dimensional model for nonlinear analysis of slender flanged reinforced concrete walls,Engineering Structures,236,,2021,10.1016/j.engstruct.2021.112105,18737323,"A new, three-dimensional, four-node, macroscopic model element for nonlinear analysis of reinforced concrete walls is developed and implemented in the structural analysis software OpenSees. The two-dimensional, beam-column formulation of the Multiple-Vertical-Line-Element Model (MVLEM) is extended by implementing the geometrical transformation of the element degrees of freedom that convert the model element from a two-node to a four-node element, and by incorporating linear-elastic element out-of-plane behavior. The three-dimensional analytical model developed, namely the MVLEM-3D, is validated based on load-deformation responses and vertical strains obtained from tests on one T-shaped wall specimen tested under uniaxial cyclic loading and four U-shaped wall specimens tested under complex multidirectional loading protocols. In addition, results of sensitivity studies on model geometric discretization and selection of an effective shear stiffness are presented. The model exhibits a high level of numerical stability and computational efficiency for all specimens investigated. Comparisons of experimental and analytical results demonstrate that the proposed MVLEM-3D captures, with good accuracy, the cyclic load-displacement behavior of non-planar walls in loading directions that are parallel to the principal axes of the cross-section. However, the analysis results overestimate the wall lateral strength in diagonal (relative to the principal axes) loading directions by 20–50%. This discrepancy is associated with the inability of the plane-sections hypothesis implemented in the model to capture the experimentally-observed nonlinear strain distributions developing along the wall cross-section due to shear lag effect. Despite this issue, the proposed model is shown to be a powerful tool for practical applications, as nonlinear response history analysis is becoming more common in engineering practice."
Jiang2020,Jing Jiang and Linchi Qu,Evolution and Emerging Trends of Sustainability in Manufacturing Based on Literature Visualization Analysis,IEEE Access,8,,2020,10.1109/ACCESS.2020.3006582,21693536,"Sustainability is necessary for improving the quality of the manufacturing industry, which is an important pillar of the economy. The existing studies can hardly realize a comprehensive reflection of the research status of sustainability in manufacturing. Thus, this study adopts science mapping as the main research method for the systematic and accurate mastery of the overall research status, development track, research hotspots, and trends within this field. Moreover, this work provides a systematic review of related studies. This study examines 6,591 articles (1999-2019), including in core journals on Web of Science as research samples, and deeply explores core journal, core author, core country, discipline distribution, keyword co-occurrence, and literature co-citation via CiteSpace software. According to results, authoritative journals regarding sustainability in manufacturing place particular emphasis on theory, method, or application in different degrees. The number of published articles in the U.S.A. tops the ranking, and the influence of Chinese research institutes and scholars is continuously enhanced, where the main research fields are engineering, environmental science, and so on. Sustainable innovativeness in the manufacturing industry is generally at a high level. Moreover, research hotspots have evolved from traditional to modern and undergone three phases: theory construction - industry or enterprise technology construction - the emergence of new problems and application of new technologies. This study further analyzes the four clusters with citation bursts, namely, additive manufacturing, power consumption, green supply chain, and green information system in terms of grasping the knowledge structures and probing the classical research results. Future research should emphasize 3D printing design and application in the information era, the business model integrating the manufacturing industry and service industry, optimization of production modeling, and so on. This study aims to provide a systematic, comprehensive, dynamic, and objective review of the literature on sustainability in manufacturing to deepen and perfect the research in this field."
Liu2023,Zirui Liu and Oleg Gaidai and Jiayao Sun and Yihan Xing,Deconvolution approach for floating wind turbines,Energy Science and Engineering,11,8,2023,10.1002/ese3.1485,20500505,"Green renewable energy is produced by floating offshore wind turbines (FOWT), a crucial component of the modern offshore wind energy industry. It is a safety concern to accurately evaluate excessive weights while the FOWT operates in adverse weather conditions. Under certain water conditions, dangerous structural bending moments may result in operational concerns. Using commercial FAST software, the study's hydrodynamic ambient wave loads were calculated and converted into FOWT structural loads. This article suggests a Monte Carlo-based engineering technique that, depending on simulations or observations, is computationally effective for predicting extreme statistics of either the load or the response process. The innovative deconvolution technique has been thoroughly explained. The suggested approach effectively uses the entire set of data to produce a clear but accurate estimate for severe response values and fatigue life. In this study, estimated extreme values obtained using a novel deconvolution approach were compared to identical values produced using the modified Weibull technique. It is expected that the enhanced new de-convolution methodology may offer a dependable and correct forecast of severe structural loads based on the overall performance of the advised de-convolution approach due to environmental wave loading."
Tempa2021,Karma Tempa and Kinley Peljor and Sangay Wangdi and Rupesh Ghalley and Kelzang Jamtsho and Samir Ghalley and Pratima Pradhan,UAV technique to localize landslide susceptibility and mitigation proposal: A case of Rinchending Goenpa landslide in Bhutan,Natural Hazards Research,1,4,2021,10.1016/j.nhres.2021.09.001,26665921,"Natural hazards such as landslides impose very high risks to the community and infrastructures. To mitigate such risk, proper long-term engineering solutions are must. Several studies conducted in the past proposed countermeasures on many occasions. Despite such interventions, the Rinchending Goenpa area still suffers high vulnerability to landslide hazard which continues every monsoon. To assess landslide characteristics in detail, we performed landslide susceptibility mapping using the unmanned aerial vehicle (UAV) technique and conducted a site investigation. High-resolution digital elevation model (DEM) was generated with the site-specific aerial photographs and processed in Agisoft PhotoScan to developed thematic layers in geographical information system (GIS) software. We implemented a multi-influencing factor (MIF) for deriving the influence of landslide conditioning factors and developed a landslide susceptibility map (LSM) using the weighted overlay method (WOM). The landslide conditioning factors include slope, elevation, aspect, topographical wetness index (TWI), and normalized difference vegetation index (NDVI). According to LSM, the areal coverage of landslide susceptibility of study area reveals 2.41% in very low susceptibility, 37.14% in low susceptibility, with highest in moderate susceptibility zone of 49.55% followed by 10.60% in high susceptibility zone, and 0.30% in the very high susceptible zone. Based on the zonal distribution of LSM, and considering findings from several other site investigation such as geophysical survey, the performance of existing countermeasures, soil characteristics, and footprint of last landslide occurrences, we proposed mitigation measures under each zone to provide long-term solutions."
Awan2022,Muhammad Mateen Afzal Awan and Muhammad Yaqoob Javed and Aamer Bilal Asghar and Krzysztof Ejsmont and Zia-Ur-rehman,Economic Integration of Renewable and Conventional Power Sources—A Case Study,Energies,15,6,2022,10.3390/en15062141,19961073,"In this study, we have presented an optimal microgrid design that ensures the uninterrupted energy supply to Mirpur University of Engineering and Technology (MUST), Azad Jammu and Kashmir AJK, Pakistan at the cheapest price by using reliable energy resources. The availability of energy resources, environmental viability, and economic feasibility are the key parameters of design. The available resources for the MUST site include the National grid, Solar photovoltaic (SPV), Battery bank, and Diesel generator. The data of electrical load, solar illumination, atmospheric temperature at the university, diesel fuel cost, SPV module lifetime, SPV degradation factor, SPV efficiency, SPV cost, battery cost, battery life, national grid energy price, load shedding and toxic emissions have been considered valuables in designing the hybrid micro-grid. The difference in net present cost (NPC) of the optimal design and the worst design is calculated by considering the above parameters. The proposed optimal microgrid design supplies energy to the load using SPV, Diesel generator, and battery bank with NPC of $250,546 and the renewable fraction of 99%. Whereas the worst design includes the Diesel generator and battery bank as energy supplying sources with the NPC of $2.14 M and a renewable fraction of 0%. Simulations performed using HOMER Pro software (HOMER Energy, HOMER Pro-3.11, Boulder, CO, USA) proved that after considering all the data and requirements mentioned above, out of 979 feasible designs, the proposed hybrid microgrid design is best suitable for MUST."
Saeed2022,Ahmed Saeed and Hadee Mohammed Najm and Amer Hassan and Shaker Qaidi and Mohanad Muayad Sabri Sabri and Nuha S. Mashaan,A Comprehensive Study on the Effect of Regular and Staggered Openings on the Seismic Performance of Shear Walls,Buildings,12,9,2022,10.3390/buildings12091293,20755309,"Shear walls have high strength and stiffness, which could be used at the same time to resist large horizontal loads and weight loads, making them pretty beneficial in several structural engineering applications. The shear walls could be included with openings, such as doors and windows, for relevant functional requirements. In the current study, a building of G + 13 stories with RC shear walls with and without openings has been investigated using ETABS Software. The seismic analysis is carried out for the determination of parameters like shear forces, drift, base shear, and story displacement for numerous models. The regular and staggered openings of the shear wall have been considered variables in the models. The dynamic analysis is carried out with the help of ETABS software. It has been observed that shear walls without openings models perform better than other models, and this is in agreement with the previous studies published in this area. This investigation also shows that the seismic behaviour of the shear wall with regular openings provides a close result to the shear wall with staggered openings. At the roof, the displacement of the model with regular openings was 38.99 mm and approximately 39.163 mm for the model with staggered openings. However, the model without a shear wall experienced a displacement of about 56 mm at the roof. Generally, it can be concluded that the openings have a substantial effect on the seismic behaviour of the shear wall, and that should be taken into consideration during the construction design. However, the type of opening (regular or staggered) has a slight effect on the behaviour of shear walls."
Khan2020,Javed Ali Khan and Lin Liu and Lijie Wen,Requirements knowledge acquisition from online user forums,IET Software,14,3,2020,10.1049/iet-sen.2019.0262,17518806,"Online discussion forums can be used for reflecting on the overall user experience of a system. If a user forum is well-structured, it can be a valuable source of requirements-related information, which can potentially be accommodated in the requirements engineering process to enhance the current and future software. However, presently, there are limited approaches for extracting such requirements-related information from the relevant community forums. To fill this gap, this study proposes an automated approach, which automatically identifies requirements information using natural language processing and machine learning. For this purpose, the authors analysed 3319 user comments collected from the seven discussion topics in the Reddit forum. Then, using a content analysis approach, they studied how frequently end-users submit such information across each discussion topic. Also, they developed an automated approach that identifies key stakeholders, who frequently contribute his rationales in the forum discussion. Further, they employed different machine learning algorithms to classify user comments into rationale elements of different types. The authors' results show that online forums, such as Reddit, can be a rich source of requirements elicitation. Also, machine learning is a promising tool to detect user's rationale and identify different kinds of requirements modelling elements."
Faezipour2020,Misagh Faezipour and Miad Faezipour,Sustainable smartphone-based healthcare systems: A systems engineering approach to assess the efficacy of respiratory monitoring apps,Sustainability (Switzerland),12,12,2020,10.3390/su12125061,20711050,"Recent technological developments along with advances in smart healthcare have been rapidly changing the healthcare industry and improving outcomes for patients. To ensure reliable smartphone-based healthcare interfaces with high levels of efficacy, a system dynamics model with sustainability indicators is proposed. The focus of this paper is smartphone-based breathing monitoring systems that could possibly use breathing sounds as the data acquisition input. This can especially be useful for the self-testing procedure of the ongoing global COVID-19 crisis in which the lungs are attacked and breathing is affected. The method of investigation is based on a systems engineering approach using system dynamics modeling. In this paper, first, a causal model for a smartphone-based respiratory function monitoring is introduced. Then, a systems thinking approach is applied to propose a system dynamics model of the smartphone-based respiratory function monitoring system. The system dynamics model investigates the level of efficacy and sustainability of the system by studying the behavior of various factors of the system including patient wellbeing and care, cost, convenience, user friendliness, in addition to other embedded software and hardware breathing monitoring system design and performance metrics (e.g., accuracy, real-time response, etc.). The sustainability level is also studied through introducing various indicators that directly relate to the three pillars of sustainability. Various scenarios have been applied and tested on the proposed model. The results depict the dynamics of the model for the efficacy and sustainability of smartphone-based breathing monitoring systems. The proposed ideas provide a clear insight to envision sustainable and effective smartphone-based healthcare monitoring systems."
Ziatdinov2022,Rushan Ziatdinov and James R. Valles,"Synthesis of Modeling, Visualization, and Programming in GeoGebra as an Effective Approach for Teaching and Learning STEM Topics",Mathematics,10,3,2022,10.3390/math10030398,22277390,"GeoGebra is an interactive geometry, algebra, statistics, and calculus application designed for teaching and learning math, science, and engineering. Its dynamic interface allows its users to accurately and interactively visualize their work, models, and results. GeoGebra employs the synthesis of three key features: modeling, visualization, and programming (MVP). Many studies have shown the positive effects of GeoGebra on the efficiency and effectiveness of learning and teaching topics related to science, technology, engineering, and mathematics. In this study, we discuss how GeoGebra provides an environment for learning that is very interactive and collaborative between the learner and the instructor. We also show how integrating GeoGebra into the learning scheme can help improve the skills and knowledge of school and university students in numerous advanced mathematical courses, such as calculus, mathematical statistics, linear algebra, linear programming, computer-aided design, computer-aided geometric design, analytic and projective geometry, and graphical representation. Therefore, this study shows the effectiveness of GeoGebra and its MVP key features in science and engineering, particularly in topics related to mathematics. Each key feature of GeoGebra is thoroughly analyzed, and further analyses, along with how GeoGebra can be helpful in different topics, are discussed."
Trunzer2020,Emanuel Trunzer and Anne Wullenweber and Birgit Vogel-Heuser,Graphical modeling notation for data collection and analysis architectures in cyber-physical systems of systems,Journal of Industrial Information Integration,19,,2020,10.1016/j.jii.2020.100155,2452414X,"Industrie 4.0 and data analytics blur the separation of operational and information technology that prevailed for industrial automation over the last decades. Decentralized control systems for production plants and robot cells collaborate actively with higher-level systems for big data analytics. In parallel, the complexity of designing and operating a system architecture for data collection and analysis increases dramatically as more experts from different domains get involved. Graphical modeling notations facilitate the design process by formalizing implicit knowledge, but currently do not exist for the combined description of field layer and data analytics. Modeling the system architecture, relevant constraints, and requirements early during the design process can increase the efficiency of the system development and deployment, especially as experts with various backgrounds are involved. In this contribution, a new graphical notation is introduced and evaluated in three industrial case-studies. The notation describes the underlying hardware and software components of cyber-physical systems of systems, the flow of data, and relevant constraints. The evaluation proved that the notation is powerful in supporting the engineering of data collection and analysis architectures in industrial automation. Future work is related to extending the scope of the modeling approach to include safety applications and real-time considerations on the field level."
Ergzen2021,Atilla Ergüzen and Erdal Erdal and Mahmut Ünver and Ahmet Özcan,Improving technological infrastructure of distance education through trustworthy platform‐independent virtual software application pools,Applied Sciences (Switzerland),11,3,2021,10.3390/app11031214,20763417,"Distance education (DE), which has evolved under the wings of information technologies in the last decade, has become a fundamental part of our modern education system. DE has not only replaced the traditional education method as in social sciences and lifelong learning opportunities but also has significantly strengthened traditional education in mathematics, science, and engineering fields that require practical and intensive study. However, it is deprived of supporting some key elements found in traditional educational approaches such as (i) modern computer laboratories with installed special software suitable for the student’s field of interest; (ii) adequate staff for mainte-nance and proper functioning of laboratories; (iii) face‐to‐face technical support; (iv) license fees. For students to overcome these shortcomings, a virtual application pool is needed where they can easily access all the necessary applications via remote access. This research aims to develop a plat-form‐independent virtual laboratory environment for DE students. This article has been developed specifically to guide DE institutions and to make a positive contribution to the literature. Technology Acceptance Model (TAM) has been used to explain student behaviors. It was concluded that students using the platform performed more successful grades (12.89%) on laboratory assessments and that the students using the developed platform were found to be more satisfied with the education process."
Matentzoglu2022,Nicolas Matentzoglu and Damien Goutte-Gattat and Shawn Zheng Kai Tan and James P. Balhoff and Seth Carbon and Anita R. Caron and William D. Duncan and Joe E. Flack and Melissa Haendel and Nomi L. Harris and William R. Hogan and Charles Tapley Hoyt and Rebecca C. Jackson and Hyeongsik Kim and Huseyin Kir and Martin Larralde and Julie A. McMurry and James A. Overton and Bjoern Peters and Clare Pilgrim and Ray Stefancsik and Sofia M.C. Robb and Sabrina Toro and Nicole A. Vasilevsky and Ramona Walls and Christopher J. Mungall and David Osumi-Sutherland,"Ontology Development Kit: A toolkit for building, maintaining and standardizing biomedical ontologies",Database,2022,,2022,10.1093/database/baac087,17580463,"Similar to managing software packages, managing the ontology life cycle involves multiple complex workflows such as preparing releases, continuous quality control checking and dependency management. To manage these processes, a diverse set of tools is required, from command-line utilities to powerful ontology-engineering environmentsr. Particularly in the biomedical domain, which has developed a set of highly diverse yet inter-dependent ontologies, standardizing release practices and metadata and establishing shared quality standards are crucial to enable interoperability. The Ontology Development Kit (ODK) provides a set of standardized, customizable and automatically executable workflows, and packages all required tooling in a single Docker image. In this paper, we provide an overview of how the ODK works, show how it is used in practice and describe how we envision it driving standardization efforts in our community. Database URL: https://github.com/INCATools/ontology-development-kit"
Kennedy2021,Matthew Kennedy and Sung Jin Lee and Michael Epstein,Modeling aerosol transmission of SARS-CoV-2 in multi-room facility,Journal of Loss Prevention in the Process Industries,69,,2021,10.1016/j.jlp.2020.104336,09504230,"The versatile and computationally attractive FATE™ facility software package for analyzing the transient behavior of facilities during normal and off-normal conditions is applied to the problem of SARS-CoV-2 virus transmission in single-and multi-room facilities. Subject to the justifiable assumptions of non-interacting virus droplets, room-wide spatially homogeneous virus droplet aerosols and droplet sedimentation in accordance with Stokes law; the FATE code tracks the virus aerosol from a human source through a facility with a practical ventilation system which reconditions, filters, and recycles the air. The results show that infection risk can be reduced by 50 percent for increased facility airflow, 70 percent for increased airflow and the inclusion of a HEPA filter on recirculated ventilation air, and nearly 90 percent for increased airflow, inclusion of a HEPA filter, and wearing a mask. These results clearly indicate that there are operational changes and engineering measures which can reduce the potential infection risk in multi-room facilities."
Mohamed2021,Mustafa Abshir Mohamed and Geylani Kardas and Moharram Challenger,Model-Driven Engineering Tools and Languages for Cyber-Physical Systems-A Systematic Literature Review,IEEE Access,9,,2021,10.1109/ACCESS.2021.3068358,21693536,"The development of Cyber-physical Systems (CPS) draws more interest from both researchers and industrial practitioners considering the opportunities they offer in almost all areas of industry. However, the engineering and management of CPS are challenging tasks due to their inherent heterogeneity and complexity characteristics. Regarding the development of CPS, there currently exists no standard methodology owing to the complexity of the domain. One of the key approaches to reduce the development complexity for CPS is Model-driven Engineering (MDE), which is frequently used in many industrial domains for software development to increase the level of platform abstraction. Nevertheless, it is always almost challenging, especially for the new researchers in this field, to determine the appropriate tools and languages to perform a particular MDE activity during CPS development. To the best of our knowledge, there is no guideline that demonstrates which language(s)/tool(s) to use for the various MDE techniques/phases for the development of CPS. This paper presents a Systematic Literature Review (SLR) study that focuses on identifying and classifying the recent research practices pertaining to CPS development by applying MDE approaches. With the objective of providing a general overview of the field, the study evaluates 140 research papers published during 2010-2018. Accordingly, a precise view of the various MDE tools and languages used in the development life-cycle of CPS, addressed MDE techniques/activities, and targeted CPS components is presented. We believe that the conducted study will guide researchers and practitioners to identify appropriate tools and languages according to the system requirements. It may also help in getting an overall understanding of the research trends for further research and development on the MDE of CPS."
Li2023,Yuan Li and Long Zhao and Yiping Chen and Na Zhang and Hongchao Fan and Zhenxin Zhang,3D LiDAR and multi-technology collaboration for preservation of built heritage in China: A review,International Journal of Applied Earth Observation and Geoinformation,116,,2023,10.1016/j.jag.2022.103156,1872826X,"In recent years, the technical application of 3D LiDAR has gradually expanded to the field of built heritage. 3D scanning, high-precision measurement, and reconstruction have enriched the methods of built heritage preservation and significantly improved the quality of heritage preservation in China. 3D LiDAR has broken through the limitations of a single technology application and played a greater role in the field of heritage preservation on different scales. Through the collaboration of multi-technology, such as 3D printing, digital mapping, internet of things, machine learning, intelligent sensors, close-range photogrammetry, infrared detection, stress wave tomography, material analysis, XR technology, reverse engineering, etc., 3D LiDAR shows its technological advantages on exploring the remote real-time monitoring and digitization of the built heritage, geological and environmental data collection, prediction of sedimentation, deformation monitoring, weather monitoring,system life cycle health detection, digital reproduction of built heritage for developing scientific problems and engineering practices such as building contour recognition, information feature matching, structural reinforcement and damaged component replacement. In addition, through the docking with GIS, HBIM, XR, and CIM, it provides fine digital models and high-precision data benchmarks which contribute to the heritage visual reproduction; and through the docking with 3Ds Max, SketchUp, and other modeling software, it has contributed to the renewal design of the built heritage, space optimization, and the scientificity and rationality of the heritage value evaluation. However, past technology applications also highlighted many problems such as limited recorded information, a large amount of data, high difficulty in collaboration, non-standardized and fragmented data, and difficulty in data mining and comprehensive utilization. There are still deficiencies in building a built heritage data backplane, and the development of a dynamic, three-dimensional, intelligent and refined heritage monitoring system, and further research is needed on these issues. This study reviews the previous academic progress and application of 3D LiDAR in the reconstruction of built heritage and multi-technology collaboration in the process of preservation, clarifies the current research hotspots and methods, the frontier issues of concern, and also clarifies the specific problems and challenges in the future."
Tamoor2022,Muhammad Tamoor and Salman Habib and Abdul Rauf Bhatti and Arslan Dawood Butt and Ahmed Bilal Awan and Emad M. Ahmed,Designing and Energy Estimation of Photovoltaic Energy Generation System and Prediction of Plant Performance with the Variation of Tilt Angle and Interrow Spacing,Sustainability (Switzerland),14,2,2022,10.3390/su14020627,20711050,"The focus of this research is to design a ground-mounted photovoltaic system at optimal tilt angle and interrow space to meet high demand of electrical energy. The Department of Electrical Engineering and Technology, GC University Faisalabad has been considered to perform the simulation test. This study is conducted using Meteonorm software for solar resource assessment. Fur-thermore, HelioScope software is used for modeling of a ground-mounted photovoltaic system, study of PV system’s performance in terms of annual generation, system losses and performance ratio and analysis of photovoltaic module’s performance, current-voltage and power-voltage curves for different irradiance levels. From SLD, it is seen that 11 strings are connected to each inverter and inverters output power are combined by using 20.0 A circuit interconnects. The performance of photovoltaic systems is impacted by tilt angle and interrow spacing. From simulation results of all cases, it is concluded that the PV system installed at 15° tilt angle with 4 feet interrow spacing are more efficient than the other installed PV systems, because total collector irradiance is maximum (1725.0 kWh/m2) as compared to other tilt angles. At 15° tilt angle, the annual production of photo-voltaic system is 2.265 GWh and performance ratio of PV system is 82.0%. It is envisioned that this work will provide the guidance to energy system designers, planners and investors to formulate strategies for the installation of photovoltaic energy systems in Pakistan and all over the world."
Akbar2020,Muhammad Azeem Akbar and Sajjad Mahmood and Hussain Alsalman and Abdul Razzaq and Abdu Gumaei and Muhammad Tanveer Riaz,Identification and prioritization of cloud based global software development best practices,IEEE Access,8,,2020,10.1109/ACCESS.2020.3031365,21693536,"The cloud based global software development (CGSD) is the most widely adopted development paradigm in software industry. The CGSD offers significant economic and strategic benefits; besides, various complexities are faced by the practitioners while deploying CGSD. Hence, this study aims to identify and prioritize the best practices that are important for the success and progression of CGSD paradigm. Using the systematic literature review a total of 30 best practices were identified and were further verified with industry experts using questionnaire survey study. The identified best practices were further prioritize using fuzzy-AHP approach. The fuzzy-AHP is novel in this domain as it successfully applied in other engineering domain to address the multicriteria decision making problems. The findings of this study will provide a prioritization-based taxonomy of the investigated best practices which assists the academic researchers and industry experts to develop and revise the strategies of CGSD."
Bik2022,Aart Bik and Penporn Koanantakool and Tatiana Shpeisman and Nicolas Vasilache and Bixia Zheng and Fredrik Kjolstad,Compiler Support for Sparse Tensor Computations in MLIR,ACM Transactions on Architecture and Code Optimization,19,4,2022,10.1145/3544559,15443973,"Sparse tensors arise in problems in science, engineering, machine learning, and data analytics. Programs that operate on such tensors can exploit sparsity to reduce storage requirements and computational time. Developing and maintaining sparse software by hand, however, is a complex and error-prone task. Therefore, we propose treating sparsity as a property of tensors, not a tedious implementation task, and letting a sparse compiler generate sparse code automatically from a sparsity-agnostic definition of the computation. This article discusses integrating this idea into MLIR."
Behroozi2020,Mahnaz Behroozi and Shivani Shirolkar and Titus Barik and Chris Parnin,Does stress impact technical interview performance?,,,,2020,10.1145/3368089.3409712,,"Software engineering candidates commonly participate in whiteboard technical interviews as part of a hiring assessment. During these sessions, candidates write code while thinking aloud as they work towards a solution, under the watchful eye of an interviewer. While technical interviews should allow for an unbiased and inclusive assessment of problem-solving ability, surprisingly, technical interviews may be instead a procedure for identifying candidates who best handle and migrate stress solely caused by being examined by an interviewer (performance anxiety). To understand if coding interviews - as administered today - can induce stress that significantly hinders performance, we conducted a randomized controlled trial with 48 Computer Science students, comparing them in private and public whiteboard settings. We found that performance is reduced by more than half, by simply being watched by an interviewer. We also observed that stress and cognitive load were significantly higher in a traditional technical interview when compared with our private interview. Consequently, interviewers may be filtering out qualified candidates by confounding assessment of problem-solving ability with unnecessary stress. We propose interview modifications to make problem-solving assessment more equitable and inclusive, such as through private focus sessions and retrospective think-aloud, allowing companies to hire from a larger and diverse pool of talent."
Walden2020,James Walden,The Impact of a Major Security Event on an Open Source Project: The Case of OpenSSL,,,,2020,10.1145/3379597.3387465,,"Context: The Heartbleed vulnerability brought OpenSSL to international attention in 2014. The almost moribund project was a key security component in public web servers and over a billion mobile devices. This vulnerability led to new investments in OpenSSL. Objective: The goal of this study is to determine how the Heart-bleed vulnerability changed the software evolution of OpenSSL. We study changes in vulnerabilities, code quality, project activity, and software engineering practices. Method: We use a mixed methods approach, collecting multiple types of quantitative data and qualitative data from web sites and an interview with a developer who worked on post-Heartbleed changes. We use regression discontinuity analysis to determine changes in levels and slopes of code and project activity metrics resulting from Heartbleed. Results: The OpenSSL project made tremendous improvements to code quality and security after Heartbleed. By the end of 2016, the number of commits per month had tripled, 91 vulnerabilities were found and fixed, code complexity decreased significantly, and OpenSSL obtained a CII best practices badge, certifying its use of good open source development practices. Conclusions: The OpenSSL project provides a model of how an open source project can adapt and improve after a security event. The evolution of OpenSSL shows that the number of known vulnerabilities is not a useful indicator of project security. A small number of vulnerabilities may simply indicate that a project does not expend much effort to finding vulnerabilities. This study suggests that project activity and CII badge best practices may be better indicators of code quality and security than vulnerability counts."
Cummaudo2020,Alex Cummaudo and Rajesh Vasa and Scott Barnett and John Grundy and Mohamed Abdelrazek,Interpreting cloud computer vision pain-points: A mining study of stack overflow,,,,2020,10.1145/3377811.3380404,02705257,"Intelligent services are becoming increasingly more pervasive; application developers want to leverage the latest advances in areas such as computer vision to provide new services and products to users, and large technology firms enable this via RESTful APIs. While such APIs promise an easy-to-integrate on-demand machine intelligence, their current design, documentation and developer interface hides much of the underlying machine learning techniques that power them. Such APIs look and feel like conventional APIs but abstract away data-driven probabilistic behaviour-the implications of a developer treating these APIs in the same way as other, traditional cloud services, such as cloud storage, is of concern. The objective of this study is to determine the various pain-points developers face when implementing systems that rely on the most mature of these intelligent services, specifically those that provide computer vision. We use Stack Overflow to mine indications of the frustrations that developers appear to face when using computer vision services, classifying their questions against two recent classification taxonomies (documentation-related and general questions). We find that, unlike mature fields like mobile development, there is a contrast in the types of questions asked by developers. These indicate a shallow understanding of the underlying technology that empower such systems. We discuss several implications of these findings via the lens of learning taxonomies to suggest how the software engineering community can improve these services and comment on the nature by which developers use them."
Laayati2022,Oussama Laayati and Hicham El Hadraoui and Nasr Guennoui and Mostafa Bouzi and Ahmed Chebak,Smart Energy Management System: Design of a Smart Grid Test Bench for Educational Purposes,Energies,15,7,2022,10.3390/en15072702,19961073,"The presented article aims to design an educational test bench setup for smart grids and renewable energies with multiple features and techniques used in a microgrid. The test bench is designed for students, laboratory engineers, and researchers, which enables electrical microgrid system studies and testing of new, advanced control algorithms to optimize the energy efficiency. The idea behind this work is to design hybrid energy sources, such as wind power, solar photovoltaic power, hydroelectric power, hydrogen energy, and different types of energy storage systems such as batteries, pumped storage, and flywheel, integrating different electrical loads. The user can visualize the state of the components of each emulated scenario through an open-source software that interacts and communicates using OPC Unified Architecture protocol. The researchers can test and validate new solutions to manage the energy behavior in the grid using machine learning and optimization algorithms integrated in the software in form of blocks that can be modified and improved, and then simulate the results. A model-based system of engineering is provided, which describes the different requirements and case studies of the designed test bench, respecting the open-source software and the frugal innovation features in which there is use of low-cost hardware and open-source software. The users obtain the opportunity to add new sources and new loads, change software platforms, and communicate with other simulators and equipment. The students can understand the different features of smart grids, such as defect classification, energy forecasting, energy optimization, and basics of production, transmission, and consumption."
Tai2020,Yaming Tai and Yu Liang Ting,English-learning mobile app designing for engineering students’ cross-disciplinary learning and collaboration: A sample practice and preliminary evaluation,Australasian Journal of Educational Technology,36,4,2020,10.14742/ajet.4999,14495554,"Engineers must be able to collaborate with experts across disciplinary boundaries to successfully address the complex challenges of a contemporary workplace. As programming has a reflexive synergistic character, it can support a way of thinking about and exploring disciplines beyond computer science. Meanwhile, programming is a medium of communication. In this study, engineering students used authoring tools of mobile app to design an English-learning app, in which English teachers provide feedbacks for revision, and a preliminary cross-disciplinary collaboration is asserted. The learning design referred to the frameworks of constructing socio-technical creativity to aid the students in employing their existing English knowledge to design mobile apps. The evaluation was conducted on the basis of the students’ designed apps and presentation, teachers’ feedback, and students’ self-reports. The results indicated that the common use of mobile apps in daily life seemed to create a boundary object for the engineering students and English teachers to express, communicate, and coordinate their perspectives and knowledge. Students could view the engineering work from different aspects and value English teachers’ expertise. By accounting for the interests of students and schools’ limited resources, this study serves as a reference for developing a method of cross-disciplinary learning and collaboration. Implications for practice or policy: • Students can acquire knowledge of English as well as programming skills in an instructional software design activity. • Cross-disciplinary collaboration could be enhanced by using mobile application as a boundary object. • Teachers could use instructional software design activity to promote cross-disciplinary learning and collaboration for engineering students"
Haibi2022,Achraf Haibi and Kenza Oufaska and Khalid El Yassini and Mohammed Boulmalf and Mohsine Bouya,Systematic Mapping Study on RFID Technology,IEEE Access,10,,2022,10.1109/ACCESS.2022.3140475,21693536,"Radio Frequency Identification (RFID) is a technology that not only serves to identify objects but also communicates other information, allowing the real-time monitoring of objects at each step in a mobile object network and the reporting of information on their current status. RFID has become one of the most promising research areas and has attracted increasing attention. This interest sparks a huge amount of literature in the field of RFID. However, the research has been conducted from different perspectives and, as a result, has led to a growing body of knowledge dispersed in different fields. To fill this gap, we carried out a systematic mapping study (SMS) based on a well-established research methodology from the medical and software engineering scientific communities, which aims to study and identify the approaches used, quantity and quality of publications, types of research, and publication trends that shaped the field of RFID research over the past two decades. Its results were based on 219 studies, rigorously selected from among 4294 studies identified in the IEEE Xplore, Scopus, and Web of Science digital libraries and classified according to the research type facet, research area facet, citation facet, and application domain facet. We synthesized and interpreted the results of this SMS to devise future research directions in the RFID domain. This breadth-first SMS provides a solid, comprehensive, and reproducible picture of state-of-the-art RFID technology; the obtained results may have implications for practitioners willing to understand and adopt RFID, including researchers, journal editors, reviewers, and universities. The results obtained revealed that (1) there is a considerable and continuous rise of RFID research activities around different parts of the globe, including in the USA and China, and other English-speaking developed countries, such as Australia, Canada, and the U.K., have a significant influence on this growth; (2) with the technological progress of RFID hardware components and increasingly demanding application domains, RFID technology brings opportunities in some new areas, such as 'IoT applications', 'Complex Environments', and 'Industry 4.0'; (3) despite the high number of studies carried out in the field of RFID, especially in the hardware design and performances subfield, a limited number of works have detailed or focused on the 'middleware' component of RFID systems, indicating that RFID data processing and management remain an open research issue; and (4) RFID domain challenges, gaps, and feasible future recommendations were highlighted in this study."
Cito2022,Jürgen Cito and Isil Dillig and Vijayaraghavan Murali and Satish Chandra,Counterfactual Explanations for Models of Code,,,,2022,10.1109/ICSE-SEIP55303.2022.9794112,02705257,"Machine learning (ML) models play an increasingly prevalent role in many software engineering tasks. However, because most models are now powered by opaque deep neural networks, it can be difficult for developers to understand why the model came to a certain conclusion and how to act upon the model's prediction. Motivated by this problem, this paper explores counterfactual explanations for models of source code. Such counterfactual explanations constitute minimal changes to the source code under which the model 'changes its mind'. We integrate counterfactual explanation generation to models of source code in a real-world setting. We describe considerations that impact both the ability to find realistic and plausible counterfactual explanations, as well as the usefulness of such explanation to the developers that use the model. In a series of experiments we investigate the efficacy of our approach on three different models, each based on a BERT-like architecture operating over source code."
Markiewicz2020,Michal Markiewicz and Piotr Dziurdzia and Tomasz Konieczny and Marek Skomorowski and Liliana Kowalczyk and Thomas Skotnicki and Pascal Urard,Software Controlled Low Cost Thermoelectric Energy Harvester for Ultra-Low Power Wireless Sensor Nodes,IEEE Access,8,,2020,10.1109/ACCESS.2020.2975424,21693536,"General hardware architecture of an energy-harvested wireless sensor network node (EH-WSN) can be divided into power, sensing, computing and communication subsystems. Interrelation between these subsystems in combination with constrained energy supply makes design and implementation of EH-WSN a complex and challenging task. Separation of these subsystems into distinct hardware modules simplifies the design process and makes the architecture and software more generic, leading to more flexible solutions. From the other hand, tightly coupling these subsystems gives more room for optimizations at the price of increased complexity of the hardware and software. Additional engineering effort could be justified by a smaller, cheaper hardware, and more energy-efficient a wireless sensor node. The aim of this paper is to push further technical and economical boundaries related to EH-WSN by proposing a novel architecture which - by tightly coupling software and hardware of power, computing, and communication subsystems - allows the wireless sensor node to be powered by a thermoelectric generator working with about 1.5°C temperature difference while keeping the cost of all electronic components used to build such a node below 9 EUR (in volume)."
Ahlgren2020,John Ahlgren and Maria Eugenia Berezin and Kinga Bojarczuk and Elena Dulskyte and Inna Dvortsova and Johann George and Natalija Gucevska and Mark Harman and Ralf Lämmel and Erik Meijer and Silvia Sapora and Justin Spahr-Summers,WES: Agent-based User Interaction Simulation on Real Infrastructure,,,,2020,10.1145/3387940.3392089,,"We introduce the Web-Enabled Simulation (WES) research agenda, and describe FACEBOOK's WW system. We describe the application of WW to reliability, integrity and privacy at FACEBOOK1, where it is used to simulate social media interactions on an infrastructure consisting of hundreds of millions of lines of code. The WES agenda draws on research from many areas of study, including Search Based Software Engineering, Machine Learning, Programming Languages, Multi Agent Systems, Graph Theory, Game AI, and AI Assisted Game Play. We conclude with a set of open problems and research challenges to motivate wider investigation."
Tahaei2022,Mohammad Tahaei and Kami Vaniea,Recruiting Participants With Programming Skills: A Comparison of Four Crowdsourcing Platforms and a CS Student Mailing List,,,,2022,10.1145/3491102.3501957,,"Reliably recruiting participants with programming skills is an ongoing challenge for empirical studies involving software development technologies, often leading to the use of crowdsourcing platforms and computer science (CS) students. In this work, we use five existing survey instruments to explore the programming skills, privacy and security attitudes, and secure development self-efficacy of participants from a CS student mailing list and four crowdsourcing platforms (Appen, Clickworker, MTurk, and Prolific). We recruited 613 participants who claimed to have programming skills and assessed recruitment channels regarding costs, quality, programming skills, as well as privacy and security attitudes. We find that 27% of crowdsourcing participants, 40% of crowdsourcing participants who self-report to be developers, and 89% of CS students answered all programming skill questions correctly. CS students were the most cost-effective recruitment channel and rated themselves lower than crowdsourcing participants about secure development self-efficacy."
Farid2021,Ahmed Bahaa Farid and Enas Mohamed Fathy and Ahmed Sharaf Eldin and Laila A. Abd-Elmegid,Software defect prediction using hybrid model (CBIL) of convolutional neural network (CNN) and bidirectional long short-term memory (Bi-LSTM),PeerJ Computer Science,7,,2021,10.7717/peerj-cs.739,23765992,"In recent years, the software industry has invested substantial effort to improve software quality in organizations. Applying proactive software defect prediction will help developers and white box testers to find the defects earlier, and this will reduce the time and effort. Traditional software defect prediction models concentrate on traditional features of source code including code complexity, lines of code, etc. However, these features fail to extract the semantics of source code. In this research, we propose a hybrid model that is called CBIL. CBIL can predict the defective areas of source code. It extracts Abstract Syntax Tree (AST) tokens as vectors from source code. Mapping and word embedding turn integer vectors into dense vectors. Then, Convolutional Neural Network (CNN) extracts the semantics of AST tokens. After that, Bidirectional Long Short-Term Memory (Bi-LSTM) keeps key features and ignores other features in order to enhance the accuracy of software defect prediction. The proposed model CBIL is evaluated on a sample of seven open-source Java projects of the PROMISE dataset. CBIL is evaluated by applying the following evaluation metrics: F -measure and area under the curve (AUC). The results display that CBIL model improves the average of F -measure by 25% compared to CNN, as CNN accomplishes the top performance among the selected baseline models. In average of AUC, CBIL model improves AUC by 18% compared to Recurrent Neural Network (RNN), as RNN accomplishes the top performance among the selected baseline models used in the experiments. Subjects Artificial Intelligence, Software Engineering"
Kalhoro2021,Shadab Kalhoro and Mobashar Rehman and Vasaki Ponnusamy and Farhan Bashir Shaikh,Extracting key factors of cyber hygiene behaviour among software engineers: A systematic literature review,IEEE Access,9,,2021,10.1109/ACCESS.2021.3097144,21693536,"The advent of new technologies and the rapid growth of internet users have given birth to the menace of cyber-crime. Unfortunately, it is increasing at an alarming pace. This situation calls for good cyber hygiene behavior to secure digital lives. Cyber hygiene behaviour holds a significant role in terms of cybersecurity across the globe. There is a dire need to understand better the user variations associated with good or bad cyber hygiene behaviour and an improved view of what users do to encourage good cyber hygiene. Cybersecurity attacks are rising due to recent advancements in ICT and the Industrial Revolution 4.0 (IR 4.0). Software development organizations are among the crucial sectors suffering from cybersecurity issues. These organizations are more vulnerable to cyber-attacks because they lack proper cybersecurity culture. Although many initiatives have been taken by academia and industry to address this rising issue, the problem still exists for Software development organizations because good cyber hygiene behaviour is not observed, which is a prerequisite to reduce cyber threats. This study performed a Systematic Literature Review (SLR) of research papers published during 2010 - 2020. The key factors influencing software engineers' cyber hygiene behaviour intention are extracted from the published literature. The study examined 35 research papers out of 5,270 found from IEEE Xplore, Emerald Insight, SpringerLink, and ScienceDirect databases. The study reviewed number of factors such as the role of personal, social, socio-cognitive, environmental, & technological factors that may individually or collectively influence software engineers' cyber hygiene behaviour. The positive and negative factors associated with the cyber hygiene behaviour of software engineers are also categorized. This study enriches the understanding of the potential factors related to software engineers' cyber hygiene behaviours. It provides valuable insights to researchers, software development organizations, governments, and individuals associated with the field of Software Engineering. This research will assist in changing the software engineers' behaviour towards cyber hygiene, which will ultimately lead to mitigate the issues of Cybersecurity."
Muhairat2020,Mohammad Muhairat and Shadi Alzu’bi and Bilal Hawashin and Mohammad Elbes and Mahmoud Al-Ayyoub,An intelligent recommender system based on association rule analysis for requirement engineering,Journal of Universal Computer Science,26,1,2020,10.3897/jucs.2020.003,09486968,"Requirement gathering is a vital step in software engineering. Even though many recent researches concentrated on the improvement of the requirement gathering process, many of their works lack completeness especially when the number of users is large. Data Mining techniques have been recently employed in various domains with promising results. In this work, we propose an intelligent recommender system for requirement engineering based on association rule analysis, which is a main category in Data Mining. Such recommender would contribute in enhancing the accuracy of the gathered requirements and provide more comprehensive results. Conducted experiments in this work prove that FP Growth outperformed Apriori in terms of execution and space consumption, while both methods were efficient in term of accuracy."
Ahmed2020,Bestoun S. Ahmed and Eduard Enoiu and Wasif Afzal and Kamal Z. Zamli,An evaluation of Monte Carlo-based hyper-heuristic for interaction testing of industrial embedded software applications,Soft Computing,24,18,2020,10.1007/s00500-020-04769-z,14337479,"Hyper-heuristic is a new methodology for the adaptive hybridization of meta-heuristic algorithms to derive a general algorithm for solving optimization problems. This work focuses on the selection type of hyper-heuristic, called the exponential Monte Carlo with counter (EMCQ). Current implementations rely on the memory-less selection that can be counterproductive as the selected search operator may not (historically) be the best performing operator for the current search instance. Addressing this issue, we propose to integrate the memory into EMCQ for combinatorial t-wise test suite generation using reinforcement learning based on the Q-learning mechanism, called Q-EMCQ. The limited application of combinatorial test generation on industrial programs can impact the use of such techniques as Q-EMCQ. Thus, there is a need to evaluate this kind of approach against relevant industrial software, with a purpose to show the degree of interaction required to cover the code as well as finding faults. We applied Q-EMCQ on 37 real-world industrial programs written in Function Block Diagram (FBD) language, which is used for developing a train control management system at Bombardier Transportation Sweden AB. The results show that Q-EMCQ is an efficient technique for test case generation. Addition- ally, unlike the t-wise test suite generation, which deals with the minimization problem, we have also subjected Q-EMCQ to a maximization problem involving the general module clustering to demonstrate the effectiveness of our approach. The results show the Q-EMCQ is also capable of outperforming the original EMCQ as well as several recent meta/hyper-heuristic including modified choice function, Tabu high-level hyper-heuristic, teaching learning-based optimization, sine cosine algorithm, and symbiotic optimization search in clustering quality within comparable execution time."
Zhang2021,Haibo Zhang and Kouichi Sakurai,A Survey of Software Clone Detection from Security Perspective,IEEE Access,9,,2021,10.1109/ACCESS.2021.3065872,21693536,"For software engineering, if two code fragments are closely similar with minor modifications or even identical due to a copy-paste behavior, that is called software/code clone. Code clones can cause trouble in software maintenance and debugging process because identifying all copied compromised code fragments in other locations is time-consuming. Researchers have been working on code clone detection issues for a long time, and the discussion mainly focuses on software engineering management and system maintenance. Another considerable issue is that code cloning provides an easy way to attackers for malicious code injection. A thorough survey work of code clone identification/detection from the security perspective is indispensable for providing a comprehensive review of existing related works and proposing future potential research directions. This paper can satisfy above requirements. We review and introduce existing security-related works following three different classifications and various comparison criteria. We then discuss three further research directions, (i) deep learning-based code clone vulnerability detection, (ii) vulnerable code clone detection for 5G-Internet of Things devices, and (iii) real-time detection methods for more efficiently detecting clone attacks. These methods are more advanced and adaptive to technological development than current technologies, and still have enough research space for future studies."
Bernius2022,Jan Philip Bernius and Stephan Krusche and Bernd Bruegge,Machine learning based feedback on textual student answers in large courses,Computers and Education: Artificial Intelligence,3,,2022,10.1016/j.caeai.2022.100081,2666920X,"Many engineering disciplines require problem-solving skills, which cannot be learned by memorization alone. Open-ended textual exercises allow students to acquire these skills. Students can learn from their mistakes when instructors provide individual feedback. However, grading these exercises is often a manual, repetitive, and time-consuming activity. The number of computer science students graduating per year has steadily increased over the last decade. This rise has led to large courses that cause a heavy workload for instructors, especially if they provide individual feedback to students. This article presents CoFee, a framework to generate and suggest computer-aided feedback for textual exercises based on machine learning. CoFee utilizes a segment-based grading concept, which links feedback to text segments. CoFee automates grading based on topic modeling and an assessment knowledge repository acquired during previous assessments. A language model builds an intermediate representation of the text segments. Hierarchical clustering identifies groups of similar text segments to reduce the grading overhead. We first demonstrated the CoFee framework in a small laboratory experiment in 2019, which showed that the grading overhead could be reduced by 85%. This experiment confirmed the feasibility of automating the grading process for problem-solving exercises. We then evaluated CoFee in a large course at the Technical University of Munich from 2019 to 2021, with up to 2, 200 enrolled students per course. We collected data from 34 exercises offered in each of these courses. On average, CoFee suggested feedback for 45% of the submissions. 92% (Positive Predictive Value) of these suggestions were precise and, therefore, accepted by the instructors."
Luburi2021,Nikola Luburić and Jelena Slivka and Goran Sladić and Gordana Milosavljević,The challenges of migrating an active learning classroom online in a crisis,Computer Applications in Engineering Education,29,6,2021,10.1002/cae.22413,10990542,"The coronavirus disease of 2019 (COVID-19) pandemic has severely crippled our globalized society. Despite the chaos, much of our civilization continued to function, thanks to contemporary information and communication technologies. In education, this situation required instructors and students to abandon the traditional face-to-face lectures and move to a fully online learning environment. Such a transition is challenging, both for the teacher tasked with creating digital educational content, and the student who needs to study in a new and isolated working environment. As educators, we have experienced these challenges when migrating our university courses to an online environment. Through this paper, we look to assist educators with building and running an online course. Before we needed to transition online, we researched and followed the best practices to establish various digital educational elements in our online classroom. We present these elements, along with guidance regarding their development and use. Next, we designed an empirical study consisting of two surveys, focus group discussions, and observations to understand the factors that influenced students' engagement with our online classroom. We used the same study to evaluate students' perceptions regarding our digital educational elements. We report the findings and define a set of recommendations from these results to help educators motivate their students and develop engaging digital educational content. Although our research is motivated by the pandemic, our findings and contributions are useful to all educators looking to establish some form of online learning. This includes developers of massive open online courses and teachers promoting blended learning in their classrooms."
Hassan2020,Ali Abdullah Hassan and Salwani Abdullah and Kamal Z. Zamli and Rozilawati Razali,Combinatorial test suites generation strategy utilizing the whale optimization algorithm,IEEE Access,8,,2020,10.1109/ACCESS.2020.3032851,21693536,"The potentially many software system input combinations make exhaustive testing practically impossible. To address this issue, combinatorial t-way testing (where t indicates the interaction strength, i.e. the number of interacting parameters (input)) was adopted to minimize the number of cases for testing. Complimentary to existing testing techniques (e.g. boundary value, equivalence partitioning, cause and effect graphing), combinatorial testing helps to detect faults caused by the faulty interaction between input parameters. In the last 15 years, applications of meta-heuristics as the backbone of t-way test suite generation have shown promising results (e.g. Particle Swarm Optimization, Cuckoo Search, Flower Pollination Algorithm, and Hyper-Heuristics (HHH), to name a few). Supporting the No Free Lunch theorem, as well as potentially offering new insights into the whole process of t-way generation, this article proposes a new strategy with constraint support based on the Whale Optimization Algorithm (WOA). Our work is the first attempt to adopt the WOA as part of a search-based software engineering (SBSE) initiative for t-way test suite generation with constraint support. The experimental results of the test-suite generation indicate that WOA produces competitive outcomes compared to some selected single-based and population-based metaheuristic algorithms."
Li2020,Dongting Li and Sibo Huang and Fangshuo Mo and Xu Wang and Yong Li,Low-frequency broadband absorbers based on coupling micro-perforated panel and space-curling chamber,Kexue Tongbao/Chinese Science Bulletin,65,15,2020,10.1360/TB-2019-0703,20959419,"Conventional acoustic absorbers, which are made of porous and fibrous materials, usually have a structural thickness comparable to the working wavelength, which inevitably hinders their applications in low frequency range. A micro-perforated panel with a backward cavity has high efficient absorption in low frequency region, yet still processes a bulky size comparable to the resonant wavelength. In the past two decades, metamaterials and metasurfaces, which possess the functionalities cannot be achieved by the natural materials, such as deep-subwavelength thickness, negative mass density or bulk modulus, negative refraction, etc., have got continuous attention and hence rapidly developed. Using the compact structures, such as membrane-type acoustic metamaterials, curling-up space structures, Helmholtz resonators with embedded apertures, to realize perfect absorption in low frequency has broadened the horizon of the acoustic absorption and provided a novel way for noise control. However, because of the dispersive nature of resonance, the aforementioned perfect absorbers cannot achieve broadband high absorption, which unavoidably confines their application. It is known that a micro-perforated panel features a broadband characteristic, while curling-up space structures have the capacity in absorbing the low frequency waves. Therefore, we come up with the idea of utilizing the micro-perforated panel and curling-up chamber to realize the high absorption in low and broad frequency region. In this work, we propose a hybrid absorber. By introducing the concept of curling chambers and coupling them to micro-perforated panels, the proposed acoustic absorber possesses remarkable properties such as deep-subwavelength thickness, broad absorption band and high absorption efficiency. Theoretical analyses, numerical simulations and experiments are carried out to reveal the underlying physical mechanism of the acoustic absorbers, whose results showed excellent agreements with each other. First, we theoretically analyze the absorption performance of each component consisting of a micro-perforated panel and a curling chamber. Then, we conduct the numerical validation and operate the experiment to verify the theoretical model. Through numerical optimizations, we finally design the two components achieving perfect absorption at lower (282 Hz) and higher (429 Hz) frequencies, respectively. Using the coupled mode theory, we present an absorber coupling the low- and high-frequency components. The hybrid absorber possesses the ability that the absorption coefficient is larger than 0.5 in the frequency range from 232 to 533 Hz, and larger than 0.8 from 262 to 469 Hz, while its structural thickness is only 10 cm (λ/12, λ is the working wavelength corresponding to the lower resonant frequency). To understand the underlying physical mechanism, we use the commercial software COMSOL MULTIPHYSICS to show the interaction (i.e., coupling) between the two components. As shown by the simulated results, at the resonant frequency of each component (282 or 429 Hz), when planar wave impinges normally on the whole absorber surface, only the corresponding component has sound energy exchange with the outside. However, at the frequency between the two resonant frequencies, both of them switch sound energy with the outside, which demonstrates the coupling effect. In this way, the hybrid absorber can realize the broadband high absorption in low frequency range. Compared to traditional bulky structures, the proposed absorber possesses the ability such as easy to fabrication, deep subwavelength and broadband high absorption, and hence may be widely used in noise control engineering."
Smolyaninov2021,Andrey Smolyaninov and Irina Pocebneva and Irina Fateeva and Konstantin Singur,Software implementation of a virtual laboratory bench for distance learning,,244,,2021,10.1051/e3sconf/202124411009,22671242,"The article discusses the functional development and integration into the educational process of a virtual laboratory stand, with the aim of digitalizing the educational processes of higher education without loss as educational services related to the completeness of mastering professional competencies in specialized engineering courses. The developed laboratory stand allows simulating technological processes implemented in an apparatus consisting of four connected tanks. Modeling such a complex technological process with many interconnected elements is necessary for the most detailed research and synthesis of automatic control systems for multiply connected objects. The program simulates two control loops, each of which can be switched to manual or automatic operation. To realize the maximum adequacy of the mathematical model, the program provides for the possibility of taking into account the influence of the actuators on the regulation process."
McCoy2021,Dakota E. McCoy and Anna V. Shneidman and Alexander L. Davis and Joanna Aizenberg,Finite-difference Time-domain (FDTD) Optical Simulations: A Primer for the Life Sciences and Bio-Inspired Engineering,Micron,151,,2021,10.1016/j.micron.2021.103160,09684328,"Light influences most ecosystems on earth, from sun-dappled forests to bioluminescent creatures in the ocean deep. Biologists have long studied nano- and micro-scale organismal adaptations to manipulate light using ever-more sophisticated microscopy, spectroscopy, and other analytical equipment. In combination with experimental tools, simulations of light interacting with objects can help researchers determine the impact of observed structures and explore how variations affect optical function. In particular, the finite-difference time-domain (FDTD) method is widely used throughout the nanophotonics community to efficiently simulate light interacting with a variety of materials and optical devices. More recently, FDTD has been used to characterize optical adaptations in nature, such as camouflage in fish and other organisms, colors in sexually-selected birds and spiders, and photosynthetic efficiency in plants. FDTD is also common in bioengineering, as the design of biologically-inspired engineered structures can be guided and optimized through FDTD simulations. Parameter sweeps are a particularly useful application of FDTD, which allows researchers to explore a range of variables and modifications in natural and synthetic systems (e.g., to investigate the optical effects of changing the sizes, shape, or refractive indices of a structure). Here, we review the use of FDTD simulations in biology and present a brief methods primer tailored for life scientists, with a focus on the commercially available software Lumerical FDTD. We give special attention to whether FDTD is the right tool to use, how experimental techniques are used to acquire and import the structures of interest, and how their optical properties such as refractive index and absorption are obtained. This primer is intended to help researchers understand FDTD, implement the method to model optical effects, and learn about the benefits and limitations of this tool. Altogether, FDTD is well-suited to (i) characterize optical adaptations and (ii) provide mechanistic explanations; by doing so, it helps (iii) make conclusions about evolutionary theory and (iv) inspire new technologies based on natural structures."
Sadiq2020,S. E. Sadiq and S. H. Bakhy and M. J. Jweeg,Crashworthiness behavior of aircraft sandwich structure with honeycomb core under bending load.,,881,1,2020,10.1088/1757-899X/881/1/012046,1757899X,"Sandwich structures have been widely used as lightweight composite parts in the aerospace and shipbuilding engineering for their high capacity of stiffness, strength and energy absorption. There are three different criteria in bending crashworthiness for sandwich structure, namely peak bending load, maximum deflection and energy absorption. In this paper, the crashworthiness criteria of sandwich structure were evaluated theoretically and numerically based on failure mode maps. A failure mode map for the loading under three-point bending was constructed, depicting the reliance of the failure mode and the load upon the ratio of the skin thickness to the span length and the relative density of honeycomb. The finite element models for the sandwich panel with a honeycomb core were developed and analyzed via Ansys soft-ware package. The obtained result elucidated a good agreement between these models and the theoretical solution, where the error ratio was not exceeded 5%.To explore the effect of honeycomb parameters on the crashworthiness criteria of sandwich structure, several parameters have been selected, including the core height, the size of cell and the thickness of cell wall. In order to obtain the optimum solution of crashworthiness, Design of Experiment (DOE) software with the technique of Response Surface Methodology (RSM) was used. Results showed that the optimum value of peak bending load (25310 N) as maximum, deflection as minimum (0.8976 mm) and energy absorption as maximum (9.9949 J) were found at 29.2424 mm core height, 5 mm cell size and 1 mm cell wall thickness. Finally, the present study provides a new basis for more studies upon the optimization of the crashworthiness of sandwich structures."
Uchida2020,Takanori Uchida and Yoshihiro Taniyama and Yuki Fukatani and Michiko Nakano and Zhiren Bai and Tadasuke Yoshida and Masaki Inui,A new wind turbine CFD modeling method based on a porous disk approach for practical wind farm design,Energies,13,12,2020,10.3390/en13123197,19961073,"In this study, the new computational fluid dynamics (CFD) porous disk (PD) wake model was proposed in order to accurately predict the time-averaged wind speed deficits in the wind turbine wake region formed on the downstream side by the 2-MW wind turbine operating at a wind speed of 10 m/s. We use the concept of forest canopy model as a new CFD PD wake model, which has many research results in the meteorological field. In the forest canopy model, an aerodynamic resistance is added as an external force term to all governing equations (Navier-Stokes equations) in the streamwise, spanwise, and vertical directions. Therefore, like the forest model, the aerodynamic resistance is added to the governing equations in the three directions as an external force term in the CFD PD wake model. In addition, we have positioned the newly proposed the LES using the CFD PD wake model approach as an intermediate method between the engineering wake model (empirical/analytical wake model) and the LES combined with actuator disk (AD) or actuator line (AL) models. The newly proposed model is intended for use in large-scale offshore wind farms (WFs) consisting of multiple wind turbines. In order to verify the validity of the new method, the optimal model parameter CRC was estimated by comparison with the time-averaged wind speed database in the wind turbine wake region with fully resolved geometries, combined with unsteady Reynolds-averaged Navier-Stokes (RANS) equations, implemented using the ANSYS(R) CFX(R) software. Here, product names (mentioned herein) may be trademarks of their respective companies. As a result, in the range from x = 5D of the near wake region to x = 10D of the far wake region, by selecting model parameter CRC, it was clarified that it is possible to accurately evaluate the time-averaged wind speed deficits at those separation distances. We also examined the effect of the spatial grid resolution using the CFD PD wake model that is proposed in the present study, clarifying that the spatial grid resolution has little effect on the simulation results shown here."
Mavlonov2020,T. Mavlonov and A. Akhmedov and R. Saidakhmedov and K. Bakhadirov,Simulation modelling of cold rolled metal strip by asymmetric technology,,883,1,2020,10.1088/1757-899X/883/1/012194,1757899X,"In the article provided the results of studies the optimization of energy-saving technologies in mechanical engineering using modern methods of simulation modeling is becoming an urgent area for the informatization of engineering production on the process of cold asymmetric rolling of metal sheets is accompanied by an inhomogeneous stress-strain state. The research considers the noncanonical areas of strain, an approach is proposed for determining the optimal technological parameters and studying the stress state of the cold rolling process using asymmetric rolling technology in the active zone of elastoplastic strains based on simulation using the application software LS-DYNA by LS-PrePost (R) version V4.6.1. To optimize energy-saving for rolling technologies, considered a stationary regime of symmetrical rolling, the kinetic energy does not change over time, and the potential energy in the active zone of elastic-plastic strains tends to increase. For the case of asymmetric rolling, a high level of kinetic energy is observed, and there is a fluctuation over time, explained by different coverage of asymmetric rollers in the zone of active elastic-plastic strains, as a result of which the stationary nature of the metal sheet rolling mode is violated."
Perin2022,Guilherme Perin and Lichao Wu and Stjepan Picek,Exploring Feature Selection Scenarios for Deep Learning-based Side-channel Analysis,IACR Transactions on Cryptographic Hardware and Embedded Systems,2022,4,2022,10.46586/tches.v2022.i4.828-861,25692925,"One of the main promoted advantages of deep learning in profiling side-channel analysis is the possibility of skipping the feature engineering process. Despite that, most recent publications consider feature selection as the attacked interval from the side-channel measurements is pre-selected. This is similar to the worst-case security assumptions in security evaluations when the random secret shares (e.g., mask shares) are known during the profiling phase: an evaluator can identify points of interest locations and efficiently trim the trace interval. To broadly understand how feature selection impacts the performance of deep learning-based profiling attacks, this paper investigates three different feature selection scenarios that could be realistically used in practical security evaluations. The scenarios range from the minimum possible number of features (worst-case security assumptions) to the whole available traces. Our results emphasize that deep neural networks as profiling models show successful key recovery independently of explored feature selection scenarios against first-order masked software implementations of AES-128. First, we show that feature selection with the worst-case security assumptions results in optimal profiling models that are highly dependent on the number of features and signal-to-noise ratio levels. Second, we demonstrate that attacking raw side-channel measurements with small deep neural networks also provides optimal models, that shortens the gap between worst-case security evaluations and online (realistic) profiling attacks. In all explored feature selection scenarios, the hyperparameter search always indicates a successful model with up to eight hidden layers for MLPs and CNNs, suggesting that complex models are not required for the considered datasets. Our results demonstrate the key recovery with less than ten attack traces for all datasets for at least one of the feature selection scenarios. Additionally, in several cases, we can recover the target key with a single attack trace."
Fioraldi2022,Andrea Fioraldi and Dominik Christian Maier and Dongjia Zhang and Davide Balzarotti,LibAFL: A Framework to Build Modular and Reusable Fuzzers,,,,2022,10.1145/3548606.3560602,15437221,"The release of AFL marked an important milestone in the area of software security testing, revitalizing fuzzing as a major research topic and spurring a large number of research studies that attempted to improve and evaluate the different aspects of the fuzzing pipeline. Many of these studies implemented their techniques by forking the AFL codebase. While this choice might seem appropriate at first, combining multiple forks into a single fuzzer requires a high engineering overhead, which hinders progress in the area and prevents fair and objective evaluations of different techniques. The highly fragmented landscape of the fuzzing ecosystem also prevents researchers from combining orthogonal techniques and makes it difficult for end users to adopt new prototype solutions. To tackle this problem, in this paper we propose LibAFL, a framework to build modular and reusable fuzzers. We discuss the different components generally used in fuzzing and map them to an extensible framework. LibAFL allows researchers and engineers to extend the core fuzzer pipeline and share their new components for further evaluations. As part of LibAFL, we integrated techniques from more than 20 previous works and conduct extensive experiments to show the benefit of our framework to combine and evaluate different approaches. We hope this can help to shed light on current advancements in fuzzing and provide a solid base for comparative and extensible research in the future."
Irwandi2020,I. Irwandi and I. M. Sari and R. Oktavia and M. Syukri,MEMS and IoT Applications in ISLE-based STEM Physics Learning Media for Mechanics Topic with LabVIEW Integration,,1462,1,2020,10.1088/1742-6596/1462/1/012066,17426596,"The development of technology in the 21st century is so rapid both in the field of sensor and data communication. In fact, almost all smartphones have used the technology. In addition to using cutting edge sensor technology, smartphones are also equipped with internet communication features so they can be categorized as IoT (Internet of Things) devices. However, both of these technologies have not been optimally used as a media for learning physics. This paper discusses the use of MEMs (Micro-electromechanical system) sensor (ADXL-345) to detect the oscillation of the spring-mass system. Data obtained by MEMs is read by IoT based Microcontrollers (NodeMCU EPS8266) and directly transferred to the intranet WIFI network via the UDP (User Datagram Protocol). The data transmitted in the UDP packet is received by a computer that is integrated with LabVIEW, a user-friendly software that is based on a Graphical Programming. Students were encouraged to conduct several experiments to investigate the dependence of the period of the spring-mass system on the mass, the spring constant, and the amplitude. The oscillation of the spring-mass system received by the computer recorded the amplitude and the period of oscillation. Then students will compare the data from various observational experiments to predict the relation among the variables. Finally, they tested their prediction by conducting a testing experiment. This learning model is using ISLE (Investigation Sciences Learning Environment) syntax where students constructed their knowledge about mechanics, especially about spring-mass system, and enjoyed their discoveries as scientists did. Since the learning media has used technology and also carried out engineering process stages and involved the calculation of mathematics to understand the measurement results, it can be said to have used the STEM (Sciences, Technology, Engineering, and Mathematics) approach. We observed that an atmosphere of ""learning is fun"" existed during the lesson."
Middleton2020,Richard S. Middleton and Bailian Chen and Dylan R. Harp and Ryan M. Kammer and Jonathan D. Ogland-Hand and Jeffrey M. Bielicki and Andres F. Clarens and Robert P. Currier and Kevin M. Ellett and Brendan A. Hoover and Dane N. McFarlane and Rajesh J. Pawar and Philip H. Stauffer and Hari S. Viswanathan and Sean P. Yaw,"Great SCO2T! Rapid tool for carbon sequestration science, engineering, and economics",Applied Computing and Geosciences,7,,2020,10.1016/j.acags.2020.100035,25901974,"CO2 capture and storage (CCS) technology is likely to be widely deployed in the coming decades in response to major climate and economics drivers: CCS is part of every clean energy pathway that limits global warming to 2 ​°C or less and receives significant CO2 tax credits in the United States. These drivers are likely to stimulate the capture, transport, and storage of hundreds of millions or billions of tonnes of CO2 annually. A key part of the CCS puzzle will be identifying and characterizing suitable storage sites for vast amounts of CO2. We introduce a new software tool called SCO2T (Sequestration of CO2 Tool, pronounced “Scott”), a dynamic CO2 injection and storage model, to rapidly characterize saline storage reservoirs. The tool is designed to rapidly screen hundreds of thousands of reservoirs, perform sensitivity and uncertainty analyses, and link sequestration engineering (injection rates, reservoir capacities, plume dimensions) to sequestration economics (costs constructed from around 70 separate economic inputs). We describe the novel science developments supporting SCO2T including a new approach to estimating CO2 injection rates and CO2 plume dimensions as well as key advances linking sequestration engineering with economics. We perform a sensitivity and uncertainty analysis of geology parameter combinations—including formation depth, thickness, permeability, porosity, and temperature—to understand the impact on carbon sequestration. Through the sensitivity analysis, we show that increasing depth and permeability both can lead to increased CO2 injection rates, increased storage potential, and reduced costs, while increasing porosity reduces costs without impacting the injection rate (CO2 is injected at a constant pressure in all cases) by increasing the reservoir capacity. Through uncertainty analysis—where formation thickness, permeability, and porosity are randomly sampled—we show that final sequestration costs are normally distributed with upper bound costs around 50% higher than the lower bound costs. While site selection decisions will ultimately require detailed site characterization and permitting, SCO2T provides an inexpensive dynamic screening tool that can help prioritize projects based on the complex interplay of reservoir, infrastructure (e.g., proximity to pipelines), and other (e.g., land use, legal) constraints on the suitability of certain regions for CCS."
Sandobalin2020,Julio Sandobalin and Emilio Insfran and Silvia Abrahao,On the effectiveness of tools to support infrastructure as code: Model-driven versus code-centric,IEEE Access,8,,2020,10.1109/ACCESS.2020.2966597,21693536,"Infrastructure as Code (IaC) is an approach for infrastructure automation that is based on software development practices. The IaC approach supports code-centric tools that use scripts to specify the creation, updating and execution of cloud infrastructure resources. Since each cloud provider offers a different type of infrastructure, the definition of an infrastructure resource (e.g., a virtual machine) implies writing several lines of code that greatly depend on the target cloud provider. Model-driven tools, meanwhile, abstract the complexity of using IaC scripts through the high-level modeling of the cloud infrastructure. In a previous work, we presented an infrastructure modeling approach and tool (Argon) for cloud provisioning that leverages model-driven engineering and supports the IaC approach. The objective of the present work is to compare a model-driven tool (Argon) with a well-known code-centric tool (Ansible) in order to provide empirical evidence of their effectiveness when defining the cloud infrastructure, and the participants' perceptions when using these tools. We, therefore, conducted a family of three experiments involving 67 Computer Science students in order to compare Argon with Ansible as regards their effectiveness, efficiency, perceived ease of use, perceived usefulness, and intention to use. We used the AB/BA crossover design to configure the individual experiments and the linear mixed model to statistically analyze the data collected and subsequently obtain empirical findings. The results of the individual experiments and meta-analysis indicate that Argon is more effective as regards supporting the IaC approach in terms of defining the cloud infrastructure. The participants also perceived that Argon is easier to use and more useful for specifying the infrastructure resources. Our findings suggest that Argon accelerates the provisioning process by modeling the cloud infrastructure and automating the generation of scripts for different DevOps tools when compared to Ansible, which is a code-centric tool that is greatly used in practice."
Ahmad2023,Jamshad Ahmad and Sonia Akram and Shafqat Ur Rehman and Nasser Bin Turki and Nehad Ali Shah,Description of soliton and lump solutions to M-truncated stochastic Biswas–Arshed model in optical communication,Results in Physics,51,,2023,10.1016/j.rinp.2023.106719,22113797,"This article negotiates the investigation of optical stochastic solitons and other exact stochastic solutions with the fractional stochastic Biswas–Arshed equation (FSBAE) describing the multiplicative white noise of optical signal propagation in birefringent fibers using the modified F-expansion method and Hirota Bilinear method. In order to manage this model, we used Itô calculus. After utilizing the aforementioned techniques and computational software, different stochastic solitary wave solutions are retrieved, including dark, bright, periodic, singular, hyperbolic, rational, combo, and trigonometric function solutions. Additionally, we also investigate several wave solutions, such as the cross-kink rational wave solution, the homoclinic breather wave solution, and M-shaped rational solution. To examine the different kinds of solitons and their dynamical behaviors, the solutions have been simulated by graphs. The discovered solutions are essential for illuminating the wave dynamics in diverse models. The obtained stochastic solitary wave solutions may be crucial in nonlinear science and engineering fields. It is impressive to see that the chosen methodologies are simple, appropriate, and effective scientific tools for determining stochastic solitary wave solutions for nonlinear engineering models (NLEMs). This is the first investigation into the lump solutions of the FSBA equation with multiplicative white noise, and the findings show how optical solitons propagate in nonlinear optics."
Tamene2021,Aiggan Tamene and Abel Afework,"Exploring barriers to the adoption and utilization of improved latrine facilities in rural Ethiopia: An Integrated Behavioral Model for Water, Sanitation and Hygiene (IBM-WASH) approach",PLoS ONE,16,1 January,2021,10.1371/journal.pone.0245289,19326203,"Background Even though evidence shows that access to and use of improved latrines is related to healthful families and the public, obstacles to the adoption and use of improved latrine facilities remain. Globally, not many inquiries appear to have been carried out to satisfactorily inform us regarding the multi-level barriers influencing the adoption and utilization of improved latrines facilities. Related studies in Ethiopia are even fewer. Methods Two qualitative data gathering methods, viz., key informant interviews and focus group discussions, were employed to collect data for this study. A total of fifteen focus group discussions were conducted with members of the community in the rural Wonago district of Ethiopia. Similarly, ten key informant interviews were conducted with water, sanitation, and hygiene officers, and health extension workers responsible for coordinating sanitation and hygiene activities. Open code software 4.03 was used for thematic analysis. Result Barriers to adoption and use of improved latrine facilities were categorized into Contextual factors (e.g. Gender, educational status, personal preference for using the field, limited space, population density, the status of land ownership), Psychosocial factors (Culture, beliefs, attitudes, and perceptions of minimal health threat from children’s feces), and Technological factors (inconveniences in acquiring materials and cost of constructing a latrine). Conclusion There are a series of multi-leveled barriers to the sustained adoption and use of latrines. Providing funding opportunities for the underprivileged and offering training on the engineering skills of latrine construction at the community level based on the contextual soil circumstances could expand the latrine coverage and use. Similarly, taking into account the variability in motivations for adopting and using latrines among our study in Ethiopia and other studies, we implore public health experts to recognize behaviors and norms in their target communities in advance of implementing sanitation interventions."
Elkhrachy2021,Ismail Elkhrachy and Quoc Bao Pham and Romulus Costache and Meriame Mohajane and Khalil Ur Rahman and Himan Shahabi and Nguyen Thi Thuy Linh and Duong Tran Anh,"Sentinel-1 remote sensing data and Hydrologic Engineering Centres River Analysis System two-dimensional integration for flash flood detection and modelling in New Cairo City, Egypt",Journal of Flood Risk Management,14,2,2021,10.1111/jfr3.12692,1753318X,"Digital surface models, land use and rainfall data were used to simulate water areas using Hydrologic Engineering Centres River Analysis System (HEC-RAS) software. Multi-temporal synthetic aperture radar (SAR) was used for the detection of flood prone area to calibrate HEC-RAS, due to the lack of data validation in the New Cairo City, Egypt. The thresholding water detection method was applied to two Sentinel-1 images, one pre- and one post-flash flood event from April 24 to 27, 2018. The threshold method was used to detect water areas from SAR Sentinel-1 images. Feature statistical agreement F1 and F2 values ranged from 73.4 to 77.7% between water areas extracted based on backscattering values between 19.97 and 16.53 in decibels (dB) and reference water areas obtained using an optical image of the Sentinel-2 satellite. The similarity between simulated HEC-RAS two-dimensional (2D) of water areas and reference water areas based on SAR data ranged between 74.2 and 89.7% using feature statistical agreement values F1 and F2. It provides a clear suggestion that, in the absence of field observations, SAR data can be used to calibrate the model. Two flood hazard maps created based on water velocity and depth were obtained from HEC-RAS 2D simulation. The obtained maps indicated that 11% of the roads and 50% of the buildings in New Cairo City are exposed to high hazard areas. Furthermore, 28% of the bare land is situated in a very high vulnerability area. We recommend the use of obtained hazard map in supporting emergency response, and designing effective emergency plans."
Edlbauer2022,Hermann Edlbauer and Junliang Wang and Thierry Crozes and Pierre Perrier and Seddik Ouacel and Clément Geffroy and Giorgos Georgiou and Eleni Chatzikyriakou and Antonio Lacerda-Santos and Xavier Waintal and D. Christian Glattli and Preden Roulleau and Jayshankar Nath and Masaya Kataoka and Janine Splettstoesser and Matteo Acciai and Maria Cecilia da Silva Figueira and Kemal Öztas and Alex Trellakis and Thomas Grange and Oleg M. Yevtushenko and Stefan Birner and Christopher Bäuerle,Semiconductor-based electron flying qubits: review on recent progress accelerated by numerical modelling,EPJ Quantum Technology,9,1,2022,10.1140/epjqt/s40507-022-00139-w,21960763,"The progress of charge manipulation in semiconductor-based nanoscale devices opened up a novel route to realise a flying qubit with a single electron. In the present review, we introduce the concept of these electron flying qubits, discuss their most promising realisations and show how numerical simulations are applicable to accelerate experimental development cycles. Addressing the technological challenges of flying qubits that are currently faced by academia and quantum enterprises, we underline the relevance of interdisciplinary cooperation to move emerging quantum industry forward. The review consists of two main sections: Pathways towards the electron flying qubit: We address three routes of single-electron transport in GaAs-based devices focusing on surface acoustic waves, hot-electron emission from quantum dot pumps and Levitons. For each approach, we discuss latest experimental results and point out how numerical simulations facilitate engineering the electron flying qubit. Numerical modelling of quantum devices: We review the full stack of numerical simulations needed for fabrication of the flying qubits. Choosing appropriate models, examples of basic quantum mechanical simulations are explained in detail. We discuss applications of open-source (KWANT) and the commercial (nextnano) platforms for modelling the flying qubits. The discussion points out the large relevance of software tools to design quantum devices tailored for efficient operation."
Gorski2020,Tomasz Gorski and Jakub Bednarski,Applying Model-Driven Engineering to Distributed Ledger Deployment,IEEE Access,8,,2020,10.1109/ACCESS.2020.3005519,21693536,"Distributed Ledger Technology (DLT) enables data storage in a decentralized manner among collaborating parties. The software architecture of such solutions encompasses models placed in the relevant architectural views. A lot of research is devoted to smart contracts and consensus algorithms, which are realized by distributed applications and can be positioned within the Logical view. However, we see the need to provide modeling support for the Deployment view of distributed ledger solutions. Especially since the chosen DLT framework has a significant impact on implementation and deployment. Besides, consistency between models and configuration deployment scripts should be ensured. So, we have applied Model-Driven Engineering (MDE) that allows on the transformation of models into more detailed models, source code, or tests. We have proposed Unified Modeling Language (UML) stereotypes and tagged values for distributed ledger deployment modeling and placed them in the UML Profile for Distributed Ledger Deployment. We have also designed the UML2Deployment model-to-code transformation for the R3 Corda DLT framework. A UML Deployment model is the source whereas a Gradle Groovy deployment script is the target of the transformation. We have provided the complete solution by incorporating the transformation into the Visual Paradigm modeling tool. Furthermore, we have designed a dedicated plug-in to validate generated deployment scripts. In the paper, we have shown how to design transformation for generating deployment scripts for the R3 Corda DLT framework with the ability to switch to another one."
Ali2021,Javed Ali and Ahmad Jusoh and Norhalimah Idris and Alhamzah F. Abbas and Ahmed H. Alsharif,Nine Years of Mobile Healthcare Research: A Bibliometric Analysis,International journal of online and biomedical engineering,17,10,2021,10.3991/ijoe.v17i10.25243,26268493,"The purpose of the paper was to explore the central keyword searched (e.g., mobile healthcare). It also aimed at identifying the valuable contributions made by authors, journals, countries, and institutions and their associations in ‘mobile healthcare' search around the world. Data was extracted from 2012 to 2020 by using Scopus database and analysed through VOSviewer software and MS Excel. PRISMA guidelines were used to screen the records. Coauthorship, Co-occurrence, Bibliographic Coupling and Co-citation analysis were executed to identify the links and collaborations among the authors, countries, author keywords and documents globally. Results showed that Yang X. had the highest association with other authors and Sood, S.K. had published more documents than others. Australia was found to have the highest association with other countries, and India was leading other countries in publications. Computers and Electrical Engineering was found to be the leading journal in publication of documents. This study, to best of our knowledge, was the first of its kind in mapping the ‘mobile healthcare' search which was designed till 2020. This will aid in shaping and understanding the central theme and set the future research directions for the researchers."
Bjelland2022,Oystein Bjelland and Bismi Rasheed and Hans Georg Schaathun and Morten D. Pedersen and Martin Steinert and Alf Inge Hellevik and Robin T. Bye,Toward a Digital Twin for Arthroscopic Knee Surgery: A Systematic Review,IEEE Access,10,,2022,10.1109/ACCESS.2022.3170108,21693536,"The use of digital twins to represent a product or process digitally is trending in many engineering disciplines. This term has also been recently introduced in the medical field. In arthroscopic surgery education, the paradigm shift from apprenticeship to simulation training has driven the need for better simulators, and the current focus is on improving simulators with respect to computational efficiency and system accuracy. However, expanding surgical simulations towards digital twins has not yet been explored. This paper introduces the digital twin concept for arthroscopic surgery, and explores its potential in light of the existing scientific literature. Thus, a systematic review was conducted to summarize and analyze the literature with respect to fast and robust design of an arthroscopic digital twin using patient-specific information, and methods for interactive surgical soft tissue simulation. The review was conducted using the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) protocol with three reliable scientific search engines: IEEE Xplore, ScienceDirect and PubMed. Eighty papers were included in the review, and the extracted data included modeling methods, tissue types, constitutive behavior, computational efficiency or accuracy, hardware configuration, haptic device description, software tools, and system architectures. Considering the review, a novel macro-level conceptual arthroscopic digital twin system is presented, and the applicability of the review findings for the identified subsystems are discussed. The proposed system integrates patient-specific images, diagnostic data, intraoperative sensor data, and surgical practice as inputs, and conceptually enables surgical skills training, preoperative planning, and a database of virtual surgeries."
Baumann2020,Einar J.M. Baumann and Stein I. Dale and Mathias C. Bellout,FieldOpt: A powerful and effective programming framework tailored for field development optimization,Computers and Geosciences,135,,2020,10.1016/j.cageo.2019.104379,00983004,"Petroleum field development involves critical decisions such as well location, well completion design and optimal control setting that have a significant impact on revenues and costs. These decisions are associated with a large degree of engineering effort that commonly involves time-consuming reservoir simulations to compute the performance of different field development scenarios. Increasingly within the petroleum industry, software tools and optimization methodology are developed and implemented to support and augment the various decision-making processes. The overall aim of these tools is to increase productivity and improve decision quality. Within this context, this work introduces an open-source, extensible, tailor-made programming framework: FieldOpt. FieldOpt's primary purpose is rapid prototyping and testing of optimization procedures to solve critical field development problems. The framework is implemented in C++ and provides an efficient integration of mathematical optimization procedures with reservoir simulation. FieldOpt's modular architecture and use of object-oriented programming allow the users to adapt and extend the code with ease. The architecture has proven successful in facilitating new optimization algorithms, new use cases, and new methodology for optimization with minimal effort and change to internal data structures and data flow. Three use cases are presented to demonstrate the optimization capabilities of FieldOpt as well as the flexibility and ease-of-use of the software regarding configuring various optimization procedures to solve a range of field development problems. The three use cases presented optimize on well control, well completion design and well placement parameters, respectively. For each case, both the configuration of the algorithms and problems within FieldOpt as well as the final solution and performance of the different optimization procedures are discussed. In all three cases, FieldOpt was able to find significant improvements or crucial information to decision making."
Pappalardo2022,Carmine M. Pappalardo and Angelo Vece and Davide Galdi and Domenico Guida,Developing a Reciprocating Mechanism for the Emergency Implementation of a Mechanical Pulmonary Ventilator using an Integrated CAD-MBD Procedure,FME Transactions,50,2,2022,10.5937/fme2201238P,2406128X,"Following the COVID-19 outbreak, the redesign of an emergency mechanical pulmonary ventilator that is cheap and easily portable became necessary in several contexts, such as emergency hotspots and environments with poor resources. To address this important issue, a general multibody approach is employed in this paper to develop a reciprocating mechanism suitable for retrofitting the existing manual mechanical ventilators through computer-aided engineering tools. By analyzing various basic articulated mechanisms typically found in engineering mechanics, a prototype is created and reproduced in a threedimensional environment using SOLIDWORKS's CAD software. Subsequently, a high-fidelity mechanical model is developed starting from the CAD geometry and employing the SIMSCAPE MULTIBODY software, an extension of the MATLAB family of programs that can effectively and efficiently perform kinematic and dynamic simulations of the mechanism of interest. As discussed in the paper, by carrying out numerous numerical experiments, the virtual simulations predict several fundamental medical parameters, such as the airflow introduced into patients, the respiratory rate, and the respiratory ratio."
Mastrone2021,Marco Nicola Mastrone and Franco Concli,CFD simulations of gearboxes: implementation of a mesh clustering algorithm for efficient simulations of complex system’s architectures,International Journal of Mechanical and Materials Engineering,16,1,2021,10.1186/s40712-021-00134-6,21982791,"In the last decade, computer-aided engineering (CAE) tools have become a determinant factor in the analysis of engineering problems. In fact, they bring a clear reduction of time in the design phase of a new product thanks to parametrical studies based on virtual prototypes. The application of such tools to gearboxes allowed engineers to study the efficiency and lubrication inside transmissions. However, the difficulties of handling the computational domain are still a concern for complex system configurations. For this reason, the authors maintain that it is fundamental to introduce time efficient algorithms that enable the effective study of any kind of gear, e.g., helical and bevel configurations. In this work, a new mesh handling strategy specifically suited for this kind of studies is presented. The methodology is based on the Global Remeshing Approach with Mesh Clustering (GRAMC) process that drastically reduces the simulation time by minimizing the effort for updating the grids. This procedure was tested on spur, helical, and bevel gears, thus demonstrating the flexibility of the approach. The comparison with experimentally measured power losses highlighted the good accuracy of the strategy. The algorithm was implemented in the opensource software OpenFOAM®."
Manda2021,Bharadwaj Manda and Pranjal Bhaskare and Ramanathan Muthuganapathy,A Convolutional Neural Network Approach to the Classification of Engineering Models,IEEE Access,9,,2021,10.1109/ACCESS.2021.3055826,21693536,"This paper presents a deep learning approach for the classification of Engineering (CAD) models using Convolutional Neural Networks (CNNs). Owing to the availability of large annotated datasets and also enough computational power in the form of GPUs, many deep learning-based solutions for object classification have been proposed of late, especially in the domain of images and graphical models. Nevertheless, very few solutions have been proposed for the task of functional classification of CAD models. Hence, for this research, CAD models have been collected from Engineering Shape Benchmark (ESB), National Design Repository (NDR) and augmented with newer models created using a modeling software to form a dataset - 'CADNET'. It is proposed to use a residual network architecture for CADNET, inspired by the popular ResNet. A weighted Light Field Descriptor (LFD) scheme is chosen as the method of feature extraction, and the generated images are fed as inputs to the CNN. The problem of class imbalance in the dataset is addressed using a class weights approach. Experiments have been conducted with other signatures such as geodesic distance etc. using deep networks as well as other network architectures on the CADNET. The LFD-based CNN approach using the proposed network architecture, along with gradient boosting yielded the best classification accuracy on CADNET."
Alrajeh2020,Dalal Alrajeh and Antoine Cailliau and Axel Van Lamsweerde,Adapting requirements models to varying environments,,,,2020,10.1145/3377811.3380927,02705257,"The engineering of high-quality software requirements generally relies on properties and assumptions about the environment in which the software-to-be has to operate. Such properties and assumptions, referred to as environment conditions in this paper, are highly subject to change over time or from one software variant to another. As a consequence, the requirements engineered for a specific set of environment conditions may no longer be adequate, complete and consistent for another set. The paper addresses this problem through a tool-supported requirements adaptation technique. A goal-oriented requirements modelling framework is considered to make requirements' refinements and dependencies on environment conditions explicit. When environment conditions change, an adapted goal model is computed that is correct with respect to the new environment conditions. The space of possible adaptations is not fixed a priori; the required changes are expected to meet one or more environment-independent goal(s) to be satisfied in any version of the system. The adapted goal model is generated using a new counterexample-guided learning procedure that ensures the correctness of the updated goal model, and prefers more local adaptations and more similar goal models."
Kang2020,Xinhui Kang,Combining Grey Relationship Analysis and Neural Network to Develop Attractive Automobile Booth Design,Computational Intelligence and Neuroscience,2020,,2020,10.1155/2020/8863727,16875273,"Miryoku engineering is a design concept based on customer preferences, with the goal of creating attractive products or spaces. However, traditional Miryoku engineering faces two main issues: (1) the upper Kansei factor ranks the weights by the number of mentions, but it does not represent the importance of customers; (2) the mapping connection between the upper Kansei factor and the lower specific conditions adopts a statistical analysis method, which easily leads to the omission of key information. With the development of computer-based artificial intelligence, it repeatedly simulates human thinking with simple calculation rules, which has the advantages of fewer errors and faster speed. Therefore, on the three-level evaluation grid diagram platform established by Miryoku engineering, this paper first uses grey relationship analysis to comprehensively evaluate the priority order of Kansei words. Secondly, for the key Kansei factors, a morphological deconstruction table that connects the original reasons and specific conditions is established. Orthogonal design is used to screen representative combinations of design elements and create sample models by using the 3D software. Finally, the neural network was used to establish a mapping function between the key Kansei factors and the representative product design elements, and based on this, the most perceptually attractive product design was discovered. As a case study, the automobile booth was used to validate the effectiveness of the proposed method and significantly improve exhibitor design decisions and attendees' satisfaction."
Sahin2020,Yasar Guneri Sahin and Ufuk Celikkan,Information technology asymmetry and gaps between higher education institutions and industry,Journal of Information Technology Education: Research,19,,2020,10.28945/4553,15393585,"Aim/Purpose This paper investigates the gaps between industry and academia perceptions of information technology fields, such as computer science, software engi-neering, and computer engineering, and it identifies areas of asymmetry be-tween curricula and industry expectations. The study mainly focuses on the skills required of IT professionals (graduated students) and on how higher education institutes equip students for industry. Background Higher education institutes have several IT-related departments. However, it is not clear whether these departments have sufficient content to equip stu-dents with industry-related skills. Rapid advances mean that some curriculum topics are redundant before the end of a standard two-or four-year degree programs. Balancing the technical/non-technical skills and adjusting the cur-ricula to better prepare the students for industry is a constant demand for higher education institutions. Several studies have demonstrated that a ge-neric curriculum is inadequate to address current IT industry needs. Methodology The study involved a comprehensive survey of IT professionals and compa-nies using a Web-based questionnaire sent directly to individual companies, academics, and employers. 64 universities and 38 companies in 24 countries were represented by the 209 participants, of whom 99 were IT professionals, 72 academics, and 38 employers. Contribution This paper is intended to guide academics in preparing dynamic curricula that can be easily adapted to current industry trends and technological develop-ments, with content directly relevant to student's careers. In addition, the re-sults may identify the skills that students need to secure employment and the courses that will provide skills in line with current industry trends. Findings The results indicate a lack of emphasis on personal and non-technical skills in undergraduate education compared to general computer science, software de-velopment, and coding courses. Employers' and software experts' responses emphasize that soft skills should not be ignored, and that, of these, analytical thinking and teamwork are the two most requested. Rather than a theoretical emphasis, courses should include hands-on projects. Rapid developments and innovations in information technologies demand that spiral and waterfall models are replaced with emerging software development models, such as Agile and Scrum development. Recommendations for Practitioners A multidisciplinary approach should be taken to the teaching of soft skills, such as communication, ethics, leadership, and customer relations. Establish-ing multiple learning tracks in IT education would equip students with special-ized knowledge and skills in IT. An effective communication channel should be established between students and industry. It is also important to reduce the distance between academics and students and to provide an interactive en-vironment for technical discussions. Enterprise level computing and Frame-work use provide job market advantages. Recommendations for Researchers Researchers and department heads, particularly those involved in curriculum design and accreditation, could use the results of this exemplary study to identify key topics for attention. Impact on Society Changes of various degrees are required in the current curricula in many higher education institutions to better meet student needs. Societies and tech-nology are dynamic in nature, and information technology-related curricula in higher education institutions should be equally dynamic. Future Research Since technology (especially information technology) transforms and ad-vances itself so rapidly, this study should be replicated t to investigate how these changes affect the gap between revised curricula and current industry expectations."
Saura2023,Jose Ramon Saura and Daniel Palacios-Marqués and Domingo Ribeiro-Soriano,Privacy concerns in social media UGC communities: Understanding user behavior sentiments in complex networks,Information Systems and e-Business Management,,,2023,10.1007/s10257-023-00631-5,16179854,"In a digital ecosystem where large amounts of data related to user actions are generated every day, important concerns have emerged about the collection, management, and analysis of these data and, according, about user privacy. In recent years, users have been accustomed to organizing in and relying on digital communities to support and achieve their goals. In this context, the present study aims to identify the main privacy concerns in user communities on social media, and how these affect users’ online behavior. In order to better understand online communities in social networks, privacy concerns, and their connection to user behavior, we developed an innovative and original methodology that combines elements of machine learning as a technical contribution. First, a complex network visualization algorithm known as ForceAtlas2 was used through the open-source software Gephi to visually identify the nodes that form the main communities belonging to the sample of UGC collected from Twitter. Then, a sentiment analysis was applied with Textblob, an algorithm that works with machine learning on which experiments were developed with support vector classifier (SVC), multinomial naïve Bayes (MNB), logistic regression (LR), random forest, and classifier (RFC) under the theoretical frameworks of computer-aided text analysis (CATA) and natural language processing (NLP). As a result, a total of 11 user communities were identified: the positive protection software and cybersecurity and eCommerce, the negative privacy settings, personal information and social engineering, and the neutral privacy concerns, hacking, false information, impersonation and cookies data. The paper concludes with a discussion of the results and their relation to user behavior in digital environments and an outline valuable and practical insights into some techniques and challenges related to users’ personal data."
Tawalbeh2020,Muhammad Tawalbeh and Tareq Salameh and Mona Albawab and Amani Al-Othman and Mamdouh El Haj Assad and Abdul H. Alami,Parametric study of a single effect lithium bromide-water absorption chiller powered by a renewable heat source,"Journal of Sustainable Development of Energy, Water and Environment Systems",8,3,2020,10.13044/j.sdewes.d7.0290,18489257,"This work investigates the performance of a single-effect absorption chiller utilizing an aqueous lithium bromide solution as the working fluid and driven by hot fluid rejected from either a geothermal power plant or the outlet of a thermal solar collector. This relatively low enthalpy return fluid, which will otherwise be reinjected back into the earth, will be utilized as the thermal energy source of the chiller. Although such chillers are considered low-grade energy refrigeration cycles, the one proposed here has an advantage in terms of economy and efficiency. A parametric analysis is performed using Engineering Equation Solver software and is used to highlight the effect of the heat exchanger size on the coefficient of performance of the chiller. The analysis proved that the proposed device can operate with excellent cooling capacity, reaching 16 kW, and a relatively high coefficient of performance (~ 0.7) while being driven by the low-grade energy. The heat source temperature, solution heat exchanger effectiveness and the size of the absorber were shown to be key parameters for the design and operation of absorption chillers. Moreover, increasing the heat source mass flow rate has a significant impact on both cooling capacity and coefficient of performance at low values (< 10 kg/s) and unnoticeable impact at higher values (> 10 kg/s)."
Li2021,Zhuohua Li and Jincheng Wang and Mingshen Sun and John C.S. Lui,MirChecker: Detecting Bugs in Rust Programs via Static Analysis,,,,2021,10.1145/3460120.3484541,15437221,"Safe system programming is often a crucial requirement due to its critical role in system software engineering. Conventional low-level programming languages such as C and assembly are efficient, but their inherent unsafe nature makes it undesirable for security-critical scenarios. Recently, Rust has become a promising alternative for safe system-level programming. While giving programmers fine-grained hardware control, its strong type system enforces many security properties including memory safety. However, Rust's security guarantee is not a silver bullet. Runtime crashes and memory-safety errors still harass Rust developers, causing damaging exploitable vulnerabilities, as reported by numerous studies. In this paper, we present and evaluate MirChecker, a fully automated bug detection framework for Rust programs by performing static analysis on Rust's Mid-level Intermediate Representation (MIR). Based on the observation of existing bugs found in Rust codebases, our approach keeps track of both numerical and symbolic information, detects potential runtime crashes and memory-safety errors by using constraint solving techniques, and outputs informative diagnostics to users. We evaluate MirChecker on both buggy code snippets extracted from existing Common Vulnerabilities and Exposures (CVE) and real-world Rust codebases. Our experiments show that MirChecker can detect all the issues in our code snippets, and is capable of performing bug finding in real-world scenarios, where it detected a total of 33 previously unknown bugs including 16 memory-safety issues from 12 Rust packages (crates) with an acceptable false-positive rate."
Altybayev2020,Alshyn Altybayev and Yelena Naydenko and Besarion Meskhi and Andrey Mozgovoy and Dmitriy Rudoy and Anastasiya Olshevskaya,Creation of integrated system for feeding management activities automation in beef breeding,,175,,2020,10.1051/e3sconf/202017503019,22671242,"The practical aspects of creating a hardware-software complex for digital information technology implementation in beef breeding feeding management system are considered. Based on systematic analysis of fattening enterprises current state in Kazakhstan, the areas most prepared for automation in the activities of feeding management in beef breeding were identified. The basic information processes are structured to ensure the adoption of optimal management decisions to comply with the technological regulations for keeping animals on the feedlot. The architecture of information infrastructure and integration of its hardware and software components are proposed, taking into account the current state of IT-solutions market. The material and technical base of the processes for feeding management activities automating was selected taking into account the current state of IT-solutions market and integration requirements, its software integration was implemented. Some methodological approaches to solving problems of software engineering are substantiated, implementation of which contributes to the achievement of practical results of increasing the management activities efficiency of personnel at the operational level."
Mandolini2022,Marco Mandolini and Agnese Brunzini and Giulia Facco and Alida Mazzoli and Archimede Forcellese and Antonio Gigante,Comparison of Three 3D Segmentation Software Tools for Hip Surgical Planning,Sensors,22,14,2022,10.3390/s22145242,14248220,"In hip arthroplasty, preoperative planning is fundamental to reaching a successful surgery. Nowadays, several software tools for computed tomography (CT) image processing are available. However, research studies comparing segmentation tools for hip surgery planning for patients affected by osteoarthritic diseases or osteoporotic fractures are still lacking. The present work compares three different software from the geometric, dimensional, and usability perspectives to identify the best three-dimensional (3D) modelling tool for the reconstruction of pathological femoral heads. Syngo.via Frontier (by Siemens Healthcare) is a medical image reading and post-processing software that allows low-skilled operators to produce prototypes. Materialise (by Mimics) is a commercial medical modelling software. 3D Slicer (by slicer.org) is an open-source development platform used in medical and biomedical fields. The 3D models reconstructed starting from the in vivo CT images of the pathological femoral head are compared with the geometries obtained from the laser scan of the in vitro bony specimens. The results show that Mimics and 3D Slicer are better for dimensional and geometric accuracy in the 3D reconstruction, while syngo.via Frontier is the easiest to use in the hospital setting."
Kim2020,Do Kyun Kim and Hui Ling Lim and Nak Kyun Cho,"An advanced technique to predict time-dependent corrosion damage of onshore, offshore, nearshore and ship structures: Part II = Application to the ship's ballast tank",International Journal of Naval Architecture and Ocean Engineering,12,,2020,10.1016/j.ijnaoe.2020.07.002,20926790,"In this study (Part II), the empirical formulation of corrosion model of a ship's ballast tank was developed to predict nonlinear time-dependent corrosion wastage based on the advanced data processing technique proposed by Part I. The detail on how to propose generalised mathematical formulation of corrosion model was precisely documented in the previous paper (Part I). The statistical scatter of corrosion data at any exposure time was investigated by the refined method and formulated based on a 2-parameter Weibull distribution which selected the best fit PDF. Throughout the nine (9) steps, empirical formulation of the ship's seawater ballast tank was successfully proposed and four (4) key step results were also obtained. The proposed method in Part I was verified and confirmed by this application of seawater ballast tank, thus making it possible to predict accurate behaviours of nonlinear time-dependent corrosion. Developed procedures and obtained corrosion damage model for ship's seawater ballast tank can be used for development of engineering software."
Schffer2020,Eike Schäffer and Andreas Mayr and Jonathan Fuchs and Martin Sjarov and Johannes Vorndran and Jörg Franke,Microservice-based architecture for engineering tools enabling a collaborative multi-user configuration of robot-based automation solutions,,86,,2020,10.1016/j.procir.2020.01.017,22128271,"Microservice (MS) architectures, especially in combination with micro front ends, are a modern, scalable and sustainable approach to software development. The modular development of individual components and the possibility of a simple, collaborative and iterative development represent a strategic advantage for companies. In addition, the MS approach allows the consistent use of current technologies, whereby new software functionalities can constantly be included. In this paper, the potentials of MS for engineering tools are shown using the example of a web-based configurator for robot-based automation solutions. On the one hand, the implemented prototype illustrates a possible MS architecture pattern and, on the other hand, clarifies how new functionalities such as the collaborative multi-user configuration or different role-based configuration sessions are enabled by such a concept. Using the example of the web-based configuration platform, it is finally shown how a MS architecture contributes to better development and easier deployment of engineering software solutions based on the divide and conquer principle."
Zotova2021,Marina Zotova and Tetiana Likhouzova and Liliya Shegai and Elena Korobeynikova,The use of moocs in online engineering education,International Journal of Engineering Pedagogy,11,3,2021,10.3991/IJEP.V11I3.20411,21924880,"The study demonstrates the possibilities of using massive open online courses in engineering education. In order to study the impact of digital learning formats on the acquisition of key engineering competencies by students, an educational experiment was carried out; it was performed as part of the study of the Fire Safety discipline on the basis of the online educational platform Khan Academy. Conceptually, the educational process was aimed at studying the capabilities of software in the field of fire safety, namely: PyroSim, Pathfinder, FireRisk, FIM, FireCategories, PromRisk, FireDistance, as well as gaining professional competencies in the process of performing practical tasks. The educational experiment involved third-year intramural and extramural students of the Faculty of Engineering (The Saint-Petersburg University of the State Fire Service of the EMERCOM of Russia) and Moscow City Pedagogical University. The online educational course was conducted with an emphasis on consolidation and acquisition of professional engineering knowledge and skills in the development of fire safety projects under simulated risk conditions using modern software and an innovative educational platform. The participants demonstrated a high level of involvement, initiative and professional aspirations in the learning process. In the context of the educational experiment, the key competencies required for 21st century engineers have been identified. The students confirmed the positive impact of digital pedagogical tools on the acquisition and development of key competencies necessary for further professional activity. Having completed an online course on the Khan Academy platform focused on the applied interaction with advanced software in the field of fire safety, students identified skills that should be developed and consolidated, namely: Teamwork skills and the ability to work in multidisciplinary teams, the ability to manage projects (management, planning, scheduling, budgeting, etc.); the ability to define, formulate and solve engineering problems; the ability to effectively prioritize; striving for lifelong learning; willingness to take a calculated risk; ability to make professional decisions; self-regulation and self-motivation. However, the development of important engineering competencies such as high ethical standards, honesty and global social, intellectual and technological responsibility; good communication skills, high emotional intelligence and cognitive flexibility; entrepreneurial spirit and customer focus requires educators to search for different pedagogical methods and practices. The assessment of the impact of digital learning formats on the acquisition of key engineering competencies was subjective, but revealed pedagogical approaches to the qualitative development of engineering personnel. An effective combination of traditional classroom and virtual interactions can create a complex paradigm of highquality engineering and technical education that is competitive in the market environment."
Ferreira2021,Isabella Ferreira and Jinghui Cheng and Bram Adams,"The ""shut the f∗∗k up"" Phenomenon: Characterizing Incivility in Open Source Code Review Discussions",Proceedings of the ACM on Human-Computer Interaction,5,CSCW2,2021,10.1145/3479497,25730142,"Code review is an important quality assurance activity for software development. Code review discussions among developers and maintainers can be heated and sometimes involve personal attacks and unnecessary disrespectful comments, demonstrating, therefore, incivility. Although incivility in public discussions has received increasing attention from researchers in different domains, the knowledge about the characteristics, causes, and consequences of uncivil communication is still very limited in the context of software development, and more specifically, code review. To address this gap in the literature, we leverage the mature social construct of incivility as a lens to understand confrontational conflicts in open source code review discussions. For that, we conducted a qualitative analysis on 1,545 emails from the Linux Kernel Mailing List (LKML) that were associated with rejected changes. We found that more than half (66.66%) of the non-technical emails included uncivil features. Particularly, frustration, name calling, and impatience are the most frequent features in uncivil emails. We also found that there are civil alternatives to address arguments, while uncivil comments can potentially be made by any people when discussing any topic. Finally, we identified various causes and consequences of such uncivil communication. Our work serves as the first study about the phenomenon of in(civility) in open source software development, paving the road for a new field of research about collaboration and communication in the context of software engineering activities."
Nor2021,Ahmad Kamal M. Nor and Srinivasa R. Pedapati and Masdi Muhammad,"Reliability engineering applications in electronic, software, nuclear and aerospace industries: A 20 year review (2000–2020)",Ain Shams Engineering Journal,12,3,2021,10.1016/j.asej.2021.02.015,20904479,"A review on reliability engineering applications in 4 industrial domains namely electronic, software, nuclear and aerospace from the 2000′s to the present day is compiled. The progress in industrial maintenance activities and Human Reliability Analysis (HRA) linked to these domains are explored. Then, the mathematical aspect of reliability evaluation, in particular multi-state system (MSS), which characterizes the system complexity in these domains are presented. Trends progression are obtained through literature of respective areas as well as available technical information and industrial circulations. Through this review, trend similarities between the mentioned domains and the challenges in reliability science implementation are uncovered. The methods employed in respective industry are explained, with each strength and weakness analysed, together with application examples. This work reveals the role of Prognostic and Health Management (PHM), HRA and analytical methodologies such as MSS and Probabilistic Risk Assessment (PRA) in managing reliability of complex industrial systems. Finally, it stresses on the importance of synergy between these frameworks to ensure a complete reliability assessment in the industry."
Antinyan2020,Vard Antinyan,Revealing the complexity of automotive software,,,,2020,10.1145/3368089.3417038,,"Software continues its procession into the core of the modern cars. Sophisticated functionalities, like connectivity and active safety, provide gratifying comfort to its users. With the sophisticated functionality, however, comes the underlying complexity that grows overwhelmingly year by year. But the invisibility of software hinders the practitioners and researchers grasping the magnitude of complexity thoroughly. Rather, from time to time, the consequences of complexity surface in forms of ultra-high design efforts, waves of defect reports, and explosions of warranty costs. This article reveals the complexity of software in four key areas of automotive software development. It points out that the existing practices are severely insufficient for systematic complexity management."
Granlund2021,Tuomas Granlund and Vlad Stirbu and Tommi Mikkonen,Towards Regulatory-Compliant MLOps: Oravizio’s Journey from a Machine Learning Experiment to a Deployed Certified Medical Product,SN Computer Science,2,5,2021,10.1007/s42979-021-00726-1,26618907,"Agile software development embraces change and manifests working software over comprehensive documentation and responding to change over following a plan. The ability to continuously release software has enabled a development approach where experimental features are put to use, and, if they stand the test of real use, they remain in production. Examples of such features include machine learning (ML) models, which are usually pre-trained, but can still evolve in production. However, many domains require more plan-driven approach to avoid hazard to environment and humans, and to mitigate risks in the process. In this paper, we start by presenting continuous software engineering practices in a regulated context, and then apply the results to the emerging practice of MLOps, or continuous delivery of ML features. Furthermore, as a practical contribution, we present a case study regarding Oravizio, first CE-certified medical software for assessing the risks of joint replacement surgeries. Towards the end of the paper, we also reflect the Oravizio experiences to MLOps in regulatory context."
Heumller2020,Robert Heumüller and Sebastian Nielebock and Jacob Krüger and Frank Ortmeier,"Publish or perish, but do not forget your software artifacts",Empirical Software Engineering,25,6,2020,10.1007/s10664-020-09851-6,15737616,"Open-science initiatives have gained substantial momentum in computer science, and particularly in software-engineering research. A critical aspect of open-science is the public availability of artifacts (e.g., tools), which facilitates the replication, reproduction, extension, and verification of results. While we experienced that many artifacts are not publicly available, we are not aware of empirical evidence supporting this subjective claim. In this article, we report an empirical study on software artifact papers (SAPs) published at the International Conference on Software Engineering (ICSE), in which we investigated whether and how researchers have published their software artifacts, and whether this had scientific impact. Our dataset comprises 789 ICSE research track papers, including 604 SAPs (76.6 %), from the years 2007 to 2017. While showing a positive trend towards artifact availability, our results are still sobering. Even in 2017, only 58.5 % of the papers that stated to have developed a software artifact made that artifact publicly available. As we did find a small, but statistically significant, positive correlation between linking to artifacts in a paper and its scientific impact in terms of citations, we hope to motivate the research community to share more artifacts. With our insights, we aim to support the advancement of open science by discussing our results in the context of existing initiatives and guidelines. In particular, our findings advocate the need for clearly communicating artifacts and the use of non-commercial, persistent archives to provide replication packages."
Soltani2020,Mozhan Soltani and Felienne Hermans and Thomas Bäck,The significance of bug report elements,Empirical Software Engineering,25,6,2020,10.1007/s10664-020-09882-z,15737616,"Open source software projects often use issue repositories, where project contributors submit bug reports. Using these repositories, more bugs in software projects may be identified and fixed. However, the content and therefore quality of bug reports vary. In this study, we aim to understand the significance of different elements in bug reports. We interviewed 35 developers to gain insights into their perceptions on the importance of various contents in bug reports. To assess our findings, we surveyed 305 developers. The results show developers find it highly important that bug reports include crash description, reproducing steps or test cases, and stack traces. Software version, fix suggestions, code snippets, and attached contents have lower importance for software debugging. Furthermore, to evaluate the quality of currently available bug reports, we mined issue repositories of 250 most popular projects on Github. Statistical analysis on the mined issues shows that crash reproducing steps, stack traces, fix suggestions, and user contents, have statistically significant impact on bug resolution times, for ∼70%, ∼76%, ∼55%, and ∼33% of the projects. However, on avarage, over 70% of bug reports lack these elements."
Komal2020,Bakhtawar Komal and Uzair Iqbal Janjua and Fozia Anwar and Tahir Mustafa Madni and Muhammad Faisal Cheema and Muhammad Noman Malik and Ahmad Raza Shahid,The Impact of Scope Creep on Project Success: An Empirical Investigation,IEEE Access,8,,2020,10.1109/ACCESS.2020.3007098,21693536,"Advocates of software engineering and software project management stated in the literature that creeping of scope is one of the most common causes for the failure of software projects. Also, advocates believed that it could occur in almost every software project, which leads to compromise in quality, delayed schedules, increase cost and decreased customer satisfaction. However, the lack of empirical evidence demands a comprehensive investigation to identify the factors of scope creep and to propose a conceptual framework to empirically evaluate the impact of scope creep on software project success. To determine the scope creep factors in this study, two exploratory methods, i.e. a Systematic Literature Review (SLR) and interview from experts are performed. Following the analysis of these methods, a conceptual framework is proposed. To empirically evaluate the proposed conceptual framework, data is collected through a survey method. Next, the collected data is analyzed through Partial Least Squares' Structural Equation Modelling (PLS-SEM). From the results, it is evident that the identified factors of scope creep are negatively associated with software project success. The results of empirical evaluation also second the findings of SLR. The outcome of the study may help the practitioners to understand the dynamics of factors, which undermine scope creep in software SMEs and to assist them in the development of effective control and mitigation strategies, therefore, to increase the project success rate."
Rivera2020,Luis F. Rivera and Hausi A. Müller and Norha M. Villegas and Gabriel Tamura and Miguel Jiménez,On the Engineering of IoT-Intensive Digital Twin Software Systems,,,,2020,10.1145/3387940.3392195,,"Digital Twins (DT) are software systems representing different aspects of a physical or conceptual counterpart - -the real twin, which is instrumented with several sensors or computing devices that generate, consume and transfer data to its DT with different purposes. In other words, DT systems are, to a large extent, IoT-intensive systems. Indeed, by exploiting and managing IoT data, artificial intelligence, and big data and simulation capabilities, DTs have emerged as a promising approach to manage the virtual manifestation of real-world entities throughout their entire lifecycle. Their proliferation will contribute to realizing the long-craved convergence of virtual and physical spaces to augment things and human capabilities. In this context, despite the proposal of noteworthy contributions, we argue that DTs have not been sufficiently investigated from a software engineering perspective. To address this, in this paper we propose GEMINIS, an architectural reference model that adopts self-adaptation, control, and model-driven engineering techniques to specify the structural and behavioural aspects of DTs and enable the evolution of their internal models. Moreover, we introduce an approach for engineering IoT-intensive Digital Twin Software Systems (DTSS) using GEMINIS' capabilities to deal with uncertain conditions that are inherent to the nature of mirrored physical environments and that might compromise the fidelity of a DT. With GEMINIS and the proposed approach, we aim to advance the engineering of DTSS as well as IoT and cyber-physical systems by providing practitioners with guidelines to model and specify inherent structural and behavioural characteristics of DTs, addressing common design concerns."
Ciniselli2022,Matteo Ciniselli and Nathan Cooper and Luca Pascarella and Antonio Mastropaolo and Emad Aghajani and Denys Poshyvanyk and Massimiliano Di Penta and Gabriele Bavota,An Empirical Study on the Usage of Transformer Models for Code Completion,IEEE Transactions on Software Engineering,48,12,2022,10.1109/TSE.2021.3128234,19393520,"Code completion aims at speeding up code writing by predicting the next code token(s) the developer is likely to write. Works in this field focused on improving the accuracy of the generated predictions, with substantial leaps forward made possible by deep learning (DL) models. However, code completion techniques are mostly evaluated in the scenario of predicting the next token to type, with few exceptions pushing the boundaries to the prediction of an entire code statement. Thus, little is known about the performance of state-of-the-art code completion approaches in more challenging scenarios in which, for example, an entire code block must be generated. We present a large-scale study exploring the capabilities of state-of-the-art Transformer-based models in supporting code completion at different granularity levels, including single tokens, one or multiple entire statements, up to entire code blocks (e.g., the iterated block of a for loop). We experimented with several variants of two recently proposed Transformer-based models, namely RoBERTa and the Text-To-Text Transfer Transformer (T5), for the task of code completion. The achieved results show that Transformer-based models, and in particular the T5, represent a viable solution for code completion, with perfect predictions ranging from ∼29%, obtained when asking the model to guess entire blocks, up to ∼69%, reached in the simpler scenario of few tokens masked from the same code statement."
Zhou2021,Zhi Quan Zhou and T. H. Tse and Matt Witheridge,Metamorphic Robustness Testing: Exposing Hidden Defects in Citation Statistics and Journal Impact Factors,IEEE Transactions on Software Engineering,47,6,2021,10.1109/TSE.2019.2915065,19393520,"We propose a robustness testing approach for software systems that process large amounts of data. Our method uses metamorphic relations to check software output for erroneous input in the absence of a tangible test oracle. We use this technique to test two major citation database systems: Scopus and the Web of Science. We report a surprising finding that the inclusion of hyphens in paper titles impedes citation counts, and that this is a result of the lack of robustness of the citation database systems in handling hyphenated paper titles. Our results are valid for the entire literature as well as for individual fields such as chemistry. We further find a strong and significant negative correlation between the journal impact factor (JIF) of IEEE Transactions on Software Engineering (TSE) and the percentage of hyphenated paper titles published in TSE. Similar results are found for ACM Transactions on Software Engineering and Methodology. A software engineering field-wide study reveals that the higher JIF-ranked journals are publishing a lower percentage of papers with hyphenated titles. Our results challenge the common belief that citation counts and JIFs are reliable measures of the impact of papers and journals, as they can be distorted simply by the presence of hyphens in paper titles."
Schneider2020,Jean Guy Schneider and Peter W. Eklund and Kevin Lee and Feifei Chen and Andrew Cain and Mohamed Abdelrazek,Adopting industry agile practices in large-scale capstone education,,,,2020,10.1145/3377814.3381715,02705257,"This paper presents the practice and experience in adopting an agile organizational model for a final-year capstone program in Software Engineering. The model developed is motivated by having real (and developing) software artifacts with incrementally changing team members working on a product-line. This in turn results in more sophisticated capstone student-project outcomes. The model proposed supports student mentoring and promotes, through its internal organization, leadership and personal responsibility. The students are supported by professional software engineers, up-skilling workshops, and academic supervisors who act as a personalized reporting and grading point for the team. The academic supervisors are themselves supported by a tribe leader, a faculty member who assumes overall responsibility for a product-line, and who acts as a report to an external industry client/sponsor. This paper describes the motivation for the capstone model, its adoption, and some preliminary observations."
Kurtukova2020,Anna Kurtukova and Aleksandr Romanov and Alexander Shelupanov,Source code authorship identification using deep neural networks,Symmetry,12,12,2020,10.3390/sym12122044,20738994,"Many open-source projects are developed by the community and have a common basis. The more source code is open, the more the project is open to contributors. The possibility of accidental or deliberate use of someone else’s source code as a closed functionality in another project (even a commercial) is not excluded. This situation could create copyright disputes. Adding a plagiarism check to the project lifecycle during software engineering solves this problem. However, not all code samples for comparing can be found in the public domain. In this case, the methods of identifying the source code author can be useful. Therefore, identifying the source code author is an important problem in software engineering, and it is also a research area in symmetry. This article discusses the problem of identifying the source code author and modern methods of solving this problem. Based on the experience of researchers in the field of natural language processing (NLP), the authors propose their technique based on a hybrid neural network and demonstrate its results both for simple cases of determining the authorship of the code and for those complicated by obfuscation and using of coding standards. The results show that the author’s technique successfully solves the essential problems of analogs and can be effective even in cases where there are no obvious signs indicating authorship. The average accuracy obtained for all programming languages was 95% in the simple case and exceeded 80% in the complicated ones."
Takhanov2021,Daulet Takhanov and Berikbol Muratuly and Zhuldyz Rashid and Adilzhan Kydrashov,Geomechanics substantiation of pillars development parameters in case of combined mining the contiguous steep ore bodies,Mining of Mineral Deposits,15,1,2021,10.33271/mining15.01.050,24153443,"Purpose. Determining the actual dimensions of the protecting and crown pillars of ore bodies by seismic survey and assessing the possibility of rock mass collapse and fracturing at the lower levels of the Zhairemskoye field. Methods. An integrated approach is used, which involves the analysis of complete ore bodies development during the combined mining. To determine the geological strength index (GSI) and rock mass rating (RMR), the mass structure is studied, as well as the survey is executed of rock fracturing on the contours of mine workings at levels of +288, +240, +192, +144 m. In addition, the physical and mechanical properties of rocks are refined using the RocLab software. Using the numerical modelling of the self-caving process, when mining the protecting and crown pillars, the processed results of numerical modelling are analysed and the possible zones of the mass deformation are assessed based on the Phase2 software. Findings. It has been determined that during the mining of ore bodies 4 and 6, protecting pillars between the quarry and the underground mine, crown pillars between the levels up to the level of +144 m, the rock displacements are possible along glide surfaces. It has been revealed that the haulage workings of levels +240 and +192 m fall into the zone of possible displacements influence, and the rock pillar between ore bodies 4 and 6 will be exposed to inelastic deformations during the mining of crown pillars to the level of +144 m. It has been found that after the crown pillar development between the levels of +240 and +192 m for ore body 6, the rock pillar destructions are possible between ore bodies 4 and 6, since during the modelling, displacements of more than 2 mm are observed. In this case, the destruction processes are possible in the rock pillar upper part. Originality. A geomechanical assessment of the rocks tendency to caving is given and problem areas of stability during the mining of ore bodies 4 and 6 in the Zhairemskoye field are identified. Practical implications. The stable parameters of protecting and crown pillars have been substantiated, which is an important aspect in the design/efficient technology of mining the contiguous ore bodies."
Wang2022,Haiying Wang and Wei Zhang and Yingzhi Zhang and Jian Xu,A bibliometric review on stability and reinforcement of special soil subgrade based on CiteSpace,Journal of Traffic and Transportation Engineering (English Edition),9,2,2022,10.1016/j.jtte.2021.07.005,20957564,"To establish a scientific foundation for further studies and to better understand special soil subgrade research foci and development directions, a visualization analysis of 2601 and 2102 article from 2005 to 2019 was conducted based on the China National Knowledge Infrastructure (CNKI) and Web of Science (WOS) core databases. Time distribution feature, country/region distribution, organization distribution, main source journal distribution, research hotspots and frontier of literature are all analyzed, and the knowledge domain maps are plotted with CiteSpace visualization software. It was found that from 2005 to 2019, the analysis uncovered that China, USA, Australia, Iran and India as the top five most productive countries publishing about subgrade stability and reinforcement with special soil, but results from the USA are the most influential ones in this field, the national/regional quantitative analysis showed that the density of international co-occurrence network is higher than that of China, which indicated that the international research on the subgrade stability and reinforcement with special soil is relatively concentrated, and the CNKI research was focused on dynamic compaction methods, adding CFG pile and geogrid to strengthen the soil subgrade and the WOS researches were focused on the use of geosynthetics to strengthen the subgrade. The current researches of CNKI were revealed to be the compression and deformation characteristics of lightweight foam soil fill subgrade whereas soft soil subgrade problems and numerical analysis methods were the current research foci in the WOS."
Suleimenov2022,Ulanbator Suleimenov and Nurlan Zhangabay and Khassen Abshenov and Akmaral Utelbayeva and Kuanysh Imanaliyev and Saule Mussayeva and Arman Moldagaliyev and Myrzabek Yermakhanov and Gulnura Raikhanova,ESTIMATING THE STRESSEDSTRAINED STATE OF THE VERTICAL MOUNTING JOINT OF THE CYLINDRICAL TANK WALL TAKING INTO CONSIDERATION IMPERFECTIONS,Eastern-European Journal of Enterprise Technologies,3,7-117,2022,10.15587/1729-4061.2022.258118,17294061,"Based on the use of a multi-level mathematical model, this paper estimates the stressed-strained state of a cylindrical reservoir in the mounting joint and considers the concentration of stresses in the joint zone. The correctness of the selected mathematical model was verified to show that for an engineering assessment of the stressed-strained state of the wall of a cylindrical tank with variable thickness, it is possible to use the ratios for a cylindrical shell with a constant wall thickness. The spread of values is no more than 1 %, which indicates the proper selection of the mathematical model. A numerical assessment of the stressedstrained state in the zone of the mounting joint proved the assumption of significant stress concentrations in the zone and indicated the determining effect exerted on the concentration of stresses by its geometric dimensions. The concentration of stresses in the joint zone of the tank wall was investigated at various sizes in the ANSYS programming environment. The result of calculating the stressed-strained state of the reservoir for various values of the dent parameters f/t and a Rt is the constructed polynomials that approximate the stress concentration coefficient Kσ. As a result of the calculations, an interpolation polynomial and an approximating stress concentration coefficient were derived, which could be used to assess the strength, durability, residual life of the tank and to normalize the limiting dimensions of the imperfection of the joint. This paper reports comparative results of the calculations of the stress concentration coefficient depending on the geometric dimensions of the imperfection of the mounting joint in the ANSYS software package, as well as using an interpolation polynomial. The results could be used to assess the strength and residual life of such structures"
Etengu2020,Richard Etengu and Saw Chin Tan and Lee Ching Kwang and Fouad Mohammed Abbou and Teong Chee Chuah,"AI-assisted framework for green-routing and load balancing in hybrid software-defined networking: Proposal, challenges and future perspective",IEEE Access,8,,2020,10.1109/ACCESS.2020.3022291,21693536,"The explosive growth of IP networks, the advent of cloud computing, and the rapid progress in wireless communications witnessed today refiect significant progress towards meeting the explosive data traffic demands. Consequently, communications service providers should deploy efficient and intelligent network solutions to accommodate the huge traffic demands and to ease the capacity pressure on their network infrastructure. Besides, vendors should develop novel energy-efficient networks to reduce network utility costs and carbon footprint. Software-defined networking (SDN) provides a suitable solution, however, complete SDN deployment is currently unachievable in the short-term. An alternative is the hybrid SDN/ open shortest path forwarding (OSPF) network, which allows the deployment of SDN in legacy networks. Nevertheless, hybrid SDN/OSPF also faces several technical, economic and organizational challenges. Although many energy-efficiency routing solutions exist in hybrid SDN/OSPF networks, they are generic and reactive by design. Moreover, these solutions are characterized by manual control plane forwarding configurations, leading to sub-optimal performance. The recent promising combination of SDN and artificial intelligence (AI) techniques such as machine learning (ML) and deep learning (DL) in traffic management and control provides tremendous opportunities. In this paper, we first provide a review of the most recent optimization approaches for global energy-efficient routing and load balancing. Next, we investigate a scalable and intelligent integrated architectural framework that leverages deep reinforcement learning (DRL) techniques to realize predictive and rate adaptive energy-efficient routing with guaranteed quality of service (QoS), in transitional hybrid SDN/OSPF networks. Based on the need to minimize global network energy consumption and improve link performance, this paper provides key research insights into the current progress in hybrid SDN/OSPF, ML and AI in the hope of stimulating more research."
Wolff2020,Sebastian Wolff and Moritz Seidenfus and Karim Gordon and Sergio Álvarez and Svenja Kalt and Markus Lienkamp,Scalable life-cycle inventory for heavy-duty vehicle production,Sustainability (Switzerland),12,13,2020,10.3390/su12135396,20711050,"The transportation sector needs to significantly lower greenhouse gas emissions. European manufacturers in particular must develop new vehicles and powertrains to comply with recent regulations and avoid fines for exceeding CO2 emissions. To answer the question regarding which powertrain concept provides the best option to lower the environmental impacts, it is necessary to evaluate all vehicle life-cycle phases. Different system boundaries and scopes of the current state of science complicate a holistic impact assessment. This paper presents a scaleable life-cycle inventory (LCI) for heavy-duty trucks and powertrains components. We combine primary and secondary data to compile a component-based inventory and apply it to internal combustion engine (ICE), hybrid and battery electric vehicles (BEV). The vehicles are configured with regard to their powertrain topology and the components are scaled according to weight models. The resulting material compositions are modeled with LCA software to obtain global warming potential and primary energy demand. Especially for BEV, decisions in product development strongly influence the vehicle's environmental impact. Our results show that the lithium-ion battery must be considered the most critical component for electrified powertrain concepts. Furthermore, the results highlight the importance of considering the vehicle production phase."
Rogala2021,Michał Rogala and Mirosław Ferdynus and Katarzyna Gawdzińska and Paweł Kochmański,The influence of different length aluminum foam filling on mechanical behavior of a square thin-walled column,Materials,14,13,2021,10.3390/ma14133630,19961944,"The demand for lightweight, strong structural profiles is currently high in the transport industry, mechanical engineering, and construction. Therefore, it is important to evaluate their properties, especially mechanical properties. The main objective of this paper is to determine energy absorption coefficients and evaluate the crush resistance of thin-walled aluminum profiles using numerical simulation and empirical verification. This paper presents the compression results of testing of thin-walled aluminum profiles filled with a porous material (cast aluminum foam). The numerical analysis was conducted using the software Abaqus/CAE. Aluminum material data were obtained from a static tensile test performed on a Shimadzu machine. The experiment was performed on an Instron CEAST 9450HES dynamic hammer. Profiles with three shapes of crush initiators filled with aluminum foam measuring40 mm–200 mm in 20 mm increments were nu-merically tested. A sample with a concave initiator filled with foams of 40 mm, 60 mm, 80 mm, and 120 mm in length was used to verify the numerical analyses. Energy absorption coefficients were determined from the analyses. The results of both analyses were tabulated to show the percentage differences. The study showed an increase in the Crush Load Efficiency (CLE) index by up to 33% for samples with the same crush initiator. In addition, it was noted that the use of porous fill does not increase the value of initiating Peak Crushing Force (PCF), which indicates the generation of much smaller overloads dangerous for vehicle passengers."
Moulisov2020,Vladimíra Moulisová and Miroslav Jiřík and Claudia Schindler and Lenka Červenková and Richard Pálek and Jáchym Rosendorf and Janine Arlt and Lukáš Bolek and Simona Šůsová and Sandor Nietzsche and Václav Liška and Uta Dahmen,Novel morphological multi-scale evaluation system for quality assessment of decellularized liver scaffolds,Journal of Tissue Engineering,11,,2020,10.1177/2041731420921121,20417314,"Decellularized scaffolds can serve as an excellent three-dimensional environment for cell repopulation. They maintain tissue-specific microarchitecture of extracellular matrix proteins with important spatial cues for cell adhesion, migration, growth, and differentiation. However, criteria for quality assessment of the three-dimensional structure of decellularized scaffolds are rather fragmented, usually study-specific, and mostly semi-quantitative. Thus, we aimed to develop a robust structural assessment system for decellularized porcine liver scaffolds. Five scaffolds of different quality were used to establish the new evaluation system. We combined conventional semi-quantitative scoring criteria with a quantitative scaffold evaluation based on automated image analysis. For the quantitation, we developed a specific open source software tool (ScaffAn) applying algorithms designed for texture analysis, segmentation, and skeletonization. ScaffAn calculates selected parameters characterizing structural features of porcine liver scaffolds such as the sinusoidal network. After evaluating individual scaffolds, the total scores predicted scaffold interaction with cells in terms of cell adhesion. Higher scores corresponded to higher numbers of cells attached to the scaffolds. Moreover, our analysis revealed that the conventional system could not identify fine differences between good quality scaffolds while the additional use of ScaffAn allowed discrimination. This led us to the conclusion that only using the combined score resulted in the best discrimination between different quality scaffolds. Overall, our newly defined evaluation system has the potential to select the liver scaffolds most suitable for recellularization, and can represent a step toward better success in liver tissue engineering."
Krier2022,Jessy Krier and Randolph R. Singh and Todor Kondić and Adelene Lai and Philippe Diderich and Jian Zhang and Paul A. Thiessen and Evan E. Bolton and Emma L. Schymanski,Discovering pesticides and their TPs in Luxembourg waters using open cheminformatics approaches,Environment International,158,,2022,10.1016/j.envint.2021.106885,18736750,"The diversity of hundreds of thousands of potential organic pollutants and the lack of (publicly available) information about many of them is a huge challenge for environmental sciences, engineering, and regulation. Suspect screening based on high-resolution liquid chromatography-mass spectrometry (LC-HRMS) has enormous potential to help characterize the presence of these chemicals in our environment, enabling the detection of known and newly emerging pollutants, as well as their potential transformation products (TPs). Here, suspect list creation (focusing on pesticides relevant for Luxembourg, incorporating data sources in 4 languages) was coupled to an automated retrieval of related TPs from PubChem based on high confidence suspect hits, to screen for pesticides and their TPs in Luxembourgish river samples. A computational workflow was established to combine LC-HRMS analysis and pre-screening of the suspects (including automated quality control steps), with spectral annotation to determine which pesticides and, in a second step, their related TPs may be present in the samples. The data analysis with Shinyscreen (https://gitlab.lcsb.uni.lu/eci/shinyscreen/), an open source software developed in house, coupled with custom-made scripts, revealed the presence of 162 potential pesticide masses and 96 potential TP masses in the samples. Further identification of these mass matches was performed using the open source approach MetFrag (https://msbi.ipb-halle.de/MetFrag/). Eventual target analysis of 36 suspects resulted in 31 pesticides and TPs confirmed at Level-1 (highest confidence), and five pesticides and TPs not confirmed due to different retention times. Spatio-temporal analysis of the results showed that TPs and pesticides followed similar trends, with a maximum number of potential detections in July. The highest detections were in the rivers Alzette and Mess and the lowest in the Sûre and Eisch. This study (a) added pesticides, classification information and related TPs into the open domain, (b) developed automated open source retrieval methods - both enhancing FAIRness (Findability, Accessibility, Interoperability and Reusability) of the data and methods; and (c) will directly support “L'Administration de la Gestion de l'Eau” on further monitoring steps in Luxembourg."
Chen2020,Hongyang Chen and Pengfei Chen and Guangba Yu,A Framework of Virtual War Room and Matrix Sketch-Based Streaming Anomaly Detection for Microservice Systems,IEEE Access,8,,2020,10.1109/ACCESS.2020.2977464,21693536,"Recently, microservice has been a popular architecture to construct cloud-native systems. This novel architecture brings agility and accelerates the software development process significantly. However, it is not easy to manage and operate microservice systems due to their scale and complexity. Many approaches are proposed to automatically operate microservice systems such as anomaly detection. Nevertheless, those methods cannot be sufficiently validated and compared due to a lack of real microservice systems, which leads to the slow process of intelligent operation. These challenges inspire us to build a system named 'VWR', a framework of Virtual War Room for operating microservice applications which allows users to simulate their microservice architectures with low overhead and inject multiple types of faults into the microservice system with chaos engineering. VWR can mimic user requests and record the end-to-end tracing data (i.e., service call chains) for each request in a way consistent with OpenTracing. With easily designed tests and the produced streaming tracing data, the users can validate the performance of their intelligent operation algorithms and improve the algorithms as needed. Besides, based on the streaming tracing data generated by VWR, we introduce a novel unsupervised anomaly detection algorithm based on Matrix Sketch and set it as a default intelligent operation algorithm in VWR. This algorithm can detect anomalies by analyzing high-dimensional performance data collected from a microservice system in a streaming manner. The experimental result in VWR shows that the matrix sketch based method can precisely detect anomalies in microservice systems and outperform some widely used anomaly detection methods such as isolation forest in some scenario. We believe more approaches on the intelligent operation of microservice systems can be constructed based on VWR."
Abu-Ghuwaleh2022,Mohammad Abu-Ghuwaleh and Rania Saadeh and Ahmad Qazza,General Master Theorems of Integrals with Applications,Mathematics,10,19,2022,10.3390/math10193547,22277390,"Many formulas of improper integrals are shown every day and need to be solved in different areas of science and engineering. Some of them can be solved, and others require approximate solutions or computer software. The main purpose of this research is to present new fundamental theorems of improper integrals that generate new formulas and tables of integrals. We present six main theorems with associated remarks that can be viewed as generalizations of Cauchy’s results and I.S. Gradshteyn integral tables. Applications to difficult problems are presented that cannot be solved with the usual techniques of residue or contour theorems. The solutions of these applications can be obtained directly, depending on the proposed theorems with an appropriate choice of functions and parameters."
deAlmeida2020,Alexandre Marques de Almeida and Marcelo Kaminski Lenzi and Ervin Kaminski Lenzi,"A survey of fractional order calculus applications of multiple-input, multiple-output (Mimo) process control",Fractal and Fractional,4,2,2020,10.3390/fractalfract4020022,25043110,"Multiple-input multiple-output (MIMO) systems are usually present in process systems engineering. Due to the interaction among the variables and loops in the MIMO system, designing efficient control systems for both servo and regulatory scenarios remains a challenging task. The literature reports the use of several techniques mainly based on classical approaches, such as the proportional-integral-derivative (PID) controller, for single-input single-output (SISO) systems control. Furthermore, control system design approaches based on derivatives and integrals of non-integer order, also known as fractional control or fractional order (FO) control, are frequently used for SISO systems control. A natural consequence, already reported in the literature, is the application of these techniques to MIMO systems to address some inherent issues. Therefore, this work discusses the state-of-the-art of fractional control applied to MIMO systems. It outlines different types of applications, fractional controllers, controller tuning rules, experimental validation, software, and appropriate loop decoupling techniques, leading to literature gaps and research opportunities. The span of publications explored in this survey ranged from the years 1997 to 2019."
Huang2020,Baofeng Huang,Light transmission performance of translucent concrete building envelope,Cogent Engineering,7,1,2020,10.1080/23311916.2020.1756145,23311916,"Energy efficient building envelopes are essential for sustainable development in civil engineering and architecture. In this preliminary investigation, a structural building envelope that is load bearing is developed for daylight harvesting. A translucent concrete panel (TCP) design is constructed using optical fibers (OFs) to transmit light and common concrete mix design. A steel mesh is embedded in the TCP to increase its structural load bearing capacity. It has the potential to save energy and reduce carbon footprint by collecting, channeling and eventually scattering the sunlight. Constructability issues including mechanical and optical losses are analyzed and discussed. Numerical models of the single OF and the whole TCP are developed using ray tracing software and the light transmission mechanisms are analyzed. Nonimaging sunlight collectors, namely compound parabolic concentrator (CPC), together with the OFs represent an efficient system for harvesting and guiding the sunlight into the interior spaces. The light transmission of a model made out of a CPC and an OF is evaluated from an energy efficiency point of view."
Bellalouna2020,Fahmi Bellalouna,Industrial case studies for digital transformation of engineering processes using the virtual reality technology,,90,,2020,10.1016/j.procir.2020.01.082,22128271,"The virtual reality technology (VR) has undergone a rapid development in recent years. The VR software and hardware have become powerful and affordable. Therefore, the VR counts to the key technologies for enabling the digital transformation. However, this significant technology has not achieved the expected breakthrough in the industrial engineering area yet. The potential of the VR to digitize and to improve the engineering process of industrial companies especially for SMEs is still undiscovered. This has led to a poor implementation and use of the VR applications with an engineering scope compared with other business sectors e.g. gaming, entertainments. This paper presents and discusses two case studies achieved within cooperation project between the University of Applied Sciences Karlsruhe and German manufacturers for fire trucks and systems for firefighting and disaster protection. The aim of the case studies is the implementation of VR applications and their evaluation in terms of using in the business processes of the involved industrial partner. Based on the experiences gathered during this cooperation project a CAD Data transformation process for an industrial VR application as best-practice approach will be outlined and discussed in this paper."
Xu2023,Fei Xu and Jiayi Wang and Yang Yang and Lu Wang and Zhen Dai and Ruiqi Han,"On methodology and application of smoothed particle hydrodynamics in fluid, solid and biomechanics",Acta Mechanica Sinica/Lixue Xuebao,39,2,2023,10.1007/s10409-022-22185-x,16143116,"Smoothed particle hydrodynamics (SPH), as one of the earliest meshfree methods, has broad prospects in modeling a wide range of problems in engineering and science, including extremely large deformation problems such as explosion and high velocity impact. This paper aims to provide a comprehensive overview on the recent advances of SPH method in the fields of fluid, solid, and biomechanics. First, the theory of SPH is described, and improved algorithms of SPH with high accuracy are summarized, such as the finite particle method (FPM). Techniques used in SPH method for simulating fluid, solid and biomechanics problems are discussed. The δ-SPH method and Godunov SPH (GSPH) based on the Riemann model are described for handling instability issues in fluid dynamics. Next, the interface contact algorithm for fluid-structure interaction is also discussed. The common algorithms for improving the tensile instability and the framework of total Lagrangian SPH are examined for challenging tasks in solid mechanics. In terms of biomechanics, the governing equations and the coupling forces based on SPH method are exemplified. Then, various typical engineering applications and recent advances are elaborated. The application of fluid mainly depicts the interaction between fluid and rigid body as well as elastomer, while some complicated fluid-structure interaction ocean engineering problems are also presented. In the aspect of solid dynamics, galaxy, geotechnical mechanics, explosion and impact, and additive manufacturing are summarized. Furthermore, the recent advancements of SPH method in biomechanics, such as hemodynamically and gut health, are discussed in general. In addition, to overcome the limitations of computational efficiency and computational scale, the multiscale adaptive resolution, the parallel algorithm and the automated mesh generation are addressed. The development of SPH software in China and abroad is also summarized. Finally, the challenging task of SPH method in the future is summarized. In future research work, the establishment of multi-scale coupled SPH model and deep learning technology in solid and biodynamics will be the focus of expanding the engineering applications of SPH methods. [Figure not available: see fulltext.]"
Thayyib2023,P. V. Thayyib and Rajesh Mamilla and Mohsin Khan and Humaira Fatima and Mohd Asim and Imran Anwar and M. K. Shamsudheen and Mohd Asif Khan,State-of-the-Art of Artificial Intelligence and Big Data Analytics Reviews in Five Different Domains: A Bibliometric Summary,Sustainability (Switzerland),15,5,2023,10.3390/su15054026,20711050,"Academicians and practitioners have recently begun to accord Artificial Intelligence (AI) and Big Data Analytics (BDA) significant consideration when exploring emerging research trends in different fields. The technique of bibliometric review has been extensively applied to the AI and BDA literature to map out existing scholarships. We summarise 711 bibliometric articles on AI & its sub-sets and BDA published in multiple fields to identify academic disciplines with significant research contributions. We pulled bibliometric review papers from the Scopus Q1 and Q2 journal database published between 2012 and 2022. The Scopus database returned 711 documents published in journals of different disciplines from 59 countries, averaging 17.9 citations per year. Multiple software and Database Analysers were used to investigate the data and illustrate the most active scientific bibliometric indicators such as authors and co-authors, citations, co-citations, countries, institutions, journal sources, and subject areas. The USA was the most influential nation (101 documents; 5405 citations), while China was the most productive nation (204 documents; 2371 citations). The most productive institution was Symbiosis International University, India (32 documents; 4.5%). The results reveal a substantial increase in bibliometric reviews in five clusters of disciplines: (a) Business & Management, (b) Engineering and Construction, (c) Healthcare, (d) Sustainable Operations & I4.0, and (e) Tourism and Hospitality Studies, the majority of which investigate the applications and use cases of AI and BDA to address real-world problems in the field. The keyword co-occurrence in the past bibliometric analyses indicates that BDA, AI, Machine Learning, Deep Learning, NLP, Fuzzy Logic, and Expert Systems will remain conspicuous research areas in these five diverse clusters of domain areas. Therefore, this paper summarises the bibliometric reviews on AI and BDA in the fields of Business, Engineering, Healthcare, Sustainable Operations, and Hospitality Tourism and serves as a starting point for novice and experienced researchers interested in these topics."
Rostami2020,Shahin Rostami and Ferrante Neri and Kiril Gyaurski,On Algorithmic Descriptions and Software Implementations for Multi-objective Optimisation: A Comparative Study,SN Computer Science,1,5,2020,10.1007/s42979-020-00265-1,26618907,"Multi-objective optimisation is a prominent subfield of optimisation with high relevance in real-world problems, such as engineering design. Over the past 2 decades, a multitude of heuristic algorithms for multi-objective optimisation have been introduced and some of them have become extremely popular. Some of the most promising and versatile algorithms have been implemented in software platforms. This article experimentally investigates the process of interpreting and implementing algorithms by examining multiple popular implementations of three well-known algorithms for multi-objective optimisation. We observed that official and broadly employed software platforms interpreted and thus implemented the same heuristic search algorithm differently. These different interpretations affect the algorithmic structure as well as the software implementation. Numerical results show that these differences cause statistically significant differences in performance."
Miltner2020,Anders Miltner and Saswat Padhi and Todd Millstein and David Walker,Data-driven inference of representation invariants,,,,2020,10.1145/3385412.3385967,,"A representation invariant is a property that holds of all values of abstract type produced by a module. Representation invariants play important roles in software engineering and program verification. In this paper, we develop a counterexample-driven algorithm for inferring a representation invariant that is sufficient to imply a desired specification for a module. The key novelty is a type-directed notion of visible inductiveness, which ensures that the algorithm makes progress toward its goal as it alternates between weakening and strengthening candidate invariants. The algorithm is parameterized by an example-based synthesis engine and a verifier, and we prove that it is sound and complete for first-order modules over finite types, assuming that the synthesizer and verifier are as well. We implement these ideas in a tool called Hanoi, which synthesizes representation invariants for recursive data types. Hanoi not only handles invariants for first-order code, but higher-order code as well. In its back end, Hanoi uses an enumerative synthesizer called Myth and an enumerative testing tool as a verifier. Because Hanoi uses testing for verification, it is not sound, though our empirical evaluation shows that it is successful on the benchmarks we investigated."
Yadav2021,Apurwa Yadav and Aarshil Patel and Manan Shah,A comprehensive review on resolving ambiguities in natural language processing,AI Open,2,,2021,10.1016/j.aiopen.2021.05.001,26666510,"Natural language processing is a known technology behind the development of some widely known AI assistants such as: SIRI, Natasha, and Watson. However, NLP is a diverse technology used for numerous purposes. NLP based tools are widely used for disambiguation in requirement engineering which will be the primary focus of this paper. A requirement document is a medium for the user to deliver one's expectations from the software. Hence, an ambiguous requirement document may eventually lead to misconceptions in a software. Various tools are available for disambiguation in RE based on different techniques. In this paper, we analyzed different disambiguation tools in order to compare and evaluate them. In our survey, we noticed that even though some disambiguation tools reflect promising results and can supposedly be relied upon, they fail to completely eliminate the ambiguities. In order to avoid ambiguities, the requirement document has to be written using formal language, which is not preferred by users due to its lack of lucidity and readability. Nevertheless, some of the tools we mentioned in this paper are still under development and in future might become capable of eliminating ambiguities. In this paper, we attempt to analyze some existing research work and present an elaborative review of various disambiguation tools."
Jones2021,Alistair Jones and Martin Leary and Stuart Bateman and Mark Easton,TPMS Designer: A tool for generating and analyzing triply periodic minimal surfaces[Formula presented],Software Impacts,10,,2021,10.1016/j.simpa.2021.100167,26659638,"Implicitly defined structures such as triply periodic minimal surfaces (TPMS) have potential applications in science, technology, engineering, arts and mathematics. TPMS Designer is a tool for rapidly generating, visualizing and analyzing implicitly defined structures. The software allows users to parametrically modify the size, aspect ratio, rotation, and resolution of a structure. Cellular volume fraction can be adjusted using surface type and isovalue options. TPMS Designer includes tools for importing and exporting to traditional computer aided design programs. Properties of a surface such as orientation and curvature can be analyzed using in-built visualization techniques."
Teymen2021,Ahmet Teymen,Statistical models for estimating the uniaxial compressive strength and elastic modulus of rocks from different hardness test methods,Heliyon,7,5,2021,10.1016/j.heliyon.2021.e06891,24058440,"In engineering projects (dams, tunnels, slope stability) the strength characteristics of the rocks affect the construction operations. It is sometimes difficult, time-consuming, and expensive to evaluate the engineering properties of solid rocks by performing direct tests. For this reason, various laboratory studies have been carried out by many researchers to predict important engineering properties such as uniaxial compressive strength (UCS) and elastic modulus (E) of rocks in a practical way. One of the engineering properties used to estimate UCS-E practically is the hardness of rocks. Hardness tests are easy to apply and non-destructive, and in many of these tests very small specimens are needed. The main objective of this study is to analyze the relations between the UCS-E of the rocks and the various hardness methods (Schmidt hammer hardness, SHH; Shore Scleroscope hardness, SSH; Vickers hardness, HV; Brinell hardness, HB; and Indentation hardness index, IHI). For this purpose, the most appropriate and meaningful relations between hardness tests and UCS-E were determined by simple regression (SR) techniques. Relationships between main engineering properties (UCS, E) and physicomechanical properties were analyzed by multiple regression (MR) techniques using SPSS software. The statistical analyses made revealed the existence of strong correlations between UCS-E and hardness properties of rocks."
Kik2020,Tomasz Kik and Jaromír Moravec and Martin Švec,Experiments and numerical simulations of the annealing temperature influence on the residual stresses level in s700mc steel welded elements,Materials,13,22,2020,10.3390/ma13225289,19961944,"The article presents the results of research on the influence of temperature and time changes of the annealing process on the values and distribution of stresses in the simulated heat-affected zone of S700MC steel welded joints. For this purpose, tests were carried out on a thermal cycle simulator, as well as heating the prepared samples in accordance with the recorded welding thermal cycles, and then annealing at temperatures from 200 to 550◦C. The stresses values in the tested samples before and after the annealing process were measured by using X-ray diffraction (XRD). The performed tests were verified with the results of numerical analyses using the finite element method (FEM) performed in the VisualWeld (SYSWELD) environment as, on the one hand, the verification of the obtained results, and, on the other hand, the source of data for the development of a methodology for conducting analyses of heat treatment processes of S700MC steel welded structures. Also presented are three examples of numerical analyses for Gas Metal Arc (GMAW), laser and hybrid welding and then the annealing process of the obtained joints at selected temperatures. The main purpose of the work was to broaden the knowledge on the influence of annealing parameters on the values and distribution of stresses in welded joints, but also to signal the possibility of using modern software in engineering practice."
Allawi2020,Mohammed Kadhim Allawi and Mohanad Kadhim Mejbel and Mahmood Hasan Oudah,Variable Valve Timing (VVT) Modelling by Lotus Engine Simulation Software,International Journal of Automotive and Mechanical Engineering,17,4,2020,10.15282/IJAME.17.4.2020.15.0635,21801606,"Variable valve timing (VVT) is an advanced modern technique applied in internal combustion engines by altering the valve lift event timing. This work aims to contribute to the continuing industrial VVT development to improve engine efficiency, fuel consumption and performance. To observe the influence on the spark-ignition (SI) engine’s performance, four valve timing strategies are selected carefully by varying the intake and exhaust valve timing. Lotus Engine Simulation, a simulation engineering software, is adapted in this study. The engine characteristics used in this modelling are spark engine, multicylinder, four strokes, port injection fuel system and constant compression ratio. A comparison between a conventional standard exhaust/intake valve timing and three other different timing cases is carried out. Results reveal that the overlap case of 98° showed a good brake-specific fuel consumption by approximately 3% less than the conventional case. An improvement of 6.2% for volume efficiency and 2.9% in brake thermal efficiency is also reported."
Sibenik2020,Goran Sibenik and Iva Kovacic,Assessment of model-based data exchange between architectural design and structural analysis,Journal of Building Engineering,32,,2020,10.1016/j.jobe.2020.101589,23527102,"The emerging availability of numerous model-based design and analysis tools promises high interoperability in terms of data exchange, thus shorter planning cycles and higher design quality. However, seamless transfer of digital models across domains without significant data inconsistencies is still a major challenge in the practice. This paper aims to identify the procedural problems in the data transfer between architectural design and structural analysis and propose improvement. This study reviewed the current data exchange practices, and concluded that data exchange based on the industry foundation classes (IFC) is the most spread in the research and software tools supporting architecture, engineering and construction industry. The study also analyzed the processes which affect the IFC-based data exchange. The interoperability of software tools through IFC files was tested in a comparative study. The testing results are reflected on the processes influencing data exchange, identifying the procedural problems and recommending the improvements. Upon conducted analysis and testing, the authors propose the following strategies for the systematic improvement of the data exchange: (a) introducing interpretation rules in the data exchange standards; (b) focusing on multiple domain-specific building data schemas instead of on the integrated schema; and (c) developing a new certification process based on the domain-specific building data schema. The improvement proposal supported by the analysis provides a roadmap for the development of model-based exchange between architectural design and structural analysis."
Alzahrani2020,Ahmed Alzahrani,Coronavirus social engineering attacks: Issues and recommendations,International Journal of Advanced Computer Science and Applications,11,5,2020,10.14569/IJACSA.2020.0110523,21565570,"During the current coronavirus pandemic, cybercriminals are exploiting people's anxieties to steal confidential information, distribute malicious software, perform ransomware attacks and use other social engineering attacks. The number of social engineering attacks is increasing day by day due to people's failure to recognize the attacks. Therefore, there is an urgent need for solutions to help people understand social engineering attacks and techniques. This paper helps individuals and industry by reviewing the most common coronavirus social engineering attacks and provides recommendations for responding to such an attack. The paper also discusses the psychology behind social engineering and introduces security awareness as a solution to reduce the risk of social engineering attacks."
Colantoni2020,Alessandro Colantoni and Luca Berardinelli and Manuel Wimmer,DevOpsML: Towards modeling DevOps processes and platforms,,,,2020,10.1145/3417990.3420203,,"DevOps and Model Driven Engineering (MDE) provide differently skilled IT stakeholders with methodologies and tools for organizing and automating continuous software engineering activities-from development to operations, and using models as key engineering artifacts, respectively. Both DevOps and MDE aim at shortening the development life-cycle, dealing with complexity, and improve software process and product quality. The integration of DevOps and MDE principles and practices in low-code engineering platforms (LCEP) are gaining attention by the research community. However, at the same time, new requirements are upcoming for DevOps and MDE as LCEPs are often used by non-technical users, to deliver fully functional software. This is in particular challenging for current DevOps processes, which are mostly considered on the technological level, and thus, excluding most of the current LCEP users. The systematic use of models and modeling to lowering the learning curve of DevOps processes and platforms seems beneficial to make them also accessible for non-technical users. In this paper, we introduce DevOpsML, a conceptual framework for modeling and combining DevOps processes and platforms. Tools along with their interfaces and capabilities are the building blocks of DevOps platform configurations, which can be mapped to software engineering processes of arbitrary complexity. We show our initial endeavors on DevOpsML and present a research roadmap how to employ the resulting DevOpsML framework for different use cases."
Gogoll2021,Jan Gogoll and Niina Zuber and Severin Kacianka and Timo Greger and Alexander Pretschner and Julian Nida-Rümelin,Ethics in the Software Development Process: from Codes of Conduct to Ethical Deliberation,Philosophy and Technology,34,4,2021,10.1007/s13347-021-00451-w,22105441,"Software systems play an ever more important role in our lives and software engineers and their companies find themselves in a position where they are held responsible for ethical issues that may arise. In this paper, we try to disentangle ethical considerations that can be performed at the level of the software engineer from those that belong in the wider domain of business ethics. The handling of ethical problems that fall into the responsibility of the engineer has traditionally been addressed by the publication of Codes of Ethics and Conduct. We argue that these Codes are barely able to provide normative orientation in software development. The main contribution of this paper is, thus, to analyze the normative features of Codes of Ethics in software engineering and to explicate how their value-based approach might prevent their usefulness from a normative perspective. Codes of Conduct cannot replace ethical deliberation because they do not and cannot offer guidance because of their underdetermined nature. This lack of orientation, we argue, triggers reactive behavior such as “cherry-picking,” “risk of indifference,” “ex-post orientation,” and the “desire to rely on gut feeling.” In the light of this, we propose to implement ethical deliberation within software development teams as a way out."
Almonte2022,Lissette Almonte and Esther Guerra and Iván Cantador and Juan de Lara,Recommender systems in model-driven engineering: A systematic mapping review,Software and Systems Modeling,21,1,2022,10.1007/s10270-021-00905-x,16191374,"Recommender systems are information filtering systems used in many online applications like music and video broadcasting and e-commerce platforms. They are also increasingly being applied to facilitate software engineering activities. Following this trend, we are witnessing a growing research interest on recommendation approaches that assist with modelling tasks and model-based development processes. In this paper, we report on a systematic mapping review (based on the analysis of 66 papers) that classifies the existing research work on recommender systems for model-driven engineering (MDE). This study aims to serve as a guide for tool builders and researchers in understanding the MDE tasks that might be subject to recommendations, the applicable recommendation techniques and evaluation methods, and the open challenges and opportunities in this field of research."
Linker2020,Johan Linåker and Björn Regnell and Daniela Damian,A method for analyzing stakeholders’ influence on an open source software ecosystem’s requirements engineering process,Requirements Engineering,25,1,2020,10.1007/s00766-019-00310-3,1432010X,"For a firm in an open source software (OSS) ecosystem, the requirements engineering (RE) process is rather multifaceted. Apart from its typical RE process, there is a competing process, external to the firm and inherent to the firm’s ecosystem. When trying to impose an agenda in competition with other firms, and aiming to align internal product planning with the ecosystem’s RE process, firms need to consider who and how influential the other stakeholders are, and what their agendas are. The aim of the presented research is to help firms identify and analyze stakeholders in OSS ecosystems, in terms of their influence and interactions, to create awareness of their agendas, their collaborators, and how they invest their resources. To arrive at a solution artifact, we applied a design science research approach where we base artifact design on the literature and earlier work. A stakeholder influence analysis (SIA) method is proposed and demonstrated in terms of applicability and utility through a case study on the Apache Hadoop OSS ecosystem. SIA uses social network constructs to measure the stakeholders’ influence and interactions and considers the special characteristics of OSS RE to help firms structure their stakeholder analysis processes in relation to an OSS ecosystem. SIA adds a strategic aspect to the stakeholder analysis process by addressing the concepts of influence and interactions, which are important to consider while acting in collaborative and meritocratic RE cultures of OSS ecosystems."
Khan2022,Rafiq Ahmad Khan and Siffat Ullah Khan and Habib Ullah Khan and Muhammad Ilyas,Systematic Literature Review on Security Risks and its Practices in Secure Software Development,IEEE Access,10,,2022,10.1109/ACCESS.2022.3140181,21693536,"Security is one of the most critical aspects of software quality. Software security refers to the process of creating and developing software that assures the integrity, confidentiality, and availability of its code, data, and services. Software development organizations treat security as an afterthought issue, and as a result, they continue to face security threats. Incorporating security at any level of the Software Development Life Cycle (SDLC) has become an urgent requirement. Several methodologies, strategies, and models have been proposed and developed to address software security, but only a few of them give reliable evidence for creating secure software applications. Software security issues, on the other hand, have not been adequately addressed, and integrating security procedures into the SDLC remains a challenge. The major purpose of this paper is to learn about software security risks and practices so that secure software development methods can be better designed. A systematic literature review (SLR) was performed to classify important studies to achieve this goal. Based on the inclusion, exclusion, and quality assessment criteria, a total of 121 studies were chosen. This study identified 145 security risks and 424 best practices that help software development organizations to manage the security in each phase of the SDLC. To pursue secure SDLC, this study prescribed different security activities, which should be followed in each phase of the SDLC. Successful integration of these activities minimizing effort, time, and budget while delivering secure software applications. The findings of this study assist software development organizations in improving the security level of their software products and also enhancing their security efficiency. This will raise the developer's awareness of secure development practices as well."
Vu2021,Chi Cuong Vu and Jooyong Kim,"Waterproof, thin, high-performance pressure sensors-hand drawing for underwater wearable applications",Science and Technology of Advanced Materials,22,1,2021,10.1080/14686996.2021.1961100,18785514,"Wearable sensors, especially pressure sensors, have become an indispensable part of life when reflecting human interactions and surroundings. However, the difficulties in technology and production-cost still limit their applicability in the field of human monitoring and healthcare. Herein, we propose a fabrication method with flexible, waterproof, thin, and high-performance circuits–based on hand-drawing for pressure sensors. The shape of the sensor is drawn on the pyralux film without assistance from any designing software and the wet-tissues coated by CNTs act as a sensing layer. Such sensor showed a sensitivity (~0.2 kPa−1) while ensuring thinness (~0.26 mm) and flexibility for touch detection or breathing monitoring. More especially, our sensor is waterproof for underwater wearable applications, which is a drawback of conventional paper-based sensors. Its outstanding capability is demonstrated in a real application when detecting touch actions to control a phone, while the sensor is dipped underwater. In addition, by leveraging machine learning technology, these touch actions were processed and classified to achieve highly accurate monitoring (up to 94%). The available materials, easy fabrication techniques, and machine learning algorithms are expected to bring significant contributions to the development of hand-drawing sensors in the future."
Sadkovyi2021,Volodymyr Sadkovyi and Volodymyr Andronov and Oleg Semkiv and Andrii Kovalov and Evgeniy Rybka and Yurii Otrosh and Mykola Udianskyi and Volodymyr Koloskov and Alexander Danilin and Pavlo Kovalov,FIRE RESISTANCE OF REINFORCED CONCRETE AND STEEL STRUCTURES,Fire resistance of reinforced concrete and steel structures,,,2021,10.15587/978-617-7319-43-5,,"The scientific bases of ensuring fire resistance of reinforced concrete and steel building structures in the conditions of modern extreme influences are laid. The current state of fire safety of buildings and structures, as well as approaches, methods and tools for its assessment are analyzed. Analysis of emergencies and fires in the world has shown that the vast majority of them occur in buildings and structures. It is shown that the cause of catastrophic consequences and destruction is the non-compliance of the actual limit of fire resistance of building structures with regulatory requirements. This is due to the imperfection of methods and means of assessing the fire resistance of building structures, including fire-retardant. To overcome the shortcomings identified during the analysis, the paper develops physical and mathematical models of thermal processes occurring in the fire-retardant reinforced concrete structure. Based on the proposed models, a computational-experimental method for estimating the fire resistance of such structures has been developed. The efficiency of the proposed method was tested by identifying the relationship between the parameters of the fire-retardant plaster coating “Neospray” and the fire resistance of fire-retardant multi-hollow reinforced concrete floor. The study of fire resistance of steel structures is proposed to be carried out using reduced samples in the form of steel plates with dimensions of 500×500×5 mm. Based on the proposed models, a calculation and experimental method for estimating the fire resistance of steel structures, as well as an algorithm and procedures for its implementation have been developed. The verification of the efficiency of the proposed method was carried out in the ANSYS software package using the aged coating “Phoenix STS” and the coating “Amotherm Steel Wb” under heating conditions at the temperature of the hydrocarbon fire. The reliability of the developed models and methods is checked. It is established that random errors in temperature measurement significantly affect the accuracy of determining the thermophysical characteristics and limits of fire resistance. In general, the efficiency of the proposed calculation and experimental methods with sufficient accuracy for engineering calculations is confirmed."
Mostafiz2021,Rubayet Bin Mostafiz and Carol J. Friedland and Md Asif Rahman and Robert V. Rohli and Eric Tate and Nazla Bushra and Arash Taghinezhad,"Comparison of Neighborhood-Scale, Residential Property Flood-Loss Assessment Methodologies",Frontiers in Environmental Science,9,,2021,10.3389/fenvs.2021.734294,2296665X,"Leading flood loss estimation models include Federal Emergency Management Agency’s (FEMA’s) Hazus, FEMA’s Flood Assessment Structure Tool (FAST), and (U.S.) Hydrologic Engineering Center’s Flood Impact Analysis (HEC-FIA), with each requiring different data input. No research to date has compared the resulting outcomes from such models at a neighborhood scale. This research examines the building and content loss estimates by Hazus Level 2, FAST, and HEC-FIA, over a levee-protected census block in Metairie, in Jefferson Parish, Louisiana. Building attribute data in National Structure Inventory (NSI) 2.0 are compared against “best available data” (BAD) collected at the individual building scale from Google Street View, Jefferson Parish building inventory, and 2019 National Building Cost Manual, to assess the sensitivity of input building inventory selection. Results suggest that use of BAD likely enhances flood loss estimation accuracy over existing reliance on default data in the software or from a national data set that generalizes over a broad scale. Although the three models give similar mean (median) building and content loss, Hazus Level 2 results diverge from those produced by FAST and HEC-FIA at the individual building level. A statistically significant difference in mean (median) building loss exists, but no significant difference is found in mean (median) content loss, between building inventory input (i.e., NSI 2.0 vs BAD), but both the building and content loss vary at the individual building scale due to difference in building-inventory-reported foundation height, foundation type, number of stories, replacement cost, and content cost. Moreover, building loss estimation also differs significantly by depth-damage function (DDF), for flood depths corresponding with the longest return periods, with content loss differing significantly by DDF at all return periods tested, from 10 to 500 years. Knowledge of the extent of estimated differences aids in understanding the degree of uncertainty in flood loss estimation. Much like the real estate industry uses comparable home values to appraise a home, flood loss planners should use multiple models to estimate flood-related losses. Moreover, results from this study can be used as a baseline for assessing losses from other hazards, thereby enhancing protection of human life and property."
KazeemIyanda2023,Falade Kazeem Iyanda and Hadi Rezazadeh and Mustafa Inc and Ali Akgül and Ibrahim Mujitaba Bashiru and Muhammad Bilal Hafeez and Marek Krawczuk,Numerical simulation of temperature distribution of heat flow on reservoir tanks connected in a series,Alexandria Engineering Journal,66,,2023,10.1016/j.aej.2022.10.062,11100168,"The flow of temperature distribution through a medium in thermodynamic studies plays an important role in understanding physical phenomena in chemical science and petroleum engineering, while temperature distribution indicates the degree of reaction that must be undergone to obtain the final product. Therefore, this paper aims to present and apply the exponential matrix algorithm (EMA), differential transformation algorithm (DTA), and Runge-Kutta (RK5) to simulate the temperature distribution in five heating tanks in series. successive preheating of multicomponent oil solutions. A mathematical model of the energy balance equations of the reservoir is considered. Two computer experiments were performed to test and investigate the relationship between two constant parameters appearing in the model. Numerical simulation of saturated steam Tsteam temperature of 500 °C and 1000 °C used to heat the tanks and initial temperature T035°Cand100°C of the first tank feed oil are considered. The fluids in the reservoirs were considered homogeneous throughout the experiment and changes in the cell configuration at two constant parameters were presented in the 2D plot control with the use of the MAPLE 18 software package. The study revealed the nature of the temperature distribution that the higher temperature distribution is obtained when heat is transferred from the first tank to the fifth tank and the reverse reaction occurs in all five reservoirs when ψ = 0.0025 and ω = 0.0025 respectively. Numerical results obtained are prototypes of oil temperature distribution performed under laboratory conditions in a thermodynamic experiment."
Dharani2021,Rajavelu Dharani and Madasamy Balasubramonian and Thanikanti Sudhakar Babu and Benedetto Nastasi,Load shifting and peak clipping for reducing energy consumption in an indian university campus,Energies,14,3,2021,10.3390/en14030558,19961073,"This paper analyzes the intelligent use of time-varying electrical load via developing efficient energy utilization patterns using demand-side management (DSM) strategies. This approach helps distribution utilities decrease maximum demand and electrical energy billing costs. A case study of DSM implementation of electric energy utility for an educational building Alagappa Chettiar Government College of Engineering and Technology (ACGCET) campus was simulated. The new optimum energy load model was established for peak and off-peak periods from the system's existing load profile using peak clipping and load shifting DSM techniques. The result reflects a significant reduction in maximum demand from 189 kW to 170 kW and a reduction in annual electricity billing cost from $11,340 to $10,200 (approximately 10%) in the upgraded system. This work highlights the importance of time of day (TOD) tariff structure consumers that aid reduction in their distribution system's maximum demand and demand charges. Copyright:"
Kempler2021,Paul A. Kempler and Shannon W. Boettcher and Shane Ardo,Reinvigorating electrochemistry education,iScience,24,5,2021,10.1016/j.isci.2021.102481,25890042,"Electrochemistry is an established discipline with modern frontiers spanning energy conversion and storage, neuroscience, and organic synthesis. In spite of the expanding opportunities for academic and industrial electrochemists, particularly in the growing energy-storage sector, rigorous training of electrochemists is generally lacking at academic institutions in the United States. In this perspective, we highlight the core concepts of electrochemistry and discuss ways in which it has been historically taught. We identify challenges faced when teaching inherently interdisciplinary electrochemical concepts and discuss how technology provides new tools for teaching, such as inexpensive electronics and open-source software, to help address these challenges. Finally, we outline example programs and discuss how new tools and approaches can be brought together to prepare scientists and engineers for careers in electrochemical technology where they can accelerate the research, development, and deployment of the clean energy technology essential to combat climate change in the coming decades."
Zeng2020,Yun Zeng and Si Zhang and Yang Zhou and Meiqiu Li,Numerical simulation of a flow field in a turbo air classifier and optimization of the process parameters,Processes,8,2,2020,10.3390/pr8020237,22279717,"Due to the rapid development of powder technology around the world, powder materials are being widely used in various fields, including metallurgy, the chemical industry, and petroleum. The turbo air classifier, as a powder production equipment, is one of the most important mechanical facilities in the industry today. In order to investigate the production efficiency of ultrafine powder and improve the classification performance in a turbo air classifier, two process parameters were optimized by analyzing the influence of the rotor cage speed and air velocity on the flow field. Numerical simulations using the ANSYS-Fluent Software, as well as material classification experiments, were implemented to verify the optimal process parameters. The simulation results provide many optimal process parameters. Several sets of the optimal process parameters were selected, and the product particle size distribution was used as the inspection index to conduct a material grading experiment. The experimental results demonstrate that the process parameters of the turbo air classifier with better classification efficiency for the products of barite and iron-ore powder were an 1800 rpm rotor cage speed and 8 m/s air inlet velocity. This research study provides theoretical guidance and engineering application value for air classifiers."
Li2021,Shuai Li and Youssef Saade and Devaraj van der Meer and Detlef Lohse,Comparison of Boundary Integral and Volume-of-Fluid methods for compressible bubble dynamics,International Journal of Multiphase Flow,145,,2021,10.1016/j.ijmultiphaseflow.2021.103834,03019322,"The Boundary Integral Method (BIM) has been widely applied to simulate oscillating bubbles, for its high efficiency and accuracy. A conventional BIM assumes the fluid surrounding the bubble to be inviscid and incompressible. Wang & Blake (J. Fluid Mech., 659, 2010, 191–224) proposed an improved model for bubbles in a weakly compressible flow, which is referred to as CBIM. In this study, an all-Mach method (AMM) implemented in the free software program Basilisk for the simulation of compressible multiphase flows, and using a geometric Volume-of-Fluid (VoF), is employed to study and estimate the accuracy of BIM and CBIM at different Mach numbers. First, for a spherical bubble, an extended Rayleigh-Plesset equation, CBIM and AMM give very close results when Ma≲0.3. However, a deviation between these three schemes gradually becomes evident as Ma increases from 0.3 to 0.6. Second, for the nonspherical deformation of a bubble close to a wall, the results obtained from CBIM and AMM show many similarities, including the evolution of the nonspherical bubble morphology, jet impact velocity, and impact pressure on the wall. Apart from the liquid compressibility, the gas inertia/density is found to be another factor that may affect the applicability of CBIM. In addition, we compare the CBIM and BIM results against an experiment of a spark-generated cavitation bubble, in which the liquid compressibility is found to play a vital role. From the perspective of engineering applications, BIM can reproduce the main features of the bubble dynamics in the first cycle if the initial conditions are set properly. The new findings provide a reference for research of bubble dynamics in both fundamental and applied problems."
Abruzzese2020,Donato Abruzzese and Andrea Micheletti and Alessandro Tiero and Manuel Cosentino and Damiano Forconi and Gianmarco Grizzi and Gianluca Scarano and Sreymom Vuth and Pierluigi Abiuso,IoT sensors for modern structural health monitoring. A new frontier,,25,,2020,10.1016/j.prostr.2020.04.043,24523216,"The problem of determining the structural safety level of buildings and civil engineering infrastructures (CEIs) is raising growing concern worldwide. Most of the reinforced concrete constructions have a design life not greater than 100 years, and today it is necessary to face the problem of assessing their level of safety and structural integrity. Such problem is even more pressing when a construction is subjected to extreme environmental conditions. The long-term goal of this study is the realization of wireless low-cost devices, and a data management software, for the structural health monitoring of buildings and CEIs, with remotely controlled sensors embedded in, or installed on, the structural elements, to measure stresses together with accelerations. Once equipped with such system, each construction can become part of the Internet of Things, permitting users and authorities to be alerted in case structural safety is diminished or compromised. A crucial aspect is the unaltered preservation of measurement data over time, which cannot just rely on third parties, and for which it is necessary the exploitation of suitable data-protection technologies. This study have been carried out by experimental testing and validation, both in lab and on site, of the monitoring devices designed and realized. Results show that it is possible to realize low-cost monitoring systems, and related installation techniques, for integration in every new or existing buildings and CEIs."
Huskov2020,Martina Husáková and Vladimír Bureš,Formal ontologies in information systems development: A systematic review,Information (Switzerland),11,2,2020,10.3390/info11020066,20782489,"Computational ontologies are machine-processable structures which represent particular domains of interest. They integrate knowledge which can be used by humans or machines for decision making and problem solving. The main aim of this systematic review is to investigate the role of formal ontologies in information systems development, i.e., how these graphs-based structures can be beneficial during the analysis and design of the information systems. Specific online databases were used to identify studies focused on the interconnections between ontologies and systems engineering. One-hundred eighty-seven studies were found during the first phase of the investigation. Twenty-seven studies were examined after the elimination of duplicate and irrelevant documents. Mind mapping was substantially helpful in organising the basic ideas and in identifying five thematic groups that show the main roles of formal ontologies in information systems development. Formal ontologies are mainly used in the interoperability of information systems, human resource management, domain knowledge representation, the involvement of semantics in unified modelling language (UML)-based modelling, and the management of programming code and documentation. We explain the main ideas in the reviewed studies and suggest possible extensions to this research."
Roy2020,Arindaam Roy and Divjeet Singh Jas and Gitanjali Jaggi and Kapil Sharma,Android Malware Detection based on Vulnerable Feature Aggregation,,173,,2020,10.1016/j.procs.2020.06.040,18770509,"Android has paved the way for the smartphone revolution. With the ever-growing advancements in technology, there is an inherent increase in the user reliance upon mobile technologies and third-party applications for communication, banking, and commerce. Needless to say, this is accompanied by steady growth in the number of attack surfaces, giving rise to new and highly advanced malicious software. Traditional malware detection approaches have revolved around pattern-based detection, which can easily be deterred using zero-day attacks. In this paper, we present a novel feature-engineering technique for android malware detection using Machine Learning. We perform static analysis to map each Application Programming Interface call to certain features, which is later aggregated to find the frequency of occurrence per feature. We empirically evaluate our approach and its robustness on 972 obfuscated android applications and 1100 benign applications and achieve an ROC-AUC score of 98.87%. We also demonstrate the scalability of our model by reducing the feature set by 75.9% and achieving a comparable ROC-AUC score of 95.67%."
Zubizarreta-Macho2020,Álvaro Zubizarreta-Macho and Jesús Mena Álvarez and Alberto Albaladejo Martínez and Juan José Segura-Egea and Javier Caviedes Brucheli and Rubén Agustín-Panadero and Roberto López Píriz and Óscar Alonso-Ezpeleta,Influence of the pecking motion frequency on the cyclic fatigue resistance of endodontic rotary files,Journal of Clinical Medicine,9,1,2020,10.3390/jcm9010045,20770383,"Purpose: To analyze the influence of the pecking motion frequency on the cyclic fatigue resistance of endodontic rotary files. Material and Methods: Sixty PlexV 25.06 endodontic rotary files were selected and distributed into three groups: 30 movements/min (n = 20), 60 movements/min (n = 20), and 120 movements/min (n = 20). A dynamic cyclic fatigue device was designed using Computer Aided Design/ Computer Aided Engineering (CAD/CAE) technology and manufactured by 3D impressions to simulate the pecking motion performed by an operator. Failures of the endodontic rotary files were detected by a Light-Emitting Diode (LED)/Light-Dependent Resistor (LDR) system controlled by an Arduino-Driver complex and management software. Endodontic rotary files were tested on an artificial root canal manufactured by wire electrical discharge machining (EDM), with similar dimensions to those of the instrument under examination. Endodontic rotary files were used following the manufacturer’s recommendations. The results were analyzed by ANOVA and Weibull statistics. Results: All pairwise comparisons revealed statistically significant differences in all three variables, except for the difference in the number of cycles between the groups with 60 and 120 movements/min (p = 0.298). The scale distribution parameter of Weibull statistics showed statistically significant differences in all three variables, except for the differences in the number of cycles between groups with 30 and 60 movements/min (p = 0.0722). No statistically significant differences in the three variables were observed for the shape distribution parameter. Conclusion: A low frequency of pecking motion is recommended to reduce the risk of failure of endodontic rotary files associated with cyclic fatigue."
Waqas2022,Hassan Waqas and Shan Ali Khan and Taseer Muhammad,Thermal analysis of magnetized flow of AA7072-AA7075/blood-based hybrid nanofluids in a rotating channel,Alexandria Engineering Journal,61,4,2022,10.1016/j.aej.2021.08.033,11100168,"Due to their wide range of applications in heat transport phenomena, researchers all over the world have completed theoretical and experimental works to examine the relevance of nanofluid. Nanofluids are made up of nanoparticles suspended in a base fluid. By incorporating nanoparticles into the base fluid, heat capacity and heat transfer rate are enhanced. The heat transfer improvement by using nanofluid is an advance category of heat transport increment. Keeping such effectiveness in mind, the current analysis is provided. The importance of heat transfer improvement and thermal engineering application is considered in the current analysis. In current article, a mathematical model is developed to scrutinize the magnetic-field and melting process aspects in thermal transport in blood-based hybrid nanofluids with different nano-sized particles of aluminum alloys AA7072-AA7075 in a revolving channel. Simultaneous impacts of heat sink/source and thermally-radiation have been considered and discussed. The melting and convective conditions are used in order to make the study more interesting. The obtained system is condensed in order to model the proposed problem in non-linear PDEs. The set of governing PDEs with boundary conditions are made dimensionless by applying suitable similarity-transformations. Afterword such dimensionless ODEs are computed numerical with aid of bvp4c solver (shooting technique) in MATLAB software. The significant findings of velocity and temperature fields against prominent parameters are also described through graphical representations. From the significant findings, we watched that radial-velocity of fluid is decayed with increasing magnetic-number while boosts up for rotational parameter. Larger Reynolds number decreases the tangential velocity of blood-based hybrid nanofluids. Temperature distribution has opposite impact for thermal Biot numbers for upper and lower walls."
Berman2020,Jeffrey W. Berman and Joseph Wartman and Michael Olsen and Jennifer L. Irish and Scott B. Miles and Troy Tanner and Kurtis Gurley and Laura Lowes and Ann Bostrom and Jacob Dafni and Michael Grilliot and Andrew Lyda and Jaqueline Peltier,Natural Hazards Reconnaissance With the NHERI RAPID Facility,Frontiers in Built Environment,6,,2020,10.3389/fbuil.2020.573067,22973362,"In 2016, the National Science Foundation (NSF) funded a multi-institution interdisciplinary team to develop and operate the Natural Hazards Reconnaissance Facility (known as the “RAPID”) as part of the Natural Hazards Engineering Research Infrastructure (NHERI) program. During the following 2 years, the RAPID facility developed its instrumentation portfolio and operational plan with input from the natural hazards community, the facility’s leadership team, and an external steering committee. In September 2018, the RAPID began field operations, which continue today and include instrumentation, software, training, and support services to conduct reconnaissance research before, during, and after natural hazard and disaster events. Over the past 2 years, the RAPID has supported the data collection efforts for over 60 projects worldwide. Projects have spanned a wide range of disciplines and hazards and have also included data collection at large-scale experimental facilities in the United States and abroad. These projects have produced an unprecedented amount of high-quality field data archived on the DesignSafe cyberinfrastructure platform. This paper describes the RAPID facility’s development, instrumentation portfolio (including the mobile application RApp), services and capabilities, and training activities. Additionally, overviews of three recent RAPID-supported projects are presented, including descriptions of field data collection workflows, details of the resulting data sets, and the impact of these project deployments on the natural hazard fields."
Rana2021,Puneet Rana and N. Srikantha and Taseer Muhammad and Gaurav Gupta,Computational study of three-dimensional flow and heat transfer of 25 nm Cu–H2O nanoliquid with convective thermal condition and radiative heat flux using modified Buongiorno model,Case Studies in Thermal Engineering,27,,2021,10.1016/j.csite.2021.101340,2214157X,"The three-dimensional steady flow of an incompressible 25 nm water-based copper nanoliquid over a bi-directional elongated surface with convective thermal boundary condition is studied. This work may meet various thermal engineering applications for instance thermal energy exchangers, solar energy collectors, geophysical transports, radiators, electronic cooling devices, and nuclear reactors. Corcione's model for effective thermal conductivity and dynamic viscosity is used in conjunction with correlations based on effective medium theory for density, specific heat, and electrical conductivity. The effects of radiative heat and convective thermal energy conditions are also taken into account. Modified Buongiorno inhomogeneous model-based governing equations transmuted to nonlinear ordinary boundary value problem. The subsequent system has been deciphered using computer algebraic system software. The method used is validated against the available data and found an exceptional agreement. Two dimensional charts are designed to analyze the impact of the key parameters involved in the model. Three dimensional surface plots are plotted for the study of the Nusselt number, friction coefficients, and Sherwood number values are tabulated for several key parameters. A regression analysis is also performed for the local Nusselt number."
Iradukunda2021,Parfait Iradukunda and Erion Bwambale,"Reservoir sedimentation and its effect on storage capacity–A case study of Murera reservoir, Kenya",Cogent Engineering,8,1,2021,10.1080/23311916.2021.1917329,23311916,"Lakes and dams located in sediment-laden rivers infill with time which reduces the storage capacity and consequently the quality of aquatic life. Anthropogenic activities in Kenya’s Murera watershed have had significant impacts on storage capacity and water quality of Murera reservoir which inturn affects aquatic ecosystems. This study focused on the reservoir sedimentation assessment and examination of the sediment impacts on the storage loss in Murera reservoir dam. This was achieved by establishing the engineering concepts using the Bathymetric Survey System (BSS) comprising of navigation twin boat system (Dual Jon-boats) with an in-built Global Positioning System (GPS) for collecting spatial data. A multi-frequency acoustic system with frequencies of 200,50 and 12 kHz was used to determine the reservoir bed level, part of sediment layers, and deposited sediments, respectively, to pre-impoundment levels of the reservoir. Data processing and analysis were performed using Depthpic 5.0.2 Surfer 15.5, and ArcGIS 10.3 software. The results show that the reservoir depth increased from the North part to the South part of the reservoir with a maximum depth of 7.78 m and the reservoir water storage of 707,862.03 m3. Sediment deposition is concentrated in the southern part compared to the north part of the reservoir. The sediment layer thickness varied from 0 m up to 0.8 m (maximum) and the total sediment storage capacity of 117,683.39 m3, this implies that the reservoir lost 14% of its actual storage capacity. Results from this study provide pertinent information to policymakers in Kenya regarding the development, prioritization and assessment of mitigation and remediation strategies to address sedimentation problems in Kenya’s water resources."
Moreno-SanSegundo2020,José Moreno-SanSegundo and Cintia Casado and Javier Marugán,Enhanced numerical simulation of photocatalytic reactors with an improved solver for the radiative transfer equation,Chemical Engineering Journal,388,,2020,10.1016/j.cej.2020.124183,13858947,"This work presents the enhanced numerical simulation of the radiation transport in three different types of photocatalytic reactor using a novel Discrete Ordinate Method model recently developed for the open-source Computational Fluid Dynamics (CFD) platform OpenFOAM. The photoreactors represent commonly used geometries and illumination sources in the field of heterogeneous photocatalysis: an annular reactor illuminated by a mercury fluorescent lamp, a tubular reactor coupled to a compound parabolic collector illuminated by sunlight, and a tubular reactor illuminated by LEDs. Simulations were carried out for different photocatalyst concentrations, considering absorption and anisotropic scattering, showing differences smaller than 2.4% with respect to the results obtained by commercial CFD software for systems with isotropic emission, such as fluorescent lamps. Moreover, the model was able to improve the simulation of solar reactors and dramatically outperformed the simulation of LED sources, due to the combined effect of quadrature rotation and cone-limit fitting for cone-shaped sources, as well as a LED-specific power-cosine light distribution. The developed model has been thoroughly validated and is now available to the open-source CFD community. It allows a comprehensive numerical simulation of radiation transport using any type of light source, with applications in numerous engineering fields where optical phenomena affect the performance of the process."
Alotaibi2021,Hammad Alotaibi,Traveling wave solutions to the nonlinear evolution equation using expansion method and addendum to kudryashov’s method,Symmetry,13,11,2021,10.3390/sym13112126,20738994,"The inspection of wave motion and propagation of diffusion, convection, dispersion, and dissipation is a key research area in mathematics, physics, engineering, and real-time application fields. This article addresses the generalized dimensional Hirota–Maccari equation by using two different methods: the exp(−ϕ(ζ)) expansion method and Addendum to Kudryashov’s method to obtain the optical traveling wave solutions. By utilizing suitable transformations, the nonlinear PDEs are transformed into ODEs. The traveling wave solutions are expressed in terms of rational functions. For certain parameter values, the obtained optical solutions are described graphically with the aid of Maple 15 software."
Khan2022,Umair Khan and Aurang Zaib and Ioan Pop and Sakhinah Abu Bakar and Anuar Ishak,Unsteady micropolar hybrid nanofluid flow past a permeable stretching/shrinking vertical plate,Alexandria Engineering Journal,61,12,2022,10.1016/j.aej.2022.05.011,11100168,"The current analysis aims to find the solution to the buoyancy effect on time-dependent flow and heat transfer induced by a hybrid micropolar nanofluid over a permeable shrinking or stretching vertical flat plate. A novel hybrid nanofluid is utilized, which consists of the agglomeration of water (pure fluid) and two dissimilar nanoparticles like silver (Ag) and titanium dioxide (TiO2). Initially, the model is developed in the form of the non-linear partial differential equations (PDEs) with three independent variables, which are transformed to the set of dimensionless ordinary differential equations (ODEs) using the appropriate similarity transformations. These dimensionless ODEs are solved numerically via the bvp4c package in MATLAB software. The consequence of various involved controlling parameters on the velocity, microrotation, friction drag, temperature, and heat transfer characteristics for the upper branch solution (UPBS) and the lower branch solution (LOBS) are thoroughly inspected. In physical engineering quantities of interest, it is deeply observed that for the case of stretching, the solution of the stable (upper) branch is possible for the entire negative and positive selected values of the stretching/shrinking parameter. In contrast, the lower branch solution exists only for negative values of the stretching/shrinking parameter for the shrinking case."
Jassim2022,Najwa Wasif Jassim and Hanan Adnan Hassan and Hadeel Ammar Mohammed and Mohammed Yousif Fattah,Utilization of waste marble powder as sustainable stabilization materials for subgrade layer,Results in Engineering,14,,2022,10.1016/j.rineng.2022.100436,25901230,"In geotechnical engineering, the practice of modifying the properties of weak soils with waste marble materials has become increasingly essential. The impact of this waste material on the soil properties has received a lot of attention in recent years. The purpose of this research is to investigate how marble dust powder affects the performance of subgrade soil. Different percentages of marble dust (MD) (0, 3, 6, 9, 12, 15) were employed. Particle size distribution, maximum dry density, optimal moisture content, and unconfined compressive strength (UCS) were all determined by laboratory testing. The study found that adding 3% marble dust to untreated soil produces the best outcomes when compared to other percentages of marble dust. The collected findings indicated that at 3% MD, the greatest value of UCS is attained. The findings revealed that adding marble dust to clay samples will lower the cost of erecting structures on poor soils, and that producing new uses for discarded marble dust will minimize pollution. For pavement application under applied traffic loading of pavement, simulation model using PLAXIS software ver. 8.2 was analyzed. It was concluded that a reduction in the vertical displacement (rutting) for flexible pavement by about (55.3%) could be obtained when the subgrade soil was stabilized with (3%) waste marble dust. Also the vertical compressive strains at top of the subgrade for flexible pavement were reduced by (38%)."
Pappalardo2022,Luca Pappalardo and Filippo Simini and Gianni Barlacchi and Roberto Pellungrini,"scikit-mobility: A Python Library for the Analysis, Generation, and Risk Assessment of Mobility Data",Journal of Statistical Software,103,4,2022,10.18637/jss.v103.i04,15487660,"The last decade has witnessed the emergence of massive mobility datasets, such as tracks generated by GPS devices, call detail records, and geo-tagged posts from social media platforms. These datasets have fostered a vast scientific production on various applications of mobility analysis, ranging from computational epidemiology to urban planning and transportation engineering. A strand of literature addresses data cleaning issues related to raw spatiotemporal trajectories, while the second line of research focuses on discovering the statistical “laws” that govern human movements. A significant effort has also been put on designing algorithms to generate synthetic trajectories able to reproduce, realistically, the laws of human mobility. Last but not least, a line of research addresses the crucial problem of privacy, proposing techniques to perform the re-identification of individuals in a database. A view on state-of-the-art cannot avoid noticing that there is no statistical software that can support scientists and practitioners with all the aspects mentioned above of mobility data analysis. In this paper, we propose scikit-mobility, a Python library that has the ambition of providing an environment to reproduce existing research, analyze mobility data, and simulate human mobility habits. scikit-mobility is efficient and easy to use as it extends pandas, a popular Python library for data analysis. Moreover, scikit-mobility provides the user with many functionalities, from visualizing trajectories to generating synthetic data, from analyzing statistical patterns to assessing the privacy risk related to the analysis of mobility datasets."
Oliveira2020,Micael J.T. Oliveira and Nick Papior and Yann Pouillon and Volker Blum and Emilio Artacho and Damien Caliste and Fabiano Corsetti and Stefano De Gironcoli and Alin M. Elena and Alberto García and Víctor M. García-Suárez and Luigi Genovese and William P. Huhn and Georg Huhs and Sebastian Kokott and Emine Küçükbenli and Ask H. Larsen and Alfio Lazzaro and Irina V. Lebedeva and Yingzhou Li and David López-Durán and Pablo López-Tarifa and Martin Lüders and Miguel A.L. Marques and Jan Minar and Stephan Mohr and Arash A. Mostofi and Alan O'Cais and Mike C. Payne and Thomas Ruh and Daniel G.A. Smith and José M. Soler and David A. Strubbe and Nicolas Tancogne-Dejean and Dominic Tildesley and Marc Torrent and Victor Wen Zhe Yu,The CECAM electronic structure library and the modular software development paradigm,Journal of Chemical Physics,153,2,2020,10.1063/5.0012901,10897690,"First-principles electronic structure calculations are now accessible to a very large community of users across many disciplines, thanks to many successful software packages, some of which are described in this special issue. The traditional coding paradigm for such packages is monolithic, i.e., regardless of how modular its internal structure may be, the code is built independently from others, essentially from the compiler up, possibly with the exception of linear-algebra and message-passing libraries. This model has endured and been quite successful for decades. The successful evolution of the electronic structure methodology itself, however, has resulted in an increasing complexity and an ever longer list of features expected within all software packages, which implies a growing amount of replication between different packages, not only in the initial coding but, more importantly, every time a code needs to be re-engineered to adapt to the evolution of computer hardware architecture. The Electronic Structure Library (ESL) was initiated by CECAM (the European Centre for Atomic and Molecular Calculations) to catalyze a paradigm shift away from the monolithic model and promote modularization, with the ambition to extract common tasks from electronic structure codes and redesign them as open-source libraries available to everybody. Such libraries include ""heavy-duty""ones that have the potential for a high degree of parallelization and adaptation to novel hardware within them, thereby separating the sophisticated computer science aspects of performance optimization and re-engineering from the computational science done by, e.g., physicists and chemists when implementing new ideas. We envisage that this modular paradigm will improve overall coding efficiency and enable specialists (whether they be computer scientists or computational scientists) to use their skills more effectively and will lead to a more dynamic evolution of software in the community as well as lower barriers to entry for new developers. The model comes with new challenges, though. The building and compilation of a code based on many interdependent libraries (and their versions) is a much more complex task than that of a code delivered in a single self-contained package. Here, we describe the state of the ESL, the different libraries it now contains, the short- and mid-term plans for further libraries, and the way the new challenges are faced. The ESL is a community initiative into which several pre-existing codes and their developers have contributed with their software and efforts, from which several codes are already benefiting, and which remains open to the community."
Weinberg2021,Joshua H. Weinberg and Ahmad Sweid and Kalyan Sajja and M. Reid Gooch and Nabeel Herial and Stavropoula Tjoumakaris and Robert H. Rosenwasser and Pascal Jabbour,Comparison of robotic-assisted carotid stenting and manual carotid stenting through the transradial approach,Journal of Neurosurgery,135,1,2021,10.3171/2020.5.JNS201421,19330693,"OBJECTIVE The objective of this study was to demonstrate the feasibility and safety of CorPath GRX robotic-assisted (RA) transradial (TR) carotid artery stenting (CAS) compared with manual TR CAS. METHODS The authors conducted a retrospective analysis of a prospectively maintained database and identified 13 consecutive patients who underwent TR CAS from June 2019 through February 2020. Patients were divided into 2 groups: RA (6 patients) and manual (7 patients). RESULTS Among 6 patients in the RA group with a mean age of 70.0 ± 7.2 years, technical success was achieved in all 6 (100%) procedures; there were no technical or access-site complications and no catheter exchanges. Transfemoral conversion was required in 1 (16.7%) case due to a tortuous aortic arch. There were no perioperative complications, including myocardial infarction, stroke, and mortality. The mean procedure duration was significantly longer in the RA group (85.0 ± 14.3 minutes [95% CI 69.9–100.0] vs 61.2 ± 17.5 minutes [95% CI 45.0–77.4], p = 0.0231). There was no significant difference in baseline characteristics, fluoroscopy time, contrast dose, radiation exposure, catheter exchanges, technical success, transfemoral conversion, technical or access-site complications, myocardial infarction, stroke, other complications, or mortality. CONCLUSIONS The authors’ results suggest that RA TR CAS is feasible, safe, and effective. Neurovascular-specific engineering and software modifications are needed prior to complete remote control. Remote control has important implications regarding patient access to lifesaving procedures for conditions such as stroke and aneurysm rupture as well as operative precision. Future clinical investigations among larger cohorts are needed to demonstrate reliable performance and patient benefit."
Daffara2020,Claudia Daffara and Riccardo Muradore and Nicola Piccinelli and Nicola Gaburro and Tullio de Rubeis and Dario Ambrosini,A cost-effective system for aerial 3d thermography of buildings,Journal of Imaging,6,8,2020,10.3390/JIMAGING6080076,2313433X,"Three-dimensional (3D) imaging and infrared (IR) thermography are powerful tools in many areas in engineering and sciences. Their joint use is of great interest in the buildings sector, allowing inspection and non-destructive testing of elements as well as an evaluation of the energy efficiency. When dealing with large and complex structures, as buildings (particularly historical) generally are, 3D thermography inspection is enhanced by Unmanned Aerial Vehicles (UAV-also known as drones). The aim of this paper is to propose a simple and cost-effective system for aerial 3D thermography of buildings. Special attention is thus payed to instrument and reconstruction software choice. After a very brief introduction to IR thermography for buildings and 3D thermography, the system is described. Some experimental results are given to validate the proposal."
Lewis2020,Austin D. Lewis and Katrina M. Groth,A dynamic bayesian network structure for joint diagnostics and prognostics of complex engineering systems,Algorithms,13,3,2020,10.3390/a13030064,19994893,"Dynamic Bayesian networks (DBNs) represent complex time-dependent causal relationships through the use of conditional probabilities and directed acyclic graph models. DBNs enable the forward and backward inference of system states, diagnosing current system health, and forecasting future system prognosis within the same modeling framework. As a result, there has been growing interest in using DBNs for reliability engineering problems and applications in risk assessment. However, there are open questions about how they can be used to support diagnostics and prognostic health monitoring of a complex engineering system (CES), e.g., power plants, processing facilities and maritime vessels. These systems' tightly integrated human, hardware, and software components and dynamic operational environments have previously been difficult to model. As part of the growing literature advancing the understanding of how DBNs can be used to improve the risk assessments and health monitoring of CESs, this paper shows the prognostic and diagnostic inference capabilities that are possible to encapsulate within a single DBN model. Using simulated accident sequence data from a model sodium fast nuclear reactor as a case study, a DBN is designed, quantified, and verified based on evidence associated with a transient overpower. The results indicate that a joint prognostic and diagnostic model that is responsive to new system evidence can be generated from operating data to represent CES health. Such a model can therefore serve as another training tool for CES operators to better prepare for accident scenarios."
SchmittLaser2020,Marcelo Schmitt Laser and Nenad Medvidovic and Duc Minh Le and Joshua Garcia,"ARCADE: An extensible workbench for architecture recovery, change, and decay evaluation",,,,2020,10.1145/3368089.3417941,,"This paper presents the design, implementation, and usage details of ARCADE, an extensible workbench for supporting the recovery of software systems' architectures, and for evaluating architectural change and decay. ARCADE has been developed and maintained over the past decade, and has been deployed in a number of research labs as well as within three large companies. ARCADE's implementation is available at https://bitbucket.org/joshuaga/arcade and the video depicting its use at https://tinyurl.com/arcade-tool-demo."
Komadja2020,Gbétoglo Charles Komadja and Sarada Prasad Pradhan and Amulya Ratna Roul and Babatunde Adebayo and Jean Baptiste Habinshuti and Luc Adissin Glodji and Azikiwe Peter Onwualu,Assessment of stability of a Himalayan road cut slope with varying degrees of weathering: A finite-element-model-based approach,Heliyon,6,11,2020,10.1016/j.heliyon.2020.e05297,24058440,"Slope stability assessment is essential for safe and sustainable development widely applied in mining, civil, and environmental engineering projects around the world. This study aimed to conduct a stability analysis of a selected Himalayan road cut slope from two different sections, named sections (A) and (B). The strength reduction factor (SRF) based on the finite element method was used to simulate the slope sections using Phase2 software. A mesh pattern of six node triangle elements was used during the numerical simulation. The Mohr-Coulomb parameters and other inputs used in the numerical modelling of the investigated slope were estimated by different geotechnical tests, namely, the direct shear test, density analysis test, rock hardness test, and Brazilian test. The results indicated that the critical SRF of the completely weathered slope profile section (A), with a relatively low overall angle, was found to be 1.25, which is approximately 50% lower than the value obtained in the moderately to highly weathered profile section (B), equal to 2.53. These results are in agreement with other published studies, which revealed that the geometry of a slope influences the weathering grade, which in turn destabilizes the slope. The results of this study will help in engineering slope design considering the influence of weathering."
Liu2022,Dongsheng Liu and Hanlong Liu and Yue Wu and Wengang Zhang and Yanlei Wang and M. Santosh,Characterization of geo-material parameters: Gene concept and big data approach in geotechnical engineering,Geosystems and Geoenvironment,1,1,2022,10.1016/j.geogeo.2021.09.003,27728838,"Due to their inherent natural properties, the basic physio-mechanical properties of geo-materials generally exhibit varying degrees of spatial variability. Therefore, describing and characterizing the global traits of geo-materials based on very limited analytical data pose a great challenge. Furthermore, the differences of their characteristics significantly impact the determination of geotechnical engineering stability as well as the relevant engineering design results. With a view to resolve this issue, in this study, we apply the concept of “Gene” from biology, and propose a novel approach to describe the “genetic” characteristics of geo-materials based on general attributes of the specific rock (soil) materials belongings to a certain region or zone. We propose the definition of genetic characteristics as well as the basic attributes in this paper. Our study provides the methodology of utilizing the big data theory to statistically analyze the geo-material parametric data identified in the previous projects and the framework for search of the genetic characteristics. We also developed a software system for big data management and analysis of genetic characteristics of geo-materials, which can fulfill the tasks of collecting, transmitting, classifying, screening, managing and analyzing the big data. Based on more than 80,000 sets of standard geo-material physio-mechanical data from a representative region, the genetic features of typical geo-materials are analyzed to obtain the corresponding gene maps. The findings from this study offer a reliable guidance for the determination of geo-material parameters, data support for engineering construction, and potential application in the mitigation of natural disasters."
Touaibi2020,Rabah Touaibi and Hasan Koten and Ozlem Boydak,Parametric study of an organic rankine cycle using different fluids,Emerging Science Journal,4,2,2020,10.28991/esj-2020-01216,26109182,"This work is an energy study of an organic Rankine cycle (ORC) for the recovery of thermal energy by comparing three organic fluids. This cycle is considered to be a promising cycle for the conversion of heat into mechanical energy suitable for low temperature heat sources; it uses more volatile organic fluids than water, which generally has high molecular weights, thus allowing operating pressures at temperatures lower than those of the traditional Rankine cycle. A thermodynamic model was developed using the Engineering Equation Solver (EES) software to determine its performance using different working fluids (toluene, R245fa and R123) under the same operating conditions, taking into account the effect of certain operating parameters and the selection of organic fluids on cycle performance. The results obtained show that the toluene organic fluid has the best thermal efficiency of the cycle compared to the other fluids; 14.38% for toluene, 13.68% for R123 and 13.19 for R245fa."
Dobrilovic2021,Dalibor Dobrilovic and Vladimir Brtka and Zeljko Stojanov and Gordana Jotanovic and Dragan Perakovic and Goran Jausevac,A model for working environment monitoring in smart manufacturing,Applied Sciences (Switzerland),11,6,2021,10.3390/app11062850,20763417,"The growing application of smart manufacturing systems and the expansion of the Industry 4.0 model have created a need for new teaching platforms for education, rapid application development, and testing. This research addresses this need with a proposal for a model of working environment monitoring in smart manufacturing, based on emerging wireless sensor technologies and the message queuing telemetry transport (MQTT) protocol. In accordance with the proposed model, a testing platform was developed. The testing platform was built on open-source hardware and software components. The testing platform was used for the validation of the model within the presented experimental environment. The results showed that the proposed model could be developed by mainly using open-source components, which can then be used to simulate different scenarios, applications, and target systems. Furthermore, the presented stable and functional platform proved to be applicable in the process of rapid prototyping, and software development for the targeted systems, as well as for student teaching as part of the engineering education process."
Alrumaih2020,Hala Alrumaih and Abdulrahman Mirza and Hessah Alsalamah,Domain Ontology for Requirements Classification in Requirements Engineering Context,IEEE Access,8,,2020,10.1109/ACCESS.2020.2993838,21693536,"Research in recent years has shown a merge between the areas of requirements engineering and semantic technologies. With the release of the semantic concept and the progress of semantic technologies, the opportunities for applying ontologies as a means to define information and knowledge semantics have become increasingly accepted in different domains. Concurrently, the implementation of most requirements classification techniques does not handle the semantic aspects of requirements. If the meaning of requirements and their relations can be handled, software developers can obtain more effective requirement classifications to produce requirements specifications of higher quality. In this study, a domain ontology is proposed to present a requirements classification technique that can be used to share and describe different classifications. The proposed ontology is built using a systematic method based on Methontology and it is implemented using Protégé. The developed ontology was successfully evaluated using validation and verification tests. The validation test included the evaluation of content and competency questions, while the verification test included the evaluation of taxonomy and the implementation of the FOCA method. The proposed ontology may represent a significant contribution to ontology libraries. In addition, this ontology can be used in several ways to increase the quality of software requirements specification documents. It could also ensure consistency between requirements, and facilitate communication between requirements engineers owing to the use of same terminologies for various software applications."
Tribst2022,João Paulo Mendes Tribst and Dayana Campanelli de Morais and Jefferson David Melo de Matos and Guilherme da Rocha Scalzer Lopes and Amanda Maria de Oliveira Dal Piva and Alexandre Luiz Souto Borges and Marco Antonio Bottino and Antonio Lanzotti and Massimo Martorelli and Pietro Ausiello,Influence of Framework Material and Posterior Implant Angulation in Full-Arch All-on-4 Implant-Supported Prosthesis Stress Concentration,Dentistry Journal,10,1,2022,10.3390/dj10010012,23046767,"This study evaluated the influence of distal implants angulation and framework material in the stress concentration of an All-on-4 full-arch prosthesis. A full-arch implant-supported prosthesis 3D model was created with different distal implant angulations and cantilever arms (30° with 10-mm cantilever; 45° with 10-mm cantilever and 45° with 6-mm cantilever) and framework materials (Cobalt–chrome [CoCr alloy], Yttria-stabilized tetragonal zirconia polycrystal [Y-TZP] and polyetheretherketone [PEEK]). Each solid was imported to computer-aided engineering software, and tetrahedral elements formed the mesh. Material properties were assigned to each solid with isotropic and homogeneous behavior. The contacts were considered bonded. A vertical load of 200 N was applied in the distal region of the cantilever arm, and stress was evaluated in Von Misses (σVM) for prosthesis components and the Maximum (σMAX) and Minimum (σMIN) Principal Stresses for the bone. Distal implants angled in 45° with a 10-mm cantilever arm showed the highest stress concentration for all structures with higher stress magnitudes when the PEEK framework was considered. However, distal implants angled in 45° with a 6-mm cantilever arm showed promising mechanical responses with the lowest stress peaks. For the All-on-4 concept, a 45° distal implants angulation is only beneficial if it is possible to reduce the cantilever’s length; otherwise, the use of 30° should be considered. Comparing with PEEK, the YTZP and CoCr concentrated stress in the framework structure, reducing the stress in the prosthetic screw."
Sang-To2021,Thanh Sang-To and Minh Hoang-Le and Samir Khatir and Seyedali Mirjalili and Magd Abdel Wahab and Thanh Cuong-Le,Forecasting of excavation problems for high-rise building in Vietnam using planet optimization algorithm,Scientific Reports,11,1,2021,10.1038/s41598-021-03097-y,20452322,"In this paper, a new method in forecasting the horizontal displacement of diaphragm wall (D.W.) for high-rise buildings is introduced. A new stochastic optimizer, called Planet Optimization Algorithm (P.O.A.), is employed to assess how proper finite element (F.E.) simulation is against field data. The process is adopted for a real phased excavation measured at the field. To automatically run the iterative optimization tasks, a source code is constructed directly in the Geotechnical Engineering Software (PLAXIS) by using Python to ensure that the operation between optimization algorithm and F.E. simulations are smooth to guarantee the accuracy of the complex calculation for the soil problem. The proposed process consists of two steps. (1) The parameters will be optimized at the early phases of the excavation. (2) The responses of D.W. displacements are forecasted at the subsequent phases. The aim of the process is to predict the displacements of D.W. of the building from the result of the nearby excavation or to provide early warning about the risks of excavation that may happen under vital phases. The proposed procedure also provides an effective method for optimization-based soil parameters updating in real engineering practice."
Su2021,Yawen Su and Guofu Chen and Moyan Li and Tengfei Shi and Diandian Fang,Design and Implementation of Web Multimedia Teaching Evaluation System Based on Artificial Intelligence and jQuery,Mobile Information Systems,2021,,2021,10.1155/2021/7318891,1875905X,"It is an important reflection of modern education and an overwhelming way to strengthen the quality of teaching. In the current environment, the traditional multimedia classroom management model can no longer adapt to the current rapidly developing network environment. How to manage more and more campus multimedia classrooms is an urgent problem to be solved. The informatization construction and application of multimedia classrooms is the key to the realization of educational informatization. The traditional multimedia classroom management model has not been able to adapt to the rapidly developing network environment, which is mainly manifested in the following aspects: electronic education management personnel cannot discover and process teaching in a timely manner. Equipment failure has not formed a set of standard troubleshooting procedures and cannot accurately record the status, use time, and maintenance records of various teaching pieces of equipment. This will not only affect the teaching quality of colleges and universities but also slow down the process of education informatization. This paper develops a web-based multimedia teaching equipment management system based on artificial intelligence and jQuery, which realizes the centralized control and management of various multimedia teaching equipment. According to the actual needs of multimedia teaching, this paper follows the design and development of software engineering, using artificial intelligence, jQuery, Ajax, and Spring MVC technology to design and develop a web-based multimedia teaching equipment management system. On the basis of realizing the centralized control and management of multiple multimedia teaching equipment, it can also track and record the use status and maintenance content of the multimedia teaching equipment to form an information knowledge base of the multimedia equipment, which is convenient for later maintenance and management. Through the use of this system, management can be systematized, standardized, and automated, reducing the tedious workload of management and maintenance personnel. It can speed up the information management process of multimedia teaching equipment and improve the work efficiency of related managers. A course can be studied online, and an online teaching system has been developed. According to our survey on Mandarin online course training in Northwest China (N = 343), we found that 81.6% of samples are satisfied with the Mandarin online training courses; 21.6% think that they have learned new teaching methods/teaching concepts from the teacher through the Mandarin training; 36.2% think that they have learned the theoretical knowledge of Mandarin through the Mandarin training. Gender, age, ethnicity, and learning experience are related to the difficulty of learning Mandarin online courses. Therefore, we can satisfy learners of different ages, learning foundations, and cultural backgrounds by designing different online course patterns, so as to enhance the high-quality promotion of Mandarin."
Patel2021,Tirth Patel and Hirakraj Bapat and Daksh Patel and Jacobus Daniel van der Walt,Identification of critical success factors (Csfs) of bim software selection: A combined approach of fcm and fuzzy dematel,Buildings,11,7,2021,10.3390/buildings11070311,20755309,"The architecture, engineering, and construction (AEC) industry has seen a significant rise in the adoption of Building Information Modeling (BIM) in the last few years. BIM software have launched with numerous robust capabilities and features to satisfy the ever-demanding needs of the AEC industry. Various factors are associated with the selection of BIM software depending on a company’s requirements and constraints. BIM software selection is a daunting process as most AEC industries are unaware of the factors to consider when making this important decision. This study focuses on identifying the critical success factors (CSFs) and their interrelationship for efficient BIM software selection. For this research, a questionnaire was developed and disseminated in two stages in India, the United States of America (U.S.A.), Germany, and the United Kingdom (U.K.). In the first stage, a total of twenty-six identified CSFs were analyzed with the factor comparison method (FCM) to identify the top fifteen CSFs. Subsequently, the identified top fifteen CSFs were further assessed by implementing Fuzzy DEMATEL to categorize them into cause-and-effect groups based on respective influence strength, depicted with a causal diagram. Out of fifteen CSFs, five and ten factors were grouped into the cause group and effect group for BIM software selection, respectively. The most important factors were identified as software functionality, BIM adoption strategies and processes, interoperability, staff competencies, BIM standards and regional regulations. The outcome of this research can help BIM user companies improve their BIM software selection framework and decision-making process during purchasing software."
Schindler2020,Daniel Schindler,Genetic engineering and synthetic genomics in yeast to understand life and boost biotechnology,Bioengineering,7,4,2020,10.3390/bioengineering7040137,23065354,"The field of genetic engineering was born in 1973 with the “construction of biologically functional bacterial plasmids in vitro”. Since then, a vast number of technologies have been developed allowing large‐scale reading and writing of DNA, as well as tools for complex modifications and alterations of the genetic code. Natural genomes can be seen as software version 1.0; synthetic genomics aims to rewrite this software with “build to understand” and “build to apply” philosophies. One of the predominant model organisms is the baker’s yeast Saccharomyces cerevisiae. Its importance ranges from ancient biotechnologies such as baking and brewing, to high‐end valuable compound synthesis on industrial scales. This tiny sugar fungus contributed greatly to enabling humankind to reach its current development status. This review discusses recent developments in the field of genetic engineering for budding yeast S. cerevisiae, and its application in biotechnology. The article highlights advances from Sc1.0 to the developments in synthetic genomics paving the way towards Sc2.0. With the synthetic genome of Sc2.0 nearing completion, the article also aims to propose perspectives for potential Sc3.0 and subsequent versions as well as its implications for basic and applied research."
Karhap2021,Pertti Karhapää and Woubshet Behutiye and Pilar Rodríguez and Markku Oivo and Dolors Costal and Xavier Franch and Sanja Aaramaa and Michał Choraś and Jari Partanen and Antonin Abherve,Strategies to manage quality requirements in agile software development: a multiple case study,Empirical Software Engineering,26,2,2021,10.1007/s10664-020-09903-x,15737616,"Agile methods can deliver software that fulfills customer needs rapidly and continuously. Quality requirements (QRs) are important in this regard; however, detailed studies on how companies applying agile methods to manage QRs are limited, as are studies on the rationale for choosing specific QR management practices and related challenges. The aim of this study was to address why practitioners manage QRs as they do and what challenges they face. We also analyzed how existing practices mitigate some of the found challenges. Lastly, we connect the contextual elements of the companies with their practices and challenges. We conducted 36 interviews with practitioners from four companies of varying sizes. Since each company operates in different domains, comparing QR management strategies and related challenges in different contexts was possible. We found that the companies apply proactive, reactive, and interactive strategies to manage QRs. Additionally, our study revealed 40 challenges in six categories that companies applying agile methods may face in QR management. We also identified nine contextual elements that affect QR management practice choices and which, importantly, can explain many related challenges. Based on these findings, we constructed a theoretical model about the connection between context, QR management practices, and challenges. Practitioners in similar contexts can learn from the practices identified in this study. Our preliminary theoretical model can help other practitioners identify what challenges they can expect to face in QR management in different developmental contexts as well as which practices to apply to mitigate these challenges."
Almarimi2020,Nuri Almarimi and Ali Ouni and Moataz Chouchen and Islem Saidani and Mohamed Wiem Mkaouer,On the detection of community smells using genetic programming-based ensemble classifier chain,,,,2020,10.1145/3372787.3390439,,"Community smells are symptoms of organizational and social issues within the software development community that often increase the project costs and impact software quality. Recent studies have identified a variety of community smells and defined them as suboptimal patterns connected to organizational-social structures in the software development community such as the lack of communication, coordination and collaboration. Recognizing the advantages of the early detection of potential community smells in a software project, we introduce a novel approach that learns from various community organizational and social practices to provide an automated support for detecting community smells. In particular, our approach learns from a set of interleaving organizational-social symptoms that characterize the existence of community smell instances in a software project. We build a multi-label learning model to detect 8 common types of community smells. We use the ensemble classifier chain (ECC) model that transforms multi-label problems into several single-label problems which are solved using genetic programming (GP) to find the optimal detection rules for each smell type. To evaluate the performance of our approach, we conducted an empirical study on a benchmark of 103 open source projects and 407 community smell instances. The statistical tests of our results show that our approach can detect the eight considered smell types with an average F-measure of 89% achieving a better performance compared to different state-of-the-art techniques. Furthermore, we found that the most influential factors that best characterize community smells include the social network density and closeness centrality as well as the standard deviation of the number of developers per time zone and per community."
Gui2020,Zhongcheng Gui and Haifeng Li,Automated Defect Detection and Visualization for the Robotic Airport Runway Inspection,IEEE Access,8,,2020,10.1109/ACCESS.2020.2986483,21693536,"Detection of both surface and subsurface defects is a vital task for maintaining the structural health and reliability of airport runways. We report the automated data collection and analysis for airport runways based on our novel robotic system, which employs a camera and a GPR (Ground Penetrating Radar) to inspect the surface and subsurface conditions, respectively. To perform the automated data analysis, we propose a novel crack detection algorithm based on the images, and a subsurface defect detection method with GPR data. Additionally, to create a composite global view of a large airport runway span, a camera/GPR data sequence from the robot is aligned accurately to create a continuous mosaic for visualization. We combine these algorithms into a software to perform automated on-site analysis. We have put our robot and software into engineering practice over 20 airports in China, achieving the performance of 70% and 67% F1-measure for crack detection and subsurface defect detection, respectively. More importantly, the results of our algorithms can satisfy the requirement of applications."
Barriga2021,Jose A. Barriga and Pedro J. Clemente and Encarna Sosa-Sanchez and Alvaro E. Prieto,"SimulateIoT: Domain Specific Language to Design, Code Generation and Execute IoT Simulation Environments",IEEE Access,9,,2021,10.1109/ACCESS.2021.3092528,21693536,"Internet of Things (IoT) is being applied to areas as smart-cities, home environment, agriculture, industry, etc. Developing, deploying and testing IoT projects require high investments on devices, fog nodes, cloud nodes, analytic nodes, hardware and software. New projects require high investments on devices, fog nodes, cloud nodes, analytic nodes, hardware and software before each system can be developed. In addition, the systems should be developed to test them, which implies time, effort and development costs. However, in order to decrease the cost associated to develop and test the system the IoT system can be simulated. Thus, simulating environments help to model the system, reasoning about it, and take advantage of the knowledge obtained to optimize it. Designing IoT simulation environments has been tackled focusing on low level aspects such as networks, motes and so on more than focusing on the high level concepts related to IoT environments. Additionally, the simulation users require high IoT knowledge and usually programming capabilities in order to implement the IoT environment simulation. The concepts to manage in an IoT simulation includes the common layers of an IoT environment including Edge, Fog and Cloud computing and heterogeneous technology. Model-driven development is an emerging software engineering area which aims to develop the software systems from domain models which capture at high level the domain concepts and relationships, generating from them the software artefacts by using code-generators. In this paper, a model-driven development approach has been developed to define, generate code and deploy IoT systems simulation. This approach makes it possible to design complex IoT simulation environments and deploy them without writing code. To do this, a domain metamodel, a graphical concrete syntax and a model to text transformation have been developed. The IoT simulation environment generated from each model includes the sensors, actuators, fog nodes, cloud nodes and analytical characteristics, which are deployed as microservices and Docker containers and where elements are connected by using publish-subscribe communication protocol. Additionally, two case studies, focused on smart building and agriculture IoT environments, are presented to show the simulation expressiveness."
Zhai2020,Juan Zhai and Yu Shi and Minxue Pan and Guian Zhou and Yongxiang Liu and Chunrong Fang and Shiqing Ma and Lin Tan and Xiangyu Zhang,C2S: Translating natural language comments to formal program specifications,,,,2020,10.1145/3368089.3409716,,"Formal program specifications are essential for various software engineering tasks, such as program verification, program synthesis, code debugging and software testing. However, manually inferring formal program specifications is not only time-consuming but also error-prone. In addition, it requires substantial expertise. Natural language comments contain rich semantics about behaviors of code, making it feasible to infer program specifications from comments. Inspired by this, we develop a tool, named C2S, to automate the specification synthesis task by translating natural language comments into formal program specifications. Our approach firstly constructs alignments between natural language word and specification tokens from existing comments and their corresponding specifications. Then for a given method comment, our approach assembles tokens that are associated with words in the comment from the alignments into specifications guided by specification syntax and the context of the target method. Our tool successfully synthesizes 1,145 specifications for 511 methods of 64 classes in 5 different projects, substantially outperforming the state-of-the-art. The generated specifications are also used to improve a number of software engineering tasks like static taint analysis, which demonstrates the high quality of the specifications."
Khan2021,Junaid Ali Khan and Saif Ur Rehman Khan and Javed Iqbal and Inayat Ur Rehman,Empirical Investigation about the Factors Affecting the Cost Estimation in Global Software Development Context,IEEE Access,9,,2021,10.1109/ACCESS.2021.3055858,21693536,"Software organization always aims at developing a quality software product using the estimated development resources, effort, and time. Global Software Development (GSD) has emerged as an essential tool to ensure optimal utilization of resources, which is performed in globally distributed settings in various geographical locations. Global software engineering focuses on reducing the cost, increasing the development speed, and accessing skilled developers worldwide. Estimating the required amount of resources and effort in the distributed development environment remains a challenging task. Thus, there is a need to focus on cost estimation models in the GSD context. We nevertheless acknowledge that several cost estimation techniques have been reported. However, to the best of our knowledge, the existing cost estimation techniques/models lack considering the additional cost drivers required to compute the accurate cost estimation in the GSD context. Motivated by this, the current work aims at identifying the other cost drivers that affect the cost estimation in the context of GSD. To achieve the targeted objectives, current state-of-the-art related to existing cost estimation techniques of GSD is reported. We adopted SLR and Empirical approach to address the formulated research questions. The current study also identifies the missing factors that would help the practitioners improve the cost estimation models. The results indicate that previously conducted work ignores the additional elements necessary for the cost estimation in the GSD context. Moreover, the current work proposes a conceptual cost estimation model tailored to fit the GSD context."
Cui2020,Zhihao Cui and Chaobing Yan,Deep integration of health information service system and data mining analysis technology,Applied Mathematics and Nonlinear Sciences,5,2,2020,10.2478/amns.2020.2.00063,24448656,"The scale and complexity of health information service system has increased dramatically, and its development activities and management are difficult to control. In the field of, Traditional methods and simple mathematical statistics methods are difficult to solve the problems caused by the explosive growth of data and information, which will adversely affect health information service system management finally. So, it is particularly important to find valuable information from the source code, design documents and collected software datasets and to guide the development and maintenance of software engineering. Therefore, some experts and scholars want to use mature data mining technologies to study the large amount of data generated in software engineering projects (commonly referred to as software knowledge base), and further explore the potential and valuable information inherently hidden behind the software data. This article initially gives a brief overview of the relevant knowledge of data mining technology and computer software technology, using decision tree graph mining algorithm to mine the function adjustment graph of the software system definition class, and then source code annotations are added to the relevant calling relationships. Data mining technology and computer software technology are deeply integrated, and the decision tree algorithm in data mining is used to mine the knowledge base of computer software. Potential defect changes are listed as key maintenance objects. The historical versions of source code change files with defects are found dynamically and corrected in time, to avoid the increase of maintenance cost in the future."
Xu2021,Yilun Xu and Gang Huang and Jan Balewski and Ravi Naik and Alexis Morvan and Bradley Mitchell and Kasra Nowrouzi and David I. Santiago and Irfan Siddiqi,QubiC: An Open-Source FPGA-Based Control and Measurement System for Superconducting Quantum Information Processors,IEEE Transactions on Quantum Engineering,2,,2021,10.1109/TQE.2021.3116540,26891808,"As quantum information processors grow in quantum bit (qubit) count and functionality, the control and measurement system becomes a limiting factor to large-scale extensibility. To tackle this challenge and keep pace with rapidly evolving classical control requirements, full control stack access is essential to system-level optimization. We design a modular field-programmable gate array (FPGA)-based system called QubiC to control and measure a superconducting quantum processing unit. The system includes room temperature electronics hardware, FPGA gateware, and engineering software. A prototype hardware module is assembled from several commercial off-the-shelf evaluation boards and in-house-developed circuit boards. Gateware and software are designed to implement basic qubit control and measurement protocols. System functionality and performance are demonstrated by performing qubit chip characterization, gate optimization, and randomized benchmarking sequences on a superconducting quantum processor operating at the Advanced Quantum Testbed at the Lawrence Berkeley National Laboratory. The single-qubit and two-qubit process fidelities are measured to be 0.9980 ± 0.0001 and 0.948 ± 0.004, respectively, by randomized benchmarking. With fast circuit sequence loading capability, the QubiC performs randomized compiling experiments efficiently and improves the feasibility of executing more complex algorithms."
Akbar2020,Muhammad Azeem Akbar and Ahmed Alsanad and Sajjad Mahmood and Abeer Abdulaziz Alsanad and Abdu Gumaei,A Systematic Study to Improve the Requirements Engineering Process in the Domain of Global Software Development,IEEE Access,8,,2020,10.1109/ACCESS.2020.2979468,21693536,"The software organizations are outsourcing their development activities across the geographical border due to huge business gains. However, the adoption of the global software development (GSD) paradigm is not straightforward; various challenges are associated with it, particularly related to the requirements engineering (RE) process. The objective of this study is to identify the barriers to the RE process faced during GSD. To achieve this, we have conducted a systematic mapping study and questionnaire survey to identify and validate the barriers of the RE process with industry practitioners. A total of 20 barriers were identified and validated with the experts. Moreover, we have performed organization types (client and vendor), organization size (small, medium, and large) and experts' levels (junior, intermediate, and senior) based analysis to provide a clear understanding of the RE barriers in the three different context. Besides, we have also developed a theoretical framework by mapping the investigated barriers into six core knowledge areas of software process improvement. The mapping results indicated that project administration is the most significant knowledge area of investigated barriers. We believe that the findings of this study will provide a framework that assists the GSD practitioners in developing an effective plan and strategies to improve the RE process in the GSD context."
Udeozor2023,Chioma Udeozor and Ryo Toyoda and Fernando Russo Abegão and Jarka Glassey,Digital games in engineering education: systematic review and future trends,European Journal of Engineering Education,48,2,2023,10.1080/03043797.2022.2093168,14695898,"The application of digital games in higher education is on the rise in engineering. With the recent COVID-19 restrictions and the move to virtual learning, the interest in and the need for virtual laboratories and technology-enhanced experiential learning tools like digital games are expected to rise. This paper presents a review of the current practices in digital game-based learning for engineering education. Most importantly, it provides insight into the application of digital game-based learning across diverse engineering disciplines. It also provides researchers and practitioners with insights into relevant journals and conferences, available games, research designs and assessment methods being used in digital game-based learning in the context of engineering. Based on predefined inclusion criteria, a total of 51 articles published within the last decade were analysed in detail. Software engineering education was found to evaluate the educational use of games most extensively. Eighteen empirical studies also reported some learning gains with digital games using different assessment methods. The findings of this review indicate increase in the dissemination of games research and possibly in the use of games for engineering education. This paper closes by highlighting future trends in digital game-based learning for engineering education."
Fischer2021,Lukas Fischer and Lisa Ehrlinger and Verena Geist and Rudolf Ramler and Florian Sobiezky and Werner Zellinger and David Brunner and Mohit Kumar and Bernhard Moser,AI System Engineering—Key Challenges and Lessons Learned †,Machine Learning and Knowledge Extraction,3,1,2021,10.3390/make3010004,25044990,"The main challenges are discussed together with the lessons learned from past and ongoing research along the development cycle of machine learning systems. This will be done by taking into account intrinsic conditions of nowadays deep learning models, data and software quality issues and human-centered artificial intelligence (AI) postulates, including confidentiality and ethical aspects. The analysis outlines a fundamental theory-practice gap which superimposes the challenges of AI system engineering at the level of data quality assurance, model building, software engineering and deployment. The aim of this paper is to pinpoint research topics to explore approaches to address these challenges."
Lewowski2022,Tomasz Lewowski and Lech Madeyski,How far are we from reproducible research on code smell detection? A systematic literature review,Information and Software Technology,144,,2022,10.1016/j.infsof.2021.106783,09505849,"Context: Code smells are symptoms of wrong design decisions or coding shortcuts that may increase defect rate and decrease maintainability. Research on code smells is accelerating, focusing on code smell detection and using code smells as defect predictors. Recent research shows that even between software developers, agreement on what constitutes a code smell is low, but several publications claim the high performance of detection algorithms—which seems counterintuitive, considering that algorithms should be taught on data labeled by developers. Objective: This paper aims to investigate the possible reasons for the inconsistencies between studies in the performance of applied machine learning algorithms compared to developers. It focuses on the reproducibility of existing studies. Methods: A systematic literature review was performed among conference and journal articles published between 1999 and 2020 to assess the state of reproducibility of the research performed in those papers. A quasi-gold standard procedure was used to validate the search. Modeling process descriptions, reproduction scripts, data sets, and techniques used for their creation were analyzed. Results: We obtained data from 46 publications. 22 of them contained a detailed description of the modeling process, 17 included any reproduction data (data set, results, or scripts) and 15 used existing data sets. In most of the publications, analyzed projects were hand-picked by the researchers. Conclusion: Most studies do not include any form of an online reproduction package, although this has started to change recently—8% of analyzed studies published before 2018 included a full reproduction package, compared to 22% in years 2018–2019. Ones that do include a package usually use a research group website or even a personal one. Dedicated archives are still rarely used for data packages. We recommend that researchers include complete reproduction packages for their studies and use well-established research data archives instead of their own websites."
Garousi2022,Vahid Garousi and David Cutting and Michael Felderer,Mining user reviews of COVID contact-tracing apps: An exploratory analysis of nine European apps,Journal of Systems and Software,184,,2022,10.1016/j.jss.2021.111136,01641212,"Context: More than 78 countries have developed COVID contact-tracing apps to limit the spread of coronavirus. However, many experts and scientists cast doubt on the effectiveness of those apps. For each app, a large number of reviews have been entered by end-users in app stores. Objective: Our goal is to gain insights into the user reviews of those apps, and to find out the main problems that users have reported. Our focus is to assess the “software in society” aspects of the apps, based on user reviews. Method: We selected nine European national apps for our analysis and used a commercial app-review analytics tool to extract and mine the user reviews. For all the apps combined, our dataset includes 39,425 user reviews. Results: Results show that users are generally dissatisfied with the nine apps under study, except the Scottish (“Protect Scotland”) app. Some of the major issues that users have complained about are high battery drainage and doubts on whether apps are really working. Conclusion: Our results show that more work is needed by the stakeholders behind the apps (e.g., app developers, decision-makers, public health experts) to improve the public adoption, software quality and public perception of these apps."
Qasim2020,Iqra Qasim and Muhammad Waseem Anwar and Farooque Azam and Hanny Tufail and Wasi Haider Butt and Muhammad Nouman Zafar,A Model-Driven Mobile HMI Framework (MMHF) for Industrial Control Systems,IEEE Access,8,,2020,10.1109/ACCESS.2020.2965259,21693536,"With the advent of software technologies, over a period of time, the Industrial Control Systems (ICSs) have grown exponentially. Whereas, almost all ICSs comprise Human Machine Interfaces (HMIs), which are the key component for monitoring and controlling complex industrial systems. For decades, traditional HMIs with simple User Interfaces (UIs) remained operational to minimize the complexities and resulting operational costs. However, due to the emergence of smartphone technologies, the perception about user interfaces has been transformed significantly and users now demand same sort of experience with industrial HMIs, as well. There are few industrial solutions, like, ICONICS GraphWorX to support the development of mobile HMI screens. However, such proprietary solutions are quite expensive. Furthermore, the underlying development approaches and source codes are not accessible in public domain. On the other hand, the state-of-the-art approaches for the development of native mobile HMI screens are hard to find in the literature. Consequently, there is dire need of a cost-effective, easy to use, open source framework for the development of native mobile HMI screens. In order to achieve this goal, here we propose, a Model-driven Mobile HMI Framework (MMHF). MMHF comprises, a Unified Modeling Language (UML) Profile for Mobile HMI (UMLPMH) for modeling of HMI screens. MMHF also includes, an open source transformation engine and a Model Driven Mobile-based HMI Code Generator (MDMHCG) to automatically transform UMLPMH models into target native mobile HMI implementations. Consequently, MMHF enables simpler way to design the HMI screens using UMLPMH and generates native Mobile HMI Screen implementations automatically using MDMHCG. The empirical evidence of MMHF is demonstrated through three (3) benchmark case studies, which prove that the MMHF is a feasible, cost effective and scalable solution to develop native HMI screens for wide-ranging ICSs."
Oduoza2020,C. F. Oduoza,Framework for sustainable risk management in the manufacturing sector,,51,,2020,10.1016/j.promfg.2020.10.180,23519789,"Risk management is a huge challenge for business managers especially in the manufacturing engineering sector, and if not proactively controlled can lead to under performance and sometimes cessation of activities for some companies. It is common knowledge that poorly managed risks can have an adverse effect on performance while proactive and systematic control of key risk variables in a business environment could generate successful outcomes. The work carried out here has developed a framework for risk management affordable and suitable for use especially by small and medium size enterprises in the manufacturing sector. Using a combination of Bayesian Belief Network (BBN) and Analytical Hierarchical Process (AHP) search algorithms, it was possible to search and identify key risk indicators that could undermine business performance (measured in terms of cost, time, quality and safety) from a system database, and thereby manage (monitor, identify, analyse, reduce, accept or reject their impact) them. The conclusion drawn from the study is that risk management for a manufacturing process can be successfully achieved if risk factors which have a negative impact on project cost, quality of delivery, lead cycle and takt time and health and safety of workers can be identified using BBN and minimised using the framework developed in this study."
Nasiri2020,Samia Nasiri and Yassine Rhazali and Mohammed Lahmer and Noureddine Chenfour,Towards a Generation of Class Diagram from User Stories in Agile Methods,,170,,2020,10.1016/j.procs.2020.03.148,18770509,"Model-Driven Architecture (MDA) is a framework for software development processes that allows an automatic transformation from a business process model to the code model. In MDA there are two transformation kinds: Transformation from the Computation independent model (CIM) to platform-independent model (PIM), transformation from PIM to platform-specific model (PSM). In this paper, we based on CIM to PIM transformation. This transformation is done by developing a platform that generates a class diagram, presented in XMI file, from specifications that are presented in user stories, which are written in natural language (English). We used a natural language processing (NLP) tool named ""Stanford CoreNLP"" for extracting of the object-oriented design elements. Applying our approach to several case studies has given good results."
Huang2020,Chao Tsai Huang and Xuan Wei Chen and Wei Wen Fu,Investigation on the fiber orientation distributions and their influence on the mechanical property of the co-injection molding products,Polymers,12,1,2020,10.3390/polym12010024,20734360,"In recent years, due to the rapid development of industrial lightweight technology, composite materials based on fiber reinforced plastics (FRP) have been widely used in the industry. However, the environmental impact of the FRPs is higher each year. To overcome this impact, co-injection molding could be one of the good solutions. But how to make the suitable control on the skin/core ratio and how to manage the glass fiber orientation features are still significant challenges. In this study, we have applied both computer-aided engineering (CAE) simulation and experimental methods to investigate the fiber feature in a co-injection system. Specifically, the fiber orientation distributions and their influence on the tensile properties for the single-shot and co-injection molding have been discovered. Results show that based on the 60:40 of skin/core ratio and same materials, the tensile properties of the co-injection system, including tensile stress and modulus, are a little weaker than that of the single-shot system. This is due to the overall fiber orientation tensor at flow direction (A11) of the co-injection system being lower than that of the single-shot system. Moreover, to discover and verify the influence of the fiber orientation features, the fiber orientation distributions (FOD) of both the co-injection and single-shot systems have been observed using micro-computerized tomography (μ-CT) technology to scan the internal structures. The scanned images were further utilizing Avizo software to perform image analyses to rebuild the fiber structure. Specifically, the fiber orientation tensor at flow direction (A11) of the co-injection system is about 89% of that of the single-shot system in the testing conditions. This is because the co-injection part has lower tensile properties. Furthermore, the difference of the fiber orientation tensor at flow direction (A11) between the co-injection and the single-shot systems is further verified based on the fiber morphology of the μ-CT scanned image. The observed result is consistent with that of the FOD estimation using μ-CT scan plus image analysis."
Zhang2021,Xiaolu Zhang and Frank Breitinger and Engelbert Luechinger and Stephen O'Shaughnessy,"Android application forensics: A survey of obfuscation, obfuscation detection and deobfuscation techniques and their impact on investigations",Forensic Science International: Digital Investigation,39,,2021,10.1016/j.fsidi.2021.301285,26662817,"Android obfuscation techniques include not only classic code obfuscation techniques that were adapted to Android, but also obfuscation methods that target the Android platform specifically. This work examines the status-quo of Android obfuscation, obfuscation detection and deobfuscation. Specifically, it first summarizes obfuscation approaches that are commonly used by app developers for code optimization, to protect their software against code theft and code tampering but are also frequently misused by malware developers to circumvent anti-malware products. Secondly, the article focuses on obfuscation detection techniques and presents various available tools and current research. Thirdly, deobfuscation (which aims at reinstating the original state before obfuscation) is discussed followed by a brief discussion how this impacts forensic investigation. We conclude that although obfuscation is widely used in Android app development (benign and malicious), available tools and the practices on how to deal with obfuscation are not standardized, and so are inherently lacking from a forensic standpoint."
Inamov2021,Aziz Inamov and Ilkhom Ruziev and Saatbay Nurjanov,Interpolyation in smoothing tin model of the earth,,1030,1,2021,10.1088/1757-899X/1030/1/012112,1757899X,"This article deals with engineering (strategic site construction, buildings and structures, material cultural heritage sites), project (underground and overhead pipelines, railways, roads, power lines, fields and minerals, irrigation networks, hydraulic structures). Improved interpolation techniques for determining the most commonly used measuring heights in the process of land surveying, solving engineering problems on topographic maps and surveying land cadastral works (agriculture, fisheries) and the state cadaster illuminated. In addition, design of linear, dotted and field-themed objects, the study of absolute heights for volume measurements, the analysis of survey results in software for the geo-information system, the mapping of Earth's dimensions, the determination of unknown altitude and land grinding. Optimized using an improved formula for implementation. In addition, there is a section with a relief section height within each relief interval, and traditional methods used to determine intermediate values have caused discomfort to specialists or users of this formula. In this paper, it is widely used that the formula developed can be used to develop a high-precision study of the absolute value of an unknown height and to develop a sequence algorithm for the implementation of modulation systems in the geo-information system family software."
Zhang2020,Hualei Zhang and Min Tu and Hua Cheng and Yongzhi Tang,Breaking mechanism and control technology of sandstone straight roof in thin bedrock stope,International Journal of Mining Science and Technology,30,2,2020,10.1016/j.ijmst.2018.10.006,20952686,"The key problem to be solved urgently is how to avoid the occurrence of support break-off and water inrush in the stoping of sandstone straight roof under the action of load transfer in unconsolidated aquifer. For this reason, taking the thin bedrock 1602 (3) working face of Huainan (the middle part of Anhui Province) Panyi Coal Mine as the engineering background, this study establishes the stope mining model by using the discrete element UDEC software and the mathematics mechanical model of the support load, and analyzes the reason of support crushing and decides to re-mining the working face by using the compulsive roof caving method. It is concluded that when the working face of sandstone straight roof is broken, the “voussoir beam” structure cannot be formed and acts on the support in the form of cantilever beam, but only when it falls to the high key stratum can the “voussoir beam” structure be formed and at this point, at this time, the bracket bears the weight of the rock layer in the range from the fractured sandstone layer to the lower critical layer. The working resistance of the support increases with the increase of the thickness and the breaking length of straight sandstone roof. When the breaking length of the roof reaches a certain extreme value, the support crushing accidents will occur. Managing roof with compulsive roof caving method can reduce the intensity of rock pressure in the stope, and the working face can be safely stoped, which provides a certain reference for similar conditions."
Shah2022,Imtiaz Ali Shah and Sardar Bilal and Ali Akgül and Mohamed Omri and Jamel Bouslimi and Noor Zeb Khan,Significance of cold cylinder in heat control in power law fluid enclosed in isosceles triangular cavity generated by natural convection: A computational approach,Alexandria Engineering Journal,61,9,2022,10.1016/j.aej.2021.12.071,11100168,"The selection of appropriate geometrical characteristics of enclosure has salient influence on performance of different thermal engineering processes and devices like microelectronics, heat exchangers, power engines, boilers, solar collectors, nuclear reactors and so forth. Specifically, the triangular cavity of different aspect ratios are used to obtain multi-objective optimization and gaining excellence in thermal performance of micro channels. Subsequently, the installation of cold cylinder in a triangular enclosure are extensively used to remove high energy dissipation in micro heat sinks and heat exchangers. So, the purpose of current effort is to probe thermal characteristics of power-law liquid describing features of shear thinning and thickening materials containing applications in lubrication and polymer industry. Heat transfer is generated by the consideration of natural convection process generated due to consideration of cold walls and cylinder with provision of non-uniform heating at base wall. No-slip velocity conditions are supposed at all walls of isosceles triangle cavity. Solution of attained differential system is simulated by capitalizing finite element based COMSOL Multiphysics software (Version 5.6). Domain discretization by distributing into triangular and rectangular elements is conceded and equations at element levels are discretized by executing weak formulation. The validation of adopted numerical procedure is established by making agreement with formerly available works in both statistical and graphical approaches. Streamlines and isotherms are plotted and discussed against various parametric regimes. This study reveals noticeable influence of involved physical parameters on average and local heat flux coefficients, kinetic energy and cutlines against involved physical parameters are also evaluated. It is inferred thorough the analysis that with uplift in Rayleigh number (Ra) produces enrichment in kinetic energy and local heat transfer coefficients whereas reverse pattern is depicted against power-law index.(n)."
Nicolaidou2020,Evangelia Nicolaidou and Thomas L. Hill and Simon A. Neild,Indirect reduced-order modelling: Using nonlinear manifolds to conserve kinetic energy: Kinetic energy-conserving reduced models,"Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences",476,2243,2020,10.1098/rspa.2020.0589,14712946,"Nonlinear dynamic analysis of complex engineering structures modelled using commercial finite element (FE) software is computationally expensive. Indirect reduced-order modelling strategies alleviate this cost by constructing low-dimensional models using a static solution dataset from the FE model. The applicability of such methods is typically limited to structures in which (a) the main source of nonlinearity is the quasi-static coupling between transverse and in-plane modes (i.e. membrane stretching); and (b) the amount of in-plane displacement is limited. We show that the second requirement arises from the fact that, in existing methods, in-plane kinetic energy is assumed to be negligible. For structures such as thin plates and slender beams with fixed/pinned boundary conditions, this is often reasonable, but in structures with free boundary conditions (e.g. cantilever beams), this assumption is violated. Here, we exploit the concept of nonlinear manifolds to show how the in-plane kinetic energy can be accounted for in the reduced dynamics, without requiring any additional information from the FE model. This new insight enables indirect reduction methods to be applied to a far wider range of structures while maintaining accuracy to higher deflection amplitudes. The accuracy of the proposed method is validated using an FE model of a cantilever beam."
Li2021,Jianjia Li and Zhifa Wang and Xiangyu Huang and Zhaodan Wang and Zehao Chen and Runting Wang and Zhao Chen and Wei Liu and Buling Wu and Fuchun Fang and Wei Qiu,Dynamic proteomic profiling of human periodontal ligament stem cells during osteogenic differentiation,Stem Cell Research and Therapy,12,1,2021,10.1186/s13287-020-02123-6,17576512,"Background: Human periodontal ligament stem cells (hPDLSCs) are ideal seed cells for periodontal regeneration. A greater understanding of the dynamic protein profiles during osteogenic differentiation contributed to the improvement of periodontal regeneration tissue engineering. Methods: Tandem Mass Tag quantitative proteomics was utilized to reveal the temporal protein expression pattern during osteogenic differentiation of hPDLSCs on days 0, 3, 7 and 14. Differentially expressed proteins (DEPs) were clustered and functional annotated by Gene Ontology (GO) terms. Pathway enrichment analysis was performed based on the Kyoto Encyclopedia of Genes and Genomes database, followed by the predicted activation using Ingenuity Pathway Analysis software. Interaction networks of redox-sensitive signalling pathways and oxidative phosphorylation (OXPHOS) were conducted and the hub protein SOD2 was validated with western blotting. Results: A total of 1024 DEPs were identified and clustered in 5 distinctive clusters representing dynamic tendencies. The GO enrichment results indicated that proteins with different tendencies show different functions. Pathway enrichment analysis found that OXPHOS was significantly involved, which further predicted continuous activation. Redox-sensitive signalling pathways with dynamic activation status showed associations with OXPHOS to various degrees, especially the sirtuin signalling pathway. SOD2, an important component of the sirtuin pathway, displays a persistent increase during osteogenesis. Data are available via ProteomeXchange with identifier PXD020908. Conclusion: This is the first in-depth dynamic proteomic analysis of osteogenic differentiation of hPDLSCs. It demonstrated a dynamic regulatory mechanism of hPDLSC osteogenesis and might provide a new perspective for research on periodontal regeneration. Graphical abstract: [Figure not available: see fulltext.]"
Zainal2021,Nurul Amira Zainal and Roslinda Nazar and Kohilavani Naganthran and Ioan Pop,Unsteady MHD mixed convection flow in hybrid nanofluid at three-dimensional stagnation point,Mathematics,9,5,2021,10.3390/math9050549,22277390,"There has been significant interest in exploring a stagnation point flow due to its numerous potential uses in engineering applications such as cooling of nuclear reactors. Hence, this study proposed a numerical analysis on the unsteady magnetohydrodynamic (MHD) mixed convection at three-dimensional stagnation point flow in Al2O3–Cu/H2O hybrid nanofluid over a permeable sheet. The ordinary differential equations are accomplished by simplifying the governing partial differential equations through suitable similarity transformation. The numerical computation is established by the MATLAB system software using the bvp4c technique. The bvp4c procedure is excellent in providing more than one solution once sufficient predictions are visible. The influence of certain functioning parameters is inspected, and notable results exposed that the rate of heat transfer is exaggerated along with the skin friction coefficient while the suction/injection and magnetic parameters are intensified. The results also signified that the rise in the volume fraction of the nanoparticle and the decline of the unsteadiness parameter demonstrates a downward attribution towards the heat transfer performance and skin friction coefficient. Conclusively, the observations are confirmed to have multiple solutions, which eventually contribute to an investigation of the analysis of the solution stability, thereby justifying the viability of the first solution."
Arestova2021,Anna Arestova and Maximilian Martin and Kai Steffen Jens Hielscher and Reinhard German,A service-oriented real-time communication scheme for autosar adaptive using opc UA and time-sensitive networking,Sensors,21,7,2021,10.3390/s21072337,14248220,"The transportation industry is facing major challenges that come along with innovative trends like autonomous driving. Due to the growing amount of network participants, smart sensors, and mixed-critical data, scalability and interoperability have become key factors of cost-efficient vehicle engineering. One solution to overcome these challenges is the AUTOSAR Adaptive software platform. Its service-oriented communication methodology allows a standardized data exchange that is not bound to a specific middleware protocol. OPC UA is a communication standard that is well-established in modern industrial automation. In addition to its Client–Server communication pattern, the newly released Publish–Subscribe (PubSub) architecture promotes scalability. PubSub is designed to work in conjunction with Time-Sensitive Networking (TSN), a collection of standards that add real-time aspects to standard Ethernet networks. TSN allows services with different requirements to share a single physical network. In this paper, we specify an integration approach of AUTOSAR Adaptive, OPC UA, and TSN. It combines the benefits of these three technologies to provide deterministic high-speed communication. Our main contribution is the architecture for the binding between Adaptive Platform and OPC UA. With a prototypical implementation, we prove that a combination of OPC UA Client–Server and PubSub qualifies as a middleware solution for service-oriented communication in AUTOSAR."
Ma2020,Ke Ma and Zhiyuan Mao and Dong He and Yichuan Zhang,Design a network architectural teaching system by auto CAD,Computer-Aided Design and Applications,17,Special Issue 2,2020,10.14733/cadaps.2020.S2.1-10,16864360,"As the scale, shape and function of engineering construction projects become more and more complex, the design of building systems has become more and more complex. In order to help students better acquire course resource information, effectively demonstrate the teaching process, and help students understand the curriculum and learn effectively, this paper designs an Auto CAD network architecture design teaching system. We analyze and compare the teaching model based on the network architecture design teaching system, and introduce the cluster analysis to the course resource management. An Auto CAD model was constructed through collaborative design among professionals based on Auto CAD technology. We analyze and compare the characteristics of traditional 2D design and inter-professional collaborative design based on Auto CAD technology, and select appropriate 3D simulation software for inter-professional collaborative design. The teaching of Auto CAD architectural design course proposed in this paper has certain innovation and practicability, and puts forward some constructive suggestions on the solution of the problem."
Wang2020,Jiaqi Wang and Qing Wang and Sen Lin and Yan Han and Shukai Cheng and Ning Wang,Relationship between the shear strength and microscopic pore parameters of saline soil with different freeze-thaw cycles and salinities,Symmetry,12,10,2020,10.3390/sym12101709,20738994,"Saline soil is a widely distributed special soil with poor engineering properties. In seasonally frozen regions, the poor properties of saline soil will cause many types of engineering damage such as road boiling, melt sinking, and subgrade instability. These engineering failures are closely related to the shear strength of saline soil. However, there are relatively few studies on saline soil in cold regions. The strength of the soil is always determined by its microstructure; therefore, the study aims to investigate the relationship between the shear strength and microscopic pore structure of saline soil with different freeze–thaw cycles and salinities. The shear strength characteristics of saline soil with different salinities subjected to different freeze–thaw cycles were obtained by triaxial tests. In addition, the microstructure of the soil samples was investigated by scanning electron microscopy (SEM) tests, and the microscopic pore parameters of the soil samples, including porosity (N), average pore diameter (D), average shape coefficient (K), surface fluctuation fractal dimension (F), and orienting probability entropy (Hm ), were obtained by image processing software quantitatively. Based on the experimental results, the influence of freeze–thaw cycles and salinity on the shear strength characteristics and microstructure of the soil samples were analyzed. Besides that, in order to effectively eliminate the collinearity between independent variables and obtain a stable and reasonable regression model, principal component regression (PCR) analysis was adopted to establish the relationship between the microscopic pore parameters and the failure strength of the soil samples. The fitting results demonstrated that the failure strength of saline soil is mainly related to the size and direction of the pores in the soil, and it has little correlation with pore shape. The failure strength of the soil was negatively correlated with the average pore diameter (D) and porosity (N), and it was positively correlated with the orienting probability entropy of the pores (Hm ). This study may provide a quantitative basis for explaining the variation mechanism of the mechanical properties of saline soil from a microscopic perspective and provide references for the symmetry between the changes of the macroscopic properties and microscopic pore structure of the saline soil in cold regions."
ZaragozaFuster2020,María Teresa Zaragoza Fuster and José Alberto García Avilés,The role of innovation labs in advancing the relevance of public service media: The cases of BBC news labs and RTVE LAB,Communication and Society,33,1,2020,10.15581/003.33.1.45-61,23867876,"As part of their social function, Public Service Media (PSM) organizations are devoted to innovation. In this context, a number of European PSM outlets have created laboratories based on the implementation of products, services and formats for multiplatform audiences. Our research focused on two case studies: a) BBC News Labs (UK) and b) RTVE Lab (Spain). We applied both quantitative and qualitative methodologies. We used content analysis techniques to examine products generated by the two laboratories between 2013 and 2017, quantifying and classifying each one according to categories related to the type and nature of the innovation. We carried out ethnographic research using participant observation over five days in each laboratory, both in London and Madrid. We also conducted open-ended interviews with five professionals at the BBC and three at RTVE, as well as with the head of EBU’s Media Strategy & Development department. Our results show that BBC News Labs implements a global innovation strategy through the design and engineering of products that facilitate journalists’ work. Projects include speech-to-text software, prototypes for voice recognition and text analysis, chatbots and object-based media. On the other hand, RTVE Lab focuses on designing and producing interactive formats, multimedia narratives and social media content, with an emphasis on experimentation. The findings reveal the main barriers and incentives to innovation within each lab and show how the transfer of innovation could increase the diversity, universality and quality of their overall output."
Hamidah2021,Ida Hamidah and Roer Eka Pawinanto and Budi Mulyanti and Jumril Yunas,A bibliometric analysis of micro electro mechanical system energy harvester research,Heliyon,7,3,2021,10.1016/j.heliyon.2021.e06406,24058440,"Micro Electro Mechanical System (MEMS) energy harvester's research interests have been increasing rapidly, indicating that the topic has given significant contributions to the sustainable development of energy alternatives. Although many research activities have been conducted and reported since several years ago, only limited efforts have been made to analyze the research's impact in this area. In this paper, we report a bibliometric analysis on the research progress in MEMS for energy harvester. VOSviewer software is used to support the analyst that includes the distributions of the publication journals, authors, affiliations and the highly cited papers reporting the progress as well as the frequency of keywords and their relationships found in the search engine. The analysis is mainly aimed to identify the research map based on publication reports. 1772 papers were initially identified and summarized based on the analysis on three focused mainstream research topics in MEMS for alternative energy, such as MEMS energy harvester, power harvesting and energy scavenging, other term analogies to MEMS such as micromachines and microsystem were included in the analysis parameter. As a result, it is found that the study on the MEMS energy harvester is mostly categorized in the engineering field, while China has been conducting the most projects. The Journal MEMS and Journal of Micromechanics and Microengineering have been the most journals publishing reports on MEMS energy harvester's research progress. Based on these analyses, some potential issues in future MEMS energy harvester research have been identified, including the contributions of new materials, the MEMS new structure's involvement, and the optimization of the vibration concepts and principles of MEMS energy harvester. These analyses would give an overview on the progress of the development and improvement in MEMS energy harvester and give a proper guideline for future MEMS research in the energy field."
Al-Hammadi2020,Mohammed A. Al-Hammadi and Wei Tian,Challenges and Barriers of Building Information Modeling Adoption in the Saudi Arabian Construction Industry,The Open Construction & Building Technology Journal,14,1,2020,10.2174/1874836802014010098,1874-8368,"© 2020 Al-Hammadi and Tian. Background: The Building Information Modeling (BIM) revolution can provide a solution for problems in the Saudi Arabian construction industry and improve its outcomes. Though this technology is increasingly and rapidly adopted in advanced countries, developing countries such as Saudi Arabia and the Gulf Cooperation Council countries (GCC) are still in the early stages of BIM adoption. Objective: This study investigates the current state of BIM technology adoption by exploring and analyzing the critical challenges and barriers to BIM technology utilization in the construction sectors. Methods: The quantitative approach is adopted via a survey questionnaire distributed to participants in the field of construction projects in the Architecture, Engineering, and Construction (AEC) industry. A total of 228 questionnaires are collected and analyzed using the statistical packaging for social science software. Results: Findings show that several significant barriers negatively affect the utilization of BIM. The major barriers to BIM adoption are related to the lack of demand, lack of experts, and poor awareness of BIM benefits, which have relative importance indexes of (RII = 89.910), (RII = 88.475), and (RII = 87.130), respectively. Meanwhile, unspecified data responsibilities, difficulty of learning BIM, lack of data sharing, and sufficient current technology constitute the lowest-ranking barriers with their relative importance indexes of (RII = 71.704), (RII = 70.807), (RII = 66.413), and (RII = 65.874), respectively. Conclusion: The findings of this study are highly significant and can become more helpful and interesting if further research can measure the methodologies to implement BIM technology in the Kingdom of Saudi Arabia."
Benabdelaziz2020,Kawtar Benabdelaziz and Badreddine Lebrouhi and Anas Maftah and Mohammed Maaroufi,Novel external cooling solution for electric vehicle battery pack,,6,,2020,10.1016/j.egyr.2019.10.043,23524847,"The future use of electric vehicles in the southern regions of the world could face several problematics related to high temperature, mostly when charging at high power. This paper proposes a new external cooling solution for cooling EV batteries pack at higher temperature conditions especially for those without effective cooling system embedded. A 3D-thermal model has been developed in order to investigate and to analyse the temperature distribution over the battery pack, the ANSYS FLUENT software has been used to solve the model development. Furthermore, different external cooling solutions have been proposed by using the engineering process design to study their impact on the internal temperature of the battery pack."
Chen2021,Liwen Chen and Jingyi Li and Yuhong Zhao and Muxi Li and Limin Li and Lei Chen and Hua Hou,Numerical simulation and optimization of indirect squeeze casting process,Engineered Science,13,,2021,10.30919/es8d1157,25769898,"In order to optimize the process of indirect squeeze casting, computer aided engineering (CAE) technology was used to predict casting defects. Taking an adapter ring casting as an example, its 3D-model was established and its casting process was designed. The results indicate that, for indirect squeeze casting, when the liquid metal injects to mold cavity by casting system, temperature of liquid metal has dropped and started to solidify. At the same time, the runner has begun to solidify. If the cross-section of runner is too small, the runner pressure has not passed the casting to produce the effect of feeding, therefore this problem will lead to casting defects such as shrinkage cavity. As a result, full consideration should be given to the feeding effect of the runner in mold design. Simulation software can be used to simulate the temperature field of casting filling and solidification to design the runner reasonably."
Thompson2020,Matthew W. Thompson and Justin B. Gilmer and Ray A. Matsumoto and Co D. Quach and Parashara Shamaprasad and Alexander H. Yang and Christopher R. Iacovella and Clare McCabe and Peter T. Cummings,"Towards molecular simulations that are transparent, reproducible, usable by others, and extensible (TRUE)",Molecular Physics,118,9-10,2020,10.1080/00268976.2020.1742938,13623028,"Systems composed of soft matter (e.g. liquids, polymers, foams, gels, colloids, and most biological materials) are ubiquitous in science and engineering, but molecular simulations of such systems pose particular computational challenges, requiring time and/or ensemble-averaged data to be collected over long simulation trajectories for property evaluation. Performing a molecular simulation of a soft matter system involves multiple steps, which have traditionally been performed by researchers in a ‘bespoke’ fashion, resulting in many published soft matter simulations not being reproducible based on the information provided in the publications. To address the issue of reproducibility and to provide tools for computational screening, we have been developing the open-source Molecular Simulation and Design Framework (MoSDeF) software suite. In this paper, we propose a set of principles to create Transparent, Reproducible, Usable by others, and Extensible (TRUE) molecular simulations. MoSDeF facilitates the publication and dissemination of TRUE simulations by automating many of the critical steps in molecular simulation, thus enhancing their reproducibilitya. We provide several examples of TRUE molecular simulations: All of the steps involved in creating, running and extracting properties from the simulations are distributed on open-source platforms (within MoSDeF and on GitHub), thus meeting the definition of TRUE simulations."
Wang2020,Qing Wang and Xiaojie Yang and Guangfei Wang and Leilei Wan and Shiwei Wang and Xiaoyong Niu and Jiannan Wu and Jinsong Pan,Osteogenic growth peptide-loaded 3D-printed PCL scaffolds for the promotion of osteogenesis through the ERK pathway,Materials and Design,193,,2020,10.1016/j.matdes.2020.108811,18734197,"This study aims to improve the osteogenic differentiation in vitro and bone formation in vivo through polycaprolactone (PCL) scaffolds modified with the osteogenic growth peptide (OGP). Three-dimensional (3D)-printed PCL scaffolds were designed with computer-aided design (CAD) software and fabricated by 3D layer-by-layer fused deposition. After the scaffold surfaces were modified by surface amination, the OGP was successfully loaded onto the scaffolds using a electrostatic self-assembly method. The effects of these scaffolds on the biocompatibility and osteogenic differentiation of rat bone marrow-derived stem cells (BMSCs) were studied in vitro. The modification did not influence the adhesion or proliferation, but had a notable osteogenic effect in vitro. This may be attributed to the activation of the mitogen-activated protein kinase/extracellular regulated protein kinase (MAPK/ERK) signalling pathway. Based on the results obtained after implantation into a rat cranial defect model, the PCL/OGP scaffolds notably promoted bone formation and accelerated mineralisation. The innovative modification method introduced in this work, will improve the application of PCL scaffolds in tissue engineering."
Souayeh2022,Basma Souayeh and Katta Ramesh and Najib Hdhiri and Essam Yasin and Mir Waqas Alam and Kawthar Alfares and Amina Yasin,Heat Transfer Attributes of Gold–Silver–Blood Hybrid Nanomaterial Flow in an EMHD Peristaltic Channel with Activation Energy,Nanomaterials,12,10,2022,10.3390/nano12101615,20794991,"The heat enhancement in hybrid nanofluid flow through the peristaltic mechanism has received great attention due to its occurrence in many engineering and biomedical systems, such as flow through canals, the cavity flow model and biomedicine. Therefore, the aim of the current study was to discuss the hybrid nanofluid flow in a symmetric peristaltic channel with diverse effects, such as electromagnetohydrodynamics (EMHD), activation energy, gyrotactic microorganisms and solar radiation. The equations governing this motion were simplified under the approximations of a low Reynolds number (LRN), a long wavelength (LWL) and Debye–Hückel linearization (DHL). The numerical solutions for the non-dimensional system of equations were tackled using the com-putational software Mathematica. The influences of diverse physical parameters on the flow and thermal characteristics were computed through pictorial interpretations. It was concluded from the results that the thermophoresis parameter and Grashof number increased the hybrid nanofluid velocity near the right wall. The nanoparticle temperature decreased with the radiation parameter and Schmidt number. The activation energy and radiation enhanced the nanoparticle volume fraction, and motile microorganisms decreased with an increase in the Peclet number and Schmidt number. The applications of the current investigation include chyme flow in the gastrointestinal tract, the control of blood flow during surgery by altering the magnetic field and novel drug delivery systems in pharmacological engineering."
Wang2022,Xiaofeng Wang and Xiao Guang Yue and Mohammed K.A. Kaabar and Arzu Akbulut and Melike Kaplan,A unique computational investigation of the exact traveling wave solutions for the fractional-order Kaup-Boussinesq and generalized Hirota Satsuma coupled KdV systems arising from water waves and interaction of long waves,Journal of Ocean Engineering and Science,,,2022,10.1016/j.joes.2022.03.012,24680133,"A novel technique, named auxiliary equation method, is applied in this research work for obtaining new traveling wave solutions for two interesting proposed systems: the Kaup-Boussinesq system and generalized Hirota-Satsuma coupled KdV system with beta time fractional derivative. Our solutions were obtained using MAPLE software. This technique shows a great potential to be applied in solving various nonlinear fractional differential equations arising from mathematical physics and ocean engineering. Since a standard equation has not been used as an auxiliary equation for this technique, different and novel solutions are obtained via this technique."
Ma2020,Zixiao Ma and Zhaoyu Wang and Yishen Wang and Ruisheng Diao and Di Shi,Mathematical representation of WECC composite load model,Journal of Modern Power Systems and Clean Energy,8,5,2020,10.35833/MPCE.2019.000296,21965420,"Composite load model of Western Electricity Coordinating Council (WECC) is a newly developed load model that has drawn great interest from the industry. To analyze its dynamic characteristics with both mathematical and engineering rigors, a detailed mathematical model is needed. Although composite load model of WECC is available in commercial software as a module and its detailed block diagrams can be found in several public reports, there is no complete mathematical representation of the full model in literature. This paper addresses a challenging problem of deriving detailed mathematical representation of composite load model of WECC from its block diagrams. In particular, we have derived the mathematical representation of the new DER_A model. The developed mathematical model is verified using both MATLAB and PSS/E to show its effectiveness in representing composite load model of WECC. The derived mathematical representation serves as an important foundation for parameter identification, order reduction and other dynamic analysis."
Klare2021,Heiko Klare and Max E. Kramer and Michael Langhammer and Dominik Werle and Erik Burger and Ralf Reussner,Enabling consistency in view-based system development — The VITRUVIUS approach,Journal of Systems and Software,171,,2021,10.1016/j.jss.2020.110815,01641212,"During the development of large software-intensive systems, developers use several modeling languages and tools to describe a system from different viewpoints. Model-driven and view-based technologies have made it easier to define domain-specific languages and transformations. Nevertheless, using several languages leads to fragmentation of information, to redundancies in the system description, and eventually to inconsistencies. Inconsistencies have negative impacts on the system's quality and are costly to fix. Often, there is no support for consistency management across multiple languages. Using a single language is no practicable solution either, as it is overly complex to define, use, and evolve such a language. View-based development is a suitable approach to deal with complex systems, and is widely used in other engineering disciplines. Still, we need to cope with the problems of fragmentation and consistency. In this paper, we present the VITRUVIUS approach for consistency in view-based modeling. We describe the approach by formalizing the notion of consistency, presenting languages for consistency preservation, and defining a model-driven development process. Furthermore, we show how existing models can be integrated. We have evaluated our approach at two case studies from component-based and embedded automotive software development, using our prototypical implementation based on the Eclipse Modeling Framework."
Meah2020,Kala Meah and Donald Hake and Stephen Drew Wilkerson,A Multidisciplinary Capstone Design Project to Satisfy ABET Student Outcomes,Education Research International,2020,,2020,10.1155/2020/9563782,20904010,"This paper presents a multidisciplinary open-ended capstone design project where students designed, built, and test drove a Formula Society of Automatic Engineers (FSAE) electric vehicle. The capstone team included students from computer, electrical, and mechanical engineering programs. Each student worked in on a subteam, namely, mechanical design, drivetrain, supervisory control and data acquisition, and battery management system. A thorough description of each subsystem is provided herein. Software architecture, system integration, and field test results are also reviewed. Team organization, faculty and industry involvement, and assessment of student outcomes are provided. This paper details the approach of building a bridge between academia and engineering practices. This paper also documents a process where undergraduate students research and master multiple technology areas and then apply them to the project's focus. ABET student outcomes 1-7 were used to design and assess the course. Peer-to-peer rating and ranking are presented as an assessment tool for the multidisciplinary nature of the project."
Soni2020,Anuja Soni and Anand Saxena and Parul Bajaj,A methodological approach for mining the user requirements using apriori algorithm,Journal of Cases on Information Technology,22,4,2020,10.4018/JCIT.2020100101,15487725,"Users of enterprise software are multiple, and their requirements are diverse. Often their specifications are masked by mundane details and at times are vague too. Acknowledging these complexities in requirements engineering, the paper proposes a multistage methodological approach based on Apriori algorithm, a data mining technique. It extracts useful information from the given data on the criteria of mutual association and sufficient frequency. The user requirements captured through interviews and brainstorming are pre-processed for eliminating unnecessary stop words and developing a uniform structure of small stories. Mutual association and occurrence of the requirements are represented through association rules and rule metrics, for example, 'Lift', 'Support', and 'Confidence'. The requirements having strong and moderate association are placed in 'Top Priority List'; those with nominal, weak, or nil association are placed in 'Low Priority List'. Gap analysis is employed to validate the defined requirements with respect to stakeholders' expectations. The complete and correct lists of requirements significantly influence the client satisfaction, software development process, and its eventual success."
Saad2021,Mustafa Saad and Al Hussein Amhedb and Mohammed Al Sharqawi,Real time DC motor position control using PID controller in LabVIEW,Journal of Robotics and Control (JRC),2,5,2021,10.18196/jrc.25104,27155072,"Direct current (DC) motors are the most used motors in control engineering application due to their simplicity of construction, easy to control and excellent performance. These motors should be well controlled to perform the required task. The functional application of DC motor is focused on this research by using LabVIEW for position control system. A closed loop real-time control system with an added 298 encoder coupled to the motor shaft is used in this control system to provide feedback position signals to the Proportional Integral Derivative (PID) controller. The PID controls the position of the DC motor at the desired position with a minimum error. The PID controller was implemented in LabVIEW software which sends the control signal to the real time DC motor through the Arduino board. In addition, LabVIEW software was developed to show the output response of motor position versus time to easily observe the performance of the system. The PID controller gains were obtained based on trial and error method. The system under these controller parameters has been tested at different positions of tracking signal and for disturbance rejection. Finally, the results showed that the designed controller had good performance characteristics where the desired position of the motor was maintained."
Shapiro2020,Marc L. Shapiro and James A. Norris and Jed C. Wilbur and Douglas S. Brungart and Odile H. Clavier,TabSINT: open-source mobile software for distributed studies of hearing,International Journal of Audiology,59,sup1,2020,10.1080/14992027.2019.1698776,17088186,"Objective: The recent emphasis on outcomes-based medical research has motivated a need for technology that allows researchers and clinicians to reach a larger and more diverse subject population for recruitment and testing. Design: This article reports on open-source mobile software (TabSINT) that enables researchers to administer customised hearing tests and questionnaires on tablets located across multiple sites. Researchers create and modify test protocols using text-based templates and deploy it to the tablets via a cloud-based repository or USB-computer connection. Results are exported locally to the tablet SD card and can also be automatically posted to a cloud-based database. Results: Between 2014 and 2019, TabSINT collected 25,000+ test results using more than 200+ unique test protocols for researchers located worldwide. Conclusions:TabSINT is a powerful software system with the potential to greatly enhance research across multiple disciplines by enabling access to subject cohorts in remote and disparate locations. Released open-source, this software is available to researchers across the world to use and adapt to their specific needs. Researchers with engineering resources can contribute to the repository to extend the capability and robustness of this software."
Reis2022,David Reis and Bruno Piedade and Filipe F. Correia and Joao Pedro Dias and Ademar Aguiar,Developing Docker and Docker-Compose Specifications: A Developers' Survey,IEEE Access,10,,2022,10.1109/ACCESS.2021.3137671,21693536,"Cloud computing and Infrastructure-as-Code (IaC), supported by technologies such as Docker, have shaped how many software systems are built and deployed. Previous research has identified typical issues for some types of IaC specification but not why they come to be, or they have delved into collaboration aspects but not into technical ones. This work aims to characterize the activities around two particular kinds of IaC specification - Dockerfiles and docker-compose.yml files. We seek to know how they can be better supported and therefore study also what approaches and tools practitioners employ. We used an online questionnaire to gather data. The first part of the study reached 68 graduate students from a study program on informatics engineering, and the second one 120 professional software developers. The results show that most of the activities of the process of developing a Dockerfile are perceived as time-consuming, especially when the respondents are beginners with this technology. We also found that solving issues using trial-and-error approaches is very common and that many developers do not use ancillary tools to support the development of Dockerfiles and docker-compose.yml files."
Shi2023,Kaihang Shi and Edward R. Smith and Erik E. Santiso and Keith E. Gubbins,"A perspective on the microscopic pressure (stress) tensor: History, current understanding, and future challenges",Journal of Chemical Physics,158,4,2023,10.1063/5.0132487,10897690,"The pressure tensor (equivalent to the negative stress tensor) at both microscopic and macroscopic levels is fundamental to many aspects of engineering and science, including fluid dynamics, solid mechanics, biophysics, and thermodynamics. In this Perspective, we review methods to calculate the microscopic pressure tensor. Connections between different pressure forms for equilibrium and nonequilibrium systems are established. We also point out several challenges in the field, including the historical controversies over the definition of the microscopic pressure tensor; the difficulties with many-body and long-range potentials; the insufficiency of software and computational tools; and the lack of experimental routes to probe the pressure tensor at the nanoscale. Possible future directions are suggested."
Malavolta2020,Ivano Malavolta and Eoin Martino Grua and Cheng Yu Lam and Randy De Vries and Franky Tan and Eric Zielinski and Michael Peters and Luuk Kaandorp,A Framework for the Automatic Execution of Measurement-based Experiments on Android Devices,,,,2020,10.1145/3417113.3422184,,"Conducting measurement-based experiments is fundamental for assessing the quality of Android apps in terms of, e.g., energy consumption, CPU, and memory usage. However, orchestrating such experiments is not trivial as it requires large boilerplate code, careful setup of measurement tools, and the adoption of various empirical best practices scattered across the literature. All together, those factors are slowing down the scientific advancement and harming experiments' replicability in the mobile software engineering area. In this paper we present Android Runner (AR), a framework for automatically executing measurement-based experiments on native and web apps running on Android devices. In AR, an experiment is defined once in a descriptive fashion, and then its execution is fully automatic, customizable, and replicable. AR is implemented in Python and it can be extended with third-party profilers. AR has been used in more than 25 scientific studies primarily targeting performance and energy efficiency."
Hocky2022,Glen M. Hocky and Andrew D. White,Natural language processing models that automate programming will transform chemistry research and teaching,Digital Discovery,1,2,2022,10.1039/D1DD00009H,2635098X,"Natural language processing models have emerged that can generate useable software and automate a number of programming tasks with high fidelity. These tools have yet to have an impact on the chemistry community. Yet, our initial testing demonstrates that this form of artificial intelligence is poised to transform chemistry and chemical engineering research. Here, we review developments that brought us to this point, examine applications in chemistry, and give our perspective on how this may fundamentally alter research and teaching."
Wang2021,Pengwen Wang and Jing Yang and Yanan Hu and Jiaofei Huo and Xiaoyang Feng,Innovative design of a helmet based on reverse engineering and 3D printing,Alexandria Engineering Journal,60,3,2021,10.1016/j.aej.2021.02.006,11100168,"Based on reverse engineering, product innovation design and 3D-printing technology, a technical route for the rapid design and development of helmet products has been constructed, and optimize the product production process, reduce the product development cycle, improve production efficiency, and complete the personalized design of helmets to improve the comfort of personnel wearing. The point cloud data of the physical model were collected by the 3D scanner, and imported to the reverse processing software (Geomagic Studio) for data processing, surface reconstruction and surface fitting. In this way, the section curve of the repaired head surface was extracted. Then, the extracted information was sent to the forward design software (SolidWorks) through the parameter exchange command for the forward design of the helmet, and the 3D model was subsequently printed. Effectively solve the traditional design method for complex surface modelling design and manufacturing, forward software measurement deviation, modelling design process is complex and tedious, time-consuming and labor-intensive, cannot well meet the product design, processing and manufacturing process is relatively cumbersome, long production cycle and other design defects, speed up enterprise product development, shorten the cycle and reduce costs. Furthermore, the adoption of 3D printing technology to output the designed 3D digital model effectively shortens the product development cycle."
Krueger2020,Ryan Krueger and Yu Huang and Xinyu Liu and Tyler Santander and Westley Weimer and Kevin Leach,Neurological divide: An fmri study of prose and codewriting,,,,2020,10.1145/3377811.3380348,02705257,"Software engineering involves writing new code or editing existing code. Recent efforts have investigated the neural processes associated with reading and comprehending code - however, we lack a thorough understanding of the human cognitive processes underlying code writing. While prose reading and writing have been studied thoroughly, that same scrutiny has not been applied to code writing. In this paper, we leverage functional brain imaging to investigate neural representations of code writing in comparison to prose writing. We present the first human study in which participants wrote code and prose while undergoing a functional magnetic resonance imaging (fMRI) brain scan, making use of a full-sized fMRI-safe QWERTY keyboard. We find that code writing and prose writing are significantly dissimilar neural tasks. While prose writing entails significant left hemisphere activity associated with language, code writing involves more activations of the right hemisphere, including regions associated with attention control, working memory, planning and spatial cognition. These findings are unlike existing work in which code and prose comprehension were studied. By contrast, we present the first evidence suggesting that code and prose writing are quite dissimilar at the neural level."
Sun2020,Bo Sun and Yu Li and Zili Wang and Zhifeng Li and Quan Xia and Yi Ren and Qiang Feng and Dezhen Yang and Cheng Qian,Physics-of-failure and computer-aided simulation fusion approach with a software system for electronics reliability analysis,Eksploatacja i Niezawodnosc,22,2,2020,10.17531/ein.2020.2.17,15072711,"Electronics, such as those used in the communication, aerospace and energy domains, often have high reliability requirements. To reduce the development and testing cost of electronics, reliability analysis needs to be incorporated into the design stage. Compared with traditional approaches, the physics of failure (PoF) methodology can better address cost reduction in the design stage. However, there are many difficulties in practical engineering applications, such as processing large amounts of engineering information simultaneously. Therefore, a flexible approach and a software system for assisting designers in developing a reliability analysis based on the PoF method in electronic product design processing are proposed. This approach integrates the PoF method and computer-aided simulation methods, such as CAD, FEM and CFD.The software system integrates functional modules such as product modeling, load-stress analysis and reliability analysis, which can help designers analyze the reliability of electronic products in actual engineering design. This system includes software and hardware that validate the simulation models. Finally, a case study is proposed in which the software system is used to analyze the filter module reliability of an industrial communication system. The results of the analysis indicate that the system can effectively promote reliability and can ensure the accuracy of analysis with high computing efficiency."
Cao2022,Xiangpeng Cao and Shiheng Yu and Hongzhi Cui and Zongjin Li,3D Printing Devices and Reinforcing Techniques for Extruded Cement-Based Materials: A Review,Buildings,12,4,2022,10.3390/buildings12040453,20755309,"The three-dimensional (3D) printing technique for cement-based materials has been ac-tively investigated and utilized in civil engineering. However, there is no systematic review of the fabricating devices. This paper reviews the software and hardware for extrusion-based 3D concrete printing. Firstly, a dedicated tool path generating software is urgently needed to meet the cementi-tious printing applications and to improve printing quality with toolpath optimizations. Secondly, the existing printing equipment was summarized and discussed, concluding the pros and cons of various 3D motion systems, material systems, and nozzle units. Suitable choices for scientific research and engineering applications were recommended. The reinforcing techniques were categorized and concluded with the existing drawbacks and the research trend. A hybrid manufacturing system of 3D printing and the reinforcing technique was then proposed with a system diagram and flowchart."
Traganos2021,Konstantinos Traganos and Paul Grefen and Irene Vanderfeesten and Jonnro Erasmus and Georgios Boultadakis and Panagiotis Bouklis,The HORSE framework: A reference architecture for cyber-physical systems in hybrid smart manufacturing,Journal of Manufacturing Systems,61,,2021,10.1016/j.jmsy.2021.09.003,02786125,"In the Industry 4.0 era, manufacturers strive to remain competitive by using advanced technologies such as collaborative robots, automated guided vehicles, augmented reality support and smart devices. However, only if these technological advancements are integrated into their system context in a seamless way, they can deliver their full potential to a manufacturing organization. This integration requires a system architecture as a blueprint for positioning and interconnection of the technologies. For this purpose, the HORSE framework, resulting from the HORSE EU H2020 project, has been developed to act as a reference architecture of a cyber-physical system to integrate various Industry 4.0 technologies and support hybrid manufacturing processes, i.e., processes in which human and robotic workers collaborate. The architecture has been created using design science research, based on well-known software engineering frameworks, established manufacturing domain standards and practical industry requirements. The value of a reference architecture is mainly established by application in practice. For this purpose, this paper presents the application and evaluation of the HORSE framework in 10 manufacturing plants across Europe, each with its own characteristics. Through the physical deployment and demonstration, the framework proved its goal to be basis for the well-structured design of an operational smart manufacturing cyber-physical system that provides horizontal, cross-functional management of manufacturing processes and vertical control of heterogeneous technologies in work cells. We report on valuable insights on the difficulties to realize such systems in specific situations. The experiences form the basis for improved adoption, further improvement and extension of the framework. In sum, this paper shows how a reference architecture framework supports the structured application of Industry 4.0 technologies in manufacturing environments that so far have relied on more traditional digital technology."
Pkknen2020,P. Pääkkönen and D. Pakkala,Extending reference architecture of big data systems towards machine learning in edge computing environments,Journal of Big Data,7,1,2020,10.1186/s40537-020-00303-y,21961115,"Background: Augmented reality, computer vision and other (e.g. network functions, Internet-of-Things (IoT)) use cases can be realised in edge computing environments with machine learning (ML) techniques. For realisation of the use cases, it has to be understood how data is collected, stored, processed, analysed, and visualised in big data systems. In order to provide services with low latency for end users, often utilisation of ML techniques has to be optimized. Also, software/service developers have to understand, how to develop and deploy ML models in edge computing environments. Therefore, architecture design of big data systems to edge computing environments may be challenging. Findings: The contribution of this paper is reference architecture (RA) design of a big data system utilising ML techniques in edge computing environments. An earlier version of the RA has been extended based on 16 realised implementation architectures, which have been developed to edge/distributed computing environments. Also, deployment of architectural elements in different environments is described. Finally, a system view is provided of the software engineering aspects of ML model development and deployment. Conclusions: The presented RA may facilitate concrete architecture design of use cases in edge computing environments. The value of RAs is reduction of development and maintenance costs of systems, reduction of risks, and facilitation of communication between different stakeholders."
Michael2022,Judith Michael and Jerome Pfeiffer and Bernhard Rumpe and Andreas Wortmann,Integration Challenges for Digital Twin Systems-of-Systems,,,,2022,10.1145/3528229.3529384,,"Research and industry leverage digital twins to monitor and control (cyber-physical) systems in various domains. For their efficient engineering, these twins need to become Systems-of-Systems (SoS), in which digital twins of smaller systems (e.g., a production machine) become parts of digital twins of larger systems (e.g., a factory). Yet, research on digital twins as SoS largely ignores reusing digital twins in SoS. Based on our experience in engineering digital twins with experts from various domains related to production systems engineering, we present insights on the challenges of composing and integrating that need to be addressed for efficient engineering of digital twins as SoS. These insights may guide future research on engineering digital twins as well as practitioners considering the challenges in building and composing digital twin systems-of-systems."
Khan2020,Mohammad Zubair Khan,Hybrid ensemble learning technique for software defect prediction,International Journal of Modern Education and Computer Science,12,1,2020,10.5815/ijmecs.2020.01.01,2075017X,"The reliability of software depends on its ability to function without error. Unfortunately, errors can be generated during any phase of software development. In the field of software engineering, the prediction of software defects during the initial stages of development has therefore become a top priority. Scientific data are used to predict the software's future release. Study shows that machine learning and hybrid algorithms are change benchmarks in the prediction of defects. During the past two decades, various approaches to software defect prediction that rely on software metrics have been proposed. This paper explores and compares well-known supervised machine learning and hybrid ensemble classifiers in eight PROMISE datasets. The experimental results showed that AdaBoost support vector machines and bagging support vector machines were the best performing classifiers in Accuracy, AUC, recall and F-measure."
Dueas2021,Santiago Dueñas and Valerio Cosentino and Jesus M. Gonzalez-Barahona and Alvaro del Castillo San Felix and Daniel Izquierdo-Cortazar and Luis Cañas-Díaz and Alberto Pérez García-Plaza,GrimoireLab: A toolset for software development analytics,PeerJ Computer Science,7,,2021,10.7717/PEERJ-CS.601,23765992,"Background: After many years of research on software repositories, the knowledge for building mature, reusable tools that perform data retrieval, storage and basic analytics is readily available. However, there is still room to improvement in the area of reusable tools implementing this knowledge. Goal: To produce a reusable toolset supporting the most common tasks when retrieving, curating and visualizing data from software repositories, allowing for the easy reproduction of data sets ready for more complex analytics, and sparing the researcher or the analyst of most of the tasks that can be automated. Method: Use our experience in building tools in this domain to identify a collection of scenarios where a reusable toolset would be convenient, and the main components of such a toolset. Then build those components, and refine them incrementally using the feedback from their use in both commercial, community-based, and academic environments. Results: GrimoireLab, an efficient toolset composed of five main components, supporting about 30 different kinds of data sources related to software development. It has been tested in many environments, for performing different kinds of studies, and providing different kinds of services. It features a common API for accessing the retrieved data, facilities for relating items from different data sources, semistructured storage for easing later analysis and reproduction, and basic facilities for visualization, preliminary analysis and drill-down in the data. It is also modular, making it easy to support new kinds of data sources and analysis. Conclusions: We present a mature toolset, widely tested in the field, that can help to improve the situation in the area of reusable tools for mining software repositories. We show some scenarios where it has already been used. We expect it will help to reduce the effort for doing studies or providing services in this area, leading to advances in reproducibility and comparison of results."
DeCarvalho2021,Halcyon Davys Pereira De Carvalho and Roberta Fagundes and Wylliams Santos,Extreme Learning Machine Applied to Software Development Effort Estimation,IEEE Access,9,,2021,10.1109/ACCESS.2021.3091313,21693536,"The project management process has been used in the area of Software Engineering to support project managers to keep projects under control. One of the essential processes in Software Engineering is to conduct an accurate and reliable estimation of the required effort to complete the project. This article's objectives are: i) to identify the variables that influence the estimation based on the correlation, and ii) to apply the Extreme Learning Machine - ELM model for effort estimation and compare it with the literature models. Thus, it was investigated which technique has better effort prediction accuracy. The models were compared with each other based on predictive precision in the criterion of absolute mean residue (MAR) and statistical tests. The main findings in this study were: i) important variables for effort estimation and; ii) the results indicated that the ELM model presents the best results compared to the models in the literature for estimating software design effort. In this way, the use of Machine Learning techniques in the effort estimation process can increase the chances of success in the accuracy of the time estimates and the project's costs."
Qasim2020,Syed Ali Qasim and Jared M. Smith and Irfan Ahmed,Control Logic Forensics Framework using Built-in Decompiler of Engineering Software in Industrial Control Systems,Forensic Science International: Digital Investigation,33,,2020,10.1016/j.fsidi.2020.301013,26662817,"In industrial control systems (ICS), attackers inject malicious control-logic into programmable logic controllers (PLCs) to sabotage physical processes, such as nuclear plants, traffic-light signals, elevators, and conveyor belts. For instance, Stuxnet operates by transfering control logic to Siemens S7-300 PLCs over the network to manipulate the motor speed of centrifuges. These devestating attacks are referred to as control-logic injection attacks. Their network traffic, if captured, contains malicious control logic that can be leveraged as a forensic artifact. In this paper, we present Reditus to recover control logic from a suspicious ICS network traffic. Reditus is based on the observation that an engineering software has a built-in decompiler that can transform the control logic into its source-code. Reditus integrates the decompiler with a (previously-captured) set of network traffic from a control-logic to recover the source code of the binary control-logic automatically. We evaluate Reditus on the network traffic of 40 control logic programs transferred from the SoMachine Basic engineering software to a Modicon M221 PLC. Our evaluation successfully demonstrates that Reditus can recover the source-code of a control logic from its network traffic."
Morandini2021,Marcelo Morandini and Thiago Adriano Coleti and Edson Oliveira and Pedro Luiz Pizzigatti Corrêa,Considerations about the efficiency and sufficiency of the utilization of the Scrum methodology: A survey for analyzing results for development teams,Computer Science Review,39,,2021,10.1016/j.cosrev.2020.100314,15740137,"SCRUM is an important strategy used for software development. However, developers may face specific issues considered relevant that might fail to lead to the best solutions implemented or result in longer development times. Literature has a significant number of ""best practices""to guide development teams; however, sometimes are very theoretical and may or may not be relevant to a particular environment or organization. This study evaluated these best practices in terms of if they were properly followed and how effective they were within various organizations. Aiming to gather information as to how tasks are actually performed as compared to SCRUM standard practices, the study performed a survey to software developers, product owners, and others. The survey focused on gathering and presenting data on the perceptions, needs, activities and issues from the various Scrum traditional roles involved in software development. A questionnaire was designed that included 15 multiple-choice questions and 2 open ended questions, Questions related to effectiveness of collecting development features/requirements, daily meetings duration, overall efficiency of the Scrum structure, and interactions with Product Owners all were distributed within multiple organizations. Overall, the study compared the implementation and use of SCRUM effectiveness based on traditional methodology SCRUM methodology. The study was designed to aid all roles involved in scrum when implementing such a framework within an organization. This study focused on specific features considered relevant for successfully developing systems using SCRUM as methodology. Among these features, the focus were on: (1) is the ticket allocation strategy effectiveness for the development processes? (2) are the daily stand-up meetings efficient and do they represent an important activity during the process? (3) is the timeline defined for each sprint appropriate? and (4) are the team members improving their capabilities on designing, coding and testing activities as they might be allocated to perform a variety of front or back-end development activities? As with other best practices, whether software development related or in other industries, theoretical guidelines should only serve as a basis for implementation as each organization has its unique characteristics, varying number of software developers, and local requirements."
Alhazmi2021,Abdulrahman Alhazmi and Nalin Asanka Gamagedara Arachchilage,I’m all ears! Listening to software developers on putting GDPR principles into software development practice,Personal and Ubiquitous Computing,25,5,2021,10.1007/s00779-021-01544-1,16174917,"Previous research has been carried out to identify the impediments that prevent developers from incorporating privacy protocols into software applications. No research has been carried out to find out why developers are not able to develop systems that preserve privacy while specifically considering the General Data Protection Regulation principles (GDPR principles). Consequently, this paper aims to examine the issues, which prevent developers from creating applications, which consider and include GDPR principles into their software systems. From our research findings, we identified the lack of familiarity with GDPR principles by developers as one of the obstacles that prevent GDPR onboarding. Those who were familiar with the principles did not have the requisite knowledge about the principles including their techniques. Developers focused on functional than on privacy requirements. Unavailability of resourceful online tools and lack of support from institutions and clients were also identified as issues inimical to the onboarding of GDPR principles."
